_id,repo_full_name,pr_id,comment_id,user_login,user_id,created_at,updated_at,body,is_review_comment,path,position,diff_hunk,reactions_total,reactions_plus1,reactions_minus1,reactions_laugh,reactions_hooray,reactions_confused,reactions_heart,repo_name
6824cd8e0ce5dcec7842e331,academic/awesome-datascience,1607818538,1398798992,rasitds,12167194,2023-11-20T08:23:49+00:00,2023-11-20T08:24:14+00:00,```suggestion\r\n- [How Generative AI Is Changing Creative Work](https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e332,academic/awesome-datascience,1607818538,1398799266,rasitds,12167194,2023-11-20T08:24:03+00:00,2023-11-20T08:24:14+00:00,```suggestion\r\n- [What is generative AI?](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e333,academic/awesome-datascience,1607818538,1398798992,rasitds,12167194,2023-11-20T08:23:49+00:00,2023-11-20T08:24:14+00:00,```suggestion\r\n- [How Generative AI Is Changing Creative Work](https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work)\r\n```,true,README.md,NaN,"@@ -1100,6 +1100,8 @@ Some data mining competition platforms\n - [Awesome Drug Synergy, Interaction and Polypharmacy Prediction](https://github.com/AstraZeneca/awesome-drug-pair-scoring)\n - [Deep Learning Interview Questions](https://www.adaface.com/blog/deep-learning-interview-questions/)\n - [Top Future Trends in Data Science in 2023](https://medium.com/the-modern-scientist/top-future-trends-in-data-science-in-2023-3e616c8998b8)\n+- [How Generative AI Is Changing Creative Work] (https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work)",0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e334,academic/awesome-datascience,1607818538,1398799266,rasitds,12167194,2023-11-20T08:24:03+00:00,2023-11-20T08:24:14+00:00,```suggestion\r\n- [What is generative AI?](https://www.techtarget.com/searchenterpriseai/definition/generative-AI)\r\n```,true,README.md,NaN,"@@ -1100,6 +1100,8 @@ Some data mining competition platforms\n - [Awesome Drug Synergy, Interaction and Polypharmacy Prediction](https://github.com/AstraZeneca/awesome-drug-pair-scoring)\n - [Deep Learning Interview Questions](https://www.adaface.com/blog/deep-learning-interview-questions/)\n - [Top Future Trends in Data Science in 2023](https://medium.com/the-modern-scientist/top-future-trends-in-data-science-in-2023-3e616c8998b8)\n+- [How Generative AI Is Changing Creative Work] (https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work)\n+- [What is generative AI?] (https://www.techtarget.com/searchenterpriseai/definition/generative-AI)",0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e335,academic/awesome-datascience,1537693201,1342467277,hmert,182906,2023-10-02T09:28:29+00:00,2023-10-02T09:28:33+00:00,please remove your referral code.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e336,academic/awesome-datascience,1537693201,1342470732,akhil-maker,58397226,2023-10-02T09:32:01+00:00,2023-10-02T09:32:02+00:00,"Sorry for mistake, the new link has been added",false,NaN,NaN,NaN,0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e337,academic/awesome-datascience,1537693201,1342467277,hmert,182906,2023-10-02T09:28:29+00:00,2023-10-02T09:28:33+00:00,please remove your referral code.,true,README.md,NaN,"@@ -625,7 +625,7 @@ This section includes some additional reading material, channels to watch, and t\n - [Software Engineering for Data Scientists](https://www.manning.com/books/software-engineering-for-data-scientists) - Early access\n - [Julia for Data Science](https://www.manning.com/books/julia-for-data-science) - Early access\n - [An Introduction to Statistical Learning](https://www.statlearning.com/) - Download Page\n-\n+- [Machine Learning For Absolute Beginners](https://amzn.eu/d/9RkOcRF)",0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e338,academic/awesome-datascience,1537693201,1342470732,akhil-maker,58397226,2023-10-02T09:32:01+00:00,2023-10-02T09:32:02+00:00,"Sorry for mistake, the new link has been added",true,README.md,NaN,"@@ -625,7 +625,7 @@ This section includes some additional reading material, channels to watch, and t\n - [Software Engineering for Data Scientists](https://www.manning.com/books/software-engineering-for-data-scientists) - Early access\n - [Julia for Data Science](https://www.manning.com/books/julia-for-data-science) - Early access\n - [An Introduction to Statistical Learning](https://www.statlearning.com/) - Download Page\n-\n+- [Machine Learning For Absolute Beginners](https://amzn.eu/d/9RkOcRF)",0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e339,academic/awesome-datascience,190734394,191044800,hmert,182906,2018-05-26T09:51:31+00:00,2018-05-26T09:51:41+00:00,Gregory Piatetsky is not a duplicate/suspended account ?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e33a,academic/awesome-datascience,190734394,191044800,hmert,182906,2018-05-26T09:51:31+00:00,2018-05-26T09:51:41+00:00,Gregory Piatetsky is not a duplicate/suspended account ?,true,README.md,4,"@@ -321,7 +321,6 @@ We are more than 3,190 data scientists and data geeks in our community.\n - [Erin Bartolo](https://twitter.com/erinbartolo) - Running with #BigData--enjoying a love/hate relationship with its hype. @iSchoolSU #DataScience Program Mgr.\n - [Greg Reda](https://twitter.com/gjreda) Working @ _GrubHub_ about data and pandas\n - [Gregory Piatetsky](https://twitter.com/kdnuggets) -  KDnuggets President, Analytics/Big Data/Data Mining/Data Science expert, KDD & SIGKDD co-founder, was Chief Scientist at 2 startups, part-time philosopher.\n-- [Gregory Piatetsky](https://twitter.com/kdnuggets) - KDnuggets President, Analytics/Big Data/Data Mining/Data Science expert, KDD & SIGKDD co-founder, was Chief Scientist at 2 startups, part-time philosopher.",0,0,0,0,0,0,0,academic_awesome-datascience
6824cd8e0ce5dcec7842e5b3,apache/airflow,2515598983,2086043119,eladkal,45845474,2025-05-13T06:45:58+00:00,2025-05-13T06:45:58+00:00,Tests looks pretty the same. Wouldn't it be simpler to use parameterized test?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5b4,apache/airflow,2515598983,2086057259,guan404ming,105915352,2025-05-13T06:54:05+00:00,2025-05-13T06:54:05+00:00,Make sense and updated. Thanks for the suggestion.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5b5,apache/airflow,2515598983,2086043119,eladkal,45845474,2025-05-13T06:45:58+00:00,2025-05-13T06:45:58+00:00,Tests looks pretty the same. Wouldn't it be simpler to use parameterized test?,true,providers/databricks/tests/unit/databricks/hooks/test_databricks_sql.py,NaN,"@@ -504,3 +506,56 @@ def test_get_openlineage_database_specific_lineage_with_old_openlineage_provider\n     )\n     with pytest.raises(AirflowOptionalProviderFeatureException, match=expected_err):\n         hook.get_openlineage_database_specific_lineage(mock.MagicMock())\n+\n+\n+def test_get_df_pandas():",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5b6,apache/airflow,2515598983,2086057259,guan404ming,105915352,2025-05-13T06:54:05+00:00,2025-05-13T06:54:05+00:00,Make sense and updated. Thanks for the suggestion.,true,providers/databricks/tests/unit/databricks/hooks/test_databricks_sql.py,NaN,"@@ -504,3 +506,56 @@ def test_get_openlineage_database_specific_lineage_with_old_openlineage_provider\n     )\n     with pytest.raises(AirflowOptionalProviderFeatureException, match=expected_err):\n         hook.get_openlineage_database_specific_lineage(mock.MagicMock())\n+\n+\n+def test_get_df_pandas():",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5b7,apache/airflow,2515088117,2086030500,eladkal,45845474,2025-05-13T06:38:33+00:00,2025-05-13T06:38:33+00:00,This needs to be under version check as needs to be compatible with Airflow 2.10.0 and sdk path did not exist in 2.10,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5b8,apache/airflow,2515088117,2086030500,eladkal,45845474,2025-05-13T06:38:33+00:00,2025-05-13T06:38:33+00:00,This needs to be under version check as needs to be compatible with Airflow 2.10.0 and sdk path did not exist in 2.10,true,providers/amazon/src/airflow/providers/amazon/aws/hooks/base_aws.py,4,"@@ -59,6 +59,7 @@\n from airflow.providers.amazon.aws.utils.identifiers import generate_uuid\n from airflow.providers.amazon.aws.utils.suppress import return_on_error\n from airflow.providers_manager import ProvidersManager\n+from airflow.sdk.exceptions import AirflowRuntimeError",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5b9,apache/airflow,2514317005,2085035587,jedcunningham,66968678,2025-05-12T16:21:47+00:00,2025-05-12T16:26:18+00:00,```suggestion\r\nAirflow 3 is released in several packages. The ``apache-airflow`` package is a meta-package that installs\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ba,apache/airflow,2514317005,2085036907,jedcunningham,66968678,2025-05-12T16:22:40+00:00,2025-05-12T16:26:18+00:00,"```suggestion\r\nall the other packages when you run Airflow as a standalone installation, and it also has several extras\r\nthat are not extending Airflow core functionality, but they are useful for the users who want to install\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5bb,apache/airflow,2514317005,2085038860,jedcunningham,66968678,2025-05-12T16:23:55+00:00,2025-05-12T16:26:18+00:00,```suggestion\r\n| s3fs                | ``pip install 'apache-airflow[s3fs]'``              | Support for S3 as Airflow FS                                               |\r\n+---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\r\n| saml                | ``pip install 'apache-airflow[saml]'``              | Support for SAML authentication in Amazon provider                         |\r\n```\r\n\r\nLet's do alphabetical.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5bc,apache/airflow,2514317005,2085041653,jedcunningham,66968678,2025-05-12T16:25:39+00:00,2025-05-12T16:26:18+00:00,"```suggestion\r\nThese are core Airflow extras that extend capabilities of core Airflow. They do not install provider\r\npackages, they just install necessary\r\n```\r\n\r\nWe don't have celery of k8s in the list, so we can probably simplify this a bit?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5bd,apache/airflow,2514317005,2085162208,potiuk,595491,2025-05-12T17:46:07+00:00,2025-05-12T17:46:07+00:00,Ah.. left-over :),false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5be,apache/airflow,2514317005,2085162793,potiuk,595491,2025-05-12T17:46:35+00:00,2025-05-12T17:46:35+00:00,Depending which alphabet you choose :) ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5bf,apache/airflow,2514317005,2085035587,jedcunningham,66968678,2025-05-12T16:21:47+00:00,2025-05-12T16:26:18+00:00,```suggestion\r\nAirflow 3 is released in several packages. The ``apache-airflow`` package is a meta-package that installs\r\n```,true,airflow-core/docs/extra-packages-ref.rst,NaN,"@@ -81,49 +81,63 @@ Core Airflow extras\n \n These are core Airflow extras that extend capabilities of core Airflow. They usually do not install provider\n packages (with the exception of ``celery`` and ``cncf.kubernetes`` extras), they just install necessary\n-python dependencies for the provided package.\n+python dependencies for the provided package. The same extras are available as ``airflow-core`` package extras.\n+\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| extra               | install command                                     | enables                                                                    |\n++=====================+=====================================================+============================================================================+\n+| async               | ``pip install 'apache-airflow[async]'``             | Async worker classes for Gunicorn                                          |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| graphviz            | ``pip install 'apache-airflow[graphviz]'``          | Graphviz renderer for converting DAG to graphical output                   |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| kerberos            | ``pip install 'apache-airflow[kerberos]'``          | Kerberos integration for Kerberized services (Hadoop, Presto, Trino)       |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| otel                | ``pip install 'apache-airflow[otel]'``              | Required for OpenTelemetry metrics                                         |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| sentry              | ``pip install 'apache-airflow[sentry]'``            | Sentry service for application logging and monitoring                      |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| standard            | ``pip install apache-airflow[standard]'``           | Standard hooks and operators                                               |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| statsd              | ``pip install 'apache-airflow[statsd]'``            | Needed by StatsD metrics                                                   |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+\n+Meta-airflow package extras\n+---------------------------\n+\n+Airflow 3 is released in several packages. the ``apache-airflow`` package is a meta-package that installs",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c0,apache/airflow,2514317005,2085036907,jedcunningham,66968678,2025-05-12T16:22:40+00:00,2025-05-12T16:26:18+00:00,"```suggestion\r\nall the other packages when you run Airflow as a standalone installation, and it also has several extras\r\nthat are not extending Airflow core functionality, but they are useful for the users who want to install\r\n```",true,airflow-core/docs/extra-packages-ref.rst,NaN,"@@ -81,49 +81,63 @@ Core Airflow extras\n \n These are core Airflow extras that extend capabilities of core Airflow. They usually do not install provider\n packages (with the exception of ``celery`` and ``cncf.kubernetes`` extras), they just install necessary\n-python dependencies for the provided package.\n+python dependencies for the provided package. The same extras are available as ``airflow-core`` package extras.\n+\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| extra               | install command                                     | enables                                                                    |\n++=====================+=====================================================+============================================================================+\n+| async               | ``pip install 'apache-airflow[async]'``             | Async worker classes for Gunicorn                                          |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| graphviz            | ``pip install 'apache-airflow[graphviz]'``          | Graphviz renderer for converting DAG to graphical output                   |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| kerberos            | ``pip install 'apache-airflow[kerberos]'``          | Kerberos integration for Kerberized services (Hadoop, Presto, Trino)       |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| otel                | ``pip install 'apache-airflow[otel]'``              | Required for OpenTelemetry metrics                                         |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| sentry              | ``pip install 'apache-airflow[sentry]'``            | Sentry service for application logging and monitoring                      |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| standard            | ``pip install apache-airflow[standard]'``           | Standard hooks and operators                                               |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| statsd              | ``pip install 'apache-airflow[statsd]'``            | Needed by StatsD metrics                                                   |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+\n+Meta-airflow package extras\n+---------------------------\n+\n+Airflow 3 is released in several packages. the ``apache-airflow`` package is a meta-package that installs\n+all the other packages when you run airflow as a standalone installation, and it also has several extras\n+that are not extending airflow core functionality, but they are useful for the users who want to install",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c1,apache/airflow,2514317005,2085038860,jedcunningham,66968678,2025-05-12T16:23:55+00:00,2025-05-12T16:26:18+00:00,```suggestion\r\n| s3fs                | ``pip install 'apache-airflow[s3fs]'``              | Support for S3 as Airflow FS                                               |\r\n+---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\r\n| saml                | ``pip install 'apache-airflow[saml]'``              | Support for SAML authentication in Amazon provider                         |\r\n```\r\n\r\nLet's do alphabetical.,true,airflow-core/docs/extra-packages-ref.rst,NaN,"@@ -81,49 +81,63 @@ Core Airflow extras\n \n These are core Airflow extras that extend capabilities of core Airflow. They usually do not install provider\n packages (with the exception of ``celery`` and ``cncf.kubernetes`` extras), they just install necessary\n-python dependencies for the provided package.\n+python dependencies for the provided package. The same extras are available as ``airflow-core`` package extras.\n+\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| extra               | install command                                     | enables                                                                    |\n++=====================+=====================================================+============================================================================+\n+| async               | ``pip install 'apache-airflow[async]'``             | Async worker classes for Gunicorn                                          |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| graphviz            | ``pip install 'apache-airflow[graphviz]'``          | Graphviz renderer for converting DAG to graphical output                   |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| kerberos            | ``pip install 'apache-airflow[kerberos]'``          | Kerberos integration for Kerberized services (Hadoop, Presto, Trino)       |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| otel                | ``pip install 'apache-airflow[otel]'``              | Required for OpenTelemetry metrics                                         |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| sentry              | ``pip install 'apache-airflow[sentry]'``            | Sentry service for application logging and monitoring                      |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| standard            | ``pip install apache-airflow[standard]'``           | Standard hooks and operators                                               |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| statsd              | ``pip install 'apache-airflow[statsd]'``            | Needed by StatsD metrics                                                   |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+\n+Meta-airflow package extras\n+---------------------------\n+\n+Airflow 3 is released in several packages. the ``apache-airflow`` package is a meta-package that installs\n+all the other packages when you run airflow as a standalone installation, and it also has several extras\n+that are not extending airflow core functionality, but they are useful for the users who want to install\n+other packages that can be used by airflow or some of its providers.\n \n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | extra               | install command                                     | enables                                                                    |\n +=====================+=====================================================+============================================================================+\n | aiobotocore         | ``pip install 'apache-airflow[aiobotocore]'``       | Support for asynchronous (deferrable) operators for Amazon integration     |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n-| async               | ``pip install 'apache-airflow[async]'``             | Async worker classes for Gunicorn                                          |\n+| cloudpickle         | ``pip install apache-airflow[cloudpickle]``         | Cloudpickle hooks and operators                                            |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | github-enterprise   | ``pip install 'apache-airflow[github-enterprise]'`` | GitHub Enterprise auth backend                                             |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | google-auth         | ``pip install 'apache-airflow[google-auth]'``       | Google auth backend                                                        |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | graphviz            | ``pip install 'apache-airflow[graphviz]'``          | Graphviz renderer for converting DAG to graphical output                   |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n-| kerberos            | ``pip install 'apache-airflow[kerberos]'``          | Kerberos integration for Kerberized services (Hadoop, Presto, Trino)       |\n-+---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | ldap                | ``pip install 'apache-airflow[ldap]'``              | LDAP authentication for users                                              |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | leveldb             | ``pip install 'apache-airflow[leveldb]'``           | Required for use leveldb extra in google provider                          |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n-| otel                | ``pip install 'apache-airflow[otel]'``              | Required for OpenTelemetry metrics                                         |\n-+---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | pandas              | ``pip install 'apache-airflow[pandas]'``            | Install Pandas library compatible with Airflow                             |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | polars              | ``pip install 'apache-airflow[polars]'``            | Polars hooks and operators                                                 |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | rabbitmq            | ``pip install 'apache-airflow[rabbitmq]'``          | RabbitMQ support as a Celery backend                                       |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n-| sentry              | ``pip install 'apache-airflow[sentry]'``            | Sentry service for application logging and monitoring                      |\n+| saml                | ``pip install 'apache-airflow[saml]'``              | Support for SAML authentication in Amazon provider                         |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | s3fs                | ``pip install 'apache-airflow[s3fs]'``              | Support for S3 as Airflow FS                                               |",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c2,apache/airflow,2514317005,2085041653,jedcunningham,66968678,2025-05-12T16:25:39+00:00,2025-05-12T16:26:18+00:00,"```suggestion\r\nThese are core Airflow extras that extend capabilities of core Airflow. They do not install provider\r\npackages, they just install necessary\r\n```\r\n\r\nWe don't have celery of k8s in the list, so we can probably simplify this a bit?",true,airflow-core/docs/extra-packages-ref.rst,NaN,"@@ -81,49 +81,63 @@ Core Airflow extras\n \n These are core Airflow extras that extend capabilities of core Airflow. They usually do not install provider\n packages (with the exception of ``celery`` and ``cncf.kubernetes`` extras), they just install necessary",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c3,apache/airflow,2514317005,2085162208,potiuk,595491,2025-05-12T17:46:07+00:00,2025-05-12T17:46:07+00:00,Ah.. left-over :),true,airflow-core/docs/extra-packages-ref.rst,NaN,"@@ -81,49 +81,63 @@ Core Airflow extras\n \n These are core Airflow extras that extend capabilities of core Airflow. They usually do not install provider\n packages (with the exception of ``celery`` and ``cncf.kubernetes`` extras), they just install necessary",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c4,apache/airflow,2514317005,2085162793,potiuk,595491,2025-05-12T17:46:35+00:00,2025-05-12T17:46:35+00:00,Depending which alphabet you choose :) ,true,airflow-core/docs/extra-packages-ref.rst,NaN,"@@ -81,49 +81,63 @@ Core Airflow extras\n \n These are core Airflow extras that extend capabilities of core Airflow. They usually do not install provider\n packages (with the exception of ``celery`` and ``cncf.kubernetes`` extras), they just install necessary\n-python dependencies for the provided package.\n+python dependencies for the provided package. The same extras are available as ``airflow-core`` package extras.\n+\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| extra               | install command                                     | enables                                                                    |\n++=====================+=====================================================+============================================================================+\n+| async               | ``pip install 'apache-airflow[async]'``             | Async worker classes for Gunicorn                                          |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| graphviz            | ``pip install 'apache-airflow[graphviz]'``          | Graphviz renderer for converting DAG to graphical output                   |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| kerberos            | ``pip install 'apache-airflow[kerberos]'``          | Kerberos integration for Kerberized services (Hadoop, Presto, Trino)       |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| otel                | ``pip install 'apache-airflow[otel]'``              | Required for OpenTelemetry metrics                                         |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| sentry              | ``pip install 'apache-airflow[sentry]'``            | Sentry service for application logging and monitoring                      |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| standard            | ``pip install apache-airflow[standard]'``           | Standard hooks and operators                                               |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+| statsd              | ``pip install 'apache-airflow[statsd]'``            | Needed by StatsD metrics                                                   |\n++---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n+\n+Meta-airflow package extras\n+---------------------------\n+\n+Airflow 3 is released in several packages. the ``apache-airflow`` package is a meta-package that installs\n+all the other packages when you run airflow as a standalone installation, and it also has several extras\n+that are not extending airflow core functionality, but they are useful for the users who want to install\n+other packages that can be used by airflow or some of its providers.\n \n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | extra               | install command                                     | enables                                                                    |\n +=====================+=====================================================+============================================================================+\n | aiobotocore         | ``pip install 'apache-airflow[aiobotocore]'``       | Support for asynchronous (deferrable) operators for Amazon integration     |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n-| async               | ``pip install 'apache-airflow[async]'``             | Async worker classes for Gunicorn                                          |\n+| cloudpickle         | ``pip install apache-airflow[cloudpickle]``         | Cloudpickle hooks and operators                                            |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | github-enterprise   | ``pip install 'apache-airflow[github-enterprise]'`` | GitHub Enterprise auth backend                                             |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | google-auth         | ``pip install 'apache-airflow[google-auth]'``       | Google auth backend                                                        |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | graphviz            | ``pip install 'apache-airflow[graphviz]'``          | Graphviz renderer for converting DAG to graphical output                   |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n-| kerberos            | ``pip install 'apache-airflow[kerberos]'``          | Kerberos integration for Kerberized services (Hadoop, Presto, Trino)       |\n-+---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | ldap                | ``pip install 'apache-airflow[ldap]'``              | LDAP authentication for users                                              |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | leveldb             | ``pip install 'apache-airflow[leveldb]'``           | Required for use leveldb extra in google provider                          |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n-| otel                | ``pip install 'apache-airflow[otel]'``              | Required for OpenTelemetry metrics                                         |\n-+---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | pandas              | ``pip install 'apache-airflow[pandas]'``            | Install Pandas library compatible with Airflow                             |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | polars              | ``pip install 'apache-airflow[polars]'``            | Polars hooks and operators                                                 |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | rabbitmq            | ``pip install 'apache-airflow[rabbitmq]'``          | RabbitMQ support as a Celery backend                                       |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n-| sentry              | ``pip install 'apache-airflow[sentry]'``            | Sentry service for application logging and monitoring                      |\n+| saml                | ``pip install 'apache-airflow[saml]'``              | Support for SAML authentication in Amazon provider                         |\n +---------------------+-----------------------------------------------------+----------------------------------------------------------------------------+\n | s3fs                | ``pip install 'apache-airflow[s3fs]'``              | Support for S3 as Airflow FS                                               |",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c5,apache/airflow,2513790576,2084726523,Lee-W,5144808,2025-05-12T13:51:01+00:00,2025-05-12T13:51:07+00:00,It would be better if we could add a new exception instead of using `AirflowExceptiuon`.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c6,apache/airflow,2513790576,2084835463,guan404ming,105915352,2025-05-12T14:37:17+00:00,2025-05-12T14:37:18+00:00,"Sure, I've updated.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c7,apache/airflow,2513790576,2084837866,eladkal,45845474,2025-05-12T14:38:35+00:00,2025-05-12T14:38:36+00:00,If we do please set it with exceptions.py in the provider itself (not in airflow-core)\r\nexample: \r\nhttps://github.com/apache/airflow/blob/1cde11a447e60d0738b0c317c3d3e8265360014f/providers/databricks/src/airflow/providers/databricks/exceptions.py#L27-L28,false,NaN,NaN,NaN,2,2,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c8,apache/airflow,2513790576,2084841870,guan404ming,105915352,2025-05-12T14:40:28+00:00,2025-05-12T14:40:28+00:00,let me fix it,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5c9,apache/airflow,2513790576,2084855929,guan404ming,105915352,2025-05-12T14:46:41+00:00,2025-05-12T14:46:42+00:00,Updated. Hope everything good.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ca,apache/airflow,2513790576,2084915007,Lee-W,5144808,2025-05-12T15:15:44+00:00,2025-05-12T15:17:56+00:00,```suggestion\r\nfrom airflow.exceptions import AirflowSkipException\r\n```\r\n\r\nnit,false,NaN,NaN,NaN,1,1,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5cb,apache/airflow,2513790576,2084918140,Lee-W,5144808,2025-05-12T15:17:22+00:00,2025-05-12T15:17:56+00:00,The exception created here is not a description and does not help much. Maybe something like `ExternalDagNotExistsError` would be better,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5cc,apache/airflow,2513790576,2084919545,Lee-W,5144808,2025-05-12T15:17:53+00:00,2025-05-12T15:17:56+00:00,and we can create multiple exceptions for different kinds of errors,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5cd,apache/airflow,2513790576,2085047153,guan404ming,105915352,2025-05-12T16:29:14+00:00,2025-05-12T16:45:53+00:00,I've created multiple exceptions for different cases and also update the test cases. Thanks for the suggestions!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ce,apache/airflow,2513790576,2084726523,Lee-W,5144808,2025-05-12T13:51:01+00:00,2025-05-12T13:51:07+00:00,It would be better if we could add a new exception instead of using `AirflowExceptiuon`.,true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -443,6 +443,10 @@ def execute_complete(self, context, event=None):\n             self.log.info(""External tasks %s has executed successfully."", self.external_task_ids)\n         elif event[""status""] == ""skipped"":\n             raise AirflowSkipException(""External job has skipped skipping."")\n+        elif event[""status""] == ""failed"":\n+            if self.soft_fail:\n+                raise AirflowSkipException(""External job has failed skipping."")\n+            raise AirflowException(""External job has failed."")",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5cf,apache/airflow,2513790576,2084835463,guan404ming,105915352,2025-05-12T14:37:17+00:00,2025-05-12T14:37:18+00:00,"Sure, I've updated.",true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -443,6 +443,10 @@ def execute_complete(self, context, event=None):\n             self.log.info(""External tasks %s has executed successfully."", self.external_task_ids)\n         elif event[""status""] == ""skipped"":\n             raise AirflowSkipException(""External job has skipped skipping."")\n+        elif event[""status""] == ""failed"":\n+            if self.soft_fail:\n+                raise AirflowSkipException(""External job has failed skipping."")\n+            raise AirflowException(""External job has failed."")",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d0,apache/airflow,2513790576,2084837866,eladkal,45845474,2025-05-12T14:38:35+00:00,2025-05-12T14:38:36+00:00,If we do please set it with exceptions.py in the provider itself (not in airflow-core)\r\nexample: \r\nhttps://github.com/apache/airflow/blob/1cde11a447e60d0738b0c317c3d3e8265360014f/providers/databricks/src/airflow/providers/databricks/exceptions.py#L27-L28,true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -443,6 +443,10 @@ def execute_complete(self, context, event=None):\n             self.log.info(""External tasks %s has executed successfully."", self.external_task_ids)\n         elif event[""status""] == ""skipped"":\n             raise AirflowSkipException(""External job has skipped skipping."")\n+        elif event[""status""] == ""failed"":\n+            if self.soft_fail:\n+                raise AirflowSkipException(""External job has failed skipping."")\n+            raise AirflowException(""External job has failed."")",2,2,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d1,apache/airflow,2513790576,2084841870,guan404ming,105915352,2025-05-12T14:40:28+00:00,2025-05-12T14:40:28+00:00,let me fix it,true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -443,6 +443,10 @@ def execute_complete(self, context, event=None):\n             self.log.info(""External tasks %s has executed successfully."", self.external_task_ids)\n         elif event[""status""] == ""skipped"":\n             raise AirflowSkipException(""External job has skipped skipping."")\n+        elif event[""status""] == ""failed"":\n+            if self.soft_fail:\n+                raise AirflowSkipException(""External job has failed skipping."")\n+            raise AirflowException(""External job has failed."")",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d2,apache/airflow,2513790576,2084855929,guan404ming,105915352,2025-05-12T14:46:41+00:00,2025-05-12T14:46:42+00:00,Updated. Hope everything good.,true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -443,6 +443,10 @@ def execute_complete(self, context, event=None):\n             self.log.info(""External tasks %s has executed successfully."", self.external_task_ids)\n         elif event[""status""] == ""skipped"":\n             raise AirflowSkipException(""External job has skipped skipping."")\n+        elif event[""status""] == ""failed"":\n+            if self.soft_fail:\n+                raise AirflowSkipException(""External job has failed skipping."")\n+            raise AirflowException(""External job has failed."")",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d3,apache/airflow,2513790576,2084915007,Lee-W,5144808,2025-05-12T15:15:44+00:00,2025-05-12T15:17:56+00:00,```suggestion\r\nfrom airflow.exceptions import AirflowSkipException\r\n```\r\n\r\nnit,true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -24,9 +23,12 @@\n from typing import TYPE_CHECKING, Any, Callable, ClassVar\n \n from airflow.configuration import conf\n-from airflow.exceptions import AirflowException, AirflowSkipException\n+from airflow.exceptions import (\n+    AirflowSkipException,\n+)",1,1,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d4,apache/airflow,2513790576,2084918140,Lee-W,5144808,2025-05-12T15:17:22+00:00,2025-05-12T15:17:56+00:00,The exception created here is not a description and does not help much. Maybe something like `ExternalDagNotExistsError` would be better,true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -455,23 +461,25 @@ def _check_for_existence(self, session) -> None:\n         dag_to_wait = DagModel.get_current(self.external_dag_id, session)\n \n         if not dag_to_wait:\n-            raise AirflowException(f""The external DAG {self.external_dag_id} does not exist."")\n+            raise AirflowExternalTaskSensorException(\n+                f""The external DAG {self.external_dag_id} does not exist.""\n+            )\n \n         if not os.path.exists(correct_maybe_zipped(dag_to_wait.fileloc)):\n-            raise AirflowException(f""The external DAG {self.external_dag_id} was deleted."")\n+            raise AirflowExternalTaskSensorException(f""The external DAG {self.external_dag_id} was deleted."")\n \n         if self.external_task_ids:\n             refreshed_dag_info = DagBag(dag_to_wait.fileloc).get_dag(self.external_dag_id)\n             for external_task_id in self.external_task_ids:\n                 if not refreshed_dag_info.has_task(external_task_id):\n-                    raise AirflowException(\n+                    raise AirflowExternalTaskSensorException(\n                         f""The external task {external_task_id} in DAG {self.external_dag_id} does not exist.""\n                     )\n \n         if self.external_task_group_id:\n             refreshed_dag_info = DagBag(dag_to_wait.fileloc).get_dag(self.external_dag_id)\n             if not refreshed_dag_info.has_task_group(self.external_task_group_id):\n-                raise AirflowException(\n+                raise AirflowExternalTaskSensorException(",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d5,apache/airflow,2513790576,2084919545,Lee-W,5144808,2025-05-12T15:17:53+00:00,2025-05-12T15:17:56+00:00,and we can create multiple exceptions for different kinds of errors,true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -455,23 +461,25 @@ def _check_for_existence(self, session) -> None:\n         dag_to_wait = DagModel.get_current(self.external_dag_id, session)\n \n         if not dag_to_wait:\n-            raise AirflowException(f""The external DAG {self.external_dag_id} does not exist."")\n+            raise AirflowExternalTaskSensorException(\n+                f""The external DAG {self.external_dag_id} does not exist.""\n+            )\n \n         if not os.path.exists(correct_maybe_zipped(dag_to_wait.fileloc)):\n-            raise AirflowException(f""The external DAG {self.external_dag_id} was deleted."")\n+            raise AirflowExternalTaskSensorException(f""The external DAG {self.external_dag_id} was deleted."")\n \n         if self.external_task_ids:\n             refreshed_dag_info = DagBag(dag_to_wait.fileloc).get_dag(self.external_dag_id)\n             for external_task_id in self.external_task_ids:\n                 if not refreshed_dag_info.has_task(external_task_id):\n-                    raise AirflowException(\n+                    raise AirflowExternalTaskSensorException(\n                         f""The external task {external_task_id} in DAG {self.external_dag_id} does not exist.""\n                     )\n \n         if self.external_task_group_id:\n             refreshed_dag_info = DagBag(dag_to_wait.fileloc).get_dag(self.external_dag_id)\n             if not refreshed_dag_info.has_task_group(self.external_task_group_id):\n-                raise AirflowException(\n+                raise AirflowExternalTaskSensorException(",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d6,apache/airflow,2513790576,2085047153,guan404ming,105915352,2025-05-12T16:29:14+00:00,2025-05-12T16:45:53+00:00,I've created multiple exceptions for different cases and also update the test cases. Thanks for the suggestions!,true,providers/standard/src/airflow/providers/standard/sensors/external_task.py,NaN,"@@ -455,23 +461,25 @@ def _check_for_existence(self, session) -> None:\n         dag_to_wait = DagModel.get_current(self.external_dag_id, session)\n \n         if not dag_to_wait:\n-            raise AirflowException(f""The external DAG {self.external_dag_id} does not exist."")\n+            raise AirflowExternalTaskSensorException(\n+                f""The external DAG {self.external_dag_id} does not exist.""\n+            )\n \n         if not os.path.exists(correct_maybe_zipped(dag_to_wait.fileloc)):\n-            raise AirflowException(f""The external DAG {self.external_dag_id} was deleted."")\n+            raise AirflowExternalTaskSensorException(f""The external DAG {self.external_dag_id} was deleted."")\n \n         if self.external_task_ids:\n             refreshed_dag_info = DagBag(dag_to_wait.fileloc).get_dag(self.external_dag_id)\n             for external_task_id in self.external_task_ids:\n                 if not refreshed_dag_info.has_task(external_task_id):\n-                    raise AirflowException(\n+                    raise AirflowExternalTaskSensorException(\n                         f""The external task {external_task_id} in DAG {self.external_dag_id} does not exist.""\n                     )\n \n         if self.external_task_group_id:\n             refreshed_dag_info = DagBag(dag_to_wait.fileloc).get_dag(self.external_dag_id)\n             if not refreshed_dag_info.has_task_group(self.external_task_group_id):\n-                raise AirflowException(\n+                raise AirflowExternalTaskSensorException(",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d7,apache/airflow,2513523036,2085018993,amoghrajesh,35884252,2025-05-12T16:10:56+00:00,2025-05-12T16:12:05+00:00,Lets remove this import,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d8,apache/airflow,2513523036,2085019238,amoghrajesh,35884252,2025-05-12T16:11:06+00:00,2025-05-12T16:12:05+00:00,Lets remove this import,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5d9,apache/airflow,2513523036,2085019493,amoghrajesh,35884252,2025-05-12T16:11:15+00:00,2025-05-12T16:12:05+00:00,Lets remove this import,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5da,apache/airflow,2513523036,2085039069,sunank200,8670962,2025-05-12T16:24:03+00:00,2025-05-12T16:24:03+00:00,Done,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5db,apache/airflow,2513523036,2085039228,sunank200,8670962,2025-05-12T16:24:10+00:00,2025-05-12T16:24:10+00:00,Done,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5dc,apache/airflow,2513523036,2085039386,sunank200,8670962,2025-05-12T16:24:16+00:00,2025-05-12T16:24:17+00:00,Done,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5dd,apache/airflow,2513523036,2085018993,amoghrajesh,35884252,2025-05-12T16:10:56+00:00,2025-05-12T16:12:05+00:00,Lets remove this import,true,airflow-core/src/airflow/models/baseoperatorlink.py,NaN,"@@ -20,3 +20,11 @@\n from __future__ import annotations\n \n from airflow.sdk.bases.operatorlink import BaseOperatorLink as BaseOperatorLink",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5de,apache/airflow,2513523036,2085019238,amoghrajesh,35884252,2025-05-12T16:11:06+00:00,2025-05-12T16:12:05+00:00,Lets remove this import,true,airflow-core/src/airflow/models/param.py,NaN,"@@ -20,3 +20,13 @@\n from __future__ import annotations\n \n from airflow.sdk.definitions.param import Param as Param, ParamsDict as ParamsDict",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5df,apache/airflow,2513523036,2085019493,amoghrajesh,35884252,2025-05-12T16:11:15+00:00,2025-05-12T16:12:05+00:00,Lets remove this import,true,airflow-core/src/airflow/secrets/cache.py,NaN,"@@ -20,3 +20,11 @@\n from __future__ import annotations\n \n from airflow.sdk.execution_time.cache import SecretCache as SecretCache",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e0,apache/airflow,2513523036,2085039069,sunank200,8670962,2025-05-12T16:24:03+00:00,2025-05-12T16:24:03+00:00,Done,true,airflow-core/src/airflow/secrets/cache.py,NaN,"@@ -20,3 +20,11 @@\n from __future__ import annotations\n \n from airflow.sdk.execution_time.cache import SecretCache as SecretCache",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e1,apache/airflow,2513523036,2085039228,sunank200,8670962,2025-05-12T16:24:10+00:00,2025-05-12T16:24:10+00:00,Done,true,airflow-core/src/airflow/models/param.py,NaN,"@@ -20,3 +20,13 @@\n from __future__ import annotations\n \n from airflow.sdk.definitions.param import Param as Param, ParamsDict as ParamsDict",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e2,apache/airflow,2513523036,2085039386,sunank200,8670962,2025-05-12T16:24:16+00:00,2025-05-12T16:24:17+00:00,Done,true,airflow-core/src/airflow/models/baseoperatorlink.py,NaN,"@@ -20,3 +20,11 @@\n from __future__ import annotations\n \n from airflow.sdk.bases.operatorlink import BaseOperatorLink as BaseOperatorLink",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e3,apache/airflow,2512268800,2083605965,eladkal,45845474,2025-05-11T19:53:38+00:00,2025-05-11T19:53:38+00:00,Whats the difference?\r\nGenerally speaking the all option would be rarely used. Most often at least one provider has rc2.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e4,apache/airflow,2512268800,2083606737,potiuk,595491,2025-05-11T19:57:09+00:00,2025-05-11T19:57:10+00:00,"""all"" means ""ALL"" - provders, airflow, helm chart , docker-stack.\r\n\r\nWe never publish them all together. ",false,NaN,NaN,NaN,1,1,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e5,apache/airflow,2512268800,2083607271,potiuk,595491,2025-05-11T19:59:20+00:00,2025-05-11T19:59:20+00:00,> Generally speaking the all option would be rarely used. Most often at least one provider has rc2.\r\n\r\nIt's not for RC2 - it's for publishing docs after release is done. Not sure what `rc2` has to do with it ?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e6,apache/airflow,2512268800,2083605965,eladkal,45845474,2025-05-11T19:53:38+00:00,2025-05-11T19:53:38+00:00,Whats the difference?\r\nGenerally speaking the all option would be rarely used. Most often at least one provider has rc2.,true,.github/workflows/publish-docs-to-s3.yml,5,"@@ -31,7 +31,7 @@ on:  # yamllint disable-line rule:truthy\n           eg: amazon common.messaging apache.kafka\n \n         required: false\n-        default: ""all""\n+        default: ""all-providers""",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e7,apache/airflow,2512268800,2083606737,potiuk,595491,2025-05-11T19:57:09+00:00,2025-05-11T19:57:10+00:00,"""all"" means ""ALL"" - provders, airflow, helm chart , docker-stack.\r\n\r\nWe never publish them all together. ",true,.github/workflows/publish-docs-to-s3.yml,5,"@@ -31,7 +31,7 @@ on:  # yamllint disable-line rule:truthy\n           eg: amazon common.messaging apache.kafka\n \n         required: false\n-        default: ""all""\n+        default: ""all-providers""",1,1,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e8,apache/airflow,2512268800,2083607271,potiuk,595491,2025-05-11T19:59:20+00:00,2025-05-11T19:59:20+00:00,> Generally speaking the all option would be rarely used. Most often at least one provider has rc2.\r\n\r\nIt's not for RC2 - it's for publishing docs after release is done. Not sure what `rc2` has to do with it ?,true,.github/workflows/publish-docs-to-s3.yml,5,"@@ -31,7 +31,7 @@ on:  # yamllint disable-line rule:truthy\n           eg: amazon common.messaging apache.kafka\n \n         required: false\n-        default: ""all""\n+        default: ""all-providers""",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5e9,apache/airflow,2512264648,2083615920,gopidesupavan,31437079,2025-05-11T20:39:06+00:00,2025-05-11T20:39:06+00:00,We can add a note about dry running a when using manual `breeze release-management publish-docs-to-s3` would better always.\r\n,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ea,apache/airflow,2512264648,2083625286,potiuk,595491,2025-05-11T21:26:57+00:00,2025-05-11T21:26:57+00:00,yeah. I am also adding another manual method - without the need of S3 credentials - via archive repo,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5eb,apache/airflow,2512264648,2083628581,potiuk,595491,2025-05-11T21:45:57+00:00,2025-05-11T21:45:57+00:00,Added the second manual update option and mentioned `--dry-run`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ec,apache/airflow,2512264648,2084147683,kaxil,8811558,2025-05-12T08:40:41+00:00,2025-05-12T08:40:42+00:00,```suggestion\r\nThis is the diagram of live documentation architecture:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ed,apache/airflow,2512264648,2084147828,kaxil,8811558,2025-05-12T08:40:46+00:00,2025-05-12T08:40:46+00:00,```suggestion\r\n* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it work\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ee,apache/airflow,2512264648,2084150164,kaxil,8811558,2025-05-12T08:41:33+00:00,2025-05-12T08:41:33+00:00,"```suggestion\r\nvia [GitHub UI](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml). The same workflow can be used to publish Airflow, Helm chart and providers documentation.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ef,apache/airflow,2512264648,2084151834,kaxil,8811558,2025-05-12T08:42:20+00:00,2025-05-12T08:42:21+00:00,"```suggestion\r\nrepository is automatically synchronized with the live S3 bucket. TODO: IMPLEMENT THIS, FOR NOW IT HAS\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f0,apache/airflow,2512264648,2084153505,kaxil,8811558,2025-05-12T08:43:01+00:00,2025-05-12T08:43:02+00:00,"```suggestion\r\nThe workflows in `apache-airflow` only update the documentation for the packages (Airflow, Helm chart,\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f1,apache/airflow,2512264648,2084937356,eladkal,45845474,2025-05-12T15:27:09+00:00,2025-05-12T15:27:09+00:00,We still need to do this if we add new provider or change his index place ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f2,apache/airflow,2512264648,2084940243,eladkal,45845474,2025-05-12T15:28:05+00:00,2025-05-12T15:28:06+00:00,"We should describe on which S3 path we do that.\r\nAs far as i remember you asked to do it on stage, review the docs and only if looks good rerun on live?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f3,apache/airflow,2512264648,2084950436,eladkal,45845474,2025-05-12T15:31:41+00:00,2025-05-12T15:31:46+00:00,Should we have a section that explains how to get permission to the S3?\r\nI assume only specific persons have access to the account,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f4,apache/airflow,2512264648,2084968415,potiuk,595491,2025-05-12T15:41:11+00:00,2025-05-12T15:41:11+00:00,Right!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f5,apache/airflow,2512264648,2084977656,potiuk,595491,2025-05-12T15:46:22+00:00,2025-05-12T15:46:23+00:00,"We do not have yet ""staging"" workflow enabled. Live is the only one that works - we will update it later, but live should be used only for now. I will add it.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f6,apache/airflow,2512264648,2084979151,potiuk,595491,2025-05-12T15:47:16+00:00,2025-05-12T15:47:16+00:00,"Sure. We can add ""Ask in #internal-airflow-ci-cd channel on slack""",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f7,apache/airflow,2512264648,2083615920,gopidesupavan,31437079,2025-05-11T20:39:06+00:00,2025-05-11T20:39:06+00:00,We can add a note about dry running a when using manual `breeze release-management publish-docs-to-s3` would better always.\r\n,true,docs/README.md,180,"@@ -37,3 +53,162 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it works\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram od live documentation architecture:\n+\n+![Live documentation architecture](images/documentation_architecture.png)\n+\n+Staging documentation architecture is similar, but uses staging bucket and staging Apache Website.\n+\n+# Typical workflows\n+\n+There are a few typical workflows that we support:\n+\n+## Publishing the documentation by the release manager\n+\n+The release manager publishes the documentation using `Publish Docs to S3` GitHub Action (accessible\n+via [GitHub UI](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml). The same  workflow can be used to publish Airflow, Helm chart and providers documentation.\n+\n+The person who triggers the build (release manager) should specify the tag name of the docs to be published\n+and the list of documentation packages to be published. Usually it is:\n+\n+* Airflow: `apache-airflow docker-stack` (later we will add `airflow-ctl` and `task-sdk`)\n+* Helm chart: `helm-chart`\n+* Providers: `provider_id1 provider_id2` or `all providers` if all providers should be published.\n+\n+Optionally - specifically if we run `all-providers` and release manager wants to exclude some providers,\n+they can specify documentation packages to exclude. Leaving ""no-docs-excluded"" will publish all packages\n+specified to be published without exclusions.\n+\n+You can also specify whether ""live"" or ""staging"" documentation should be published. The default is ""live"".\n+\n+Example screenshot of the workflow triggered from the GitHub UI:\n+\n+![Publishing airflow](images/publish_airflow.png)\n+\n+Right after the workflow succeeds and documentation is published, in live bucket, the `airflow-site-archive`\n+repository is automatically synchronized with the live S3 bucket. TO DO: IMPLEMENT THIS, FOR NOW IT HAS\n+TO BE MANUALLY SYNCHRONIZED VIA [Sync s3 to GitHub](https://github.com/apache/airflow-site-archive/actions/workflows/s3-to-github.yml)\n+workflow in `airflow-site-archive` repository. The `airflow-site-archive` essentially keeps the history of\n+snapshots of the `live` documentation.\n+\n+Also another thing that is triggered automatically (TODO: IMPLEMENT THIS) is building the `airflow-site`\n+and re-publishing it. This is needed in order to refresh version numbers of published documentation. After\n+you publish new version of the documentation, the ""stable"" version of the documentation is automatically\n+replaced with the new version, however building the `airflow-site` is needed to make sure that the drop-down\n+version numbers are updated properly.\n+\n+\n+## Publishing changes to the website (including theme)\n+\n+The workflows in `apache-airflow` only update the documentation for the documentation packages (Airflow, Helm chart,\n+Providers, Docker Stack) that we publish from airflow sources. If we want to publish changes to the website\n+itself or to the theme (css, javascript) we need to do it in `airflow-site` repository.\n+\n+Publishing of airflow-site happens automatically when a PR from `airflow-site` is merged to `main` or when\n+the [Build docs](https://github.com/apache/airflow-site/actions/workflows/build.yml) workflow is triggered\n+manually in the main branch of `airflow-site` repository. The workflow builds the website and publishes it to\n+`publish` branch of `airflow-site` repository, which in turn gets picked up by the ASF servers and is\n+published as the official website. This includes any changes to `.htaccess` of the website.\n+\n+Such a main build also publishes latest ""sphinx-airflow-theme"" package to GitHub so that the next build\n+of documentation can automatically pick it up from there. This means that if you want to make changes to\n+`javascript` or `css` that are part of the theme, you need to do it in `ariflow-site` repository and\n+merge it to `main` branch in order to be able to run the documentation build in `apache-airflow` repository\n+and pick up the latest version of the theme.\n+\n+The version of sphinx theme is fixed in both repositories:\n+\n+* https://github.com/apache/airflow-site/blob/main/sphinx_airflow_theme/sphinx_airflow_theme/__init__.py#L21\n+* https://github.com/apache/airflow/blob/main/devel-common/pyproject.toml#L77 in ""docs"" section\n+\n+In case of bigger changes to the theme, we\n+can first iterate on the website and merge a new theme version, and only after that we can switch to the new\n+version of the theme.\n+\n+\n+# Fixing historical documentation\n+\n+Sometimes we need to update historical documentation (modify generated `html`) - for example when we find\n+bad links or when we change some of the structure in the documentation. This can be done via the\n+`airflow-site-archive` repository. The workflow is as follows:\n+\n+* Get the latest version of the documentation from S3 to `airflow-site-archive` repository using\n+  `Sync s3 to GitHub` workflow. This will download the latest version of the documentation from S3 to\n+  `airflow-site-archive` repository (this should be normally not needed, if automated synchronization works).\n+\n+* Make the changes to the documentation in `airflow-site-archive` repository. This can be done using any\n+  text editors, scripts etc. Those files are generated as `html` files and are not meant to be regenerated,\n+  they should be modified as `html` files in-place\n+\n+* Commit the changes to `airflow-site-archive` repository and push them to `some` branch of the repository.\n+\n+* Run `Sync GitHub to S3` workflow in `airflow-site-archive` repository. This will upload the modified\n+  documentation to S3 bucket.\n+\n+* You can choose, whether to sync the changes to `live` or `staging` bucket. The default is `live`.\n+\n+* By default the workflow will synchronize all documentation modified in single - last commit pushed to\n+  the branch you specified. You can also specify ""full_sync"" to synchronize all files in the repository.\n+\n+* In case you specify ""full_sync"", you can also synchronize `all` docs or only selected documentation\n+  packages (for example `apache-airflow` or `docker-stack` or `amazon` or `helm-chart`) - you can specify\n+ more than one package separated by  spaces.\n+\n+* After you synchronize the changes to S3, the Sync `S3 to GitHub` workflow will be triggered\n+  automatically and the changes will be synchronized to `airflow-site-archive` `main` branch - so there\n+  is no need to merge your changes to `main` branch of `airflow-site-archive` repository.\n+\n+![Sync GitHub to S3](images/sync_github_to_s3.png)\n+\n+\n+## Manually publishing documentation\n+\n+The regular publishing workflows involve running Github Actions workflow and they cover majority of cases,\n+however sometimes some manual updates and cherry-picks are needed, when we discover problems with the\n+publishing and doc building code - for example when we find that we need to fix extensions to sphinx.\n+\n+In such case, release manager or a committer can build and publish documentation locally - providing that\n+they configure AWS credentials to be able to upload files to S3.\n+\n+You can checkout locally a version of airflow repo that you need and apply any cherry-picks you need before\n+running publishing.\n+\n+This is done using breeze. You also need to have aws CLI installed and configured credentials to be able\n+to upload files to S3. You can get credentials from one of the admins of Airflow's AWS account. The\n+region to set for AWS is `us-east-2`.\n+",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f8,apache/airflow,2512264648,2083625286,potiuk,595491,2025-05-11T21:26:57+00:00,2025-05-11T21:26:57+00:00,yeah. I am also adding another manual method - without the need of S3 credentials - via archive repo,true,docs/README.md,180,"@@ -37,3 +53,162 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it works\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram od live documentation architecture:\n+\n+![Live documentation architecture](images/documentation_architecture.png)\n+\n+Staging documentation architecture is similar, but uses staging bucket and staging Apache Website.\n+\n+# Typical workflows\n+\n+There are a few typical workflows that we support:\n+\n+## Publishing the documentation by the release manager\n+\n+The release manager publishes the documentation using `Publish Docs to S3` GitHub Action (accessible\n+via [GitHub UI](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml). The same  workflow can be used to publish Airflow, Helm chart and providers documentation.\n+\n+The person who triggers the build (release manager) should specify the tag name of the docs to be published\n+and the list of documentation packages to be published. Usually it is:\n+\n+* Airflow: `apache-airflow docker-stack` (later we will add `airflow-ctl` and `task-sdk`)\n+* Helm chart: `helm-chart`\n+* Providers: `provider_id1 provider_id2` or `all providers` if all providers should be published.\n+\n+Optionally - specifically if we run `all-providers` and release manager wants to exclude some providers,\n+they can specify documentation packages to exclude. Leaving ""no-docs-excluded"" will publish all packages\n+specified to be published without exclusions.\n+\n+You can also specify whether ""live"" or ""staging"" documentation should be published. The default is ""live"".\n+\n+Example screenshot of the workflow triggered from the GitHub UI:\n+\n+![Publishing airflow](images/publish_airflow.png)\n+\n+Right after the workflow succeeds and documentation is published, in live bucket, the `airflow-site-archive`\n+repository is automatically synchronized with the live S3 bucket. TO DO: IMPLEMENT THIS, FOR NOW IT HAS\n+TO BE MANUALLY SYNCHRONIZED VIA [Sync s3 to GitHub](https://github.com/apache/airflow-site-archive/actions/workflows/s3-to-github.yml)\n+workflow in `airflow-site-archive` repository. The `airflow-site-archive` essentially keeps the history of\n+snapshots of the `live` documentation.\n+\n+Also another thing that is triggered automatically (TODO: IMPLEMENT THIS) is building the `airflow-site`\n+and re-publishing it. This is needed in order to refresh version numbers of published documentation. After\n+you publish new version of the documentation, the ""stable"" version of the documentation is automatically\n+replaced with the new version, however building the `airflow-site` is needed to make sure that the drop-down\n+version numbers are updated properly.\n+\n+\n+## Publishing changes to the website (including theme)\n+\n+The workflows in `apache-airflow` only update the documentation for the documentation packages (Airflow, Helm chart,\n+Providers, Docker Stack) that we publish from airflow sources. If we want to publish changes to the website\n+itself or to the theme (css, javascript) we need to do it in `airflow-site` repository.\n+\n+Publishing of airflow-site happens automatically when a PR from `airflow-site` is merged to `main` or when\n+the [Build docs](https://github.com/apache/airflow-site/actions/workflows/build.yml) workflow is triggered\n+manually in the main branch of `airflow-site` repository. The workflow builds the website and publishes it to\n+`publish` branch of `airflow-site` repository, which in turn gets picked up by the ASF servers and is\n+published as the official website. This includes any changes to `.htaccess` of the website.\n+\n+Such a main build also publishes latest ""sphinx-airflow-theme"" package to GitHub so that the next build\n+of documentation can automatically pick it up from there. This means that if you want to make changes to\n+`javascript` or `css` that are part of the theme, you need to do it in `ariflow-site` repository and\n+merge it to `main` branch in order to be able to run the documentation build in `apache-airflow` repository\n+and pick up the latest version of the theme.\n+\n+The version of sphinx theme is fixed in both repositories:\n+\n+* https://github.com/apache/airflow-site/blob/main/sphinx_airflow_theme/sphinx_airflow_theme/__init__.py#L21\n+* https://github.com/apache/airflow/blob/main/devel-common/pyproject.toml#L77 in ""docs"" section\n+\n+In case of bigger changes to the theme, we\n+can first iterate on the website and merge a new theme version, and only after that we can switch to the new\n+version of the theme.\n+\n+\n+# Fixing historical documentation\n+\n+Sometimes we need to update historical documentation (modify generated `html`) - for example when we find\n+bad links or when we change some of the structure in the documentation. This can be done via the\n+`airflow-site-archive` repository. The workflow is as follows:\n+\n+* Get the latest version of the documentation from S3 to `airflow-site-archive` repository using\n+  `Sync s3 to GitHub` workflow. This will download the latest version of the documentation from S3 to\n+  `airflow-site-archive` repository (this should be normally not needed, if automated synchronization works).\n+\n+* Make the changes to the documentation in `airflow-site-archive` repository. This can be done using any\n+  text editors, scripts etc. Those files are generated as `html` files and are not meant to be regenerated,\n+  they should be modified as `html` files in-place\n+\n+* Commit the changes to `airflow-site-archive` repository and push them to `some` branch of the repository.\n+\n+* Run `Sync GitHub to S3` workflow in `airflow-site-archive` repository. This will upload the modified\n+  documentation to S3 bucket.\n+\n+* You can choose, whether to sync the changes to `live` or `staging` bucket. The default is `live`.\n+\n+* By default the workflow will synchronize all documentation modified in single - last commit pushed to\n+  the branch you specified. You can also specify ""full_sync"" to synchronize all files in the repository.\n+\n+* In case you specify ""full_sync"", you can also synchronize `all` docs or only selected documentation\n+  packages (for example `apache-airflow` or `docker-stack` or `amazon` or `helm-chart`) - you can specify\n+ more than one package separated by  spaces.\n+\n+* After you synchronize the changes to S3, the Sync `S3 to GitHub` workflow will be triggered\n+  automatically and the changes will be synchronized to `airflow-site-archive` `main` branch - so there\n+  is no need to merge your changes to `main` branch of `airflow-site-archive` repository.\n+\n+![Sync GitHub to S3](images/sync_github_to_s3.png)\n+\n+\n+## Manually publishing documentation\n+\n+The regular publishing workflows involve running Github Actions workflow and they cover majority of cases,\n+however sometimes some manual updates and cherry-picks are needed, when we discover problems with the\n+publishing and doc building code - for example when we find that we need to fix extensions to sphinx.\n+\n+In such case, release manager or a committer can build and publish documentation locally - providing that\n+they configure AWS credentials to be able to upload files to S3.\n+\n+You can checkout locally a version of airflow repo that you need and apply any cherry-picks you need before\n+running publishing.\n+\n+This is done using breeze. You also need to have aws CLI installed and configured credentials to be able\n+to upload files to S3. You can get credentials from one of the admins of Airflow's AWS account. The\n+region to set for AWS is `us-east-2`.\n+",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5f9,apache/airflow,2512264648,2083628581,potiuk,595491,2025-05-11T21:45:57+00:00,2025-05-11T21:45:57+00:00,Added the second manual update option and mentioned `--dry-run`,true,docs/README.md,180,"@@ -37,3 +53,162 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it works\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram od live documentation architecture:\n+\n+![Live documentation architecture](images/documentation_architecture.png)\n+\n+Staging documentation architecture is similar, but uses staging bucket and staging Apache Website.\n+\n+# Typical workflows\n+\n+There are a few typical workflows that we support:\n+\n+## Publishing the documentation by the release manager\n+\n+The release manager publishes the documentation using `Publish Docs to S3` GitHub Action (accessible\n+via [GitHub UI](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml). The same  workflow can be used to publish Airflow, Helm chart and providers documentation.\n+\n+The person who triggers the build (release manager) should specify the tag name of the docs to be published\n+and the list of documentation packages to be published. Usually it is:\n+\n+* Airflow: `apache-airflow docker-stack` (later we will add `airflow-ctl` and `task-sdk`)\n+* Helm chart: `helm-chart`\n+* Providers: `provider_id1 provider_id2` or `all providers` if all providers should be published.\n+\n+Optionally - specifically if we run `all-providers` and release manager wants to exclude some providers,\n+they can specify documentation packages to exclude. Leaving ""no-docs-excluded"" will publish all packages\n+specified to be published without exclusions.\n+\n+You can also specify whether ""live"" or ""staging"" documentation should be published. The default is ""live"".\n+\n+Example screenshot of the workflow triggered from the GitHub UI:\n+\n+![Publishing airflow](images/publish_airflow.png)\n+\n+Right after the workflow succeeds and documentation is published, in live bucket, the `airflow-site-archive`\n+repository is automatically synchronized with the live S3 bucket. TO DO: IMPLEMENT THIS, FOR NOW IT HAS\n+TO BE MANUALLY SYNCHRONIZED VIA [Sync s3 to GitHub](https://github.com/apache/airflow-site-archive/actions/workflows/s3-to-github.yml)\n+workflow in `airflow-site-archive` repository. The `airflow-site-archive` essentially keeps the history of\n+snapshots of the `live` documentation.\n+\n+Also another thing that is triggered automatically (TODO: IMPLEMENT THIS) is building the `airflow-site`\n+and re-publishing it. This is needed in order to refresh version numbers of published documentation. After\n+you publish new version of the documentation, the ""stable"" version of the documentation is automatically\n+replaced with the new version, however building the `airflow-site` is needed to make sure that the drop-down\n+version numbers are updated properly.\n+\n+\n+## Publishing changes to the website (including theme)\n+\n+The workflows in `apache-airflow` only update the documentation for the documentation packages (Airflow, Helm chart,\n+Providers, Docker Stack) that we publish from airflow sources. If we want to publish changes to the website\n+itself or to the theme (css, javascript) we need to do it in `airflow-site` repository.\n+\n+Publishing of airflow-site happens automatically when a PR from `airflow-site` is merged to `main` or when\n+the [Build docs](https://github.com/apache/airflow-site/actions/workflows/build.yml) workflow is triggered\n+manually in the main branch of `airflow-site` repository. The workflow builds the website and publishes it to\n+`publish` branch of `airflow-site` repository, which in turn gets picked up by the ASF servers and is\n+published as the official website. This includes any changes to `.htaccess` of the website.\n+\n+Such a main build also publishes latest ""sphinx-airflow-theme"" package to GitHub so that the next build\n+of documentation can automatically pick it up from there. This means that if you want to make changes to\n+`javascript` or `css` that are part of the theme, you need to do it in `ariflow-site` repository and\n+merge it to `main` branch in order to be able to run the documentation build in `apache-airflow` repository\n+and pick up the latest version of the theme.\n+\n+The version of sphinx theme is fixed in both repositories:\n+\n+* https://github.com/apache/airflow-site/blob/main/sphinx_airflow_theme/sphinx_airflow_theme/__init__.py#L21\n+* https://github.com/apache/airflow/blob/main/devel-common/pyproject.toml#L77 in ""docs"" section\n+\n+In case of bigger changes to the theme, we\n+can first iterate on the website and merge a new theme version, and only after that we can switch to the new\n+version of the theme.\n+\n+\n+# Fixing historical documentation\n+\n+Sometimes we need to update historical documentation (modify generated `html`) - for example when we find\n+bad links or when we change some of the structure in the documentation. This can be done via the\n+`airflow-site-archive` repository. The workflow is as follows:\n+\n+* Get the latest version of the documentation from S3 to `airflow-site-archive` repository using\n+  `Sync s3 to GitHub` workflow. This will download the latest version of the documentation from S3 to\n+  `airflow-site-archive` repository (this should be normally not needed, if automated synchronization works).\n+\n+* Make the changes to the documentation in `airflow-site-archive` repository. This can be done using any\n+  text editors, scripts etc. Those files are generated as `html` files and are not meant to be regenerated,\n+  they should be modified as `html` files in-place\n+\n+* Commit the changes to `airflow-site-archive` repository and push them to `some` branch of the repository.\n+\n+* Run `Sync GitHub to S3` workflow in `airflow-site-archive` repository. This will upload the modified\n+  documentation to S3 bucket.\n+\n+* You can choose, whether to sync the changes to `live` or `staging` bucket. The default is `live`.\n+\n+* By default the workflow will synchronize all documentation modified in single - last commit pushed to\n+  the branch you specified. You can also specify ""full_sync"" to synchronize all files in the repository.\n+\n+* In case you specify ""full_sync"", you can also synchronize `all` docs or only selected documentation\n+  packages (for example `apache-airflow` or `docker-stack` or `amazon` or `helm-chart`) - you can specify\n+ more than one package separated by  spaces.\n+\n+* After you synchronize the changes to S3, the Sync `S3 to GitHub` workflow will be triggered\n+  automatically and the changes will be synchronized to `airflow-site-archive` `main` branch - so there\n+  is no need to merge your changes to `main` branch of `airflow-site-archive` repository.\n+\n+![Sync GitHub to S3](images/sync_github_to_s3.png)\n+\n+\n+## Manually publishing documentation\n+\n+The regular publishing workflows involve running Github Actions workflow and they cover majority of cases,\n+however sometimes some manual updates and cherry-picks are needed, when we discover problems with the\n+publishing and doc building code - for example when we find that we need to fix extensions to sphinx.\n+\n+In such case, release manager or a committer can build and publish documentation locally - providing that\n+they configure AWS credentials to be able to upload files to S3.\n+\n+You can checkout locally a version of airflow repo that you need and apply any cherry-picks you need before\n+running publishing.\n+\n+This is done using breeze. You also need to have aws CLI installed and configured credentials to be able\n+to upload files to S3. You can get credentials from one of the admins of Airflow's AWS account. The\n+region to set for AWS is `us-east-2`.\n+",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5fa,apache/airflow,2512264648,2084147683,kaxil,8811558,2025-05-12T08:40:41+00:00,2025-05-12T08:40:42+00:00,```suggestion\r\nThis is the diagram of live documentation architecture:\r\n```,true,docs/README.md,NaN,"@@ -37,3 +54,189 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it works\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram od live documentation architecture:",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5fb,apache/airflow,2512264648,2084147828,kaxil,8811558,2025-05-12T08:40:46+00:00,2025-05-12T08:40:46+00:00,```suggestion\r\n* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it work\r\n```,true,docs/README.md,NaN,"@@ -37,3 +54,189 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it works",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5fc,apache/airflow,2512264648,2084150164,kaxil,8811558,2025-05-12T08:41:33+00:00,2025-05-12T08:41:33+00:00,"```suggestion\r\nvia [GitHub UI](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml). The same workflow can be used to publish Airflow, Helm chart and providers documentation.\r\n```",true,docs/README.md,NaN,"@@ -37,3 +54,189 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it works\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram od live documentation architecture:\n+\n+![Live documentation architecture](images/documentation_architecture.png)\n+\n+Staging documentation architecture is similar, but uses staging bucket and staging Apache Website.\n+\n+# Typical workflows\n+\n+There are a few typical workflows that we support:\n+\n+## Publishing the documentation by the release manager\n+\n+The release manager publishes the documentation using `Publish Docs to S3` GitHub Action (accessible\n+via [GitHub UI](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml). The same  workflow can be used to publish Airflow, Helm chart and providers documentation.",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5fd,apache/airflow,2512264648,2084151834,kaxil,8811558,2025-05-12T08:42:20+00:00,2025-05-12T08:42:21+00:00,"```suggestion\r\nrepository is automatically synchronized with the live S3 bucket. TODO: IMPLEMENT THIS, FOR NOW IT HAS\r\n```",true,docs/README.md,NaN,"@@ -37,3 +54,189 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it works\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram od live documentation architecture:\n+\n+![Live documentation architecture](images/documentation_architecture.png)\n+\n+Staging documentation architecture is similar, but uses staging bucket and staging Apache Website.\n+\n+# Typical workflows\n+\n+There are a few typical workflows that we support:\n+\n+## Publishing the documentation by the release manager\n+\n+The release manager publishes the documentation using `Publish Docs to S3` GitHub Action (accessible\n+via [GitHub UI](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml). The same  workflow can be used to publish Airflow, Helm chart and providers documentation.\n+\n+The person who triggers the build (release manager) should specify the tag name of the docs to be published\n+and the list of documentation packages to be published. Usually it is:\n+\n+* Airflow: `apache-airflow docker-stack` (later we will add `airflow-ctl` and `task-sdk`)\n+* Helm chart: `helm-chart`\n+* Providers: `provider_id1 provider_id2` or `all providers` if all providers should be published.\n+\n+Optionally - specifically if we run `all-providers` and release manager wants to exclude some providers,\n+they can specify documentation packages to exclude. Leaving ""no-docs-excluded"" will publish all packages\n+specified to be published without exclusions.\n+\n+You can also specify whether ""live"" or ""staging"" documentation should be published. The default is ""live"".\n+\n+Example screenshot of the workflow triggered from the GitHub UI:\n+\n+![Publishing airflow](images/publish_airflow.png)\n+\n+Right after the workflow succeeds and documentation is published, in live bucket, the `airflow-site-archive`\n+repository is automatically synchronized with the live S3 bucket. TO DO: IMPLEMENT THIS, FOR NOW IT HAS",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5fe,apache/airflow,2512264648,2084153505,kaxil,8811558,2025-05-12T08:43:01+00:00,2025-05-12T08:43:02+00:00,"```suggestion\r\nThe workflows in `apache-airflow` only update the documentation for the packages (Airflow, Helm chart,\r\n```",true,docs/README.md,NaN,"@@ -37,3 +54,189 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it works\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram od live documentation architecture:\n+\n+![Live documentation architecture](images/documentation_architecture.png)\n+\n+Staging documentation architecture is similar, but uses staging bucket and staging Apache Website.\n+\n+# Typical workflows\n+\n+There are a few typical workflows that we support:\n+\n+## Publishing the documentation by the release manager\n+\n+The release manager publishes the documentation using `Publish Docs to S3` GitHub Action (accessible\n+via [GitHub UI](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml). The same  workflow can be used to publish Airflow, Helm chart and providers documentation.\n+\n+The person who triggers the build (release manager) should specify the tag name of the docs to be published\n+and the list of documentation packages to be published. Usually it is:\n+\n+* Airflow: `apache-airflow docker-stack` (later we will add `airflow-ctl` and `task-sdk`)\n+* Helm chart: `helm-chart`\n+* Providers: `provider_id1 provider_id2` or `all providers` if all providers should be published.\n+\n+Optionally - specifically if we run `all-providers` and release manager wants to exclude some providers,\n+they can specify documentation packages to exclude. Leaving ""no-docs-excluded"" will publish all packages\n+specified to be published without exclusions.\n+\n+You can also specify whether ""live"" or ""staging"" documentation should be published. The default is ""live"".\n+\n+Example screenshot of the workflow triggered from the GitHub UI:\n+\n+![Publishing airflow](images/publish_airflow.png)\n+\n+Right after the workflow succeeds and documentation is published, in live bucket, the `airflow-site-archive`\n+repository is automatically synchronized with the live S3 bucket. TO DO: IMPLEMENT THIS, FOR NOW IT HAS\n+TO BE MANUALLY SYNCHRONIZED VIA [Sync s3 to GitHub](https://github.com/apache/airflow-site-archive/actions/workflows/s3-to-github.yml)\n+workflow in `airflow-site-archive` repository. The `airflow-site-archive` essentially keeps the history of\n+snapshots of the `live` documentation.\n+\n+Also another thing that is triggered automatically (TODO: IMPLEMENT THIS) is building the `airflow-site`\n+and re-publishing it. This is needed in order to refresh version numbers of published documentation. After\n+you publish new version of the documentation, the ""stable"" version of the documentation is automatically\n+replaced with the new version, however building the `airflow-site` is needed to make sure that the drop-down\n+version numbers are updated properly.\n+\n+\n+## Publishing changes to the website (including theme)\n+\n+The workflows in `apache-airflow` only update the documentation for the documentation packages (Airflow, Helm chart,",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e5ff,apache/airflow,2512264648,2084937356,eladkal,45845474,2025-05-12T15:27:09+00:00,2025-05-12T15:27:09+00:00,We still need to do this if we add new provider or change his index place ,true,dev/README_RELEASE_PROVIDERS.md,151,"@@ -486,137 +485,6 @@ If you want to disable this behaviour, set the env **CLEAN_LOCAL_TAGS** to false\n breeze release-management tag-providers\n ```\n \n-## Prepare documentation\n-\n-Documentation is an essential part of the product and should be made available to users.\n-In our cases, documentation  for the released versions is published in a separate repository -\n-[`apache/airflow-site`](https://github.com/apache/airflow-site), but the documentation source code\n-and build tools are available in the `apache/airflow` repository, so you have to coordinate between\n-the two repositories to be able to build the documentation.\n-\n-Documentation for providers can be found in the `/docs/apache-airflow-providers` directory\n-and the `/docs/apache-airflow-providers-*/` directory. The first directory contains the package contents\n-lists and should be updated every time a new version of Provider distributions is released.\n-\n-- First, copy the airflow-site repository and set the environment variable ``AIRFLOW_SITE_DIRECTORY``.\n-\n-```shell script\n-git clone https://github.com/apache/airflow-site.git airflow-site\n-cd airflow-site\n-export AIRFLOW_SITE_DIRECTORY=""$(pwd -P)""\n-```\n-\n-Note if this is not the first time you clone the repo make sure main branch is rebased:\n-\n-```shell script\n-cd ""${AIRFLOW_SITE_DIRECTORY}""\n-git checkout main\n-git pull --rebase\n-```\n-\n-- Then you can go to the directory and build the necessary documentation packages\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-breeze build-docs --clean-build apache-airflow-providers all-providers --include-removed-providers --include-commits\n-```\n-\n-Usually when we release packages we also build documentation for the ""documentation-only"" packages. This\n-means that unless we release just few selected packages or if we need to deliberately skip some packages\n-we should release documentation for all Provider distributions and the above command is the one to use.\n-\n-If we want to just release some providers you can release them using package names:\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-breeze build-docs apache-airflow-providers cncf.kubernetes sftp --clean-build --include-commits\n-```\n-\n-Alternatively, if you have set the environment variable: `DISTRIBUTIONS_LIST` above, just run the command:\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-breeze build-docs --clean-build --include-commits\n-```\n-\n-Or using `--distributions-list` argument:\n-\n-```shell script\n-breeze build-docs --distributions-list PACKAGE1,PACKAGE2 --include-commits\n-```\n-\n-- Now you can preview the documentation.\n-\n-```shell script\n-./docs/start_doc_server.sh\n-```\n-\n-If you encounter error like:\n-\n-```shell script\n-airflow git:(main) ./docs/start_doc_server.sh\n-./docs/start_doc_server.sh: line 22: cd: /Users/eladkal/PycharmProjects/airflow/docs/_build: No such file or directory\n-```\n-\n-That probably means that the doc folder is empty thus it can not build the doc server.\n-This indicates that previous step of building the docs did not work.\n-\n-- Copy the documentation to the ``airflow-site`` repository\n-\n-All providers (including overriding documentation for doc-only changes) - note that publishing is\n-way faster on multi-cpu machines when you are publishing multiple providers:\n-\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-\n-breeze release-management publish-docs apache-airflow-providers all-providers --include-removed-providers \\n-    --override-versioned --run-in-parallel\n-\n-breeze release-management add-back-references all-providers\n-```\n-\n-If you have providers as list of provider ids because you just released them you can build them with\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-\n-breeze release-management publish-docs amazon apache.beam google ....\n-breeze release-management add-back-references all-providers\n-```\n-\n-Alternatively, if you have set the environment variable: `DISTRIBUTIONS_LIST` above, just run the command:\n-\n-```shell script\n-breeze release-management publish-docs\n-breeze release-management add-back-references all-providers\n-```\n-\n-Or using `--distributions-list` argument:\n-\n-```shell script\n-breeze release-management publish-docs --distributions-list PACKAGE1,PACKAGE2\n-breeze release-management add-back-references all-providers\n-```\n-\n-\n-Review the state of removed, suspended, new packages in\n-[the docs index](https://github.com/apache/airflow-site/blob/master/landing-pages/site/content/en/docs/_index.md):\n-\n-- If you publish a new package, you must add it to the list of packages in the index.\n-- If there are changes to suspension or removal status of a package  you must move it appropriate section.\n-\n-- Create the commit and push changes.\n-\n-```shell script\n-cd ""${AIRFLOW_SITE_DIRECTORY}""\n-branch=""add-documentation-$(date ""+%Y-%m-%d%n"")""\n-git checkout -b ""${branch}""\n-git add .\n-git commit -m ""Add documentation for packages - $(date ""+%Y-%m-%d%n"")""\n-git push --set-upstream origin ""${branch}""\n-```",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e600,apache/airflow,2512264648,2084940243,eladkal,45845474,2025-05-12T15:28:05+00:00,2025-05-12T15:28:06+00:00,"We should describe on which S3 path we do that.\r\nAs far as i remember you asked to do it on stage, review the docs and only if looks good rerun on live?",true,dev/README_RELEASE_PROVIDERS.md,204,"@@ -1283,6 +1136,38 @@ If you want to disable this behaviour, set the env **CLEAN_LOCAL_TAGS** to false\n breeze release-management tag-providers\n ```\n \n+## Publish documentation\n+\n+Documentation is an essential part of the product and should be made available to users.\n+In our cases, documentation for the released versions is published in S3 bucket, and the site is\n+kept in a separate repository - [`apache/airflow-site`](https://github.com/apache/airflow-site),\n+but the documentation source code and build tools are available in the `apache/airflow` repository, so\n+you need to run several workflows to publish the documentation. More details about it can be found in\n+[Docs README](../docs/README.md) showing the architecture and workflows including manual workflows for\n+emergency cases.\n+\n+There are two steps to publish the documentation:\n+\n+1. Publish the documentation to S3 bucket.\n+\n+The release manager publishes the documentation using GitHub Actions workflow\n+[Publish Docs to S3](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml).\n+\n+You can specify the tag to use to build the docs and list of providers to publish (separated by spaces) or\n+``all-providers`` in case you want to publish all providers (optionally you can exclude\n+some of those providers)\n+\n+After that step, the provider documentation should be available under the usual urls (same as in PyPI packages)\n+but stable links and drop-down boxes should not be updated.",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e601,apache/airflow,2512264648,2084950436,eladkal,45845474,2025-05-12T15:31:41+00:00,2025-05-12T15:31:46+00:00,Should we have a section that explains how to get permission to the S3?\r\nI assume only specific persons have access to the account,true,docs/README.md,164,"@@ -37,3 +54,196 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it work\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram of live documentation architecture:\n+\n+![Live documentation architecture](images/documentation_architecture.png)\n+\n+Staging documentation architecture is similar, but uses staging bucket and staging Apache Website.\n+\n+# Typical workflows\n+\n+There are a few typical workflows that we support:\n+\n+## Publishing the documentation by the release manager\n+\n+The release manager publishes the documentation using GitHub Actions workflow\n+[Publish Docs to S3](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml).\n+The same workflow can be used to publish Airflow, Helm chart and providers documentation.\n+\n+The person who triggers the build (release manager) should specify the tag name of the docs to be published\n+and the list of documentation packages to be published. Usually it is:\n+\n+* Airflow: `apache-airflow docker-stack` (later we will add `airflow-ctl` and `task-sdk`)\n+* Helm chart: `helm-chart`\n+* Providers: `provider_id1 provider_id2` or `all providers` if all providers should be published.\n+\n+Optionally - specifically if we run `all-providers` and release manager wants to exclude some providers,\n+they can specify documentation packages to exclude. Leaving ""no-docs-excluded"" will publish all packages\n+specified to be published without exclusions.\n+\n+You can also specify whether ""live"" or ""staging"" documentation should be published. The default is ""live"".\n+\n+Example screenshot of the workflow triggered from the GitHub UI:\n+\n+![Publishing airflow or providers](images/publish_airflow.png)\n+\n+Note that this just publishes the documentation but does not update the ""site"" with version numbers or\n+stable links to providers and airflow - if you release a new documentation version it will be available\n+with direct URL (say https://apache.airflow.org/docs/apache-airflow/3.0.1/) but the main site will still\n+point to previous version of the documentation as `stable` and the version drop-downs will not be updated.\n+\n+In order to do it, you need to run the [Build docs](https://github.com/apache/airflow-site/actions/workflows/build.yml)\n+workflow in `airflow-site` repository. This will build the website and publish it to the `publish`\n+branch of `airflow-site` repository, including refreshing of the version numbers in the drop-downs and\n+stable links.\n+\n+![Publishing site](images/publish_site.png)\n+\n+\n+Some time after the workflow succeeds and documentation is published, in live bucket, the `airflow-site-archive`\n+repository is automatically synchronized with the live S3 bucket. TODO: IMPLEMENT THIS, FOR NOW IT HAS\n+TO BE MANUALLY SYNCHRONIZED VIA [Sync s3 to GitHub](https://github.com/apache/airflow-site-archive/actions/workflows/s3-to-github.yml)\n+workflow in `airflow-site-archive` repository. The `airflow-site-archive` essentially keeps the history of\n+snapshots of the `live` documentation.\n+\n+## Publishing changes to the website (including theme)\n+\n+The workflows in `apache-airflow` only update the documentation for the packages (Airflow, Helm chart,\n+Providers, Docker Stack) that we publish from airflow sources. If we want to publish changes to the website\n+itself or to the theme (css, javascript) we need to do it in `airflow-site` repository.\n+\n+Publishing of airflow-site happens automatically when a PR from `airflow-site` is merged to `main` or when\n+the [Build docs](https://github.com/apache/airflow-site/actions/workflows/build.yml) workflow is triggered\n+manually in the main branch of `airflow-site` repository. The workflow builds the website and publishes it to\n+`publish` branch of `airflow-site` repository, which in turn gets picked up by the ASF servers and is\n+published as the official website. This includes any changes to `.htaccess` of the website.\n+\n+Such a main build also publishes latest ""sphinx-airflow-theme"" package to GitHub so that the next build\n+of documentation can automatically pick it up from there. This means that if you want to make changes to\n+`javascript` or `css` that are part of the theme, you need to do it in `ariflow-site` repository and\n+merge it to `main` branch in order to be able to run the documentation build in `apache-airflow` repository\n+and pick up the latest version of the theme.\n+\n+The version of sphinx theme is fixed in both repositories:\n+\n+* https://github.com/apache/airflow-site/blob/main/sphinx_airflow_theme/sphinx_airflow_theme/__init__.py#L21\n+* https://github.com/apache/airflow/blob/main/devel-common/pyproject.toml#L77 in ""docs"" section\n+\n+In case of bigger changes to the theme, we\n+can first iterate on the website and merge a new theme version, and only after that we can switch to the new\n+version of the theme.\n+\n+\n+# Fixing historical documentation\n+\n+Sometimes we need to update historical documentation (modify generated `html`) - for example when we find\n+bad links or when we change some of the structure in the documentation. This can be done via the\n+`airflow-site-archive` repository. The workflow is as follows:\n+\n+1. Get the latest version of the documentation from S3 to `airflow-site-archive` repository using\n+   `Sync s3 to GitHub` workflow. This will download the latest version of the documentation from S3 to\n+   `airflow-site-archive` repository (this should be normally not needed, if automated synchronization works).\n+2. Make the changes to the documentation in `airflow-site-archive` repository. This can be done using any\n+   text editors, scripts etc. Those files are generated as `html` files and are not meant to be regenerated,\n+   they should be modified as `html` files in-place\n+3. Commit the changes to `airflow-site-archive` repository and push them to `some` branch of the repository.\n+4. Run `Sync GitHub to S3` workflow in `airflow-site-archive` repository. This will upload the modified\n+   documentation to S3 bucket.\n+5. You can choose, whether to sync the changes to `live` or `staging` bucket. The default is `live`.\n+6. By default the workflow will synchronize all documentation modified in single - last commit pushed to\n+   the branch you specified. You can also specify ""full_sync"" to synchronize all files in the repository.\n+7. In case you specify ""full_sync"", you can also synchronize `all` docs or only selected documentation\n+   packages (for example `apache-airflow` or `docker-stack` or `amazon` or `helm-chart`) - you can specify\n+   more than one package separated by  spaces.\n+8. After you synchronize the changes to S3, the Sync `S3 to GitHub` workflow will be triggered\n+   automatically and the changes will be synchronized to `airflow-site-archive` `main` branch - so there\n+   is no need to merge your changes to `main` branch of `airflow-site-archive` repository. You can safely\n+   delete the branch you created in step 3.\n+\n+![Sync GitHub to S3](images/sync_github_to_s3.png)\n+\n+\n+## Manually publishing documentation directly to S3",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e602,apache/airflow,2512264648,2084968415,potiuk,595491,2025-05-12T15:41:11+00:00,2025-05-12T15:41:11+00:00,Right!,true,dev/README_RELEASE_PROVIDERS.md,151,"@@ -486,137 +485,6 @@ If you want to disable this behaviour, set the env **CLEAN_LOCAL_TAGS** to false\n breeze release-management tag-providers\n ```\n \n-## Prepare documentation\n-\n-Documentation is an essential part of the product and should be made available to users.\n-In our cases, documentation  for the released versions is published in a separate repository -\n-[`apache/airflow-site`](https://github.com/apache/airflow-site), but the documentation source code\n-and build tools are available in the `apache/airflow` repository, so you have to coordinate between\n-the two repositories to be able to build the documentation.\n-\n-Documentation for providers can be found in the `/docs/apache-airflow-providers` directory\n-and the `/docs/apache-airflow-providers-*/` directory. The first directory contains the package contents\n-lists and should be updated every time a new version of Provider distributions is released.\n-\n-- First, copy the airflow-site repository and set the environment variable ``AIRFLOW_SITE_DIRECTORY``.\n-\n-```shell script\n-git clone https://github.com/apache/airflow-site.git airflow-site\n-cd airflow-site\n-export AIRFLOW_SITE_DIRECTORY=""$(pwd -P)""\n-```\n-\n-Note if this is not the first time you clone the repo make sure main branch is rebased:\n-\n-```shell script\n-cd ""${AIRFLOW_SITE_DIRECTORY}""\n-git checkout main\n-git pull --rebase\n-```\n-\n-- Then you can go to the directory and build the necessary documentation packages\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-breeze build-docs --clean-build apache-airflow-providers all-providers --include-removed-providers --include-commits\n-```\n-\n-Usually when we release packages we also build documentation for the ""documentation-only"" packages. This\n-means that unless we release just few selected packages or if we need to deliberately skip some packages\n-we should release documentation for all Provider distributions and the above command is the one to use.\n-\n-If we want to just release some providers you can release them using package names:\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-breeze build-docs apache-airflow-providers cncf.kubernetes sftp --clean-build --include-commits\n-```\n-\n-Alternatively, if you have set the environment variable: `DISTRIBUTIONS_LIST` above, just run the command:\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-breeze build-docs --clean-build --include-commits\n-```\n-\n-Or using `--distributions-list` argument:\n-\n-```shell script\n-breeze build-docs --distributions-list PACKAGE1,PACKAGE2 --include-commits\n-```\n-\n-- Now you can preview the documentation.\n-\n-```shell script\n-./docs/start_doc_server.sh\n-```\n-\n-If you encounter error like:\n-\n-```shell script\n-airflow git:(main) ./docs/start_doc_server.sh\n-./docs/start_doc_server.sh: line 22: cd: /Users/eladkal/PycharmProjects/airflow/docs/_build: No such file or directory\n-```\n-\n-That probably means that the doc folder is empty thus it can not build the doc server.\n-This indicates that previous step of building the docs did not work.\n-\n-- Copy the documentation to the ``airflow-site`` repository\n-\n-All providers (including overriding documentation for doc-only changes) - note that publishing is\n-way faster on multi-cpu machines when you are publishing multiple providers:\n-\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-\n-breeze release-management publish-docs apache-airflow-providers all-providers --include-removed-providers \\n-    --override-versioned --run-in-parallel\n-\n-breeze release-management add-back-references all-providers\n-```\n-\n-If you have providers as list of provider ids because you just released them you can build them with\n-\n-```shell script\n-cd ""${AIRFLOW_REPO_ROOT}""\n-\n-breeze release-management publish-docs amazon apache.beam google ....\n-breeze release-management add-back-references all-providers\n-```\n-\n-Alternatively, if you have set the environment variable: `DISTRIBUTIONS_LIST` above, just run the command:\n-\n-```shell script\n-breeze release-management publish-docs\n-breeze release-management add-back-references all-providers\n-```\n-\n-Or using `--distributions-list` argument:\n-\n-```shell script\n-breeze release-management publish-docs --distributions-list PACKAGE1,PACKAGE2\n-breeze release-management add-back-references all-providers\n-```\n-\n-\n-Review the state of removed, suspended, new packages in\n-[the docs index](https://github.com/apache/airflow-site/blob/master/landing-pages/site/content/en/docs/_index.md):\n-\n-- If you publish a new package, you must add it to the list of packages in the index.\n-- If there are changes to suspension or removal status of a package  you must move it appropriate section.\n-\n-- Create the commit and push changes.\n-\n-```shell script\n-cd ""${AIRFLOW_SITE_DIRECTORY}""\n-branch=""add-documentation-$(date ""+%Y-%m-%d%n"")""\n-git checkout -b ""${branch}""\n-git add .\n-git commit -m ""Add documentation for packages - $(date ""+%Y-%m-%d%n"")""\n-git push --set-upstream origin ""${branch}""\n-```",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e603,apache/airflow,2512264648,2084977656,potiuk,595491,2025-05-12T15:46:22+00:00,2025-05-12T15:46:23+00:00,"We do not have yet ""staging"" workflow enabled. Live is the only one that works - we will update it later, but live should be used only for now. I will add it.",true,dev/README_RELEASE_PROVIDERS.md,204,"@@ -1283,6 +1136,38 @@ If you want to disable this behaviour, set the env **CLEAN_LOCAL_TAGS** to false\n breeze release-management tag-providers\n ```\n \n+## Publish documentation\n+\n+Documentation is an essential part of the product and should be made available to users.\n+In our cases, documentation for the released versions is published in S3 bucket, and the site is\n+kept in a separate repository - [`apache/airflow-site`](https://github.com/apache/airflow-site),\n+but the documentation source code and build tools are available in the `apache/airflow` repository, so\n+you need to run several workflows to publish the documentation. More details about it can be found in\n+[Docs README](../docs/README.md) showing the architecture and workflows including manual workflows for\n+emergency cases.\n+\n+There are two steps to publish the documentation:\n+\n+1. Publish the documentation to S3 bucket.\n+\n+The release manager publishes the documentation using GitHub Actions workflow\n+[Publish Docs to S3](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml).\n+\n+You can specify the tag to use to build the docs and list of providers to publish (separated by spaces) or\n+``all-providers`` in case you want to publish all providers (optionally you can exclude\n+some of those providers)\n+\n+After that step, the provider documentation should be available under the usual urls (same as in PyPI packages)\n+but stable links and drop-down boxes should not be updated.",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e604,apache/airflow,2512264648,2084979151,potiuk,595491,2025-05-12T15:47:16+00:00,2025-05-12T15:47:16+00:00,"Sure. We can add ""Ask in #internal-airflow-ci-cd channel on slack""",true,docs/README.md,164,"@@ -37,3 +54,196 @@ Documentation for general overview and summaries not connected with any specific\n \n * `docker-stack-docs` - documentation for Docker Stack'\n * `providers-summary-docs` - documentation for provider summary page\n+\n+# Architecture of documentation for Airflow\n+\n+Building documentation for Airflow is optimized for speed and for convenience workflows of the release\n+managers and committers who publish and fix the documentation - that's why it's a little complex, as we have\n+multiple repositories and multiple sources of the documentation involved.\n+\n+There are few repositories under `apache` organization which are used to build the documentation for Airflow:\n+\n+* `apache-airflow` - the repository with the code and the documentation sources for Airflow distributions,\n+   provider distributions, providers summary and docker summary: [apache-airflow](https://github.com/apache/airflow)\n+   from here we publish the documentation to S3 bucket where the documentation is hosted.\n+* `airflow-site` - the repository with the website theme and content where we keep sources of the website\n+   structure, navigation, theme for the website [airflow-site](https://github.com/apache/airflow). From here\n+   we publish the website to the ASF servers so they are publish as the [official website](https://airflow.apache.org)\n+* `airflow-site-archive` - here we keep the archived historical versions of the generated documentation\n+   of all the documentation packages that we keep on S3. This repository is automatically synchronized from\n+   the S3 buckets and is only used in case we need to perform a bulk update of historical documentation. Here only\n+   generated `html`, `css`, `js` and `images` files are kept, no sources of the documentation are kept here.\n+\n+We have two S3 buckets where we can publish the documentation generated from `apache-airflow` repository:\n+\n+* `s3://live-docs-airflow-apache-org/docs/` - live, [official documentation](https://airflow.apache.org/docs/)\n+* `s3://staging-docs-airflow-apache-org/docs/` - staging documentation [official documentation](https://staging-airflow.apache.org/docs/) TODO: make it work\n+\n+# Diagrams of the documentation architecture\n+\n+This is the diagram of live documentation architecture:\n+\n+![Live documentation architecture](images/documentation_architecture.png)\n+\n+Staging documentation architecture is similar, but uses staging bucket and staging Apache Website.\n+\n+# Typical workflows\n+\n+There are a few typical workflows that we support:\n+\n+## Publishing the documentation by the release manager\n+\n+The release manager publishes the documentation using GitHub Actions workflow\n+[Publish Docs to S3](https://github.com/apache/airflow/actions/workflows/publish-docs-to-s3.yml).\n+The same workflow can be used to publish Airflow, Helm chart and providers documentation.\n+\n+The person who triggers the build (release manager) should specify the tag name of the docs to be published\n+and the list of documentation packages to be published. Usually it is:\n+\n+* Airflow: `apache-airflow docker-stack` (later we will add `airflow-ctl` and `task-sdk`)\n+* Helm chart: `helm-chart`\n+* Providers: `provider_id1 provider_id2` or `all providers` if all providers should be published.\n+\n+Optionally - specifically if we run `all-providers` and release manager wants to exclude some providers,\n+they can specify documentation packages to exclude. Leaving ""no-docs-excluded"" will publish all packages\n+specified to be published without exclusions.\n+\n+You can also specify whether ""live"" or ""staging"" documentation should be published. The default is ""live"".\n+\n+Example screenshot of the workflow triggered from the GitHub UI:\n+\n+![Publishing airflow or providers](images/publish_airflow.png)\n+\n+Note that this just publishes the documentation but does not update the ""site"" with version numbers or\n+stable links to providers and airflow - if you release a new documentation version it will be available\n+with direct URL (say https://apache.airflow.org/docs/apache-airflow/3.0.1/) but the main site will still\n+point to previous version of the documentation as `stable` and the version drop-downs will not be updated.\n+\n+In order to do it, you need to run the [Build docs](https://github.com/apache/airflow-site/actions/workflows/build.yml)\n+workflow in `airflow-site` repository. This will build the website and publish it to the `publish`\n+branch of `airflow-site` repository, including refreshing of the version numbers in the drop-downs and\n+stable links.\n+\n+![Publishing site](images/publish_site.png)\n+\n+\n+Some time after the workflow succeeds and documentation is published, in live bucket, the `airflow-site-archive`\n+repository is automatically synchronized with the live S3 bucket. TODO: IMPLEMENT THIS, FOR NOW IT HAS\n+TO BE MANUALLY SYNCHRONIZED VIA [Sync s3 to GitHub](https://github.com/apache/airflow-site-archive/actions/workflows/s3-to-github.yml)\n+workflow in `airflow-site-archive` repository. The `airflow-site-archive` essentially keeps the history of\n+snapshots of the `live` documentation.\n+\n+## Publishing changes to the website (including theme)\n+\n+The workflows in `apache-airflow` only update the documentation for the packages (Airflow, Helm chart,\n+Providers, Docker Stack) that we publish from airflow sources. If we want to publish changes to the website\n+itself or to the theme (css, javascript) we need to do it in `airflow-site` repository.\n+\n+Publishing of airflow-site happens automatically when a PR from `airflow-site` is merged to `main` or when\n+the [Build docs](https://github.com/apache/airflow-site/actions/workflows/build.yml) workflow is triggered\n+manually in the main branch of `airflow-site` repository. The workflow builds the website and publishes it to\n+`publish` branch of `airflow-site` repository, which in turn gets picked up by the ASF servers and is\n+published as the official website. This includes any changes to `.htaccess` of the website.\n+\n+Such a main build also publishes latest ""sphinx-airflow-theme"" package to GitHub so that the next build\n+of documentation can automatically pick it up from there. This means that if you want to make changes to\n+`javascript` or `css` that are part of the theme, you need to do it in `ariflow-site` repository and\n+merge it to `main` branch in order to be able to run the documentation build in `apache-airflow` repository\n+and pick up the latest version of the theme.\n+\n+The version of sphinx theme is fixed in both repositories:\n+\n+* https://github.com/apache/airflow-site/blob/main/sphinx_airflow_theme/sphinx_airflow_theme/__init__.py#L21\n+* https://github.com/apache/airflow/blob/main/devel-common/pyproject.toml#L77 in ""docs"" section\n+\n+In case of bigger changes to the theme, we\n+can first iterate on the website and merge a new theme version, and only after that we can switch to the new\n+version of the theme.\n+\n+\n+# Fixing historical documentation\n+\n+Sometimes we need to update historical documentation (modify generated `html`) - for example when we find\n+bad links or when we change some of the structure in the documentation. This can be done via the\n+`airflow-site-archive` repository. The workflow is as follows:\n+\n+1. Get the latest version of the documentation from S3 to `airflow-site-archive` repository using\n+   `Sync s3 to GitHub` workflow. This will download the latest version of the documentation from S3 to\n+   `airflow-site-archive` repository (this should be normally not needed, if automated synchronization works).\n+2. Make the changes to the documentation in `airflow-site-archive` repository. This can be done using any\n+   text editors, scripts etc. Those files are generated as `html` files and are not meant to be regenerated,\n+   they should be modified as `html` files in-place\n+3. Commit the changes to `airflow-site-archive` repository and push them to `some` branch of the repository.\n+4. Run `Sync GitHub to S3` workflow in `airflow-site-archive` repository. This will upload the modified\n+   documentation to S3 bucket.\n+5. You can choose, whether to sync the changes to `live` or `staging` bucket. The default is `live`.\n+6. By default the workflow will synchronize all documentation modified in single - last commit pushed to\n+   the branch you specified. You can also specify ""full_sync"" to synchronize all files in the repository.\n+7. In case you specify ""full_sync"", you can also synchronize `all` docs or only selected documentation\n+   packages (for example `apache-airflow` or `docker-stack` or `amazon` or `helm-chart`) - you can specify\n+   more than one package separated by  spaces.\n+8. After you synchronize the changes to S3, the Sync `S3 to GitHub` workflow will be triggered\n+   automatically and the changes will be synchronized to `airflow-site-archive` `main` branch - so there\n+   is no need to merge your changes to `main` branch of `airflow-site-archive` repository. You can safely\n+   delete the branch you created in step 3.\n+\n+![Sync GitHub to S3](images/sync_github_to_s3.png)\n+\n+\n+## Manually publishing documentation directly to S3",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e605,apache/airflow,2512064060,2083499815,eladkal,45845474,2025-05-11T11:44:02+00:00,2025-05-11T11:44:31+00:00,Do we need to have specific implementation?\r\nThe hook doesn't implement specific implementation for pandas so why would it need one for polars?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e606,apache/airflow,2512064060,2083499888,eladkal,45845474,2025-05-11T11:44:27+00:00,2025-05-11T11:44:31+00:00,I think if you parameterized the `test_get_df_pandas` to include polars that would work,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e607,apache/airflow,2512064060,2083501851,guan404ming,105915352,2025-05-11T11:53:12+00:00,2025-05-11T11:56:59+00:00,"Since the `ElasticsearchSQLHook`has its own `ElasticsearchSQLCursor` which is not compatable with polars thus if we comment the implementation above and try like \r\n\r\n```python\r\n@pytest.mark.parametrize(\r\n        ""df_type"",\r\n        [""pandas"", ""polars""],\r\n    )\r\n    def test_get_df(self, df_type):\r\n        statement = ""SELECT * FROM hollywood.actors""\r\n        df = self.db_hook.get_df(statement, df_type=df_type)\r\n\r\n        assert list(df.columns) == [""index"", ""name"", ""firstname"", ""age""]\r\n        assert df.values.tolist() == ROWS\r\n\r\n        self.conn.close.assert_called_once_with()\r\n        self.spy_agency.assert_spy_called(self.cur.close)\r\n        self.spy_agency.assert_spy_called(self.cur.execute)\r\n```\r\n\r\n\r\nwould get this error\r\n```\r\n___________________________________________________________ TestElasticsearchSQLHook.test_get_df[polars] ___________________________________________________________\r\nproviders/elasticsearch/tests/unit/elasticsearch/hooks/test_elasticsearch.py:175: in test_get_df\r\n    df = self.db_hook.get_df(statement, df_type=df_type)\r\nproviders/common/sql/src/airflow/providers/common/sql/hooks/sql.py:458: in get_df\r\n    return self._get_polars_df(sql, parameters, **kwargs)\r\nproviders/common/sql/src/airflow/providers/common/sql/hooks/sql.py:513: in _get_polars_df\r\n    return pl.read_database(sql, connection=conn, execute_options=execute_options, **kwargs)\r\n.venv/lib/python3.12/site-packages/polars/io/database/functions.py:251: in read_database\r\n    ).to_polars(\r\n.venv/lib/python3.12/site-packages/polars/io/database/_executor.py:563: in to_polars\r\n    raise NotImplementedError(msg)\r\n ```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e608,apache/airflow,2512064060,2083502123,guan404ming,105915352,2025-05-11T11:54:21+00:00,2025-05-11T11:55:28+00:00,I implement it to prevent user to use the `df_type=polars` when using `get_df` since there is incompatible issue which would lead to error below.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e609,apache/airflow,2512064060,2083507819,eladkal,45845474,2025-05-11T12:12:29+00:00,2025-05-11T12:12:44+00:00,OK can you just add a todo comment in the `_get_polars_df` provide some context about why it's incompatible and what needs to be done to make it work (just leaving instructions to whoever decide to tackle this),false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e60a,apache/airflow,2512064060,2083510135,guan404ming,105915352,2025-05-11T12:23:35+00:00,2025-05-11T12:24:29+00:00,"Sure, I've updated with todo comment and this pr link hoping that would help.\r\nIf there are more information or discussion about this integration on whether polars or Elasticsearch document in the future, I would get back here and try implement it.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e60b,apache/airflow,2512064060,2083499815,eladkal,45845474,2025-05-11T11:44:02+00:00,2025-05-11T11:44:31+00:00,Do we need to have specific implementation?\r\nThe hook doesn't implement specific implementation for pandas so why would it need one for polars?,true,providers/elasticsearch/src/airflow/providers/elasticsearch/hooks/elasticsearch.py,NaN,"@@ -219,6 +219,14 @@ def get_uri(self) -> str:\n \n         return uri\n \n+    def _get_polars_df(\n+        self,\n+        sql,\n+        parameters: list | tuple | Mapping[str, Any] | None = None,\n+        **kwargs,\n+    ):\n+        raise NotImplementedError(""Polars is not supported for Elasticsearch"")",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e60c,apache/airflow,2512064060,2083499888,eladkal,45845474,2025-05-11T11:44:27+00:00,2025-05-11T11:44:31+00:00,I think if you parameterized the `test_get_df_pandas` to include polars that would work,true,providers/elasticsearch/tests/unit/elasticsearch/hooks/test_elasticsearch.py,4,"@@ -177,6 +177,10 @@ def test_get_df_pandas(self):\n         self.spy_agency.assert_spy_called(self.cur.close)\n         self.spy_agency.assert_spy_called(self.cur.execute)\n \n+    def test_get_df_polars(self):",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e60d,apache/airflow,2512064060,2083501851,guan404ming,105915352,2025-05-11T11:53:12+00:00,2025-05-11T11:56:59+00:00,"Since the `ElasticsearchSQLHook`has its own `ElasticsearchSQLCursor` which is not compatable with polars thus if we comment the implementation above and try like \r\n\r\n```python\r\n@pytest.mark.parametrize(\r\n        ""df_type"",\r\n        [""pandas"", ""polars""],\r\n    )\r\n    def test_get_df(self, df_type):\r\n        statement = ""SELECT * FROM hollywood.actors""\r\n        df = self.db_hook.get_df(statement, df_type=df_type)\r\n\r\n        assert list(df.columns) == [""index"", ""name"", ""firstname"", ""age""]\r\n        assert df.values.tolist() == ROWS\r\n\r\n        self.conn.close.assert_called_once_with()\r\n        self.spy_agency.assert_spy_called(self.cur.close)\r\n        self.spy_agency.assert_spy_called(self.cur.execute)\r\n```\r\n\r\n\r\nwould get this error\r\n```\r\n___________________________________________________________ TestElasticsearchSQLHook.test_get_df[polars] ___________________________________________________________\r\nproviders/elasticsearch/tests/unit/elasticsearch/hooks/test_elasticsearch.py:175: in test_get_df\r\n    df = self.db_hook.get_df(statement, df_type=df_type)\r\nproviders/common/sql/src/airflow/providers/common/sql/hooks/sql.py:458: in get_df\r\n    return self._get_polars_df(sql, parameters, **kwargs)\r\nproviders/common/sql/src/airflow/providers/common/sql/hooks/sql.py:513: in _get_polars_df\r\n    return pl.read_database(sql, connection=conn, execute_options=execute_options, **kwargs)\r\n.venv/lib/python3.12/site-packages/polars/io/database/functions.py:251: in read_database\r\n    ).to_polars(\r\n.venv/lib/python3.12/site-packages/polars/io/database/_executor.py:563: in to_polars\r\n    raise NotImplementedError(msg)\r\n ```",true,providers/elasticsearch/tests/unit/elasticsearch/hooks/test_elasticsearch.py,4,"@@ -177,6 +177,10 @@ def test_get_df_pandas(self):\n         self.spy_agency.assert_spy_called(self.cur.close)\n         self.spy_agency.assert_spy_called(self.cur.execute)\n \n+    def test_get_df_polars(self):",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e60e,apache/airflow,2512064060,2083502123,guan404ming,105915352,2025-05-11T11:54:21+00:00,2025-05-11T11:55:28+00:00,I implement it to prevent user to use the `df_type=polars` when using `get_df` since there is incompatible issue which would lead to error below.,true,providers/elasticsearch/src/airflow/providers/elasticsearch/hooks/elasticsearch.py,NaN,"@@ -219,6 +219,14 @@ def get_uri(self) -> str:\n \n         return uri\n \n+    def _get_polars_df(\n+        self,\n+        sql,\n+        parameters: list | tuple | Mapping[str, Any] | None = None,\n+        **kwargs,\n+    ):\n+        raise NotImplementedError(""Polars is not supported for Elasticsearch"")",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e60f,apache/airflow,2512064060,2083507819,eladkal,45845474,2025-05-11T12:12:29+00:00,2025-05-11T12:12:44+00:00,OK can you just add a todo comment in the `_get_polars_df` provide some context about why it's incompatible and what needs to be done to make it work (just leaving instructions to whoever decide to tackle this),true,providers/elasticsearch/tests/unit/elasticsearch/hooks/test_elasticsearch.py,4,"@@ -177,6 +177,10 @@ def test_get_df_pandas(self):\n         self.spy_agency.assert_spy_called(self.cur.close)\n         self.spy_agency.assert_spy_called(self.cur.execute)\n \n+    def test_get_df_polars(self):",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e610,apache/airflow,2512064060,2083510135,guan404ming,105915352,2025-05-11T12:23:35+00:00,2025-05-11T12:24:29+00:00,"Sure, I've updated with todo comment and this pr link hoping that would help.\r\nIf there are more information or discussion about this integration on whether polars or Elasticsearch document in the future, I would get back here and try implement it.",true,providers/elasticsearch/tests/unit/elasticsearch/hooks/test_elasticsearch.py,4,"@@ -177,6 +177,10 @@ def test_get_df_pandas(self):\n         self.spy_agency.assert_spy_called(self.cur.close)\n         self.spy_agency.assert_spy_called(self.cur.execute)\n \n+    def test_get_df_polars(self):",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e611,apache/airflow,2511928488,2083855483,amoghrajesh,35884252,2025-05-12T05:41:41+00:00,2025-05-12T05:42:53+00:00,Should this not be `ser_input`?,false,NaN,NaN,NaN,1,1,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e612,apache/airflow,2511928488,2083855989,amoghrajesh,35884252,2025-05-12T05:42:18+00:00,2025-05-12T05:42:53+00:00,Lets just rename the `content_json_for_volume` -- its not that anymore.\r\nhttps://github.com/apache/airflow/pull/50448/files#diff-a82432d2e0fd9cf3be813faa6d4361eb102df82c56583caab8ebe30f10a7eb1dR290,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e613,apache/airflow,2511928488,2083855483,amoghrajesh,35884252,2025-05-12T05:41:41+00:00,2025-05-12T05:42:53+00:00,Should this not be `ser_input`?,true,providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/executors/kubernetes_executor_utils.py,NaN,"@@ -398,8 +398,8 @@ def run_next(self, next_job: KubernetesJobType) -> None:\n                     ""python"",\n                     ""-m"",\n                     ""airflow.sdk.execution_time.execute_workload"",\n-                    ""--json-path"",\n-                    ""/tmp/execute/input.json"",\n+                    ""--json-string"",\n+                    ""temp"",",1,1,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e614,apache/airflow,2511928488,2083855989,amoghrajesh,35884252,2025-05-12T05:42:18+00:00,2025-05-12T05:42:53+00:00,Lets just rename the `content_json_for_volume` -- its not that anymore.\r\nhttps://github.com/apache/airflow/pull/50448/files#diff-a82432d2e0fd9cf3be813faa6d4361eb102df82c56583caab8ebe30f10a7eb1dR290,true,providers/cncf/kubernetes/src/airflow/providers/cncf/kubernetes/pod_generator.py,NaN,"@@ -358,35 +358,9 @@ def construct_pod(\n         if content_json_for_volume:\n             import shlex\n \n-            input_file_path = ""/tmp/execute/input.json""\n-            execute_volume = V1Volume(\n-                name=""execute-volume"",\n-                empty_dir=V1EmptyDirVolumeSource(),\n-            )\n-\n-            execute_volume_mount = V1VolumeMount(\n-                name=""execute-volume"",\n-                mount_path=""/tmp/execute"",\n-                read_only=False,\n-            )\n-\n             escaped_json = shlex.quote(content_json_for_volume)",0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e615,apache/airflow,2511811713,2083465079,ashb,34150,2025-05-11T09:26:18+00:00,2025-05-11T09:26:26+00:00,Waaaait. Webserver should never parse DAGs. What's going on here?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e616,apache/airflow,2511811713,2083465727,rawwar,20266953,2025-05-11T09:28:50+00:00,2025-05-11T09:28:51+00:00,"read_dags_from_db is set to True. So, its not actually parsing DAGs, right?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_airflow
6824cd8e0ce5dcec7842e743,apache/superset,2515147575,2085694841,korbit-ai[bot],131444098,2025-05-12T23:46:05+00:00,2025-05-13T00:20:45+00:00,"### Excessive Base Wait Time in Backoff Strategy <sub>![category Performance](https://img.shields.io/badge/Performance-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe base wait time of 60 seconds in the exponential backoff is unnecessarily long for HTTP 429 rate limit retries.\n\n###### Why this matters\nWith a 60-second base wait time and exponential backoff (2^n), subsequent retries will wait for 60s, 120s, 240s, 480s - leading to extremely long total wait times that could significantly impact performance.\n\n###### Suggested change ∙ *Feature Preview*\nReduce the base wait time to a more reasonable value like 1-5 seconds. This still provides adequate spacing between retries while maintaining responsiveness:\n```python\nwait_seconds: float = 2.0,\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:c8467b9e-04d5-40da-9e56-2a5dcc9530f8 -->\n\n\n[](c8467b9e-04d5-40da-9e56-2a5dcc9530f8)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e744,apache/superset,2515147575,2085694846,korbit-ai[bot],131444098,2025-05-12T23:46:05+00:00,2025-05-12T23:46:05+00:00,"### Fragmented string formatting in retry message <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nUsing concatenated print statements makes the retry message harder to read and maintain. The f-strings are split unnecessarily across multiple lines.\n\n###### Why this matters\nSplit string formatting makes the message structure harder to follow and introduces potential formatting inconsistencies. Single f-string would be clearer.\n\n###### Suggested change ∙ *Feature Preview*\n```python\nprint(f""HTTP 429 received from {url}. Retrying in {sleep_time:.1f}s (attempt {attempt}/{max_attempts})..."")\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:aa4ab7b4-1121-408b-802c-bca06e289402 -->\n\n\n[](aa4ab7b4-1121-408b-802c-bca06e289402)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e745,apache/superset,2515147575,2085694848,korbit-ai[bot],131444098,2025-05-12T23:46:05+00:00,2025-05-13T00:36:49+00:00,"### Incorrect Data Source for Airports <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe airports data is being loaded from the wrong file. It's using 'flight_data.csv.gz' instead of 'airports.csv.gz'.\n\n###### Why this matters\nThis will cause incorrect data to be loaded for airports, leading to wrong lat/long information when joining with the flights data. The subsequent join operations will produce invalid results.\n\n###### Suggested change ∙ *Feature Preview*\nUse the correct file name for loading airports data:\n```python\nairports = read_example_csv(\n    ""airports.csv.gz"", encoding=""latin-1"", compression=""gzip""\n)\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:c4aa6c08-587c-4b87-8186-49aa8f45644e -->\n\n\n[](c4aa6c08-587c-4b87-8186-49aa8f45644e)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e746,apache/superset,2515147575,2085694841,korbit-ai[bot],131444098,2025-05-12T23:46:05+00:00,2025-05-13T00:20:45+00:00,"### Excessive Base Wait Time in Backoff Strategy <sub>![category Performance](https://img.shields.io/badge/Performance-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe base wait time of 60 seconds in the exponential backoff is unnecessarily long for HTTP 429 rate limit retries.\n\n###### Why this matters\nWith a 60-second base wait time and exponential backoff (2^n), subsequent retries will wait for 60s, 120s, 240s, 480s - leading to extremely long total wait times that could significantly impact performance.\n\n###### Suggested change ∙ *Feature Preview*\nReduce the base wait time to a more reasonable value like 1-5 seconds. This still provides adequate spacing between retries while maintaining responsiveness:\n```python\nwait_seconds: float = 2.0,\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/500ea470-02b5-4a35-bf32-1c9520c44f05)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:c8467b9e-04d5-40da-9e56-2a5dcc9530f8 -->\n\n\n[](c8467b9e-04d5-40da-9e56-2a5dcc9530f8)",true,superset/examples/helpers.py,21,"@@ -119,3 +123,41 @@ def get_example_url(filepath: str) -> str:\n     paths like ``datasets/examples/slack/messages.csv``.\n     """"""\n     return f""{BASE_URL}{filepath}""\n+\n+\n+def read_example_csv(\n+    filepath: str,\n+    max_attempts: int = 5,\n+    wait_seconds: float = 60,",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e747,apache/superset,2515147575,2085694846,korbit-ai[bot],131444098,2025-05-12T23:46:05+00:00,2025-05-12T23:46:05+00:00,"### Fragmented string formatting in retry message <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nUsing concatenated print statements makes the retry message harder to read and maintain. The f-strings are split unnecessarily across multiple lines.\n\n###### Why this matters\nSplit string formatting makes the message structure harder to follow and introduces potential formatting inconsistencies. Single f-string would be clearer.\n\n###### Suggested change ∙ *Feature Preview*\n```python\nprint(f""HTTP 429 received from {url}. Retrying in {sleep_time:.1f}s (attempt {attempt}/{max_attempts})..."")\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/4c206eb8-4cae-4772-893c-0bcbf25d43b4)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:aa4ab7b4-1121-408b-802c-bca06e289402 -->\n\n\n[](aa4ab7b4-1121-408b-802c-bca06e289402)",true,superset/examples/helpers.py,NaN,"@@ -119,3 +123,41 @@\n     paths like ``datasets/examples/slack/messages.csv``.\n     """"""\n     return f""{BASE_URL}{filepath}""\n+\n+\n+def read_example_csv(\n+    filepath: str,\n+    max_attempts: int = 5,\n+    wait_seconds: float = 60,\n+    **kwargs: Any,\n+) -> pd.DataFrame:\n+    """"""\n+    Load a CSV from the Superset example data mirror with retry/backoff on HTTP 429.\n+\n+    Args:\n+        filepath: Relative path within the examples repo (e.g. ""datasets/foo.csv"").\n+        max_attempts: Max number of retry attempts.\n+        wait_seconds: Base wait time in seconds, will be multiplied\n+          exponentially per attempt.\n+        **kwargs: Passed directly to pandas.read_csv().\n+\n+    Returns:\n+        pd.DataFrame\n+    """"""\n+\n+    url = get_example_url(filepath)\n+\n+    for attempt in range(1, max_attempts + 1):\n+        try:\n+            return pd.read_csv(url, **kwargs)\n+        except HTTPError as e:\n+            if e.code == 429 and attempt < max_attempts:\n+                sleep_time = wait_seconds * (2 ** (attempt - 1))\n+                print(\n+                    f""HTTP 429 received from {url}. "",\n+                    f""Retrying in {sleep_time:.1f}s "",\n+                    ""(attempt {attempt}/{max_attempts})..."",\n+                )",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e748,apache/superset,2515147575,2085694848,korbit-ai[bot],131444098,2025-05-12T23:46:05+00:00,2025-05-13T00:36:49+00:00,"### Incorrect Data Source for Airports <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe airports data is being loaded from the wrong file. It's using 'flight_data.csv.gz' instead of 'airports.csv.gz'.\n\n###### Why this matters\nThis will cause incorrect data to be loaded for airports, leading to wrong lat/long information when joining with the flights data. The subsequent join operations will produce invalid results.\n\n###### Suggested change ∙ *Feature Preview*\nUse the correct file name for loading airports data:\n```python\nairports = read_example_csv(\n    ""airports.csv.gz"", encoding=""latin-1"", compression=""gzip""\n)\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/ab7f23f5-1c2d-4d29-beed-78df10891c0b)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:c4aa6c08-587c-4b87-8186-49aa8f45644e -->\n\n\n[](c4aa6c08-587c-4b87-8186-49aa8f45644e)",true,superset/examples/flights.py,NaN,"@@ -37,12 +37,14 @@ def load_flights(only_metadata: bool = False, force: bool = False) -> None:\n         table_exists = database.has_table(Table(tbl_name, schema))\n \n         if not only_metadata and (not table_exists or force):\n-            flight_data_url = get_example_url(""flight_data.csv.gz"")\n-            pdf = pd.read_csv(flight_data_url, encoding=""latin-1"", compression=""gzip"")\n+            pdf = read_example_csv(\n+                ""flight_data.csv.gz"", encoding=""latin-1"", compression=""gzip""\n+            )\n \n             # Loading airports info to join and get lat/long\n-            airports_url = get_example_url(""airports.csv.gz"")\n-            airports = pd.read_csv(airports_url, encoding=""latin-1"", compression=""gzip"")\n+            airports = read_example_csv(\n+                ""flight_data.csv.gz"", encoding=""latin-1"", compression=""gzip""\n+            )",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e749,apache/superset,2514517609,2085305712,korbit-ai[bot],131444098,2025-05-12T19:21:13+00:00,2025-05-12T21:03:58+00:00,"### Overly Complex Type Extraction <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nComplex type extraction using Parameters and NonNullable makes the code harder to read and understand at a glance.\n\n###### Why this matters\nThe nested type utility operations obscure the actual type being used, making it difficult for developers to quickly understand what type of parameter info should be.\n\n###### Suggested change ∙ *Feature Preview*\n```typescript\n// Create a dedicated type at the top of the file or in types.ts\ntype TreeDropInfo = Parameters<NonNullable<TreeProps<FlatLayerDataNode>['onDrop']>>[0];\n\n// Then use it in the function\nonDrop = (info: TreeDropInfo) => {\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:2c3d1e01-582a-429a-b543-781c8ccbb9ff -->\n\n\n[](2c3d1e01-582a-429a-b543-781c8ccbb9ff)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e74a,apache/superset,2514517609,2085305712,korbit-ai[bot],131444098,2025-05-12T19:21:13+00:00,2025-05-12T21:03:58+00:00,"### Overly Complex Type Extraction <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nComplex type extraction using Parameters and NonNullable makes the code harder to read and understand at a glance.\n\n###### Why this matters\nThe nested type utility operations obscure the actual type being used, making it difficult for developers to quickly understand what type of parameter info should be.\n\n###### Suggested change ∙ *Feature Preview*\n```typescript\n// Create a dedicated type at the top of the file or in types.ts\ntype TreeDropInfo = Parameters<NonNullable<TreeProps<FlatLayerDataNode>['onDrop']>>[0];\n\n// Then use it in the function\nonDrop = (info: TreeDropInfo) => {\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/cc90ede1-f3c5-4dcd-a770-eee533cfdbd5)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:2c3d1e01-582a-429a-b543-781c8ccbb9ff -->\n\n\n[](2c3d1e01-582a-429a-b543-781c8ccbb9ff)",true,superset-frontend/src/explore/components/controls/LayerConfigsControl/FlatLayerTree.tsx,19,"@@ -120,7 +118,9 @@ export const FlatLayerTree = forwardRef<HTMLDivElement, FlatLayerTreeProps>(\n \n     const treeData = layerConfigsToTreeData(layerConfigs);\n \n-    const onDrop: TreeProps['onDrop'] = info => {\n+    const onDrop = (\n+      info: Parameters<NonNullable<TreeProps<FlatLayerDataNode>['onDrop']>>[0],",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e74b,apache/superset,2513234086,2084224233,korbit-ai[bot],131444098,2025-05-12T09:17:39+00:00,2025-05-12T10:00:33+00:00,"### Incorrect Time Series Flag Logic <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe is_timeseries flag is incorrectly tied to the presence of aggregation, when it should be based on whether an X-axis is set (as in the first query object).\n\n###### Why this matters\nThis could cause incorrect data processing for time series data when no aggregation is selected, potentially leading to visualization errors or data misrepresentation.\n\n###### Suggested change ∙ *Feature Preview*\nThe is_timeseries flag should maintain the same logic as the first query object:\n```typescript\n      ...(isXAxisSet(formData) ? {} : { is_timeseries: true }),\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:75dc8d09-d748-4e67-bf96-cce6394a188d -->\n\n\n[](75dc8d09-d748-4e67-bf96-cce6394a188d)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e74c,apache/superset,2513234086,2084224239,korbit-ai[bot],131444098,2025-05-12T09:17:39+00:00,2025-05-12T09:17:39+00:00,"### Missing Essential Post-Processing <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nWhen aggregation is disabled, all post-processing operations are skipped, including pivoting which might still be necessary for proper data transformation.\n\n###### Why this matters\nThis could result in incorrectly structured data when pivoting is required but aggregation is not selected, leading to visualization failures.\n\n###### Suggested change ∙ *Feature Preview*\nSeparate the pivot operation from the aggregation condition:\n```typescript\n      post_processing: [\n        pivotOperator(formData, baseQueryObject),\n        ...(formData.aggregation ? [aggregationOperator(formData, baseQueryObject)] : []),\n      ],\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:94b9b312-ed06-4d0d-9c84-4ddc8b957ff8 -->\n\n\n[](94b9b312-ed06-4d0d-9c84-4ddc8b957ff8)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e74d,apache/superset,2513234086,2084430777,LevisNgigi,111055098,2025-05-12T11:02:15+00:00,2025-05-12T11:02:15+00:00,"Thanks! I tried separating pivotOperator, but without aggregation, the time column is removed, which causes pivot to fail. So I kept it conditional to avoid data errors when the time column isn’t present. Also, not all post-processing is skipped, only the aggregation step is skipped when no aggregation method is selected.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e74e,apache/superset,2513234086,2084431377,korbit-ai[bot],131444098,2025-05-12T11:02:41+00:00,2025-05-12T11:02:41+00:00,That makes sense - I understand now that pivot requires the time column which is only present with aggregation. Thanks for explaining the technical constraint.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e74f,apache/superset,2513234086,2084224233,korbit-ai[bot],131444098,2025-05-12T09:17:39+00:00,2025-05-12T10:00:33+00:00,"### Incorrect Time Series Flag Logic <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe is_timeseries flag is incorrectly tied to the presence of aggregation, when it should be based on whether an X-axis is set (as in the first query object).\n\n###### Why this matters\nThis could cause incorrect data processing for time series data when no aggregation is selected, potentially leading to visualization errors or data misrepresentation.\n\n###### Suggested change ∙ *Feature Preview*\nThe is_timeseries flag should maintain the same logic as the first query object:\n```typescript\n      ...(isXAxisSet(formData) ? {} : { is_timeseries: true }),\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/526a4bc5-4e0c-409a-8916-83a32dae1c19)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:75dc8d09-d748-4e67-bf96-cce6394a188d -->\n\n\n[](75dc8d09-d748-4e67-bf96-cce6394a188d)",true,superset-frontend/plugins/plugin-chart-echarts/src/BigNumber/BigNumberWithTrendline/buildQuery.ts,17,"@@ -52,15 +52,19 @@ export default function buildQuery(formData: QueryFormData) {\n     {\n       ...baseQueryObject,\n       columns: [\n-        ...(isXAxisSet(formData)\n-          ? ensureIsArray(getXAxisColumn(formData))\n+        ...(formData.aggregation\n+          ? isXAxisSet(formData)\n+            ? ensureIsArray(getXAxisColumn(formData))\n+            : []\n           : []),\n       ],\n-      ...(isXAxisSet(formData) ? {} : { is_timeseries: true }),\n-      post_processing: [\n-        pivotOperator(formData, baseQueryObject),\n-        aggregationOperator(formData, baseQueryObject),\n-      ],\n+      is_timeseries: !!formData.aggregation,",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e750,apache/superset,2513234086,2084224239,korbit-ai[bot],131444098,2025-05-12T09:17:39+00:00,2025-05-12T09:17:39+00:00,"### Missing Essential Post-Processing <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nWhen aggregation is disabled, all post-processing operations are skipped, including pivoting which might still be necessary for proper data transformation.\n\n###### Why this matters\nThis could result in incorrectly structured data when pivoting is required but aggregation is not selected, leading to visualization failures.\n\n###### Suggested change ∙ *Feature Preview*\nSeparate the pivot operation from the aggregation condition:\n```typescript\n      post_processing: [\n        pivotOperator(formData, baseQueryObject),\n        ...(formData.aggregation ? [aggregationOperator(formData, baseQueryObject)] : []),\n      ],\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6f1fe1b8-933f-4b7d-aad1-60a025293765)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:94b9b312-ed06-4d0d-9c84-4ddc8b957ff8 -->\n\n\n[](94b9b312-ed06-4d0d-9c84-4ddc8b957ff8)",true,superset-frontend/plugins/plugin-chart-echarts/src/BigNumber/BigNumberWithTrendline/buildQuery.ts,23,"@@ -52,15 +52,19 @@\n     {\n       ...baseQueryObject,\n       columns: [\n-        ...(isXAxisSet(formData)\n-          ? ensureIsArray(getXAxisColumn(formData))\n+        ...(formData.aggregation\n+          ? isXAxisSet(formData)\n+            ? ensureIsArray(getXAxisColumn(formData))\n+            : []\n           : []),\n       ],\n-      ...(isXAxisSet(formData) ? {} : { is_timeseries: true }),\n-      post_processing: [\n-        pivotOperator(formData, baseQueryObject),\n-        aggregationOperator(formData, baseQueryObject),\n-      ],\n+      is_timeseries: !!formData.aggregation,\n+      post_processing: formData.aggregation\n+        ? [\n+            pivotOperator(formData, baseQueryObject),\n+            aggregationOperator(formData, baseQueryObject),\n+          ]\n+        : [],",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e751,apache/superset,2513234086,2084430777,LevisNgigi,111055098,2025-05-12T11:02:15+00:00,2025-05-12T11:02:15+00:00,"Thanks! I tried separating pivotOperator, but without aggregation, the time column is removed, which causes pivot to fail. So I kept it conditional to avoid data errors when the time column isn’t present. Also, not all post-processing is skipped, only the aggregation step is skipped when no aggregation method is selected.",true,superset-frontend/plugins/plugin-chart-echarts/src/BigNumber/BigNumberWithTrendline/buildQuery.ts,23,"@@ -52,15 +52,19 @@\n     {\n       ...baseQueryObject,\n       columns: [\n-        ...(isXAxisSet(formData)\n-          ? ensureIsArray(getXAxisColumn(formData))\n+        ...(formData.aggregation\n+          ? isXAxisSet(formData)\n+            ? ensureIsArray(getXAxisColumn(formData))\n+            : []\n           : []),\n       ],\n-      ...(isXAxisSet(formData) ? {} : { is_timeseries: true }),\n-      post_processing: [\n-        pivotOperator(formData, baseQueryObject),\n-        aggregationOperator(formData, baseQueryObject),\n-      ],\n+      is_timeseries: !!formData.aggregation,\n+      post_processing: formData.aggregation\n+        ? [\n+            pivotOperator(formData, baseQueryObject),\n+            aggregationOperator(formData, baseQueryObject),\n+          ]\n+        : [],",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e752,apache/superset,2513234086,2084431377,korbit-ai[bot],131444098,2025-05-12T11:02:41+00:00,2025-05-12T11:02:41+00:00,That makes sense - I understand now that pivot requires the time column which is only present with aggregation. Thanks for explaining the technical constraint.,true,superset-frontend/plugins/plugin-chart-echarts/src/BigNumber/BigNumberWithTrendline/buildQuery.ts,23,"@@ -52,15 +52,19 @@\n     {\n       ...baseQueryObject,\n       columns: [\n-        ...(isXAxisSet(formData)\n-          ? ensureIsArray(getXAxisColumn(formData))\n+        ...(formData.aggregation\n+          ? isXAxisSet(formData)\n+            ? ensureIsArray(getXAxisColumn(formData))\n+            : []\n           : []),\n       ],\n-      ...(isXAxisSet(formData) ? {} : { is_timeseries: true }),\n-      post_processing: [\n-        pivotOperator(formData, baseQueryObject),\n-        aggregationOperator(formData, baseQueryObject),\n-      ],\n+      is_timeseries: !!formData.aggregation,\n+      post_processing: formData.aggregation\n+        ? [\n+            pivotOperator(formData, baseQueryObject),\n+            aggregationOperator(formData, baseQueryObject),\n+          ]\n+        : [],",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e753,apache/superset,2511788720,2083321958,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:46:26+00:00,"### Unclear build process documentation <sub>![category Documentation](https://img.shields.io/badge/Documentation-7c3aed)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe comment only explains what the --build flag does, but not why two different Docker Compose commands are being used or why --no-cache is needed.\n\n###### Why this matters\nWithout understanding the purpose of using both commands and the --no-cache flag, maintainers might inadvertently remove or modify these important build steps.\n\n###### Suggested change ∙ *Feature Preview*\n# Rebuild all images from scratch (--no-cache) to ensure latest dependencies\n# Then start containers with updated images\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:d8ca08aa-d5a8-418d-b265-2ee6acf89153 -->\n\n\n[](d8ca08aa-d5a8-418d-b265-2ee6acf89153)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e754,apache/superset,2511788720,2083321959,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:18:25+00:00,"### Poor Code Structure with Multiple Empty Lines <sub>![category Design](https://img.shields.io/badge/Design-0d9488)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nMultiple empty lines between commands reduce code readability and violate clean code principles.\n\n###### Why this matters\nPoor code formatting makes the script harder to maintain and understand, violating the KISS principle.\n\n###### Suggested change ∙ *Feature Preview*\nRemove extra empty lines between commands to maintain clean and consistent formatting.\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:2c1f15fb-1aa5-40ef-9ff2-313cf9e2aedc -->\n\n\n[](2c1f15fb-1aa5-40ef-9ff2-313cf9e2aedc)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e755,apache/superset,2511788720,2083321964,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:26:26+00:00,"### Inconsistent Error Message Format <sub>![category Logging](https://img.shields.io/badge/Logging-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nInconsistent error message format. The script uses [ERROR] in one place and 'Error:' in another, lacking a standardized logging format.\n\n###### Why this matters\nInconsistent error message formats make it harder to parse logs and create monitoring rules. This impacts log aggregation and automated error detection.\n\n###### Suggested change ∙ *Feature Preview*\nStandardize error message format throughout the script:\n```bash\necho ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""\n# ...\necho ""[ERROR] 'docker compose' is not available. Please install Docker Compose v2.""\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:5e3e621d-c84b-41b9-b737-9b8d82a97c6d -->\n\n\n[](5e3e621d-c84b-41b9-b737-9b8d82a97c6d)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e756,apache/superset,2511788720,2083321966,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T01:46:25+00:00,"### Missing Success Logging <sub>![category Logging](https://img.shields.io/badge/Logging-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nMissing informational logging about the script's progress and successful operations.\n\n###### Why this matters\nWithout success logs, it's difficult to track the script's execution flow and verify successful operations in production environments.\n\n###### Suggested change ∙ *Feature Preview*\nAdd informational logging for key operations:\n```bash\necho ""[INFO] Checking Docker daemon status...""\nif ! pgrep -x dockerd > /dev/null 2>&1; then\n  echo ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""\n  exit 1\nfi\necho ""[INFO] Docker daemon is running""\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:4063b577-6680-4e05-acbc-8b7e036429c1 -->\n\n\n[](4063b577-6680-4e05-acbc-8b7e036429c1)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e757,apache/superset,2511788720,2083321968,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:54:29+00:00,"### Inconsistent Docker Compose Version Usage <sub>![category Performance](https://img.shields.io/badge/Performance-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe script mixes 'docker-compose' (v1) and 'docker compose' (v2) commands, and runs both build commands redundantly.\n\n###### Why this matters\nThis will cause confusion and unnecessary rebuilds, doubling the build time and potentially causing conflicts between the two versions.\n\n###### Suggested change ∙ *Feature Preview*\nChoose one version of Docker Compose and use it consistently:\n```bash\nif docker compose version > /dev/null 2>&1; then\n  docker compose up --build\nelse\n  echo ""Error: 'docker compose' is not available. Please install Docker Compose v2.""\n  exit 1\nfi\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:9423dadd-2a74-473c-8faa-930d26b93752 -->\n\n\n[](9423dadd-2a74-473c-8faa-930d26b93752)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e758,apache/superset,2511788720,2083321971,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:38:24+00:00,"### Incorrect Version Check <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe script checks for 'docker-compose' (v1) but suggests installing Docker Compose v2 in the error message.\n\n###### Why this matters\nUsers might have v2 installed (docker compose) but the script will fail because it's checking for v1 (docker-compose), leading to false negative errors.\n\n###### Suggested change ∙ *Feature Preview*\nUpdate the version check to look for Docker Compose v2:\n```bash\nif ! docker compose version > /dev/null 2>&1; then\n  echo ""Error: 'docker compose' is not available. Please install Docker Compose v2.""\n  exit 1\nfi\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:02934ba8-9f57-4b90-997a-743aec90363c -->\n\n\n[](02934ba8-9f57-4b90-997a-743aec90363c)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e759,apache/superset,2511788720,2083321972,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-10T23:48:25+00:00,"### Incorrect HOME directory reference in sudo command <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe $HOME variable in this command will resolve to the root user's home directory instead of the dockeruser's home directory when executed with sudo.\n\n###### Why this matters\nThis will cause the Docker rootless setup to fail as it will look for the setup script in the wrong directory.\n\n###### Suggested change ∙ *Feature Preview*\nReplace `$HOME` with `/home/$USERNAME` to ensure the correct home directory is used:\n```bash\nsudo -u ""$USERNAME"" bash -c ""/home/$USERNAME/bin/dockerd-rootless-setuptool.sh install""\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:1c535b31-c4c3-4192-a3e3-1db06f76c7b9 -->\n\n\n[](1c535b31-c4c3-4192-a3e3-1db06f76c7b9)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e75a,apache/superset,2511788720,2083321977,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:58:27+00:00,"### Silent failure in Docker installation <sub>![category Error Handling](https://img.shields.io/badge/Error%20Handling-ea580c)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe '|| true' silently ignores any installation failures, which could lead to an incomplete or failed Docker installation without proper error notification.\n\n###### Why this matters\nThe script will continue executing even if the Docker installation fails, potentially causing issues in subsequent steps that depend on a successful installation.\n\n###### Suggested change ∙ *Feature Preview*\nRemove the '|| true' and add proper error handling:\n```bash\nif ! sudo -u ""$USERNAME"" bash -c ""curl -fsSL $DOCKER_INSTALL_URL | sh""; then\n    echo ""❌ Docker installation failed""\n    exit 1\nfi\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:2e3e531c-8457-4825-a418-a68eedfeb34b -->\n\n\n[](2e3e531c-8457-4825-a418-a68eedfeb34b)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e75b,apache/superset,2511788720,2083321978,korbit-ai[bot],131444098,2025-05-10T23:47:41+00:00,2025-05-11T00:58:20+00:00,### Undocumented package dependencies <sub>![category Documentation](https://img.shields.io/badge/Documentation-7c3aed)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe purpose of each required package should be documented.\n\n###### Why this matters\nFuture maintainers may remove packages they think are unnecessary without understanding their purpose.\n\n###### Suggested change ∙ *Feature Preview*\n# Required packages:\n# - uidmap: For user namespace mapping\n# - dbus-user-session: For systemd user service management\n# - curl: For downloading Docker installation script\nREQUIRED_PKGS=(uidmap dbus-user-session curl)\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:b638ad45-bbb2-4a96-9e7d-56029ab9ba09 -->\n\n\n[](b638ad45-bbb2-4a96-9e7d-56029ab9ba09),false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e75c,apache/superset,2511788720,2083321980,korbit-ai[bot],131444098,2025-05-10T23:47:41+00:00,2025-05-11T01:14:25+00:00,"### Hardcoded configuration value <sub>![category Design](https://img.shields.io/badge/Design-0d9488)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe username is hardcoded in the script, limiting flexibility and reusability.\n\n###### Why this matters\nThe script cannot be easily adapted for different installation scenarios where a different username might be preferred.\n\n###### Suggested change ∙ *Feature Preview*\nMake the username configurable through a command-line argument with a default value:\n```bash\nUSERNAME=${1:-""dockeruser""}\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:aa14f08b-4a2d-47d4-8ac9-762207e60744 -->\n\n\n[](aa14f08b-4a2d-47d4-8ac9-762207e60744)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e75d,apache/superset,2511788720,2083321958,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:46:26+00:00,"### Unclear build process documentation <sub>![category Documentation](https://img.shields.io/badge/Documentation-7c3aed)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe comment only explains what the --build flag does, but not why two different Docker Compose commands are being used or why --no-cache is needed.\n\n###### Why this matters\nWithout understanding the purpose of using both commands and the --no-cache flag, maintainers might inadvertently remove or modify these important build steps.\n\n###### Suggested change ∙ *Feature Preview*\n# Rebuild all images from scratch (--no-cache) to ensure latest dependencies\n# Then start containers with updated images\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9e0d9d4a-f252-478e-b40c-04244dadb2fb)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:d8ca08aa-d5a8-418d-b265-2ee6acf89153 -->\n\n\n[](d8ca08aa-d5a8-418d-b265-2ee6acf89153)",true,loonar/install-superset.sh,14,"@@ -0,0 +1,18 @@\n+#!/bin/bash\n+\n+# Ensure Docker daemon is running before running compose\n+if ! pgrep -x dockerd > /dev/null 2>&1; then\n+  echo ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""\n+  exit 1\n+fi\n+\n+# The --build argument ensures all the layers are up-to-date\n+if command -v docker-compose > /dev/null 2>&1; then\n+  docker-compose build --no-cache\n+  \n+  \n+  docker compose up --build",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e75e,apache/superset,2511788720,2083321959,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:18:25+00:00,"### Poor Code Structure with Multiple Empty Lines <sub>![category Design](https://img.shields.io/badge/Design-0d9488)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nMultiple empty lines between commands reduce code readability and violate clean code principles.\n\n###### Why this matters\nPoor code formatting makes the script harder to maintain and understand, violating the KISS principle.\n\n###### Suggested change ∙ *Feature Preview*\nRemove extra empty lines between commands to maintain clean and consistent formatting.\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/b7919ebe-c9a0-4d2b-adda-642fa330304d)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:2c1f15fb-1aa5-40ef-9ff2-313cf9e2aedc -->\n\n\n[](2c1f15fb-1aa5-40ef-9ff2-313cf9e2aedc)",true,loonar/install-superset.sh,14,"@@ -0,0 +1,18 @@\n+#!/bin/bash\n+\n+# Ensure Docker daemon is running before running compose\n+if ! pgrep -x dockerd > /dev/null 2>&1; then\n+  echo ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""\n+  exit 1\n+fi\n+\n+# The --build argument ensures all the layers are up-to-date\n+if command -v docker-compose > /dev/null 2>&1; then\n+  docker-compose build --no-cache\n+  \n+  \n+  docker compose up --build",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e75f,apache/superset,2511788720,2083321964,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:26:26+00:00,"### Inconsistent Error Message Format <sub>![category Logging](https://img.shields.io/badge/Logging-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nInconsistent error message format. The script uses [ERROR] in one place and 'Error:' in another, lacking a standardized logging format.\n\n###### Why this matters\nInconsistent error message formats make it harder to parse logs and create monitoring rules. This impacts log aggregation and automated error detection.\n\n###### Suggested change ∙ *Feature Preview*\nStandardize error message format throughout the script:\n```bash\necho ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""\n# ...\necho ""[ERROR] 'docker compose' is not available. Please install Docker Compose v2.""\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/87ae8938-4e2c-462d-b269-affacda89383)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:5e3e621d-c84b-41b9-b737-9b8d82a97c6d -->\n\n\n[](5e3e621d-c84b-41b9-b737-9b8d82a97c6d)",true,loonar/install-superset.sh,5,"@@ -0,0 +1,18 @@\n+#!/bin/bash\n+\n+# Ensure Docker daemon is running before running compose\n+if ! pgrep -x dockerd > /dev/null 2>&1; then\n+  echo ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e760,apache/superset,2511788720,2083321966,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T01:46:25+00:00,"### Missing Success Logging <sub>![category Logging](https://img.shields.io/badge/Logging-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nMissing informational logging about the script's progress and successful operations.\n\n###### Why this matters\nWithout success logs, it's difficult to track the script's execution flow and verify successful operations in production environments.\n\n###### Suggested change ∙ *Feature Preview*\nAdd informational logging for key operations:\n```bash\necho ""[INFO] Checking Docker daemon status...""\nif ! pgrep -x dockerd > /dev/null 2>&1; then\n  echo ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""\n  exit 1\nfi\necho ""[INFO] Docker daemon is running""\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5ca1a364-2174-4a46-83c7-3d8a96b84ced)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:4063b577-6680-4e05-acbc-8b7e036429c1 -->\n\n\n[](4063b577-6680-4e05-acbc-8b7e036429c1)",true,loonar/install-superset.sh,4,"@@ -0,0 +1,18 @@\n+#!/bin/bash\n+\n+# Ensure Docker daemon is running before running compose\n+if ! pgrep -x dockerd > /dev/null 2>&1; then",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e761,apache/superset,2511788720,2083321968,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:54:29+00:00,"### Inconsistent Docker Compose Version Usage <sub>![category Performance](https://img.shields.io/badge/Performance-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe script mixes 'docker-compose' (v1) and 'docker compose' (v2) commands, and runs both build commands redundantly.\n\n###### Why this matters\nThis will cause confusion and unnecessary rebuilds, doubling the build time and potentially causing conflicts between the two versions.\n\n###### Suggested change ∙ *Feature Preview*\nChoose one version of Docker Compose and use it consistently:\n```bash\nif docker compose version > /dev/null 2>&1; then\n  docker compose up --build\nelse\n  echo ""Error: 'docker compose' is not available. Please install Docker Compose v2.""\n  exit 1\nfi\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c7cfaabc-7000-4e2a-8f11-388940e550e6)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:9423dadd-2a74-473c-8faa-930d26b93752 -->\n\n\n[](9423dadd-2a74-473c-8faa-930d26b93752)",true,loonar/install-superset.sh,14,"@@ -0,0 +1,18 @@\n+#!/bin/bash\n+\n+# Ensure Docker daemon is running before running compose\n+if ! pgrep -x dockerd > /dev/null 2>&1; then\n+  echo ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""\n+  exit 1\n+fi\n+\n+# The --build argument ensures all the layers are up-to-date\n+if command -v docker-compose > /dev/null 2>&1; then\n+  docker-compose build --no-cache\n+  \n+  \n+  docker compose up --build",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e762,apache/superset,2511788720,2083321971,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:38:24+00:00,"### Incorrect Version Check <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe script checks for 'docker-compose' (v1) but suggests installing Docker Compose v2 in the error message.\n\n###### Why this matters\nUsers might have v2 installed (docker compose) but the script will fail because it's checking for v1 (docker-compose), leading to false negative errors.\n\n###### Suggested change ∙ *Feature Preview*\nUpdate the version check to look for Docker Compose v2:\n```bash\nif ! docker compose version > /dev/null 2>&1; then\n  echo ""Error: 'docker compose' is not available. Please install Docker Compose v2.""\n  exit 1\nfi\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/dcf3a62a-8850-4ddd-9df2-f84cef5838c5)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:02934ba8-9f57-4b90-997a-743aec90363c -->\n\n\n[](02934ba8-9f57-4b90-997a-743aec90363c)",true,loonar/install-superset.sh,10,"@@ -0,0 +1,18 @@\n+#!/bin/bash\n+\n+# Ensure Docker daemon is running before running compose\n+if ! pgrep -x dockerd > /dev/null 2>&1; then\n+  echo ""[ERROR] Docker daemon is not running. Please start Docker with: sudo systemctl start docker""\n+  exit 1\n+fi\n+\n+# The --build argument ensures all the layers are up-to-date\n+if command -v docker-compose > /dev/null 2>&1; then",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e763,apache/superset,2511788720,2083321972,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-10T23:48:25+00:00,"### Incorrect HOME directory reference in sudo command <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe $HOME variable in this command will resolve to the root user's home directory instead of the dockeruser's home directory when executed with sudo.\n\n###### Why this matters\nThis will cause the Docker rootless setup to fail as it will look for the setup script in the wrong directory.\n\n###### Suggested change ∙ *Feature Preview*\nReplace `$HOME` with `/home/$USERNAME` to ensure the correct home directory is used:\n```bash\nsudo -u ""$USERNAME"" bash -c ""/home/$USERNAME/bin/dockerd-rootless-setuptool.sh install""\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3d93b8c8-574d-4388-97b4-a61dc1bfca7b)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:1c535b31-c4c3-4192-a3e3-1db06f76c7b9 -->\n\n\n[](1c535b31-c4c3-4192-a3e3-1db06f76c7b9)",true,loonar/install-docker-rootless.sh,67,"@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+# This script sets up Docker in rootless mode for improved security.\n+# Rootless Docker allows a non-root user to run containers without requiring elevated privileges,\n+# reducing the risk of system compromise in case of container breakout.\n+\n+set -e\n+\n+USERNAME=""dockeruser""\n+DOCKER_INSTALL_URL=""https://get.docker.com/rootless""\n+\n+# Check if the user already exists\n+if id ""$USERNAME"" &>/dev/null; then\n+  echo ""👤 User $USERNAME already exists. Skipping creation.""\n+else\n+  echo ""👤 Creating user $USERNAME...""\n+  useradd -m -s /bin/bash ""$USERNAME""\n+fi\n+\n+# Check if dependencies are already installed\n+REQUIRED_PKGS=(uidmap dbus-user-session curl)\n+MISSING_PKGS=()\n+for pkg in ""${REQUIRED_PKGS[@]}""; do\n+  dpkg -s ""$pkg"" &>/dev/null || MISSING_PKGS+=(""$pkg"")\n+done\n+if [ ${#MISSING_PKGS[@]} -eq 0 ]; then\n+  echo ""📦 Dependencies already installed.""\n+else\n+  echo ""📦 Installing dependencies: ${MISSING_PKGS[*]}...""\n+  apt update && apt install -y ""${MISSING_PKGS[@]}""\n+fi\n+\n+# Check if rootless Docker is already installed\n+if sudo -u ""$USERNAME"" test -x ""/home/$USERNAME/bin/dockerd-rootless-setuptool.sh""; then\n+  echo ""🐳 Rootless Docker already installed for $USERNAME.""\n+else\n+  echo ""🐳 Installing rootless Docker for $USERNAME...""\n+  sudo -u ""$USERNAME"" bash -c ""curl -fsSL $DOCKER_INSTALL_URL | sh || true""\n+fi\n+\n+# Check if rootlesskit failed due to AppArmor restriction\n+if grep -q ""apparmor_restrict_unprivileged_userns"" /home/$USERNAME/.docker/rootlesskit.log 2>/dev/null || \\n+   grep -q ""permission denied"" /home/$USERNAME/.docker/rootlesskit.log 2>/dev/null; then\n+  echo ""⚠️ Detected AppArmor restriction. Applying AppArmor exception for rootlesskit...""\n+\n+  APPARMOR_FILE=""/etc/apparmor.d/home.$USERNAME.bin.rootlesskit""\n+  if [ -f ""$APPARMOR_FILE"" ]; then\n+    echo ""🔒 AppArmor profile already exists. Skipping creation.""\n+  else\n+    cat <<EOT | sudo tee ""$APPARMOR_FILE""\n+# ref: https://ubuntu.com/blog/ubuntu-23-10-restricted-unprivileged-user-namespaces\n+abi <abi/4.0>,\n+include <tunables/global>\n+\n+/home/$USERNAME/bin/rootlesskit flags=(unconfined) {\n+  userns,\n+\n+  # Site-specific additions and overrides. See local/README for details.\n+  include if exists <local/home.$USERNAME.bin.rootlesskit>\n+}\n+EOT\n+  fi\n+\n+  sudo systemctl restart apparmor.service\n+  echo ""🔁 AppArmor profile applied and reloaded.""\n+  echo ""🔄 Re-running rootless Docker setup...""\n+  sudo -u ""$USERNAME"" bash -c ""$HOME/bin/dockerd-rootless-setuptool.sh install""",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e764,apache/superset,2511788720,2083321977,korbit-ai[bot],131444098,2025-05-10T23:47:40+00:00,2025-05-11T02:58:27+00:00,"### Silent failure in Docker installation <sub>![category Error Handling](https://img.shields.io/badge/Error%20Handling-ea580c)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe '|| true' silently ignores any installation failures, which could lead to an incomplete or failed Docker installation without proper error notification.\n\n###### Why this matters\nThe script will continue executing even if the Docker installation fails, potentially causing issues in subsequent steps that depend on a successful installation.\n\n###### Suggested change ∙ *Feature Preview*\nRemove the '|| true' and add proper error handling:\n```bash\nif ! sudo -u ""$USERNAME"" bash -c ""curl -fsSL $DOCKER_INSTALL_URL | sh""; then\n    echo ""❌ Docker installation failed""\n    exit 1\nfi\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/9245104e-59fd-4574-80f8-26fbfb43c277)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:2e3e531c-8457-4825-a418-a68eedfeb34b -->\n\n\n[](2e3e531c-8457-4825-a418-a68eedfeb34b)",true,loonar/install-docker-rootless.sh,38,"@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+# This script sets up Docker in rootless mode for improved security.\n+# Rootless Docker allows a non-root user to run containers without requiring elevated privileges,\n+# reducing the risk of system compromise in case of container breakout.\n+\n+set -e\n+\n+USERNAME=""dockeruser""\n+DOCKER_INSTALL_URL=""https://get.docker.com/rootless""\n+\n+# Check if the user already exists\n+if id ""$USERNAME"" &>/dev/null; then\n+  echo ""👤 User $USERNAME already exists. Skipping creation.""\n+else\n+  echo ""👤 Creating user $USERNAME...""\n+  useradd -m -s /bin/bash ""$USERNAME""\n+fi\n+\n+# Check if dependencies are already installed\n+REQUIRED_PKGS=(uidmap dbus-user-session curl)\n+MISSING_PKGS=()\n+for pkg in ""${REQUIRED_PKGS[@]}""; do\n+  dpkg -s ""$pkg"" &>/dev/null || MISSING_PKGS+=(""$pkg"")\n+done\n+if [ ${#MISSING_PKGS[@]} -eq 0 ]; then\n+  echo ""📦 Dependencies already installed.""\n+else\n+  echo ""📦 Installing dependencies: ${MISSING_PKGS[*]}...""\n+  apt update && apt install -y ""${MISSING_PKGS[@]}""\n+fi\n+\n+# Check if rootless Docker is already installed\n+if sudo -u ""$USERNAME"" test -x ""/home/$USERNAME/bin/dockerd-rootless-setuptool.sh""; then\n+  echo ""🐳 Rootless Docker already installed for $USERNAME.""\n+else\n+  echo ""🐳 Installing rootless Docker for $USERNAME...""\n+  sudo -u ""$USERNAME"" bash -c ""curl -fsSL $DOCKER_INSTALL_URL | sh || true""",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e765,apache/superset,2511788720,2083321978,korbit-ai[bot],131444098,2025-05-10T23:47:41+00:00,2025-05-11T00:58:20+00:00,### Undocumented package dependencies <sub>![category Documentation](https://img.shields.io/badge/Documentation-7c3aed)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe purpose of each required package should be documented.\n\n###### Why this matters\nFuture maintainers may remove packages they think are unnecessary without understanding their purpose.\n\n###### Suggested change ∙ *Feature Preview*\n# Required packages:\n# - uidmap: For user namespace mapping\n# - dbus-user-session: For systemd user service management\n# - curl: For downloading Docker installation script\nREQUIRED_PKGS=(uidmap dbus-user-session curl)\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/6e06d117-251e-4e07-b624-d5b3ebd897e1)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:b638ad45-bbb2-4a96-9e7d-56029ab9ba09 -->\n\n\n[](b638ad45-bbb2-4a96-9e7d-56029ab9ba09),true,loonar/install-docker-rootless.sh,21,"@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+# This script sets up Docker in rootless mode for improved security.\n+# Rootless Docker allows a non-root user to run containers without requiring elevated privileges,\n+# reducing the risk of system compromise in case of container breakout.\n+\n+set -e\n+\n+USERNAME=""dockeruser""\n+DOCKER_INSTALL_URL=""https://get.docker.com/rootless""\n+\n+# Check if the user already exists\n+if id ""$USERNAME"" &>/dev/null; then\n+  echo ""👤 User $USERNAME already exists. Skipping creation.""\n+else\n+  echo ""👤 Creating user $USERNAME...""\n+  useradd -m -s /bin/bash ""$USERNAME""\n+fi\n+\n+# Check if dependencies are already installed\n+REQUIRED_PKGS=(uidmap dbus-user-session curl)",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e766,apache/superset,2511788720,2083321980,korbit-ai[bot],131444098,2025-05-10T23:47:41+00:00,2025-05-11T01:14:25+00:00,"### Hardcoded configuration value <sub>![category Design](https://img.shields.io/badge/Design-0d9488)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe username is hardcoded in the script, limiting flexibility and reusability.\n\n###### Why this matters\nThe script cannot be easily adapted for different installation scenarios where a different username might be preferred.\n\n###### Suggested change ∙ *Feature Preview*\nMake the username configurable through a command-line argument with a default value:\n```bash\nUSERNAME=${1:-""dockeruser""}\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d0dcc85c-7a84-4ad9-97bf-0663099b2022)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:aa14f08b-4a2d-47d4-8ac9-762207e60744 -->\n\n\n[](aa14f08b-4a2d-47d4-8ac9-762207e60744)",true,loonar/install-docker-rootless.sh,9,"@@ -0,0 +1,90 @@\n+#!/bin/bash\n+\n+# This script sets up Docker in rootless mode for improved security.\n+# Rootless Docker allows a non-root user to run containers without requiring elevated privileges,\n+# reducing the risk of system compromise in case of container breakout.\n+\n+set -e\n+\n+USERNAME=""dockeruser""",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e767,apache/superset,2507724992,2080186719,korbit-ai[bot],131444098,2025-05-08T17:50:51+00:00,2025-05-08T20:00:11+00:00,"### Inefficient Owner Name Transformation <sub>![category Performance](https://img.shields.io/badge/Performance-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nString splitting and joining operations are performed unnecessarily on every save operation to transform owner names back to the redux format.\n\n###### Why this matters\nThis transformation could be expensive for large sets of owners and is being performed every time the chart properties are saved, when it could be handled once during initial data loading or maintained in the correct format throughout.\n\n###### Suggested change ∙ *Feature Preview*\nStore owners in the desired format from the beginning or create a mapping when owners are first fetched. Example:\n```typescript\nconst ownerMap = chart?.owners?.reduce((acc, owner) => {\n  acc[owner.id] = {\n    first_name: owner.first_name,\n    last_name: owner.last_name\n  };\n  return acc;\n}, {});\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:f3e3c061-054f-46bc-9d99-d9de244e5a4c -->\n\n\n[](f3e3c061-054f-46bc-9d99-d9de244e5a4c)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e768,apache/superset,2507724992,2080186722,korbit-ai[bot],131444098,2025-05-08T17:50:51+00:00,2025-05-08T21:58:18+00:00,"### Unsafe Name Splitting Logic <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe code assumes that the owner's name always has at least a first name and last name, and splits the name based on spaces. This can cause issues with names that don't follow this format.\n\n###### Why this matters\nNames that contain only one word, or names with special characters (like hyphens or apostrophes) could be incorrectly parsed, leading to incorrect owner information in the redux state.\n\n###### Suggested change ∙ *Feature Preview*\nAdd validation and fallback handling for different name formats:\n```typescript\nconst nameParts = owner.label.split(' ');\nconst first_name = nameParts[0] || '';\nconst last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';\nreturn {\n  id: owner.value,\n  first_name,\n  last_name,\n};\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:bf412400-eb0a-4e39-ba46-40b2c8dbd5b1 -->\n\n\n[](bf412400-eb0a-4e39-ba46-40b2c8dbd5b1)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e769,apache/superset,2507724992,2080210495,korbit-ai[bot],131444098,2025-05-08T18:08:11+00:00,2025-05-08T20:32:05+00:00,"### Embedded Name Parsing Logic <sub>![category Design](https://img.shields.io/badge/Design-0d9488)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe owner name parsing logic is directly embedded in the submit handler, making it difficult to reuse and test. This logic should be extracted into a separate utility function.\n\n###### Why this matters\nHaving business logic directly in event handlers reduces code reusability and makes it harder to maintain or modify the name parsing logic if it needs to be used elsewhere.\n\n###### Suggested change ∙ *Feature Preview*\nExtract the logic into a separate utility function:\n```typescript\nconst parseOwnerName = (owner: { label: string; value: number }) => {\n  const nameParts = owner.label.split(' ');\n  return {\n    id: owner.value,\n    first_name: nameParts[0] || '',\n    last_name: nameParts.length > 1 ? nameParts.slice(1).join(' ') : ''\n  };\n};\n\n// Usage in component\nconst newOwners = selectedOwnersArray.map(parseOwnerName);\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:d9e444ca-2cf2-4fdc-958f-d869452f076b -->\n\n\n[](d9e444ca-2cf2-4fdc-958f-d869452f076b)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e76a,apache/superset,2507724992,2080210496,korbit-ai[bot],131444098,2025-05-08T18:08:11+00:00,2025-05-08T18:12:24+00:00,"### Complex Name Parsing Without Types <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe name parsing logic is complex and lacks a clear type definition for the owner parameter, making the transformation from label to first/last name hard to follow.\n\n###### Why this matters\nWithout proper typing and a dedicated function, this name parsing logic is brittle and its purpose isn't immediately clear to future maintainers.\n\n###### Suggested change ∙ *Feature Preview*\n```typescript\ninterface Owner {\n  label: string;\n  value: number;\n}\n\nfunction parseOwnerName(owner: Owner): { first_name: string; last_name: string } {\n  const nameParts = owner.label.split(' ');\n  return {\n    first_name: nameParts[0] || '',\n    last_name: nameParts.length > 1 ? nameParts.slice(1).join(' ') : ''\n  };\n}\n\nconst newOwners = selectedOwnersArray.map((owner: Owner) => ({\n  id: owner.value,\n  ...parseOwnerName(owner)\n}));\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:a57ef742-4b39-475b-9e70-1d8d5ddf1145 -->\n\n\n[](a57ef742-4b39-475b-9e70-1d8d5ddf1145)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e76b,apache/superset,2507724992,2080224863,ethan-l-geotab,165913720,2025-05-08T18:19:07+00:00,2025-05-08T18:19:07+00:00,"I think it's a pretty specific case where this one component handles owners in a specific way which when trying to update redux, the ""shape"" of the object is different, so i have to translate it back to the same shape which originally existed.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e76c,apache/superset,2507724992,2080228625,korbit-ai[bot],131444098,2025-05-08T18:21:49+00:00,2025-05-08T18:21:49+00:00,"I see - since this is specifically about matching the Redux state shape in this component, keeping the transformation logic local makes sense. Thanks for clarifying the context.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e76d,apache/superset,2507724992,2080230511,rusackas,812905,2025-05-08T18:23:02+00:00,2025-05-08T18:23:02+00:00,"This is probably OK, but I'm _slightly_ nervous about things like middle names. This would mean if we have Michael J. Fox sign up, it'd be ""Michael"" ""J. Fox"" instead of ""Michael J."" ""Fox.""\r\n\r\nIs there other first/last name logic currently in place, or is this a new concept?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e76e,apache/superset,2507724992,2080233094,korbit-ai[bot],131444098,2025-05-08T18:24:55+00:00,2025-05-08T18:24:55+00:00,"Valid concern about middle name handling. Before suggesting changes, could you clarify if there's existing first/last name parsing logic elsewhere in the codebase we should match? If so, we should reuse that pattern instead of introducing a new one.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e76f,apache/superset,2507724992,2080355064,ethan-l-geotab,165913720,2025-05-08T19:35:31+00:00,2025-05-08T19:35:31+00:00,"I thought about that too actually. ""Updating"" only happens around user ID's. As for display, I'm on the same line as you where it's ""probably okay"". \r\n\r\nThere is no real existing ""full_name"" split. I tried checking out how dashboards/dataset list handles it, but they're both different :(. \r\n\r\nI dont think there is a true way of doing it unless we retrieve it from the backend.\r\n\r\nI have tried doing something like this in the backend to ""pass ownership data"" along with the put request \r\nand then just removing the modification to the owner field and using just the returned owner from the put request, but I'm not sure that's the best design either...\r\n<img width=""701"" alt=""image"" src=""https://github.com/user-attachments/assets/04b2e056-4ed4-4914-bfe3-8c37a0d77ebc"" />\r\n<img width=""544"" alt=""image"" src=""https://github.com/user-attachments/assets/1562c78e-a209-4cfa-9b94-4b66ce3fe8fb"" />\r\n \r\nI've also tried seeing if I could sort of ""cache"" the load options and see if it was split there when im getting the eligible users to add as owners, but that won't work either because it's only `item.text` and makes no differentiation of first and last names.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e770,apache/superset,2507724992,2082152322,rusackas,812905,2025-05-09T17:31:26+00:00,2025-05-09T17:31:26+00:00,"> There is no real existing ""full_name"" split. I tried checking out how dashboards/dataset list handles it, but they're both different :(.\r\n\r\nThis is what I was worried about. If there's any logic to be DRY'd up (moved to a util or something) that would be preferable than implementing another bit of inline logic.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e771,apache/superset,2507724992,2080186719,korbit-ai[bot],131444098,2025-05-08T17:50:51+00:00,2025-05-08T20:00:11+00:00,"### Inefficient Owner Name Transformation <sub>![category Performance](https://img.shields.io/badge/Performance-4f46e5)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nString splitting and joining operations are performed unnecessarily on every save operation to transform owner names back to the redux format.\n\n###### Why this matters\nThis transformation could be expensive for large sets of owners and is being performed every time the chart properties are saved, when it could be handled once during initial data loading or maintained in the correct format throughout.\n\n###### Suggested change ∙ *Feature Preview*\nStore owners in the desired format from the beginning or create a mapping when owners are first fetched. Example:\n```typescript\nconst ownerMap = chart?.owners?.reduce((acc, owner) => {\n  acc[owner.id] = {\n    first_name: owner.first_name,\n    last_name: owner.last_name\n  };\n  return acc;\n}, {});\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3509b88c-43b2-496a-95a3-5d955a6c3566)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:f3e3c061-054f-46bc-9d99-d9de244e5a4c -->\n\n\n[](f3e3c061-054f-46bc-9d99-d9de244e5a4c)",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,22 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const [first_name, ...last_name_parts] = owner.label.split(' ');\n+        return {\n+          id: owner.value,\n+          first_name,\n+          last_name: last_name_parts.join(' '),\n+        };\n+      });",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e772,apache/superset,2507724992,2080186722,korbit-ai[bot],131444098,2025-05-08T17:50:51+00:00,2025-05-08T21:58:18+00:00,"### Unsafe Name Splitting Logic <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe code assumes that the owner's name always has at least a first name and last name, and splits the name based on spaces. This can cause issues with names that don't follow this format.\n\n###### Why this matters\nNames that contain only one word, or names with special characters (like hyphens or apostrophes) could be incorrectly parsed, leading to incorrect owner information in the redux state.\n\n###### Suggested change ∙ *Feature Preview*\nAdd validation and fallback handling for different name formats:\n```typescript\nconst nameParts = owner.label.split(' ');\nconst first_name = nameParts[0] || '';\nconst last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';\nreturn {\n  id: owner.value,\n  first_name,\n  last_name,\n};\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/3978e966-ac14-4e9e-9b88-70edcc7d19a1)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:bf412400-eb0a-4e39-ba46-40b2c8dbd5b1 -->\n\n\n[](bf412400-eb0a-4e39-ba46-40b2c8dbd5b1)",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,22 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const [first_name, ...last_name_parts] = owner.label.split(' ');",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e773,apache/superset,2507724992,2080210495,korbit-ai[bot],131444098,2025-05-08T18:08:11+00:00,2025-05-08T20:32:05+00:00,"### Embedded Name Parsing Logic <sub>![category Design](https://img.shields.io/badge/Design-0d9488)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe owner name parsing logic is directly embedded in the submit handler, making it difficult to reuse and test. This logic should be extracted into a separate utility function.\n\n###### Why this matters\nHaving business logic directly in event handlers reduces code reusability and makes it harder to maintain or modify the name parsing logic if it needs to be used elsewhere.\n\n###### Suggested change ∙ *Feature Preview*\nExtract the logic into a separate utility function:\n```typescript\nconst parseOwnerName = (owner: { label: string; value: number }) => {\n  const nameParts = owner.label.split(' ');\n  return {\n    id: owner.value,\n    first_name: nameParts[0] || '',\n    last_name: nameParts.length > 1 ? nameParts.slice(1).join(' ') : ''\n  };\n};\n\n// Usage in component\nconst newOwners = selectedOwnersArray.map(parseOwnerName);\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5e46bad0-2e9a-456d-b8e0-6bd3e7b7c71c)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:d9e444ca-2cf2-4fdc-958f-d869452f076b -->\n\n\n[](d9e444ca-2cf2-4fdc-958f-d869452f076b)",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,24 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const nameParts = owner.label.split(' ');\n+        const first_name = nameParts[0] || '';\n+        const last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';\n+        return {\n+          id: owner.value,\n+          first_name,\n+          last_name,\n+        };\n+      });",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e774,apache/superset,2507724992,2080210496,korbit-ai[bot],131444098,2025-05-08T18:08:11+00:00,2025-05-08T18:12:24+00:00,"### Complex Name Parsing Without Types <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe name parsing logic is complex and lacks a clear type definition for the owner parameter, making the transformation from label to first/last name hard to follow.\n\n###### Why this matters\nWithout proper typing and a dedicated function, this name parsing logic is brittle and its purpose isn't immediately clear to future maintainers.\n\n###### Suggested change ∙ *Feature Preview*\n```typescript\ninterface Owner {\n  label: string;\n  value: number;\n}\n\nfunction parseOwnerName(owner: Owner): { first_name: string; last_name: string } {\n  const nameParts = owner.label.split(' ');\n  return {\n    first_name: nameParts[0] || '',\n    last_name: nameParts.length > 1 ? nameParts.slice(1).join(' ') : ''\n  };\n}\n\nconst newOwners = selectedOwnersArray.map((owner: Owner) => ({\n  id: owner.value,\n  ...parseOwnerName(owner)\n}));\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/8dc4544e-e622-477b-9a61-6e1b7e850401)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:a57ef742-4b39-475b-9e70-1d8d5ddf1145 -->\n\n\n[](a57ef742-4b39-475b-9e70-1d8d5ddf1145)",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,24 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const nameParts = owner.label.split(' ');\n+        const first_name = nameParts[0] || '';\n+        const last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e775,apache/superset,2507724992,2080224863,ethan-l-geotab,165913720,2025-05-08T18:19:07+00:00,2025-05-08T18:19:07+00:00,"I think it's a pretty specific case where this one component handles owners in a specific way which when trying to update redux, the ""shape"" of the object is different, so i have to translate it back to the same shape which originally existed.",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,24 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const nameParts = owner.label.split(' ');\n+        const first_name = nameParts[0] || '';\n+        const last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';\n+        return {\n+          id: owner.value,\n+          first_name,\n+          last_name,\n+        };\n+      });",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e776,apache/superset,2507724992,2080228625,korbit-ai[bot],131444098,2025-05-08T18:21:49+00:00,2025-05-08T18:21:49+00:00,"I see - since this is specifically about matching the Redux state shape in this component, keeping the transformation logic local makes sense. Thanks for clarifying the context.",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,24 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const nameParts = owner.label.split(' ');\n+        const first_name = nameParts[0] || '';\n+        const last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';\n+        return {\n+          id: owner.value,\n+          first_name,\n+          last_name,\n+        };\n+      });",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e777,apache/superset,2507724992,2080230511,rusackas,812905,2025-05-08T18:23:02+00:00,2025-05-08T18:23:02+00:00,"This is probably OK, but I'm _slightly_ nervous about things like middle names. This would mean if we have Michael J. Fox sign up, it'd be ""Michael"" ""J. Fox"" instead of ""Michael J."" ""Fox.""\r\n\r\nIs there other first/last name logic currently in place, or is this a new concept?",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,24 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const nameParts = owner.label.split(' ');\n+        const first_name = nameParts[0] || '';\n+        const last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e778,apache/superset,2507724992,2080233094,korbit-ai[bot],131444098,2025-05-08T18:24:55+00:00,2025-05-08T18:24:55+00:00,"Valid concern about middle name handling. Before suggesting changes, could you clarify if there's existing first/last name parsing logic elsewhere in the codebase we should match? If so, we should reuse that pattern instead of introducing a new one.",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,24 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const nameParts = owner.label.split(' ');\n+        const first_name = nameParts[0] || '';\n+        const last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e779,apache/superset,2507724992,2080355064,ethan-l-geotab,165913720,2025-05-08T19:35:31+00:00,2025-05-08T19:35:31+00:00,"I thought about that too actually. ""Updating"" only happens around user ID's. As for display, I'm on the same line as you where it's ""probably okay"". \r\n\r\nThere is no real existing ""full_name"" split. I tried checking out how dashboards/dataset list handles it, but they're both different :(. \r\n\r\nI dont think there is a true way of doing it unless we retrieve it from the backend.\r\n\r\nI have tried doing something like this in the backend to ""pass ownership data"" along with the put request \r\nand then just removing the modification to the owner field and using just the returned owner from the put request, but I'm not sure that's the best design either...\r\n<img width=""701"" alt=""image"" src=""https://github.com/user-attachments/assets/04b2e056-4ed4-4914-bfe3-8c37a0d77ebc"" />\r\n<img width=""544"" alt=""image"" src=""https://github.com/user-attachments/assets/1562c78e-a209-4cfa-9b94-4b66ce3fe8fb"" />\r\n \r\nI've also tried seeing if I could sort of ""cache"" the load options and see if it was split there when im getting the eligible users to add as owners, but that won't work either because it's only `item.text` and makes no differentiation of first and last names.",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,24 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const nameParts = owner.label.split(' ');\n+        const first_name = nameParts[0] || '';\n+        const last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e77a,apache/superset,2507724992,2082152322,rusackas,812905,2025-05-09T17:31:26+00:00,2025-05-09T17:31:26+00:00,"> There is no real existing ""full_name"" split. I tried checking out how dashboards/dataset list handles it, but they're both different :(.\r\n\r\nThis is what I was worried about. If there's any logic to be DRY'd up (moved to a util or something) that would be preferable than implementing another bit of inline logic.",true,superset-frontend/src/explore/components/PropertiesModal/index.tsx,NaN,"@@ -185,12 +185,24 @@ function PropertiesModal({\n         body: JSON.stringify(payload),\n       });\n       // update the redux state\n+      // convert the tags to the format used in the selectOwners back to original redux format\n+      const selectedOwnersArray = ensureIsArray(selectedOwners);\n+      const newOwners = selectedOwnersArray.map((owner: any) => {\n+        const nameParts = owner.label.split(' ');\n+        const first_name = nameParts[0] || '';\n+        const last_name = nameParts.length > 1 ? nameParts.slice(1).join(' ') : '';",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e77b,apache/superset,2505910089,2078805430,korbit-ai[bot],131444098,2025-05-08T02:59:31+00:00,2025-05-08T05:06:24+00:00,"### Incomplete ValidationError Documentation <sub>![category Documentation](https://img.shields.io/badge/Documentation-7c3aed)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe docstring doesn't explain when this error is raised or what conditions trigger it.\n\n###### Why this matters\nWithout context about when this error occurs, developers may have difficulty troubleshooting or properly handling this exception.\n\n###### Suggested change ∙ *Feature Preview*\nclass MultiCatalogDisabledValidationError(ValidationError):\n    """"""\n    Validation error raised when attempting to use a non-default catalog\n    in a database connection where multi-catalog support is disabled.\n    """"""\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:6474a029-cd2c-4b59-b12a-f2e7c5dd4b16 -->\n\n\n[](6474a029-cd2c-4b59-b12a-f2e7c5dd4b16)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e77c,apache/superset,2505910089,2078805431,korbit-ai[bot],131444098,2025-05-08T02:59:32+00:00,2025-05-08T05:02:30+00:00,"### Unclear Error Message for Catalog Selection <sub>![category Error Handling](https://img.shields.io/badge/Error%20Handling-ea580c)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe error message 'Only the default catalog is supported for this connection' may be too vague for users to understand what action they need to take.\n\n###### Why this matters\nUsers may not understand what a 'default catalog' is or how to correct their selection, leading to confusion and potential support tickets.\n\n###### Suggested change ∙ *Feature Preview*\nEnhance the error message to be more specific and actionable:\n```python\nclass MultiCatalogDisabledValidationError(ValidationError):\n    def __init__(self) -> None:\n        super().__init__(\n            [_('Multi-catalog support is disabled. Please use the default catalog or enable multi-catalog support in your database configuration.')],\n            field_name='catalog',\n        )\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:f82026d2-d4c6-4294-8d67-31fae16f2f2f -->\n\n\n[](f82026d2-d4c6-4294-8d67-31fae16f2f2f)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e77d,apache/superset,2505910089,2078805433,korbit-ai[bot],131444098,2025-05-08T02:59:32+00:00,2025-05-08T03:15:41+00:00,"### Missing Catalog Persistence <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe catalog value is assigned but not stored in self._properties, causing potential inconsistency between the local variable and the properties dict.\n\n###### Why this matters\nThis could lead to the catalog value not being properly persisted during the update operation, which contradicts the intended functionality.\n\n###### Suggested change ∙ *Feature Preview*\nStore the catalog value in properties:\n```python\nelif ""catalog"" not in self._properties:\n    catalog = self._properties[""catalog""] = self._model.catalog\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:357699ca-06c8-4f74-9ac1-ef011a10150d -->\n\n\n[](357699ca-06c8-4f74-9ac1-ef011a10150d)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e77e,apache/superset,2505910089,2078805435,korbit-ai[bot],131444098,2025-05-08T02:59:32+00:00,2025-05-08T05:14:29+00:00,"### Unclear method name and redundant type casting comment <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe comment 'we know we have a valid model' is redundant with the type casting, and the method name doesn't clearly indicate it handles catalog and database configuration.\n\n###### Why this matters\nUnclear method naming and redundant comments make the code harder to understand at a glance.\n\n###### Suggested change ∙ *Feature Preview*\nRename method to be more specific and remove redundant comment:\n```python\ndef _validate_database_and_catalog_config(self, exceptions: list[ValidationError]) -> None:\n    self._model = cast(SqlaTable, self._model)\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:6c9f03c2-f4cd-492e-a789-15e65473862a -->\n\n\n[](6c9f03c2-f4cd-492e-a789-15e65473862a)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e77f,apache/superset,2505910089,2078805830,Vitor-Avila,96086495,2025-05-08T03:00:13+00:00,2025-05-08T03:00:14+00:00,"Not entirely sure why we didn't want to allow a DB re-map here. This validation was not really working tho since `database_id` here would never get a value (as it was reading from `database` as opposed to `database_id`). \r\n\r\nThat said, I know there are valid use-cases to re-map datasets to other connections and since it's currently possible, I've removed this check.",false,NaN,NaN,NaN,1,1,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e780,apache/superset,2505910089,2078806564,Vitor-Avila,96086495,2025-05-08T03:01:21+00:00,2025-05-08T03:01:22+00:00,didn't want to do `if not catalog` as technically `catalog` can be `None`.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e781,apache/superset,2505910089,2078806867,Vitor-Avila,96086495,2025-05-08T03:01:55+00:00,2025-05-08T03:01:55+00:00,"same here, supporting `schema = None`",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e782,apache/superset,2505910089,2078815334,Vitor-Avila,96086495,2025-05-08T03:15:35+00:00,2025-05-08T03:15:35+00:00,"That's not really needed. The `update()` logic would only update keys present in `self._properties`. There's no need to include `self._properties[""catalog""]` in this case, since its value is already what's in the object.\r\n\r\nI'm just assigning the value to the local `catalog` variable to validate uniqueness after.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e783,apache/superset,2505910089,2078815620,korbit-ai[bot],131444098,2025-05-08T03:16:06+00:00,2025-05-08T03:16:06+00:00,"I see, makes sense. The local catalog variable is sufficient for validation and storing it in properties would be redundant. Thanks for clarifying.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e784,apache/superset,2505910089,2078816489,Vitor-Avila,96086495,2025-05-08T03:17:47+00:00,2025-05-08T03:17:47+00:00,"I'd say that `validate_dataset_source` makes sense, as these configs appear in the **Source** tab of the dataset editor. But I'll keep this open to hear from others too :) ",false,NaN,NaN,NaN,1,1,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e785,apache/superset,2505910089,2078816947,Vitor-Avila,96086495,2025-05-08T03:18:29+00:00,2025-05-08T03:18:29+00:00,Seems to be in accordance to other `Exceptions` we have.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e786,apache/superset,2505910089,2078817132,korbit-ai[bot],131444098,2025-05-08T03:18:46+00:00,2025-05-08T03:18:46+00:00,You're right - the documentation style is consistent with the other exceptions in this file. The brief docstring works in this context.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e787,apache/superset,2505910089,2078817599,Vitor-Avila,96086495,2025-05-08T03:19:31+00:00,2025-05-08T03:19:31+00:00,np!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e788,apache/superset,2505910089,2078817823,korbit-ai[bot],131444098,2025-05-08T03:19:56+00:00,2025-05-08T03:19:56+00:00,"Got it, thanks for explaining the rationale! 👍",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e789,apache/superset,2505910089,2079686796,betodealmeida,1534870,2025-05-08T13:09:11+00:00,2025-05-08T13:27:53+00:00,"What happens if we pass `null` as the catalog, wouldn't the backend also use the default catalog? Then you can just do:\r\n\r\n```js\r\ncatalog: datasource.database?.allow_multi_catalog ? datasource.catalog : null,\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e78a,apache/superset,2505910089,2079688003,betodealmeida,1534870,2025-05-08T13:09:53+00:00,2025-05-08T13:27:53+00:00,This would have to be updated depending on the answer to my question above.,false,NaN,NaN,NaN,1,1,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e78b,apache/superset,2505910089,2079690033,betodealmeida,1534870,2025-05-08T13:11:08+00:00,2025-05-08T13:27:53+00:00,❤️,false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e78c,apache/superset,2505910089,2079706349,betodealmeida,1534870,2025-05-08T13:20:32+00:00,2025-05-08T13:27:53+00:00,"Yeah, we should be able to do this, there's valid use cases.",false,NaN,NaN,NaN,1,0,0,0,0,0,1,apache_superset
6824cd8e0ce5dcec7842e78d,apache/superset,2505910089,2080079470,Vitor-Avila,96086495,2025-05-08T16:36:35+00:00,2025-05-08T16:36:35+00:00,"hmm I think that wouldn't work properly. Let's say you're remapping a dataset to a BQ connection that has `allow_multi_catalog` disabled. `get_default_catalog` for this connection would return the project ID from the service account, and then if we're passing `null` this check would fail:\r\n``` python\r\nif (\r\n    ""catalog"" in self._properties\r\n    and catalog != default_catalog\r\n    and not db.allow_multi_catalog\r\n):\r\n    exceptions.append(MultiCatalogDisabledValidationError())\r\n```\r\n\r\nI think it makes sense to have this check at the API layer as `catalog=None` is technically valid.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e78e,apache/superset,2505910089,2078805430,korbit-ai[bot],131444098,2025-05-08T02:59:31+00:00,2025-05-08T05:06:24+00:00,"### Incomplete ValidationError Documentation <sub>![category Documentation](https://img.shields.io/badge/Documentation-7c3aed)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe docstring doesn't explain when this error is raised or what conditions trigger it.\n\n###### Why this matters\nWithout context about when this error occurs, developers may have difficulty troubleshooting or properly handling this exception.\n\n###### Suggested change ∙ *Feature Preview*\nclass MultiCatalogDisabledValidationError(ValidationError):\n    """"""\n    Validation error raised when attempting to use a non-default catalog\n    in a database connection where multi-catalog support is disabled.\n    """"""\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d5ea4df9-fdf4-4865-95b6-c1be7d1dd8c3)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:6474a029-cd2c-4b59-b12a-f2e7c5dd4b16 -->\n\n\n[](6474a029-cd2c-4b59-b12a-f2e7c5dd4b16)",true,superset/commands/dataset/exceptions.py,NaN,"@@ -33,6 +33,18 @@ def get_dataset_exist_error_msg(table: Table) -> str:\n     return _(""Dataset %(table)s already exists"", table=table)\n \n \n+class MultiCatalogDisabledValidationError(ValidationError):\n+    """"""\n+    Validation error for using a non-default catalog when multi-catalog is disabled\n+    """"""",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e78f,apache/superset,2505910089,2078805431,korbit-ai[bot],131444098,2025-05-08T02:59:32+00:00,2025-05-08T05:02:30+00:00,"### Unclear Error Message for Catalog Selection <sub>![category Error Handling](https://img.shields.io/badge/Error%20Handling-ea580c)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe error message 'Only the default catalog is supported for this connection' may be too vague for users to understand what action they need to take.\n\n###### Why this matters\nUsers may not understand what a 'default catalog' is or how to correct their selection, leading to confusion and potential support tickets.\n\n###### Suggested change ∙ *Feature Preview*\nEnhance the error message to be more specific and actionable:\n```python\nclass MultiCatalogDisabledValidationError(ValidationError):\n    def __init__(self) -> None:\n        super().__init__(\n            [_('Multi-catalog support is disabled. Please use the default catalog or enable multi-catalog support in your database configuration.')],\n            field_name='catalog',\n        )\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/c5727bd4-1c6f-470a-b04b-3d35fdc04621)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:f82026d2-d4c6-4294-8d67-31fae16f2f2f -->\n\n\n[](f82026d2-d4c6-4294-8d67-31fae16f2f2f)",true,superset/commands/dataset/exceptions.py,5,"@@ -33,6 +33,18 @@\n     return _(""Dataset %(table)s already exists"", table=table)\n \n \n+class MultiCatalogDisabledValidationError(ValidationError):",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e790,apache/superset,2505910089,2078805433,korbit-ai[bot],131444098,2025-05-08T02:59:32+00:00,2025-05-08T03:15:41+00:00,"### Missing Catalog Persistence <sub>![category Functionality](https://img.shields.io/badge/Functionality-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe catalog value is assigned but not stored in self._properties, causing potential inconsistency between the local variable and the properties dict.\n\n###### Why this matters\nThis could lead to the catalog value not being properly persisted during the update operation, which contradicts the intended functionality.\n\n###### Suggested change ∙ *Feature Preview*\nStore the catalog value in properties:\n```python\nelif ""catalog"" not in self._properties:\n    catalog = self._properties[""catalog""] = self._model.catalog\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/268f8793-a7d0-45eb-b6e6-b9854e1f1bab)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:357699ca-06c8-4f74-9ac1-ef011a10150d -->\n\n\n[](357699ca-06c8-4f74-9ac1-ef011a10150d)",true,superset/commands/dataset/update.py,114,"@@ -128,15 +106,68 @@ def validate(self) -> None:\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)\n+        database_id = self._properties.pop(""database_id"", None)\n+        catalog = self._properties.get(""catalog"")\n+        new_db_connection: Database | None = None\n+\n+        if database_id and database_id != self._model.database.id:\n+            if new_db_connection := DatasetDAO.get_database_by_id(database_id):\n+                self._properties[""database""] = new_db_connection\n+            else:\n+                exceptions.append(DatabaseNotFoundValidationError())\n+        db = new_db_connection or self._model.database\n+        default_catalog = db.get_default_catalog()\n+\n+        # If multi-catalog is disabled, and catalog provided is not\n+        # the default one, fail\n+        if (\n+            ""catalog"" in self._properties\n+            and catalog != default_catalog\n+            and not db.allow_multi_catalog\n+        ):\n+            exceptions.append(MultiCatalogDisabledValidationError())\n+\n+        # If the DB connection does not support multi-catalog,\n+        # use the default catalog\n+        elif not db.allow_multi_catalog:\n+            catalog = self._properties[""catalog""] = default_catalog\n+\n+        # Fallback to using the previous value if not provided\n+        elif ""catalog"" not in self._properties:\n+            catalog = self._model.catalog",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e791,apache/superset,2505910089,2078805435,korbit-ai[bot],131444098,2025-05-08T02:59:32+00:00,2025-05-08T05:14:29+00:00,"### Unclear method name and redundant type casting comment <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe comment 'we know we have a valid model' is redundant with the type casting, and the method name doesn't clearly indicate it handles catalog and database configuration.\n\n###### Why this matters\nUnclear method naming and redundant comments make the code harder to understand at a glance.\n\n###### Suggested change ∙ *Feature Preview*\nRename method to be more specific and remove redundant comment:\n```python\ndef _validate_database_and_catalog_config(self, exceptions: list[ValidationError]) -> None:\n    self._model = cast(SqlaTable, self._model)\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/aec1573a-4769-4026-af64-7ed2824a7fd7)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:6c9f03c2-f4cd-492e-a789-15e65473862a -->\n\n\n[](6c9f03c2-f4cd-492e-a789-15e65473862a)",true,superset/commands/dataset/update.py,85,"@@ -128,15 +106,68 @@\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e792,apache/superset,2505910089,2078805830,Vitor-Avila,96086495,2025-05-08T03:00:13+00:00,2025-05-08T03:00:14+00:00,"Not entirely sure why we didn't want to allow a DB re-map here. This validation was not really working tho since `database_id` here would never get a value (as it was reading from `database` as opposed to `database_id`). \r\n\r\nThat said, I know there are valid use-cases to re-map datasets to other connections and since it's currently possible, I've removed this check.",true,superset/commands/dataset/update.py,67,"@@ -86,38 +90,12 @@ def validate(self) -> None:\n         if not self._model:\n             raise DatasetNotFoundError()\n \n-        # Check ownership\n+        # Check permission to update the dataset\n         try:\n             security_manager.raise_for_ownership(self._model)\n         except SupersetSecurityException as ex:\n             raise DatasetForbiddenError() from ex\n \n-        database_id = self._properties.get(""database"")\n-\n-        catalog = self._properties.get(""catalog"")\n-        if not catalog:\n-            catalog = self._properties[""catalog""] = (\n-                self._model.database.get_default_catalog()\n-            )\n-\n-        table = Table(\n-            self._properties.get(""table_name""),  # type: ignore\n-            self._properties.get(""schema""),\n-            catalog,\n-        )\n-\n-        # Validate uniqueness\n-        if not DatasetDAO.validate_update_uniqueness(\n-            self._model.database,\n-            table,\n-            self._model_id,\n-        ):\n-            exceptions.append(DatasetExistsValidationError(table))\n-\n-        # Validate/Populate database not allowed to change\n-        if database_id and database_id != self._model:\n-            exceptions.append(DatabaseChangeValidationError())",1,1,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e793,apache/superset,2505910089,2078806564,Vitor-Avila,96086495,2025-05-08T03:01:21+00:00,2025-05-08T03:01:22+00:00,didn't want to do `if not catalog` as technically `catalog` can be `None`.,true,superset/commands/dataset/update.py,105,"@@ -128,15 +106,68 @@ def validate(self) -> None:\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)\n+        database_id = self._properties.pop(""database_id"", None)\n+        catalog = self._properties.get(""catalog"")\n+        new_db_connection: Database | None = None\n+\n+        if database_id and database_id != self._model.database.id:\n+            if new_db_connection := DatasetDAO.get_database_by_id(database_id):\n+                self._properties[""database""] = new_db_connection\n+            else:\n+                exceptions.append(DatabaseNotFoundValidationError())\n+        db = new_db_connection or self._model.database\n+        default_catalog = db.get_default_catalog()\n+\n+        # If multi-catalog is disabled, and catalog provided is not\n+        # the default one, fail\n+        if (\n+            ""catalog"" in self._properties\n+            and catalog != default_catalog\n+            and not db.allow_multi_catalog\n+        ):\n+            exceptions.append(MultiCatalogDisabledValidationError())",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e794,apache/superset,2505910089,2078806867,Vitor-Avila,96086495,2025-05-08T03:01:55+00:00,2025-05-08T03:01:55+00:00,"same here, supporting `schema = None`",true,superset/commands/dataset/update.py,120,"@@ -128,15 +106,68 @@ def validate(self) -> None:\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)\n+        database_id = self._properties.pop(""database_id"", None)\n+        catalog = self._properties.get(""catalog"")\n+        new_db_connection: Database | None = None\n+\n+        if database_id and database_id != self._model.database.id:\n+            if new_db_connection := DatasetDAO.get_database_by_id(database_id):\n+                self._properties[""database""] = new_db_connection\n+            else:\n+                exceptions.append(DatabaseNotFoundValidationError())\n+        db = new_db_connection or self._model.database\n+        default_catalog = db.get_default_catalog()\n+\n+        # If multi-catalog is disabled, and catalog provided is not\n+        # the default one, fail\n+        if (\n+            ""catalog"" in self._properties\n+            and catalog != default_catalog\n+            and not db.allow_multi_catalog\n+        ):\n+            exceptions.append(MultiCatalogDisabledValidationError())\n+\n+        # If the DB connection does not support multi-catalog,\n+        # use the default catalog\n+        elif not db.allow_multi_catalog:\n+            catalog = self._properties[""catalog""] = default_catalog\n+\n+        # Fallback to using the previous value if not provided\n+        elif ""catalog"" not in self._properties:\n+            catalog = self._model.catalog\n+\n+        schema = (\n+            self._properties[""schema""]\n+            if ""schema"" in self._properties\n+            else self._model.schema\n+        )",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e795,apache/superset,2505910089,2078815334,Vitor-Avila,96086495,2025-05-08T03:15:35+00:00,2025-05-08T03:15:35+00:00,"That's not really needed. The `update()` logic would only update keys present in `self._properties`. There's no need to include `self._properties[""catalog""]` in this case, since its value is already what's in the object.\r\n\r\nI'm just assigning the value to the local `catalog` variable to validate uniqueness after.",true,superset/commands/dataset/update.py,114,"@@ -128,15 +106,68 @@ def validate(self) -> None:\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)\n+        database_id = self._properties.pop(""database_id"", None)\n+        catalog = self._properties.get(""catalog"")\n+        new_db_connection: Database | None = None\n+\n+        if database_id and database_id != self._model.database.id:\n+            if new_db_connection := DatasetDAO.get_database_by_id(database_id):\n+                self._properties[""database""] = new_db_connection\n+            else:\n+                exceptions.append(DatabaseNotFoundValidationError())\n+        db = new_db_connection or self._model.database\n+        default_catalog = db.get_default_catalog()\n+\n+        # If multi-catalog is disabled, and catalog provided is not\n+        # the default one, fail\n+        if (\n+            ""catalog"" in self._properties\n+            and catalog != default_catalog\n+            and not db.allow_multi_catalog\n+        ):\n+            exceptions.append(MultiCatalogDisabledValidationError())\n+\n+        # If the DB connection does not support multi-catalog,\n+        # use the default catalog\n+        elif not db.allow_multi_catalog:\n+            catalog = self._properties[""catalog""] = default_catalog\n+\n+        # Fallback to using the previous value if not provided\n+        elif ""catalog"" not in self._properties:\n+            catalog = self._model.catalog",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e796,apache/superset,2505910089,2078815620,korbit-ai[bot],131444098,2025-05-08T03:16:06+00:00,2025-05-08T03:16:06+00:00,"I see, makes sense. The local catalog variable is sufficient for validation and storing it in properties would be redundant. Thanks for clarifying.",true,superset/commands/dataset/update.py,114,"@@ -128,15 +106,68 @@ def validate(self) -> None:\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)\n+        database_id = self._properties.pop(""database_id"", None)\n+        catalog = self._properties.get(""catalog"")\n+        new_db_connection: Database | None = None\n+\n+        if database_id and database_id != self._model.database.id:\n+            if new_db_connection := DatasetDAO.get_database_by_id(database_id):\n+                self._properties[""database""] = new_db_connection\n+            else:\n+                exceptions.append(DatabaseNotFoundValidationError())\n+        db = new_db_connection or self._model.database\n+        default_catalog = db.get_default_catalog()\n+\n+        # If multi-catalog is disabled, and catalog provided is not\n+        # the default one, fail\n+        if (\n+            ""catalog"" in self._properties\n+            and catalog != default_catalog\n+            and not db.allow_multi_catalog\n+        ):\n+            exceptions.append(MultiCatalogDisabledValidationError())\n+\n+        # If the DB connection does not support multi-catalog,\n+        # use the default catalog\n+        elif not db.allow_multi_catalog:\n+            catalog = self._properties[""catalog""] = default_catalog\n+\n+        # Fallback to using the previous value if not provided\n+        elif ""catalog"" not in self._properties:\n+            catalog = self._model.catalog",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e797,apache/superset,2505910089,2078816489,Vitor-Avila,96086495,2025-05-08T03:17:47+00:00,2025-05-08T03:17:47+00:00,"I'd say that `validate_dataset_source` makes sense, as these configs appear in the **Source** tab of the dataset editor. But I'll keep this open to hear from others too :) ",true,superset/commands/dataset/update.py,85,"@@ -128,15 +106,68 @@\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)",1,1,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e798,apache/superset,2505910089,2078816947,Vitor-Avila,96086495,2025-05-08T03:18:29+00:00,2025-05-08T03:18:29+00:00,Seems to be in accordance to other `Exceptions` we have.,true,superset/commands/dataset/exceptions.py,NaN,"@@ -33,6 +33,18 @@ def get_dataset_exist_error_msg(table: Table) -> str:\n     return _(""Dataset %(table)s already exists"", table=table)\n \n \n+class MultiCatalogDisabledValidationError(ValidationError):\n+    """"""\n+    Validation error for using a non-default catalog when multi-catalog is disabled\n+    """"""",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e799,apache/superset,2505910089,2078817132,korbit-ai[bot],131444098,2025-05-08T03:18:46+00:00,2025-05-08T03:18:46+00:00,You're right - the documentation style is consistent with the other exceptions in this file. The brief docstring works in this context.,true,superset/commands/dataset/exceptions.py,NaN,"@@ -33,6 +33,18 @@ def get_dataset_exist_error_msg(table: Table) -> str:\n     return _(""Dataset %(table)s already exists"", table=table)\n \n \n+class MultiCatalogDisabledValidationError(ValidationError):\n+    """"""\n+    Validation error for using a non-default catalog when multi-catalog is disabled\n+    """"""",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e79a,apache/superset,2505910089,2078817599,Vitor-Avila,96086495,2025-05-08T03:19:31+00:00,2025-05-08T03:19:31+00:00,np!,true,superset/commands/dataset/update.py,114,"@@ -128,15 +106,68 @@ def validate(self) -> None:\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)\n+        database_id = self._properties.pop(""database_id"", None)\n+        catalog = self._properties.get(""catalog"")\n+        new_db_connection: Database | None = None\n+\n+        if database_id and database_id != self._model.database.id:\n+            if new_db_connection := DatasetDAO.get_database_by_id(database_id):\n+                self._properties[""database""] = new_db_connection\n+            else:\n+                exceptions.append(DatabaseNotFoundValidationError())\n+        db = new_db_connection or self._model.database\n+        default_catalog = db.get_default_catalog()\n+\n+        # If multi-catalog is disabled, and catalog provided is not\n+        # the default one, fail\n+        if (\n+            ""catalog"" in self._properties\n+            and catalog != default_catalog\n+            and not db.allow_multi_catalog\n+        ):\n+            exceptions.append(MultiCatalogDisabledValidationError())\n+\n+        # If the DB connection does not support multi-catalog,\n+        # use the default catalog\n+        elif not db.allow_multi_catalog:\n+            catalog = self._properties[""catalog""] = default_catalog\n+\n+        # Fallback to using the previous value if not provided\n+        elif ""catalog"" not in self._properties:\n+            catalog = self._model.catalog",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e79b,apache/superset,2505910089,2078817823,korbit-ai[bot],131444098,2025-05-08T03:19:56+00:00,2025-05-08T03:19:56+00:00,"Got it, thanks for explaining the rationale! 👍",true,superset/commands/dataset/update.py,114,"@@ -128,15 +106,68 @@ def validate(self) -> None:\n         except ValidationError as ex:\n             exceptions.append(ex)\n \n+        self._validate_dataset_source(exceptions)\n         self._validate_semantics(exceptions)\n \n         if exceptions:\n             raise DatasetInvalidError(exceptions=exceptions)\n \n-    def _validate_semantics(self, exceptions: list[ValidationError]) -> None:\n+    def _validate_dataset_source(self, exceptions: list[ValidationError]) -> None:\n         # we know we have a valid model\n         self._model = cast(SqlaTable, self._model)\n+        database_id = self._properties.pop(""database_id"", None)\n+        catalog = self._properties.get(""catalog"")\n+        new_db_connection: Database | None = None\n+\n+        if database_id and database_id != self._model.database.id:\n+            if new_db_connection := DatasetDAO.get_database_by_id(database_id):\n+                self._properties[""database""] = new_db_connection\n+            else:\n+                exceptions.append(DatabaseNotFoundValidationError())\n+        db = new_db_connection or self._model.database\n+        default_catalog = db.get_default_catalog()\n+\n+        # If multi-catalog is disabled, and catalog provided is not\n+        # the default one, fail\n+        if (\n+            ""catalog"" in self._properties\n+            and catalog != default_catalog\n+            and not db.allow_multi_catalog\n+        ):\n+            exceptions.append(MultiCatalogDisabledValidationError())\n+\n+        # If the DB connection does not support multi-catalog,\n+        # use the default catalog\n+        elif not db.allow_multi_catalog:\n+            catalog = self._properties[""catalog""] = default_catalog\n+\n+        # Fallback to using the previous value if not provided\n+        elif ""catalog"" not in self._properties:\n+            catalog = self._model.catalog",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e79c,apache/superset,2505910089,2079686796,betodealmeida,1534870,2025-05-08T13:09:11+00:00,2025-05-08T13:27:53+00:00,"What happens if we pass `null` as the catalog, wouldn't the backend also use the default catalog? Then you can just do:\r\n\r\n```js\r\ncatalog: datasource.database?.allow_multi_catalog ? datasource.catalog : null,\r\n```",true,superset-frontend/src/components/Datasource/DatasourceModal.tsx,143,"@@ -120,71 +120,83 @@ const DatasourceModal: FunctionComponent<DatasourceModalProps> = ({\n   const [isEditing, setIsEditing] = useState<boolean>(false);\n   const dialog = useRef<any>(null);\n   const [modal, contextHolder] = Modal.useModal();\n-  const buildPayload = (datasource: Record<string, any>) => ({\n-    table_name: datasource.table_name,\n-    database_id: datasource.database?.id,\n-    sql: datasource.sql,\n-    filter_select_enabled: datasource.filter_select_enabled,\n-    fetch_values_predicate: datasource.fetch_values_predicate,\n-    schema:\n-      datasource.tableSelector?.schema ||\n-      datasource.databaseSelector?.schema ||\n-      datasource.schema,\n-    description: datasource.description,\n-    main_dttm_col: datasource.main_dttm_col,\n-    normalize_columns: datasource.normalize_columns,\n-    always_filter_main_dttm: datasource.always_filter_main_dttm,\n-    offset: datasource.offset,\n-    default_endpoint: datasource.default_endpoint,\n-    cache_timeout:\n-      datasource.cache_timeout === '' ? null : datasource.cache_timeout,\n-    is_sqllab_view: datasource.is_sqllab_view,\n-    template_params: datasource.template_params,\n-    extra: datasource.extra,\n-    is_managed_externally: datasource.is_managed_externally,\n-    external_url: datasource.external_url,\n-    metrics: datasource?.metrics?.map((metric: DatasetObject['metrics'][0]) => {\n-      const metricBody: any = {\n-        expression: metric.expression,\n-        description: metric.description,\n-        metric_name: metric.metric_name,\n-        metric_type: metric.metric_type,\n-        d3format: metric.d3format || null,\n-        currency: !isDefined(metric.currency)\n-          ? null\n-          : JSON.stringify(metric.currency),\n-        verbose_name: metric.verbose_name,\n-        warning_text: metric.warning_text,\n-        uuid: metric.uuid,\n-        extra: buildExtraJsonObject(metric),\n-      };\n-      if (!Number.isNaN(Number(metric.id))) {\n-        metricBody.id = metric.id;\n-      }\n-      return metricBody;\n-    }),\n-    columns: datasource?.columns?.map(\n-      (column: DatasetObject['columns'][0]) => ({\n-        id: typeof column.id === 'number' ? column.id : undefined,\n-        column_name: column.column_name,\n-        type: column.type,\n-        advanced_data_type: column.advanced_data_type,\n-        verbose_name: column.verbose_name,\n-        description: column.description,\n-        expression: column.expression,\n-        filterable: column.filterable,\n-        groupby: column.groupby,\n-        is_active: column.is_active,\n-        is_dttm: column.is_dttm,\n-        python_date_format: column.python_date_format || null,\n-        uuid: column.uuid,\n-        extra: buildExtraJsonObject(column),\n-      }),\n-    ),\n-    owners: datasource.owners.map(\n-      (o: Record<string, number>) => o.value || o.id,\n-    ),\n-  });\n+  const buildPayload = (datasource: Record<string, any>) => {\n+    const payload: Record<string, any> = {\n+      table_name: datasource.table_name,\n+      database_id: datasource.database?.id,\n+      sql: datasource.sql,\n+      filter_select_enabled: datasource.filter_select_enabled,\n+      fetch_values_predicate: datasource.fetch_values_predicate,\n+      schema:\n+        datasource.tableSelector?.schema ||\n+        datasource.databaseSelector?.schema ||\n+        datasource.schema,\n+      description: datasource.description,\n+      main_dttm_col: datasource.main_dttm_col,\n+      normalize_columns: datasource.normalize_columns,\n+      always_filter_main_dttm: datasource.always_filter_main_dttm,\n+      offset: datasource.offset,\n+      default_endpoint: datasource.default_endpoint,\n+      cache_timeout:\n+        datasource.cache_timeout === '' ? null : datasource.cache_timeout,\n+      is_sqllab_view: datasource.is_sqllab_view,\n+      template_params: datasource.template_params,\n+      extra: datasource.extra,\n+      is_managed_externally: datasource.is_managed_externally,\n+      external_url: datasource.external_url,\n+      metrics: datasource?.metrics?.map(\n+        (metric: DatasetObject['metrics'][0]) => {\n+          const metricBody: any = {\n+            expression: metric.expression,\n+            description: metric.description,\n+            metric_name: metric.metric_name,\n+            metric_type: metric.metric_type,\n+            d3format: metric.d3format || null,\n+            currency: !isDefined(metric.currency)\n+              ? null\n+              : JSON.stringify(metric.currency),\n+            verbose_name: metric.verbose_name,\n+            warning_text: metric.warning_text,\n+            uuid: metric.uuid,\n+            extra: buildExtraJsonObject(metric),\n+          };\n+          if (!Number.isNaN(Number(metric.id))) {\n+            metricBody.id = metric.id;\n+          }\n+          return metricBody;\n+        },\n+      ),\n+      columns: datasource?.columns?.map(\n+        (column: DatasetObject['columns'][0]) => ({\n+          id: typeof column.id === 'number' ? column.id : undefined,\n+          column_name: column.column_name,\n+          type: column.type,\n+          advanced_data_type: column.advanced_data_type,\n+          verbose_name: column.verbose_name,\n+          description: column.description,\n+          expression: column.expression,\n+          filterable: column.filterable,\n+          groupby: column.groupby,\n+          is_active: column.is_active,\n+          is_dttm: column.is_dttm,\n+          python_date_format: column.python_date_format || null,\n+          uuid: column.uuid,\n+          extra: buildExtraJsonObject(column),\n+        }),\n+      ),\n+      owners: datasource.owners.map(\n+        (o: Record<string, number>) => o.value || o.id,\n+      ),\n+    };\n+    // Handle catalog based on database's allow_multi_catalog setting\n+    // If multi-catalog is disabled, don't include catalog in payload\n+    // The backend will use the default catalog\n+    // If multi-catalog is enabled, include the selected catalog\n+    if (datasource.database?.allow_multi_catalog) {\n+      payload.catalog = datasource.catalog;\n+    }",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e79d,apache/superset,2505910089,2079688003,betodealmeida,1534870,2025-05-08T13:09:53+00:00,2025-05-08T13:27:53+00:00,This would have to be updated depending on the answer to my question above.,true,superset-frontend/src/features/datasets/types.ts,4,"@@ -62,6 +62,7 @@ export type DatasetObject = {\n   filter_select_enabled?: boolean;\n   fetch_values_predicate?: string;\n   schema?: string;\n+  catalog?: string;",1,1,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e79e,apache/superset,2505910089,2079690033,betodealmeida,1534870,2025-05-08T13:11:08+00:00,2025-05-08T13:27:53+00:00,❤️,true,superset/commands/dataset/update.py,4,"@@ -14,6 +14,8 @@\n # KIND, either express or implied.  See the License for the\n # specific language governing permissions and limitations\n # under the License.\n+from __future__ import annotations",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e79f,apache/superset,2505910089,2079706349,betodealmeida,1534870,2025-05-08T13:20:32+00:00,2025-05-08T13:27:53+00:00,"Yeah, we should be able to do this, there's valid use cases.",true,superset/commands/dataset/update.py,67,"@@ -86,38 +90,12 @@ def validate(self) -> None:\n         if not self._model:\n             raise DatasetNotFoundError()\n \n-        # Check ownership\n+        # Check permission to update the dataset\n         try:\n             security_manager.raise_for_ownership(self._model)\n         except SupersetSecurityException as ex:\n             raise DatasetForbiddenError() from ex\n \n-        database_id = self._properties.get(""database"")\n-\n-        catalog = self._properties.get(""catalog"")\n-        if not catalog:\n-            catalog = self._properties[""catalog""] = (\n-                self._model.database.get_default_catalog()\n-            )\n-\n-        table = Table(\n-            self._properties.get(""table_name""),  # type: ignore\n-            self._properties.get(""schema""),\n-            catalog,\n-        )\n-\n-        # Validate uniqueness\n-        if not DatasetDAO.validate_update_uniqueness(\n-            self._model.database,\n-            table,\n-            self._model_id,\n-        ):\n-            exceptions.append(DatasetExistsValidationError(table))\n-\n-        # Validate/Populate database not allowed to change\n-        if database_id and database_id != self._model:\n-            exceptions.append(DatabaseChangeValidationError())",1,0,0,0,0,0,1,apache_superset
6824cd8e0ce5dcec7842e7a0,apache/superset,2505910089,2080079470,Vitor-Avila,96086495,2025-05-08T16:36:35+00:00,2025-05-08T16:36:35+00:00,"hmm I think that wouldn't work properly. Let's say you're remapping a dataset to a BQ connection that has `allow_multi_catalog` disabled. `get_default_catalog` for this connection would return the project ID from the service account, and then if we're passing `null` this check would fail:\r\n``` python\r\nif (\r\n    ""catalog"" in self._properties\r\n    and catalog != default_catalog\r\n    and not db.allow_multi_catalog\r\n):\r\n    exceptions.append(MultiCatalogDisabledValidationError())\r\n```\r\n\r\nI think it makes sense to have this check at the API layer as `catalog=None` is technically valid.",true,superset-frontend/src/components/Datasource/DatasourceModal.tsx,143,"@@ -120,71 +120,83 @@ const DatasourceModal: FunctionComponent<DatasourceModalProps> = ({\n   const [isEditing, setIsEditing] = useState<boolean>(false);\n   const dialog = useRef<any>(null);\n   const [modal, contextHolder] = Modal.useModal();\n-  const buildPayload = (datasource: Record<string, any>) => ({\n-    table_name: datasource.table_name,\n-    database_id: datasource.database?.id,\n-    sql: datasource.sql,\n-    filter_select_enabled: datasource.filter_select_enabled,\n-    fetch_values_predicate: datasource.fetch_values_predicate,\n-    schema:\n-      datasource.tableSelector?.schema ||\n-      datasource.databaseSelector?.schema ||\n-      datasource.schema,\n-    description: datasource.description,\n-    main_dttm_col: datasource.main_dttm_col,\n-    normalize_columns: datasource.normalize_columns,\n-    always_filter_main_dttm: datasource.always_filter_main_dttm,\n-    offset: datasource.offset,\n-    default_endpoint: datasource.default_endpoint,\n-    cache_timeout:\n-      datasource.cache_timeout === '' ? null : datasource.cache_timeout,\n-    is_sqllab_view: datasource.is_sqllab_view,\n-    template_params: datasource.template_params,\n-    extra: datasource.extra,\n-    is_managed_externally: datasource.is_managed_externally,\n-    external_url: datasource.external_url,\n-    metrics: datasource?.metrics?.map((metric: DatasetObject['metrics'][0]) => {\n-      const metricBody: any = {\n-        expression: metric.expression,\n-        description: metric.description,\n-        metric_name: metric.metric_name,\n-        metric_type: metric.metric_type,\n-        d3format: metric.d3format || null,\n-        currency: !isDefined(metric.currency)\n-          ? null\n-          : JSON.stringify(metric.currency),\n-        verbose_name: metric.verbose_name,\n-        warning_text: metric.warning_text,\n-        uuid: metric.uuid,\n-        extra: buildExtraJsonObject(metric),\n-      };\n-      if (!Number.isNaN(Number(metric.id))) {\n-        metricBody.id = metric.id;\n-      }\n-      return metricBody;\n-    }),\n-    columns: datasource?.columns?.map(\n-      (column: DatasetObject['columns'][0]) => ({\n-        id: typeof column.id === 'number' ? column.id : undefined,\n-        column_name: column.column_name,\n-        type: column.type,\n-        advanced_data_type: column.advanced_data_type,\n-        verbose_name: column.verbose_name,\n-        description: column.description,\n-        expression: column.expression,\n-        filterable: column.filterable,\n-        groupby: column.groupby,\n-        is_active: column.is_active,\n-        is_dttm: column.is_dttm,\n-        python_date_format: column.python_date_format || null,\n-        uuid: column.uuid,\n-        extra: buildExtraJsonObject(column),\n-      }),\n-    ),\n-    owners: datasource.owners.map(\n-      (o: Record<string, number>) => o.value || o.id,\n-    ),\n-  });\n+  const buildPayload = (datasource: Record<string, any>) => {\n+    const payload: Record<string, any> = {\n+      table_name: datasource.table_name,\n+      database_id: datasource.database?.id,\n+      sql: datasource.sql,\n+      filter_select_enabled: datasource.filter_select_enabled,\n+      fetch_values_predicate: datasource.fetch_values_predicate,\n+      schema:\n+        datasource.tableSelector?.schema ||\n+        datasource.databaseSelector?.schema ||\n+        datasource.schema,\n+      description: datasource.description,\n+      main_dttm_col: datasource.main_dttm_col,\n+      normalize_columns: datasource.normalize_columns,\n+      always_filter_main_dttm: datasource.always_filter_main_dttm,\n+      offset: datasource.offset,\n+      default_endpoint: datasource.default_endpoint,\n+      cache_timeout:\n+        datasource.cache_timeout === '' ? null : datasource.cache_timeout,\n+      is_sqllab_view: datasource.is_sqllab_view,\n+      template_params: datasource.template_params,\n+      extra: datasource.extra,\n+      is_managed_externally: datasource.is_managed_externally,\n+      external_url: datasource.external_url,\n+      metrics: datasource?.metrics?.map(\n+        (metric: DatasetObject['metrics'][0]) => {\n+          const metricBody: any = {\n+            expression: metric.expression,\n+            description: metric.description,\n+            metric_name: metric.metric_name,\n+            metric_type: metric.metric_type,\n+            d3format: metric.d3format || null,\n+            currency: !isDefined(metric.currency)\n+              ? null\n+              : JSON.stringify(metric.currency),\n+            verbose_name: metric.verbose_name,\n+            warning_text: metric.warning_text,\n+            uuid: metric.uuid,\n+            extra: buildExtraJsonObject(metric),\n+          };\n+          if (!Number.isNaN(Number(metric.id))) {\n+            metricBody.id = metric.id;\n+          }\n+          return metricBody;\n+        },\n+      ),\n+      columns: datasource?.columns?.map(\n+        (column: DatasetObject['columns'][0]) => ({\n+          id: typeof column.id === 'number' ? column.id : undefined,\n+          column_name: column.column_name,\n+          type: column.type,\n+          advanced_data_type: column.advanced_data_type,\n+          verbose_name: column.verbose_name,\n+          description: column.description,\n+          expression: column.expression,\n+          filterable: column.filterable,\n+          groupby: column.groupby,\n+          is_active: column.is_active,\n+          is_dttm: column.is_dttm,\n+          python_date_format: column.python_date_format || null,\n+          uuid: column.uuid,\n+          extra: buildExtraJsonObject(column),\n+        }),\n+      ),\n+      owners: datasource.owners.map(\n+        (o: Record<string, number>) => o.value || o.id,\n+      ),\n+    };\n+    // Handle catalog based on database's allow_multi_catalog setting\n+    // If multi-catalog is disabled, don't include catalog in payload\n+    // The backend will use the default catalog\n+    // If multi-catalog is enabled, include the selected catalog\n+    if (datasource.database?.allow_multi_catalog) {\n+      payload.catalog = datasource.catalog;\n+    }",0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e7a1,apache/superset,2504434481,2077751365,korbit-ai[bot],131444098,2025-05-07T14:21:30+00:00,2025-05-07T17:14:40+00:00,"### Missing Error Handling in API Registration <sub>![category Error Handling](https://img.shields.io/badge/Error%20Handling-ea580c)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nMultiple sequential API registrations are performed without error handling, which could fail silently if an API is already registered or if registration fails.\n\n###### Why this matters\nIf API registration fails, the application might continue with partially initialized views, leading to inconsistent application state and potential security vulnerabilities.\n\n###### Suggested change ∙ *Feature Preview*\n```python\ndef init_views(self) -> None:\n    try:\n        # Adds api for roles\n        appbuilder.add_api(SupersetSecurityManager.role_api)\n        appbuilder.add_api(SupersetSecurityManager.permission_api)\n        appbuilder.add_api(SupersetSecurityManager.user_api)\n        appbuilder.add_api(SupersetSecurityManager.view_menu_api)\n        appbuilder.add_api(SupersetSecurityManager.permission_view_menu_api)\n        super().init_views()\n    except Exception as e:\n        # Log the specific error with context\n        logger.error(f'Failed to initialize Superset views: {str(e)}')\n        raise RuntimeError(f'Failed to initialize Superset views: {str(e)}') from e\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5bd5e4bb-696d-42a5-af90-09990037fa97/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5bd5e4bb-696d-42a5-af90-09990037fa97?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5bd5e4bb-696d-42a5-af90-09990037fa97?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5bd5e4bb-696d-42a5-af90-09990037fa97?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/5bd5e4bb-696d-42a5-af90-09990037fa97)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:6baa02b0-587d-437c-8ba7-2ada26bd4d21 -->\n\n\n[](6baa02b0-587d-437c-8ba7-2ada26bd4d21)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e7a2,apache/superset,2504434481,2077751371,korbit-ai[bot],131444098,2025-05-07T14:21:30+00:00,2025-05-07T17:42:45+00:00,"### Unconditional exposure of sensitive security APIs <sub>![category Security](https://img.shields.io/badge/Security-e11d48)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nSecurity-sensitive APIs are unconditionally exposed despite the comment suggesting they should be optional.\n\n###### Why this matters\nExposing all security APIs by default could lead to increased attack surface and potential security vulnerabilities when these endpoints are not needed.\n\n###### Suggested change ∙ *Feature Preview*\nMove the sensitive API endpoints behind a configuration flag or environment variable:\n```python\nfrom superset.app import SupersetAppInitializer\nfrom superset.extensions import appbuilder\nfrom superset.security import SupersetSecurityManager\n\n\nclass MySupsersetAppInitializer(SupersetAppInitializer):\n    def init_views(self) -> None:\n        # Adds api for roles\n        appbuilder.add_api(SupersetSecurityManager.role_api)\n\n        # Optional APIs controlled by configuration\n        if app.config.get('ENABLE_SECURITY_APIS', False):\n            appbuilder.add_api(SupersetSecurityManager.permission_api)\n            appbuilder.add_api(SupersetSecurityManager.user_api)\n            appbuilder.add_api(SupersetSecurityManager.view_menu_api)\n            appbuilder.add_api(SupersetSecurityManager.permission_view_menu_api)\n\n        super().init_views()\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/0a040184-fed9-43f4-8c29-35f761ee8c3c/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/0a040184-fed9-43f4-8c29-35f761ee8c3c?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/0a040184-fed9-43f4-8c29-35f761ee8c3c?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/0a040184-fed9-43f4-8c29-35f761ee8c3c?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/0a040184-fed9-43f4-8c29-35f761ee8c3c)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:103bb622-4f29-4a1b-b454-996885117dd0 -->\n\n\n[](103bb622-4f29-4a1b-b454-996885117dd0)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e7a3,apache/superset,2504434481,2077751374,korbit-ai[bot],131444098,2025-05-07T14:21:30+00:00,2025-05-07T17:38:43+00:00,"### Exposed AWS Account ID <sub>![category Security](https://img.shields.io/badge/Security-e11d48)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nAWS Account ID is hardcoded directly in the script\n\n###### Why this matters\nExposing AWS Account IDs in code can aid attackers in targeting specific AWS resources and should be treated as sensitive information\n\n###### Suggested change ∙ *Feature Preview*\nMove the AWS Account ID to a secure environment variable or configuration management system:\n```bash\nAWS_ACCOUNT_ID=""${AWS_ACCOUNT_ID:?'AWS_ACCOUNT_ID environment variable is required'}""\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/f1e9c816-e1ef-4251-b9ec-786f1152d476/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/f1e9c816-e1ef-4251-b9ec-786f1152d476?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/f1e9c816-e1ef-4251-b9ec-786f1152d476?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/f1e9c816-e1ef-4251-b9ec-786f1152d476?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/f1e9c816-e1ef-4251-b9ec-786f1152d476)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:25ae2f95-2ab2-4ff4-b477-1efe326308ac -->\n\n\n[](25ae2f95-2ab2-4ff4-b477-1efe326308ac)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e7a4,apache/superset,2504434481,2077751382,korbit-ai[bot],131444098,2025-05-07T14:21:30+00:00,2025-05-07T16:54:20+00:00,"### Obsolete comment with typo <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nThe comment is obsolete since the code below it is already uncommented, and contains a typo ('ect' instead of 'etc').\n\n###### Why this matters\nObsolete comments create confusion and cognitive overhead for developers trying to understand the code's current state.\n\n###### Suggested change ∙ *Feature Preview*\nRemove the obsolete comment since the APIs are already enabled.\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d1f01190-43bc-4beb-8b15-62ec4c0c6d5e/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d1f01190-43bc-4beb-8b15-62ec4c0c6d5e?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d1f01190-43bc-4beb-8b15-62ec4c0c6d5e?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d1f01190-43bc-4beb-8b15-62ec4c0c6d5e?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/d1f01190-43bc-4beb-8b15-62ec4c0c6d5e)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:332d238a-2e62-4edd-85c9-b4576cabeba7 -->\n\n\n[](332d238a-2e62-4edd-85c9-b4576cabeba7)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e7a5,apache/superset,2504434481,2077751386,korbit-ai[bot],131444098,2025-05-07T14:21:30+00:00,2025-05-07T16:06:23+00:00,### Class name typo <sub>![category Readability](https://img.shields.io/badge/Readability-0284c7)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nClass name contains a typo ('Supserset' instead of 'Superset') which affects readability and searchability.\n\n###### Why this matters\nTypos in class names can make the code harder to find through searches and may cause confusion when referenced in documentation or logs.\n\n###### Suggested change ∙ *Feature Preview*\nRename the class to `MySuperset AppInitializer`\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/1e05c720-37c1-44db-a169-d1f8319444b3/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/1e05c720-37c1-44db-a169-d1f8319444b3?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/1e05c720-37c1-44db-a169-d1f8319444b3?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/1e05c720-37c1-44db-a169-d1f8319444b3?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/1e05c720-37c1-44db-a169-d1f8319444b3)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:e8b08625-5360-43a8-829c-ec44efef51f2 -->\n\n\n[](e8b08625-5360-43a8-829c-ec44efef51f2),false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842e7a6,apache/superset,2504434481,2077751389,korbit-ai[bot],131444098,2025-05-07T14:21:30+00:00,2025-05-07T17:02:22+00:00,### Unverified Git Source Code Pull <sub>![category Security](https://img.shields.io/badge/Security-e11d48)</sub>\n\n<details>\n  <summary>Tell me more</summary>\n\n###### What is the issue?\nUnvalidated git pull from master branch without verifying source or commit integrity\n\n###### Why this matters\nAutomatically pulling from git without verification could allow injection of malicious code if the repository is compromised\n\n###### Suggested change ∙ *Feature Preview*\nAdd commit verification and specific commit/tag pulling:\n```bash\ngit fetch origin master\ngit verify-commit HEAD\ngit reset --hard origin/master\n```\n\n\n###### Provide feedback to improve future suggestions\n[![Nice Catch](https://img.shields.io/badge/👍%20Nice%20Catch-71BC78)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7f15762e-245e-4c39-b585-b150329b1900/upvote) [![Incorrect](https://img.shields.io/badge/👎%20Incorrect-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7f15762e-245e-4c39-b585-b150329b1900?what_not_true=true)  [![Not in Scope](https://img.shields.io/badge/👎%20Out%20of%20PR%20scope-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7f15762e-245e-4c39-b585-b150329b1900?what_out_of_scope=true) [![Not in coding standard](https://img.shields.io/badge/👎%20Not%20in%20our%20standards-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7f15762e-245e-4c39-b585-b150329b1900?what_not_in_standard=true) [![Other](https://img.shields.io/badge/👎%20Other-white)](https://app.korbit.ai/feedback/aa91ff46-6083-4491-9416-b83dd1994b51/7f15762e-245e-4c39-b585-b150329b1900)\n</details>\n\n<sub>\n\n💬 Looking for more details? Reply to this comment to chat with Korbit.\n</sub>\n\n<!--- korbi internal id:014f2b94-756d-4539-9213-fe1c0dc8c8ef -->\n\n\n[](014f2b94-756d-4539-9213-fe1c0dc8c8ef),false,NaN,NaN,NaN,0,0,0,0,0,0,0,apache_superset
6824cd8e0ce5dcec7842ebf5,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,1025604159,984583867,twiecki,674200,2022-09-30T13:18:54+00:00,2022-12-29T10:30:36+00:00,"<p>I don't think you have to pass tune, and does it reuqire that many samples?</p>\n<br/>\n\n _Reply via <a href='https://app.reviewnb.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/pull/548/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='21'/>",false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebf6,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,1025604159,1058713535,miemiekurisu,3900532,2022-12-29T03:47:27+00:00,2022-12-29T03:47:27+00:00,"<p>I'm not quite sure about the sample size, </p><p>because according to the PYMC2 and PYMC3 version, it samples one chain on 20000 samples and burned the 1/4 of 20000 (5000).</p><p> </p><p>I try to keep the code minimal modified with PYMC2 and PYMC3 version, maybe ""15000 sample/ one chain/5000 tune"" is more appropriate?</p><p> </p>",false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebf7,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,1025604159,1058871221,twiecki,674200,2022-12-29T10:30:32+00:00,2022-12-29T10:30:33+00:00,<p>You can also just leave it.</p>,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebf8,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,1025604159,984583867,twiecki,674200,2022-09-30T13:18:54+00:00,2022-12-29T10:30:36+00:00,"<p>I don't think you have to pass tune, and does it reuqire that many samples?</p>\n<br/>\n\n _Reply via <a href='https://app.reviewnb.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/pull/548/discussion/'>ReviewNB</a>_ <div id='ReviewNBCommentContext-DoNotDelete' style='display:none' data-state='RESOLVED' data-cellIndex='21'/>",true,Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC_current.ipynb,1,"@@ -0,0 +1,1487 @@\n+{",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebf9,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,1025604159,1058713535,miemiekurisu,3900532,2022-12-29T03:47:27+00:00,2022-12-29T03:47:27+00:00,"<p>I'm not quite sure about the sample size, </p><p>because according to the PYMC2 and PYMC3 version, it samples one chain on 20000 samples and burned the 1/4 of 20000 (5000).</p><p> </p><p>I try to keep the code minimal modified with PYMC2 and PYMC3 version, maybe ""15000 sample/ one chain/5000 tune"" is more appropriate?</p><p> </p>",true,Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC_current.ipynb,1,"@@ -0,0 +1,1487 @@\n+{",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebfa,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,1025604159,1058871221,twiecki,674200,2022-12-29T10:30:32+00:00,2022-12-29T10:30:33+00:00,<p>You can also just leave it.</p>,true,Chapter4_TheGreatestTheoremNeverTold/Ch4_LawOfLargeNumbers_PyMC_current.ipynb,1,"@@ -0,0 +1,1487 @@\n+{",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebfb,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,220945633,223492776,CamDavidsonPilon,884032,2018-10-08T20:48:12+00:00,2018-10-08T20:48:12+00:00,👍 ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebfc,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,220945633,223492776,CamDavidsonPilon,884032,2018-10-08T20:48:12+00:00,2018-10-08T20:48:12+00:00,👍 ,true,Chapter6_Priorities/BanditsD3.html,5,"@@ -87,4 +87,4 @@\n    \n         </div>\n \n-<script type=""text/javascript"" src=""https://dl.dropbox.com/s/2qhdohtgzszp3yx/d3bandits.js""></script>\n+<script type=""text/javascript"" src=""https://cdn.rawgit.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter6_Priorities/d3bandits.js""></script>",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebfd,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,217526860,227878784,ysgit,898918,2018-10-24T17:06:31+00:00,2018-10-25T14:55:30+00:00,"the line is missing a closing `)`\r\n```suggestion\r\naxes.prop_cycle : cycler('color', ['348ABD', 'A60628', '7A68A6', '467821', 'D55E00', 'CC79A7', '56B4E9', '009E73', 'F0E442', '0072B2'])  # color cycle for plot lines\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebfe,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,217526860,228208499,michrzan,8075164,2018-10-25T14:55:53+00:00,2018-10-25T14:55:54+00:00,thanks,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ebff,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,217526860,227878784,ysgit,898918,2018-10-24T17:06:31+00:00,2018-10-25T14:55:30+00:00,"the line is missing a closing `)`\r\n```suggestion\r\naxes.prop_cycle : cycler('color', ['348ABD', 'A60628', '7A68A6', '467821', 'D55E00', 'CC79A7', '56B4E9', '009E73', 'F0E442', '0072B2'])  # color cycle for plot lines\r\n```",true,styles/matplotlibrc,NaN,"@@ -237,7 +237,7 @@ axes.labelsize      : large  # fontsize of the x any y labels\n #axes.unicode_minus  : True    # use unicode for the minus symbol\n                                # rather than hyphen.  See\n                                # http://en.wikipedia.org/wiki/Plus_and_minus_signs#Character_codes\n-axes.color_cycle    : 348ABD, A60628, 7A68A6, 467821,D55E00,  CC79A7, 56B4E9, 009E73, F0E442, 0072B2  # color cycle for plot lines\n+axes.prop_cycle : cycler('color', ['348ABD', 'A60628', '7A68A6', '467821', 'D55E00', 'CC79A7', '56B4E9', '009E73', 'F0E442', '0072B2']  # color cycle for plot lines",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec00,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,217526860,228208499,michrzan,8075164,2018-10-25T14:55:53+00:00,2018-10-25T14:55:54+00:00,thanks,true,styles/matplotlibrc,NaN,"@@ -237,7 +237,7 @@ axes.labelsize      : large  # fontsize of the x any y labels\n #axes.unicode_minus  : True    # use unicode for the minus symbol\n                                # rather than hyphen.  See\n                                # http://en.wikipedia.org/wiki/Plus_and_minus_signs#Character_codes\n-axes.color_cycle    : 348ABD, A60628, 7A68A6, 467821,D55E00,  CC79A7, 56B4E9, 009E73, F0E442, 0072B2  # color cycle for plot lines\n+axes.prop_cycle : cycler('color', ['348ABD', 'A60628', '7A68A6', '467821', 'D55E00', 'CC79A7', '56B4E9', '009E73', 'F0E442', '0072B2']  # color cycle for plot lines",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec01,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,151275956,149567242,CamDavidsonPilon,884032,2017-11-08T03:30:04+00:00,2017-11-08T15:20:43+00:00,"does this mean strictly less than 2.7, so _not_ including 2.7?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec02,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,151275956,149683118,geoffharcourt,319471,2017-11-08T14:28:02+00:00,2017-11-08T15:20:43+00:00,"Yeah, I cribbed it from a blog post about this issue elsewhere. Maybe `< 3.0` is a better idea?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec03,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,151275956,149686144,CamDavidsonPilon,884032,2017-11-08T14:38:08+00:00,2017-11-08T15:20:43+00:00,I think so yea,false,NaN,NaN,NaN,2,2,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec04,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,151275956,153050450,geoffharcourt,319471,2017-11-25T17:26:49+00:00,2017-11-25T17:26:49+00:00,@CamDavidsonPilon I think this is now sorted and OK to merge.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec05,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,151275956,149567242,CamDavidsonPilon,884032,2017-11-08T03:30:04+00:00,2017-11-08T15:20:43+00:00,"does this mean strictly less than 2.7, so _not_ including 2.7?",true,requirements.txt,NaN,"@@ -5,6 +5,6 @@ pymc==2.3.4\n pyzmq>=13.1.0\n scipy>=0.12.0\n tornado>=3.0.2\n-wsgiref>=0.1.2\n+wsgiref>=0.1.2; python_version < '2.7'",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec06,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,151275956,149683118,geoffharcourt,319471,2017-11-08T14:28:02+00:00,2017-11-08T15:20:43+00:00,"Yeah, I cribbed it from a blog post about this issue elsewhere. Maybe `< 3.0` is a better idea?",true,requirements.txt,NaN,"@@ -5,6 +5,6 @@ pymc==2.3.4\n pyzmq>=13.1.0\n scipy>=0.12.0\n tornado>=3.0.2\n-wsgiref>=0.1.2\n+wsgiref>=0.1.2; python_version < '2.7'",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec07,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,151275956,149686144,CamDavidsonPilon,884032,2017-11-08T14:38:08+00:00,2017-11-08T15:20:43+00:00,I think so yea,true,requirements.txt,NaN,"@@ -5,6 +5,6 @@ pymc==2.3.4\n pyzmq>=13.1.0\n scipy>=0.12.0\n tornado>=3.0.2\n-wsgiref>=0.1.2\n+wsgiref>=0.1.2; python_version < '2.7'",2,2,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec08,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,151275956,153050450,geoffharcourt,319471,2017-11-25T17:26:49+00:00,2017-11-25T17:26:49+00:00,@CamDavidsonPilon I think this is now sorted and OK to merge.,true,requirements.txt,NaN,"@@ -5,6 +5,6 @@ pymc==2.3.4\n pyzmq>=13.1.0\n scipy>=0.12.0\n tornado>=3.0.2\n-wsgiref>=0.1.2\n+wsgiref>=0.1.2; python_version < '2.7'",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec09,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,89302834,83344464,CamDavidsonPilon,884032,2016-10-14T01:17:18+00:00,2016-10-14T01:17:18+00:00,niiiice\n,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec0a,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,89302834,83344464,CamDavidsonPilon,884032,2016-10-14T01:17:18+00:00,2016-10-14T01:17:18+00:00,niiiice\n,true,to_latex_pdf.sh,10,"@@ -1,6 +1,4 @@\n-cd Chapter1_Introduction/ && ipython nbconvert Chapter1.ipynb --to latex --post PDF  --template article\n-cd ../Chapter2_MorePyMC/ && ipython nbconvert Chapter2.ipynb --to latex --post PDF --template article\n-cd ../Chapter3_MCMC/ && ipython nbconvert Chapter3.ipynb --to latex --post PDF --template article\n-cd ../Chapter4_TheGreatestTheoremNeverTold/ && ipython nbconvert Chapter4.ipynb --to latex --post PDF --template article\n-cd ../Chapter5_LossFunctions/ && ipython nbconvert Chapter5.ipynb --to latex --post PDF --template article\n-cd ../Chapter6_Priorities/ && ipython nbconvert Chapter6.ipynb --to latex --post PDF --template article\n+find Prologue Chapter* -name ""*.ipynb"" | grep -v ""PyMC2"" | xargs ipython3 nbconvert --to pdf --template article\n+\n+# merge all files:\n+pdfjoin Prologue.pdf Ch*.pdf DontOverfit.pdf MachineLearning.pdf",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec0b,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,19615424,16078860,cebe,189796,2014-08-11T20:43:14+00:00,2014-08-11T20:43:14+00:00,:+1: \n,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec0c,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,19615424,16078860,cebe,189796,2014-08-11T20:43:14+00:00,2014-08-11T20:43:14+00:00,:+1: \n,true,Chapter3_MCMC/IntroMCMC.ipynb,5,"@@ -992,7 +992,7 @@\n       ""\n"",\n       ""The MAP can also be used as a solution to the inference problem, as mathematically it  is the *most likely* value for the unknowns. But as mentioned earlier in this chapter,  this location ignores the uncertainty and doesn't return a distribution.\n"",\n       ""\n"",\n-      ""Typically, it is always a good idea, and rarely a bad idea, to prepend your call to `mcmc` with a call to `MAP(model).fit()`. The intermediate call to `fit` is hardly computationally intensive, and will save you time later due to a shorter burn-in period. \n"",\n+      ""Most often it is a good idea, and rarely a bad idea, to prepend your call to `mcmc` with a call to `MAP(model).fit()`. The intermediate call to `fit` is hardly computationally intensive, and will save you time later due to a shorter burn-in period. \n"",",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec0d,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,17922581,14495853,CamDavidsonPilon,884032,2014-07-03T03:37:18+00:00,2014-07-03T03:37:18+00:00,nice!\n,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec0e,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,17922581,14495853,CamDavidsonPilon,884032,2014-07-03T03:37:18+00:00,2014-07-03T03:37:18+00:00,nice!\n,true,.gitignore,4,"@@ -2,3 +2,4 @@\n *.pyc\n *~\n *.png\n+**/.ipynb_checkpoints",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec0f,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,13528700,10574781,cebe,189796,2014-03-13T17:13:41+00:00,2014-03-13T17:15:49+00:00,?\n,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec10,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,13528700,10574850,pushmatrix,190692,2014-03-13T17:14:59+00:00,2014-03-13T17:15:49+00:00,"Alas, the price of removing one is to add another.\n\nDidn't see this one.\n",false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec11,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,13528700,10574781,cebe,189796,2014-03-13T17:13:41+00:00,2014-03-13T17:15:49+00:00,?\n,true,Chapter2_MorePyMC/MorePyMC.ipynb,NaN,"@@ -8,7 +8,7 @@\n   {\n    ""cells"": [\n     {\n-     ""cell_type"": ""markdown"",\n+     ""cell_type"": ""markdown"",wi",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec12,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,13528700,10574850,pushmatrix,190692,2014-03-13T17:14:59+00:00,2014-03-13T17:15:49+00:00,"Alas, the price of removing one is to add another.\n\nDidn't see this one.\n",true,Chapter2_MorePyMC/MorePyMC.ipynb,NaN,"@@ -8,7 +8,7 @@\n   {\n    ""cells"": [\n     {\n-     ""cell_type"": ""markdown"",\n+     ""cell_type"": ""markdown"",wi",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec13,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,10889651,8401768,rolisz,426313,2013-12-17T13:43:56+00:00,2014-01-24T20:18:59+00:00,"""we talked about objective priors rarely are _objective_"" I think there is a word missing here.\n",false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec14,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,10889651,8401768,rolisz,426313,2013-12-17T13:43:56+00:00,2014-01-24T20:18:59+00:00,"""we talked about objective priors rarely are _objective_"" I think there is a word missing here.\n",true,Chapter6_Priorities/Priors.ipynb,NaN,"@@ -1393,7 +1393,17 @@\n       ""\n"",\n       ""## Jefferys Priors\n"",\n       ""\n"",\n-      ""TODO.""\n+      ""Earlier, we talked about objective priors rarely are *objective*. Partly what we mean by this is that we want a prior that doesn't bias our posterior estimates. The flat prior seems like a reasonable choice as it assigns equal probability to all values. \n"",",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec15,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,5885292,4359399,CamDavidsonPilon,884032,2013-05-23T12:36:47+00:00,2013-05-23T13:00:23+00:00,What is happening here? Was this `jQuery...` peice auto-inserted or was it your choice? \n,false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec16,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,5885292,4359625,sash-ko,220472,2013-05-23T12:49:57+00:00,2013-05-23T13:00:23+00:00,"Oh, it shouldn't be here. Probably auto-replace happened after I checked all my changes.\n",false,NaN,NaN,NaN,0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec17,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,5885292,4359399,CamDavidsonPilon,884032,2013-05-23T12:36:47+00:00,2013-05-23T13:00:23+00:00,What is happening here? Was this `jQuery...` peice auto-inserted or was it your choice? \n,true,Chapter2_MorePyMC/MorePyMC.ipynb,NaN,"@@ -200,7 +200,7 @@\n       ""\n"",\n       ""`some_variable = mc.DiscreteUniform( \""discrete_uni_var\"", 0, 4 )`\n"",\n       ""\n"",\n-      ""where 0,4 are the `DiscreteUniform`-specific upper and lower bound on the random variable. The [PyMC docs](http://pymc-devs.github.com/pymc/distributions.html) contain the specific parameters for stochastic variables. (Or use `??` if you are using IPython!)\n"",\n+      ""where 0,4 are the `DiscreteUniform`-specific upper and lower bound on the random variable. The [PyMC docs](http://pymc-devs.github.com/pymc/distributions.html) contain the specific parameters for stochastic variables. (Or use `jQuery17206780988641548902_1369305611013` if you are using IPython!)\n"",",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8e0ce5dcec7842ec18,CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers,5885292,4359625,sash-ko,220472,2013-05-23T12:49:57+00:00,2013-05-23T13:00:23+00:00,"Oh, it shouldn't be here. Probably auto-replace happened after I checked all my changes.\n",true,Chapter2_MorePyMC/MorePyMC.ipynb,NaN,"@@ -200,7 +200,7 @@\n       ""\n"",\n       ""`some_variable = mc.DiscreteUniform( \""discrete_uni_var\"", 0, 4 )`\n"",\n       ""\n"",\n-      ""where 0,4 are the `DiscreteUniform`-specific upper and lower bound on the random variable. The [PyMC docs](http://pymc-devs.github.com/pymc/distributions.html) contain the specific parameters for stochastic variables. (Or use `??` if you are using IPython!)\n"",\n+      ""where 0,4 are the `DiscreteUniform`-specific upper and lower bound on the random variable. The [PyMC docs](http://pymc-devs.github.com/pymc/distributions.html) contain the specific parameters for stochastic variables. (Or use `jQuery17206780988641548902_1369305611013` if you are using IPython!)\n"",",0,0,0,0,0,0,0,CamDavidsonPilon_Probabilistic-Programming-and-Bayesian-Methods-for-Hackers
6824cd8f0ce5dcec7842ed45,d2l-ai/d2l-en,1476022738,1295075663,astonzhang,22279212,2023-08-15T20:26:23+00:00,2023-08-15T20:26:23+00:00,please revert this for now,false,NaN,NaN,NaN,1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed46,d2l-ai/d2l-en,1476022738,1295075830,astonzhang,22279212,2023-08-15T20:26:33+00:00,2023-08-15T20:26:33+00:00,remember to change it back :),false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed47,d2l-ai/d2l-en,1476022738,1295091658,AnirudhDagar,23621655,2023-08-15T20:42:25+00:00,2023-08-15T20:42:25+00:00,yes ofc!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed48,d2l-ai/d2l-en,1476022738,1295191759,AnirudhDagar,23621655,2023-08-15T22:18:06+00:00,2023-08-15T22:18:06+00:00,done,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed49,d2l-ai/d2l-en,1476022738,1295256249,AnirudhDagar,23621655,2023-08-16T00:14:10+00:00,2023-08-16T00:14:10+00:00,done,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed4a,d2l-ai/d2l-en,1476022738,1295075663,astonzhang,22279212,2023-08-15T20:26:23+00:00,2023-08-15T20:26:23+00:00,please revert this for now,true,d2l/__init__.py,NaN,"@@ -9,4 +9,4 @@\n \n """"""\n \n-__version__ = ""1.0.0-beta0""\n+__version__ = ""1.0.0""",1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed4b,d2l-ai/d2l-en,1476022738,1295075830,astonzhang,22279212,2023-08-15T20:26:33+00:00,2023-08-15T20:26:33+00:00,remember to change it back :),true,.github/workflow_scripts/utils.sh,NaN,"@@ -1,7 +1,7 @@\n #!/bin/bash\n \n # By default, all builds are cached\n-DISABLE_CACHE=false  # Eg. 'true' or 'false'\n+DISABLE_CACHE=true  # Eg. 'true' or 'false'",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed4c,d2l-ai/d2l-en,1476022738,1295091658,AnirudhDagar,23621655,2023-08-15T20:42:25+00:00,2023-08-15T20:42:25+00:00,yes ofc!,true,.github/workflow_scripts/utils.sh,NaN,"@@ -1,7 +1,7 @@\n #!/bin/bash\n \n # By default, all builds are cached\n-DISABLE_CACHE=false  # Eg. 'true' or 'false'\n+DISABLE_CACHE=true  # Eg. 'true' or 'false'",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed4d,d2l-ai/d2l-en,1476022738,1295191759,AnirudhDagar,23621655,2023-08-15T22:18:06+00:00,2023-08-15T22:18:06+00:00,done,true,d2l/__init__.py,NaN,"@@ -9,4 +9,4 @@\n \n """"""\n \n-__version__ = ""1.0.0-beta0""\n+__version__ = ""1.0.0""",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed4e,d2l-ai/d2l-en,1476022738,1295256249,AnirudhDagar,23621655,2023-08-16T00:14:10+00:00,2023-08-16T00:14:10+00:00,done,true,.github/workflow_scripts/utils.sh,NaN,"@@ -1,7 +1,7 @@\n #!/bin/bash\n \n # By default, all builds are cached\n-DISABLE_CACHE=false  # Eg. 'true' or 'false'\n+DISABLE_CACHE=true  # Eg. 'true' or 'false'",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed4f,d2l-ai/d2l-en,1387838350,1226593981,AnirudhDagar,23621655,2023-06-12T12:30:24+00:00,2023-06-12T12:32:11+00:00,"```suggestion\r\n        return jnp.maximum(1 - d2l.abs(x), 0)\r\n\r\n```\r\n\r\nAdd an extra line to better format code.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed50,d2l-ai/d2l-en,1387838350,1226593981,AnirudhDagar,23621655,2023-06-12T12:30:24+00:00,2023-06-12T12:32:11+00:00,"```suggestion\r\n        return jnp.maximum(1 - d2l.abs(x), 0)\r\n\r\n```\r\n\r\nAdd an extra line to better format code.",true,chapter_attention-mechanisms-and-transformers/attention-pooling.md,NaN,"@@ -90,10 +90,7 @@ if tab.selected('tensorflow'):\n if tab.selected('jax'):\n     def epanechikov(x):\n         return jnp.maximum(1 - d2l.abs(x), 0)",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed51,d2l-ai/d2l-en,1360470543,1202084438,AnirudhDagar,23621655,2023-05-23T11:04:22+00:00,2023-05-23T11:04:46+00:00,```suggestion\r\nA good language model is able to predict with high-accuracy\r\nthe most probable tokens that follow a given context.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed52,d2l-ai/d2l-en,1360470543,1202335538,cx-olquinjica,50518346,2023-05-23T13:28:56+00:00,2023-05-23T13:29:48+00:00,That looks much better @AnirudhDagar ,false,NaN,NaN,NaN,1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed53,d2l-ai/d2l-en,1360470543,1202084438,AnirudhDagar,23621655,2023-05-23T11:04:22+00:00,2023-05-23T11:04:46+00:00,```suggestion\r\nA good language model is able to predict with high-accuracy\r\nthe most probable tokens that follow a given context.\r\n```,true,chapter_recurrent-neural-networks/language-model.md,NaN,"@@ -171,8 +171,8 @@ in the rest of the chapter.\n \n Next, let's discuss about how to measure the language model quality, which will be used to evaluate our models in the subsequent sections.\n One way is to check how surprising the text is.\n-A good language model is able to predict with\n-high-accuracy tokens that what we will see next.\n+A good language model is able to predict tokens with\n+high accuracy. That what we will see next.",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed54,d2l-ai/d2l-en,1360470543,1202335538,cx-olquinjica,50518346,2023-05-23T13:28:56+00:00,2023-05-23T13:29:48+00:00,That looks much better @AnirudhDagar ,true,chapter_recurrent-neural-networks/language-model.md,NaN,"@@ -171,8 +171,8 @@ in the rest of the chapter.\n \n Next, let's discuss about how to measure the language model quality, which will be used to evaluate our models in the subsequent sections.\n One way is to check how surprising the text is.\n-A good language model is able to predict with\n-high-accuracy tokens that what we will see next.\n+A good language model is able to predict tokens with\n+high accuracy. That what we will see next.",1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed55,d2l-ai/d2l-en,1241812306,1830258930,yunggiras,160676660,2024-11-06T00:56:07+00:00,2024-11-06T00:56:13+00:00,.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed56,d2l-ai/d2l-en,1241812306,1830258930,yunggiras,160676660,2024-11-06T00:56:07+00:00,2024-11-06T00:56:13+00:00,.,true,chapter_hyperparameter-optimization/hyperopt-intro.md,42,"@@ -258,7 +259,7 @@ While random search is very simple, it is the better alternative to grid\n search, which simply evaluates a fixed set of hyperparameters. Random search\n somewhat mitigates the curse of dimensionality :cite:`bellman-science66`, and\n can be far more efficient than grid search if the criterion most strongly\n-depends on a small subset of the hyperparameters.\n+depends on a small subset of the hyperparameters :cite:`bergstra-jmlr12a`.",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed57,d2l-ai/d2l-en,1223034228,1092307411,astonzhang,22279212,2023-01-31T18:08:37+00:00,2023-01-31T18:08:53+00:00,:citet:,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed58,d2l-ai/d2l-en,1223034228,1092307411,astonzhang,22279212,2023-01-31T18:08:37+00:00,2023-01-31T18:08:53+00:00,:citet:,true,chapter_hyperparameter-optimization/hyperopt-intro.md,NaN,"@@ -272,8 +273,8 @@ depends on a small subset of the hyperparameters.\n     3. Give a rough estimate of the number of floating point values you need to store during a forward pass on this graph. Hint: FashionMNIST has 60000 cases. Assume the required memory is dominated by the activations after each layer, and look up the layer widths in :numref:`sec_mlp-implementation`.\n     5. Apart from the sheer amount of compute and storage required, what other issues would gradient-based hyperparameter optimization run into? Hint: Re-read about vanishing and exploding gradients in :numref:`sec_numerical_stability`.\n     6. *Advanced*: Read :cite:`maclaurin-icml15` for an elegant (yet still somewhat unpractical) approach to gradient-based HPO.\n-3. Grid search is another HPO baseline, where we define an equi-spaced grid for each hyperparameter, then iterate over the (combinatorial) Cartesian product in order to suggest configurations.\n-    1. We stated above that random search can be much more efficient than grid search for HPO on a sizable number of hyperparameters, if the criterion most strongly depends on a small subset of the hyperparameters. Why is this? Hint: Read :cite:`bergstra2011algorithms`.\n+3. Grid search is another HPO baseline, where we define an equispaced grid for each hyperparameter, then iterate over the (combinatorial) Cartesian product in order to suggest configurations.\n+    1. We stated above that random search can be much more efficient than grid search for HPO on a sizable number of hyperparameters, if the criterion most strongly depends on a small subset of the hyperparameters. Why is this? Hint: Read :cite:`bergstra-jmlr12a`.",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed59,d2l-ai/d2l-en,1167922394,1051498850,astonzhang,22279212,2022-12-17T23:42:46+00:00,2022-12-17T23:44:30+00:00,```suggestion\r\nautomatic differentiation \r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed5a,d2l-ai/d2l-en,1167922394,1051498855,astonzhang,22279212,2022-12-17T23:43:06+00:00,2022-12-17T23:44:30+00:00,```suggestion\r\nworks backwards through this graph\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed5b,d2l-ai/d2l-en,1167922394,1051498850,astonzhang,22279212,2022-12-17T23:42:46+00:00,2022-12-17T23:44:30+00:00,```suggestion\r\nautomatic differentiation \r\n```,true,chapter_preliminaries/autograd.md,NaN,"@@ -24,13 +24,13 @@ the framework builds a *computational graph*\n that tracks how each value depends on others.\n To calculate derivatives, \n automatic differentiation packages ",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed5c,d2l-ai/d2l-en,1167922394,1051498855,astonzhang,22279212,2022-12-17T23:43:06+00:00,2022-12-17T23:44:30+00:00,```suggestion\r\nworks backwards through this graph\r\n```,true,chapter_preliminaries/autograd.md,NaN,"@@ -24,13 +24,13 @@ the framework builds a *computational graph*\n that tracks how each value depends on others.\n To calculate derivatives, \n automatic differentiation packages \n-then work backwards through this graph\n+then works backwards through this graph",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed5d,d2l-ai/d2l-en,1163182055,1047626768,astonzhang,22279212,2022-12-13T19:13:51+00:00,2022-12-13T19:15:52+00:00,"```suggestion\r\nIt provides convenient features to handle neural networks. For example, it handles the model parameters, provides the `nn.compact` decorator to simplify code, invokes the `__call__` method among other things.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed5e,d2l-ai/d2l-en,1163182055,1047627131,astonzhang,22279212,2022-12-13T19:14:12+00:00,2022-12-13T19:15:52+00:00,```suggestion\r\nHere we also redirect `__call__` to the `forward` method. We do this to make our code more similar to other framework implementations.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed5f,d2l-ai/d2l-en,1163182055,1047628044,astonzhang,22279212,2022-12-13T19:15:03+00:00,2022-12-13T19:15:52+00:00,```suggestion\r\n            # normalization section\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed60,d2l-ai/d2l-en,1163182055,1047626768,astonzhang,22279212,2022-12-13T19:13:51+00:00,2022-12-13T19:15:52+00:00,"```suggestion\r\nIt provides convenient features to handle neural networks. For example, it handles the model parameters, provides the `nn.compact` decorator to simplify code, invokes the `__call__` method among other things.\r\n```",true,chapter_linear-regression/oo-design.md,NaN,"@@ -292,6 +292,12 @@ You may notice that `Module` is a subclass of `tf.keras.Model`, the base class o\n It provides convenient features to handle neural networks. For example, it invokes the `call` method in the built-in `__call__` method. Here we redirect `call` to the `forward` function, saving its arguments as a class attribute. We do this to make our code more similar to other framework implementations.\n :end_tab:\n \n+:begin_tab:`jax`\n+You may notice that `Module` is a subclass of `linen.Module`, the base class of neural networks in Flax.\n+It provides convenient features to handle neural networks. For example, it handles the model parameters, provides `nn.compact` decorator to simplify code, invokes the `__call__` method among other things.",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed61,d2l-ai/d2l-en,1163182055,1047627131,astonzhang,22279212,2022-12-13T19:14:12+00:00,2022-12-13T19:15:52+00:00,```suggestion\r\nHere we also redirect `__call__` to the `forward` method. We do this to make our code more similar to other framework implementations.\r\n```,true,chapter_linear-regression/oo-design.md,NaN,"@@ -292,6 +292,12 @@ You may notice that `Module` is a subclass of `tf.keras.Model`, the base class o\n It provides convenient features to handle neural networks. For example, it invokes the `call` method in the built-in `__call__` method. Here we redirect `call` to the `forward` function, saving its arguments as a class attribute. We do this to make our code more similar to other framework implementations.\n :end_tab:\n \n+:begin_tab:`jax`\n+You may notice that `Module` is a subclass of `linen.Module`, the base class of neural networks in Flax.\n+It provides convenient features to handle neural networks. For example, it handles the model parameters, provides `nn.compact` decorator to simplify code, invokes the `__call__` method among other things.\n+Here we also redirect `__call__` to the `forward` function. We do this to make our code more similar to other framework implementations.",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed62,d2l-ai/d2l-en,1163182055,1047628044,astonzhang,22279212,2022-12-13T19:15:03+00:00,2022-12-13T19:15:52+00:00,```suggestion\r\n            # normalization section\r\n```,true,chapter_linear-regression/oo-design.md,NaN,"@@ -369,13 +388,17 @@ class Trainer(d2l.HyperParameters):  #@save\n             else:\n                 batch_stats = {}\n \n-            # Flax uses optax under the hood for a single state obj TrainState\n-            # (more will be discussed later in the batch normalization section)\n+            # Flax uses optax under the hood for a single state obj TrainState.\n+            # More will be discussed later in the dropout and batch\n+            # normalization section.",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed63,d2l-ai/d2l-en,1158259505,1045189590,astonzhang,22279212,2022-12-11T08:08:29+00:00,2022-12-11T08:08:30+00:00,"```suggestion\r\n$||x-x_1||$, then the function values will be highly correlated. We can visualize the process of determining $f(x)$ from $f(x_1)$ both in the space of functions, and in the joint distribution over $f(x_1), f(x)$. Let's initially consider an $x$ such that $k(x,x_1) = 0.9$, and $k(x,x)=1$, meaning that the value of $f(x)$ is moderately correlated with the value of $f(x_1)$. In the joint distribution, the contours of constant probability will be relatively narrow ellipses.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed64,d2l-ai/d2l-en,1158259505,1045189590,astonzhang,22279212,2022-12-11T08:08:29+00:00,2022-12-11T08:08:30+00:00,"```suggestion\r\n$||x-x_1||$, then the function values will be highly correlated. We can visualize the process of determining $f(x)$ from $f(x_1)$ both in the space of functions, and in the joint distribution over $f(x_1), f(x)$. Let's initially consider an $x$ such that $k(x,x_1) = 0.9$, and $k(x,x)=1$, meaning that the value of $f(x)$ is moderately correlated with the value of $f(x_1)$. In the joint distribution, the contours of constant probability will be relatively narrow ellipses.\r\n```",true,chapter_gaussian-processes/gp-intro.md,NaN,"@@ -124,17 +126,28 @@ The off-diagonal expression $k(x,x_1) = k(x_1,x)$\n tells us how correlated the function values will be --- how strongly determined $f(x)$\n will be from $f(x_1)$. \n We've seen already that if we use a large length-scale, relative to the distance between $x$ and $x_1$, \n-$||x-x_1||$, then the function values will be highly correlated. We can visualize the process of determining $f(x)$ from $f(x_1)$ both in the space of functions, and in the joint distribution over $f(x_1), f(x)$. Let's initially consider an $x$ such that $k(x,x_1) = 0.7$, and $k(x,x)=1$, meaning that the value of $f(x)$ is moderately correlated with the value of $f(x_1)$. In the joint distribution, the contours of constant probability will be relatively narrow ellipses.\n+$||x-x_1||$, then the function values will be highly correlated. We can visualize the process of determining $f(x)$ from $f(x_1)$ both in the space of functions, and in the joint distribution over $f(x_1), f(x)$. Let's initially consider an $x$ such that $k(x,x_1) = 0.$9, and $k(x,x)=1$, meaning that the value of $f(x)$ is moderately correlated with the value of $f(x_1)$. In the joint distribution, the contours of constant probability will be relatively narrow ellipses.",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed65,d2l-ai/d2l-en,1155858064,1044661642,astonzhang,22279212,2022-12-09T17:17:29+00:00,2022-12-09T17:17:29+00:00,```suggestion\r\n[Discussions](https://discuss.d2l.ai/t/12092)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed66,d2l-ai/d2l-en,1155858064,1044661870,astonzhang,22279212,2022-12-09T17:17:47+00:00,2022-12-09T17:17:47+00:00,```suggestion\r\n[Discussions](https://discuss.d2l.ai/t/12093)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed67,d2l-ai/d2l-en,1155858064,1044662265,astonzhang,22279212,2022-12-09T17:18:19+00:00,2022-12-09T17:18:20+00:00,Need this ID\r\n\r\n```suggestion\r\n[Discussions](https://discuss.d2l.ai/t/???)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed68,d2l-ai/d2l-en,1155858064,1044662403,astonzhang,22279212,2022-12-09T17:18:31+00:00,2022-12-09T17:18:31+00:00,```suggestion\r\n[Discussions](https://discuss.d2l.ai/t/12094)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed69,d2l-ai/d2l-en,1155858064,1044661642,astonzhang,22279212,2022-12-09T17:17:29+00:00,2022-12-09T17:17:29+00:00,```suggestion\r\n[Discussions](https://discuss.d2l.ai/t/12092)\r\n```,true,chapter_hyperparameter_optimization/hyperopt-api.md,NaN,"@@ -253,10 +253,17 @@ quickly outperforms random search afterwards.\n ![Example any-time performance plot to compare two algorithms A and B.](../img/example_anytime_performance.svg)\n :label:`example_anytime_performance`\n \n-\n ## Summary\n \n This section laid out a simple, yet flexible interface to implement various HPO\n algorithms that we will look at in this chapter. Similar interfaces can be found\n in popular open-source HPO frameworks. We also looked at how we can compare HPO\n algorithms, and potential pitfall one needs to be aware. \n+\n+## Exercises\n+\n+\n+\n+:begin_tab:`pytorch`\n+[Discussions](https://discuss.d2l.ai/t/hyperparameter-optimization-api/12092)",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed6a,d2l-ai/d2l-en,1155858064,1044661870,astonzhang,22279212,2022-12-09T17:17:47+00:00,2022-12-09T17:17:47+00:00,```suggestion\r\n[Discussions](https://discuss.d2l.ai/t/12093)\r\n```,true,chapter_hyperparameter_optimization/rs-async.md,NaN,"@@ -217,3 +218,8 @@ become available, and, hence, ensures that all workers are busy at any point in\n time. While random search is easy to distribute asynchronously and does not\n require any change of the actual algorithm, other methods require some additional\n modifications.\n+\n+\n+:begin_tab:`pytorch`\n+[Discussions](https://discuss.d2l.ai/t/asynchronous-random-search/12093)",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed6b,d2l-ai/d2l-en,1155858064,1044662265,astonzhang,22279212,2022-12-09T17:18:19+00:00,2022-12-09T17:18:20+00:00,Need this ID\r\n\r\n```suggestion\r\n[Discussions](https://discuss.d2l.ai/t/???)\r\n```,true,chapter_hyperparameter_optimization/sh-async.md,NaN,"@@ -229,3 +231,8 @@ configurations as quickly as possible to the next rung level, even if this means\n promoting some wrong ones. In practice, this usually does not hurt much, and the\n gains of asynchronous versus synchronous scheduling are usually much higher\n than the loss of the suboptimal decision making.\n+\n+\n+:begin_tab:`pytorch`\n+[Discussions](https://discuss.d2l.ai/t/asynchronous-successive-halving/???)",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed6c,d2l-ai/d2l-en,1155858064,1044662403,astonzhang,22279212,2022-12-09T17:18:31+00:00,2022-12-09T17:18:31+00:00,```suggestion\r\n[Discussions](https://discuss.d2l.ai/t/12094)\r\n```,true,chapter_hyperparameter_optimization/sh-intro.md,NaN,"@@ -262,3 +262,8 @@ computation of the HPO instead of just reducing the wall-clock time.\n \n We implemented and evaluated successive halving, a simple yet efficient\n multi-fidelity HPO algorithm.\n+\n+\n+:begin_tab:`pytorch`\n+[Discussions](https://discuss.d2l.ai/t/multi-fidelity-hyperparameter-optimization/12094)",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed6d,d2l-ai/d2l-en,1154799550,1043886750,astonzhang,22279212,2022-12-08T22:33:35+00:00,2022-12-08T22:33:36+00:00,```suggestion\r\nfrom d2l import jax as d2l  # Use Jax as the backend\r\n```,false,NaN,NaN,NaN,1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed6e,d2l-ai/d2l-en,1154799550,1043892846,astonzhang,22279212,2022-12-08T22:43:29+00:00,2022-12-08T22:43:30+00:00,"```suggestion\r\n    # Each column/row corresponds to each query/key\r\n    k = d2l.astype(kernel(dists), d2l.float32)\r\n```",false,NaN,NaN,NaN,1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed6f,d2l-ai/d2l-en,1154799550,1043894427,astonzhang,22279212,2022-12-08T22:46:15+00:00,2022-12-08T22:46:15+00:00,"```suggestion\r\nx = jax.random.normal(key1, (10,))  # Dummy input\r\n```",false,NaN,NaN,NaN,1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed70,d2l-ai/d2l-en,1154799550,1043894507,astonzhang,22279212,2022-12-08T22:46:23+00:00,2022-12-08T22:46:23+00:00,"```suggestion\r\nparams = net.init(key2, x)  # Initialization call\r\n```",false,NaN,NaN,NaN,1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed71,d2l-ai/d2l-en,1154799550,1043912799,astonzhang,22279212,2022-12-08T23:20:08+00:00,2022-12-08T23:20:09+00:00,```suggestion\r\n# Any call of a random function in JAX requires a key to be \r\n```,false,NaN,NaN,NaN,1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed72,d2l-ai/d2l-en,1154799550,1043913160,astonzhang,22279212,2022-12-08T23:20:57+00:00,2022-12-08T23:20:57+00:00,```suggestion\r\n# JAX arrays are immutable. `jax.numpy.ndarray.at` index\r\n# update operators create a new array with the corresponding\r\n# modifications made\r\n```,false,NaN,NaN,NaN,1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed73,d2l-ai/d2l-en,1154799550,1043886750,astonzhang,22279212,2022-12-08T22:33:35+00:00,2022-12-08T22:33:36+00:00,```suggestion\r\nfrom d2l import jax as d2l  # Use Jax as the backend\r\n```,true,d2l/__init__.py,NaN,"@@ -5,6 +5,7 @@\n from d2l import mxnet as d2l  # Use MXNet as the backend\n from d2l import torch as d2l  # Use PyTorch as the backend\n from d2l import tensorflow as d2l  # Use TensorFlow as the backend\n+from d2l import jax as d2l # Use Jax as the backend",1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed74,d2l-ai/d2l-en,1154799550,1043892846,astonzhang,22279212,2022-12-08T22:43:29+00:00,2022-12-08T22:43:30+00:00,"```suggestion\r\n    # Each column/row corresponds to each query/key\r\n    k = d2l.astype(kernel(dists), d2l.float32)\r\n```",true,chapter_attention-mechanisms-and-transformers/attention-pooling.md,NaN,"@@ -122,26 +138,32 @@ Recall attention pooling in :eqref:`eq_attention_pooling`. Let each validation f\n %%tab all\n def nadaraya_watson(x_train, y_train, x_val, kernel):\n     dists = d2l.reshape(x_train, (-1, 1)) - d2l.reshape(x_val, (1, -1))\n-    k = kernel(dists)  # Each column/row corresponds to each query/key\n-    attention_w = k / k.sum(0)  # Normalization over keys for each query\n+    k = d2l.astype(kernel(dists), d2l.float32)  # Each column/row corresponds to each query/key",1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed75,d2l-ai/d2l-en,1154799550,1043894427,astonzhang,22279212,2022-12-08T22:46:15+00:00,2022-12-08T22:46:15+00:00,"```suggestion\r\nx = jax.random.normal(key1, (10,))  # Dummy input\r\n```",true,chapter_builders-guide/use-gpu.md,NaN,"@@ -434,17 +508,31 @@ with strategy.scope():\n         tf.keras.layers.Dense(1)])\n ```\n \n+```{.python .input}\n+%%tab jax\n+net = nn.Sequential([nn.Dense(1)])\n+\n+key1, key2 = jax.random.split(jax.random.PRNGKey(0))\n+x = jax.random.normal(key1, (10,)) # Dummy input",1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed76,d2l-ai/d2l-en,1154799550,1043894507,astonzhang,22279212,2022-12-08T22:46:23+00:00,2022-12-08T22:46:23+00:00,"```suggestion\r\nparams = net.init(key2, x)  # Initialization call\r\n```",true,chapter_builders-guide/use-gpu.md,NaN,"@@ -434,17 +508,31 @@ with strategy.scope():\n         tf.keras.layers.Dense(1)])\n ```\n \n+```{.python .input}\n+%%tab jax\n+net = nn.Sequential([nn.Dense(1)])\n+\n+key1, key2 = jax.random.split(jax.random.PRNGKey(0))\n+x = jax.random.normal(key1, (10,)) # Dummy input\n+params = net.init(key2, x) # Initialization call",1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed77,d2l-ai/d2l-en,1154799550,1043912799,astonzhang,22279212,2022-12-08T23:20:08+00:00,2022-12-08T23:20:09+00:00,```suggestion\r\n# Any call of a random function in JAX requires a key to be \r\n```,true,chapter_preliminaries/ndarray.md,NaN,"@@ -295,6 +317,14 @@ torch.randn(3, 4)\n tf.random.normal(shape=[3, 4])\n ```\n \n+```{.python .input}\n+%%tab jax\n+# any call of a random function in JAX requires a key to be ",1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed78,d2l-ai/d2l-en,1154799550,1043913160,astonzhang,22279212,2022-12-08T23:20:57+00:00,2022-12-08T23:20:57+00:00,```suggestion\r\n# JAX arrays are immutable. `jax.numpy.ndarray.at` index\r\n# update operators create a new array with the corresponding\r\n# modifications made\r\n```,true,chapter_preliminaries/ndarray.md,NaN,"@@ -369,6 +404,15 @@ X_var[1, 2].assign(9)\n X_var\n ```\n \n+```{.python .input}\n+%%tab jax\n+# JAX arrays are immutable\n+# `jax.numpy.ndarray.at` index update operators create\n+# a new array with the corresponding modifications made",1,1,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed79,d2l-ai/d2l-en,1154171499,1043421380,AnirudhDagar,23621655,2022-12-08T14:32:44+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed7a,d2l-ai/d2l-en,1154171499,1043421628,AnirudhDagar,23621655,2022-12-08T14:32:57+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed7b,d2l-ai/d2l-en,1154171499,1043422991,AnirudhDagar,23621655,2022-12-08T14:34:14+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed7c,d2l-ai/d2l-en,1154171499,1043423801,AnirudhDagar,23621655,2022-12-08T14:34:56+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed7d,d2l-ai/d2l-en,1154171499,1043425443,AnirudhDagar,23621655,2022-12-08T14:36:22+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed7e,d2l-ai/d2l-en,1154171499,1043427052,AnirudhDagar,23621655,2022-12-08T14:37:48+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed7f,d2l-ai/d2l-en,1154171499,1043427255,AnirudhDagar,23621655,2022-12-08T14:37:59+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed80,d2l-ai/d2l-en,1154171499,1043434700,AnirudhDagar,23621655,2022-12-08T14:44:40+00:00,2022-12-08T14:44:41+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed81,d2l-ai/d2l-en,1154171499,1043435224,AnirudhDagar,23621655,2022-12-08T14:45:09+00:00,2022-12-08T14:45:10+00:00,@astonzhang changes made to this file.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed82,d2l-ai/d2l-en,1154171499,1043421380,AnirudhDagar,23621655,2022-12-08T14:32:44+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,true,chapter_attention-mechanisms-and-transformers/attention-pooling.md,2,"@@ -1,21 +1,22 @@\n-# Attention Pooling\n+# Attention Pooling by Similarity",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed83,d2l-ai/d2l-en,1154171499,1043421628,AnirudhDagar,23621655,2022-12-08T14:32:57+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,true,chapter_attention-mechanisms-and-transformers/attention-cues.md,1,"@@ -1,255 +0,0 @@\n-```{.python .input}",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed84,d2l-ai/d2l-en,1154171499,1043422991,AnirudhDagar,23621655,2022-12-08T14:34:14+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,true,chapter_attention-mechanisms-and-transformers/queries-keys-values.md,1,"@@ -0,0 +1,138 @@\n+```{.python .input  n=1}",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed85,d2l-ai/d2l-en,1154171499,1043423801,AnirudhDagar,23621655,2022-12-08T14:34:56+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,true,chapter_convolutional-modern/cnn-design.md,1,"@@ -6,140 +6,52 @@ tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])\n # Designing Convolution Network Architectures",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed86,d2l-ai/d2l-en,1154171499,1043425443,AnirudhDagar,23621655,2022-12-08T14:36:22+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,true,chapter_multilayer-perceptrons/mlp-implementation.md,4,"@@ -65,6 +65,27 @@ one weight matrix and one bias vector.\n As always, we allocate memory\n for the gradients of the loss with respect to these parameters.\n \n+:begin_tab:`mxnet`",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed87,d2l-ai/d2l-en,1154171499,1043427052,AnirudhDagar,23621655,2022-12-08T14:37:48+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,true,chapter_recurrent-modern/gru.md,40,"@@ -236,18 +236,27 @@ except that the update equations are more complex.\n %%tab pytorch, mxnet, tensorflow\n @d2l.add_to_class(GRUScratch)\n def forward(self, inputs, H=None):\n+    if H is None:",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed88,d2l-ai/d2l-en,1154171499,1043427255,AnirudhDagar,23621655,2022-12-08T14:37:59+00:00,2022-12-08T14:40:55+00:00,@astonzhang changes made to this file.,true,chapter_recurrent-modern/lstm.md,5,"@@ -330,13 +330,27 @@ returns the final state and the stacked outputs as expected.\n %%tab pytorch, mxnet, tensorflow\n @d2l.add_to_class(LSTMScratch)\n def forward(self, inputs, H_C=None):\n-    H, C = None, None if H_C is None else H_C\n+    if H_C is None:",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed89,d2l-ai/d2l-en,1154171499,1043434700,AnirudhDagar,23621655,2022-12-08T14:44:40+00:00,2022-12-08T14:44:41+00:00,@astonzhang changes made to this file.,true,static/build.yml,6,"@@ -8,8 +8,8 @@ dependencies:\n     - -f https://download.pytorch.org/whl/torch_stable.html\n     - torchvision==0.13.0+cu102\n     - -f https://download.pytorch.org/whl/torch_stable.html\n-    - tensorflow==2.9.1\n-    - tensorflow-probability==0.17.0\n+    - tensorflow==2.11.0",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed8a,d2l-ai/d2l-en,1154171499,1043435224,AnirudhDagar,23621655,2022-12-08T14:45:09+00:00,2022-12-08T14:45:10+00:00,@astonzhang changes made to this file.,true,chapter_recurrent-modern/bi-rnn.md,14,"@@ -118,7 +118,7 @@ from d2l import jax as d2l\n from jax import numpy as jnp\n ```\n \n-### Implementation from Scratch\n+## Implementation from Scratch",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed8b,d2l-ai/d2l-en,1152931073,1042902074,astonzhang,22279212,2022-12-08T04:03:12+00:00,2022-12-08T04:06:15+00:00,redundant?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed8c,d2l-ai/d2l-en,1152931073,1042902800,astonzhang,22279212,2022-12-08T04:05:13+00:00,2022-12-08T04:06:15+00:00,redundant?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed8d,d2l-ai/d2l-en,1152931073,1042902916,astonzhang,22279212,2022-12-08T04:05:38+00:00,2022-12-08T04:06:15+00:00,tab all? tab.selected('jax') is used below,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed8e,d2l-ai/d2l-en,1152931073,1043179000,AnirudhDagar,23621655,2022-12-08T10:17:34+00:00,2022-12-08T10:17:34+00:00,What is redundant?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed8f,d2l-ai/d2l-en,1152931073,1043185724,AnirudhDagar,23621655,2022-12-08T10:21:22+00:00,2022-12-08T10:21:22+00:00,"Ohh, i see, somehow a couple of tabs were copied. Turns out the final commit doesn't include the final state of the saved notebook on my local machine. Sorry my bad.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed90,d2l-ai/d2l-en,1152931073,1042902074,astonzhang,22279212,2022-12-08T04:03:12+00:00,2022-12-08T04:06:15+00:00,redundant?,true,chapter_recurrent-modern/seq2seq.md,NaN,"@@ -453,11 +527,35 @@ where the last dimension of the tensor stores the predicted token distribution.\n ```{.python .input}\n %%tab all\n decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n-state = decoder.init_state(encoder(X))\n-outputs, state = decoder(X, state)\n+if tab.selected('mxnet', 'pytorch', 'tensorflow'):\n+    state = decoder.init_state(encoder(X))\n+    outputs, state = decoder(X, state)\n+if tab.selected('jax'):\n+    state = decoder.init_state(encoder.init_with_output(d2l.get_key(), X)[0])\n+    (outputs, state), _ = decoder.init_with_output(d2l.get_key(), X, state)\n+\n \n d2l.check_shape(outputs, (batch_size, num_steps, vocab_size))\n-if tab.selected('mxnet', 'pytorch'):\n+if tab.selected('mxnet', 'pytorch', 'jax'):\n+    d2l.check_shape(state, (num_layers, batch_size, num_hiddens))\n+if tab.selected('tensorflow'):\n+    d2l.check_len(state, num_layers)\n+    d2l.check_shape(state[0], (batch_size, num_hiddens))\n+```\n+\n+```{.python .input}\n+%%tab all\n+decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n+if tab.selected('mxnet', 'pytorch', 'tensorflow'):\n+    state = decoder.init_state(encoder(X))\n+    outputs, state = decoder(X, state)\n+if tab.selected('jax'):\n+    state = decoder.init_state(encoder.init_with_output(d2l.get_key(), X)[0])\n+    (outputs, state), _ = decoder.init_with_output(d2l.get_key(), X, state)\n+\n+\n+d2l.check_shape(outputs, (batch_size, num_steps, vocab_size))\n+if tab.selected('mxnet', 'pytorch', 'jax'):",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed91,d2l-ai/d2l-en,1152931073,1042902800,astonzhang,22279212,2022-12-08T04:05:13+00:00,2022-12-08T04:06:15+00:00,redundant?,true,chapter_recurrent-modern/seq2seq.md,NaN,"@@ -522,32 +637,75 @@ of any irrelevant prediction\n with zero equals to zero.\n \n ```{.python .input}\n-%%tab all\n+%%tab pytorch, mxnet, tensorflow\n @d2l.add_to_class(Seq2Seq)\n def loss(self, Y_hat, Y):\n     l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)\n     mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)\n     return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask)\n ```\n \n+```{.python .input}\n+%%tab jax\n+@d2l.add_to_class(Seq2Seq)\n+@partial(jax.jit, static_argnums=(0, 5))\n+def loss(self, params, X, Y, state, averaged=False):\n+    Y_hat = state.apply_fn({'params': params}, *X,\n+                           rngs={'dropout': jax.random.PRNGKey(0)})\n+    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n+    Y = d2l.reshape(Y, (-1,))\n+    fn = optax.softmax_cross_entropy_with_integer_labels\n+    l = fn(Y_hat, Y)\n+    mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)\n+    return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask), {}\n+```\n+\n ## [**Training**]\n :label:`sec_seq2seq_training`\n \n Now we can [**create and train an RNN encoder-decoder model**]\n for sequence to sequence learning on the machine translation dataset.\n \n ```{.python .input}\n-%%tab all\n+%%tab pytorch, tensorflow, mxnet\n+data = d2l.MTFraEng(batch_size=128, num_steps=10)\n+embed_size, num_hiddens, num_layers, dropout = 128, 128, 2, 0.2\n+if tab.selected('mxnet', 'pytorch', 'jax'):\n+    encoder = Seq2SeqEncoder(\n+        len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n+    decoder = Seq2SeqDecoder(\n+        len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n+    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n+                    lr=0.01)\n+    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\n+if tab.selected('tensorflow'):\n+    with d2l.try_gpu():\n+        encoder = Seq2SeqEncoder(\n+            len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n+        decoder = Seq2SeqDecoder(\n+            len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n+        model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n+                        lr=0.001)\n+    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1)\n+trainer.fit(model, data)\n+```\n+\n+```{.python .input}\n+%%tab jax\n data = d2l.MTFraEng(batch_size=128) \n embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2\n-if tab.selected('mxnet', 'pytorch'):\n+if tab.selected('mxnet', 'pytorch', 'jax'):\n     encoder = Seq2SeqEncoder(\n         len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)\n     decoder = Seq2SeqDecoder(\n         len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n+if tab.selected('mxnet', 'pytorch'):\n     model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n                     lr=0.001)\n-    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\n+if tab.selected('jax'):\n+    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],\n+                    lr=0.001, training=True)\n+    trainer = d2l.Trainer(max_epochs=70, gradient_clip_val=1, num_gpus=1)",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed92,d2l-ai/d2l-en,1152931073,1042902916,astonzhang,22279212,2022-12-08T04:05:38+00:00,2022-12-08T04:06:15+00:00,tab all? tab.selected('jax') is used below,true,chapter_recurrent-modern/seq2seq.md,NaN,"@@ -522,32 +637,75 @@ of any irrelevant prediction\n with zero equals to zero.\n \n ```{.python .input}\n-%%tab all\n+%%tab pytorch, mxnet, tensorflow\n @d2l.add_to_class(Seq2Seq)\n def loss(self, Y_hat, Y):\n     l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)\n     mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)\n     return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask)\n ```\n \n+```{.python .input}\n+%%tab jax\n+@d2l.add_to_class(Seq2Seq)\n+@partial(jax.jit, static_argnums=(0, 5))\n+def loss(self, params, X, Y, state, averaged=False):\n+    Y_hat = state.apply_fn({'params': params}, *X,\n+                           rngs={'dropout': jax.random.PRNGKey(0)})\n+    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n+    Y = d2l.reshape(Y, (-1,))\n+    fn = optax.softmax_cross_entropy_with_integer_labels\n+    l = fn(Y_hat, Y)\n+    mask = d2l.astype(d2l.reshape(Y, -1) != self.tgt_pad, d2l.float32)\n+    return d2l.reduce_sum(l * mask) / d2l.reduce_sum(mask), {}\n+```\n+\n ## [**Training**]\n :label:`sec_seq2seq_training`\n \n Now we can [**create and train an RNN encoder-decoder model**]\n for sequence to sequence learning on the machine translation dataset.\n \n ```{.python .input}\n-%%tab all\n+%%tab pytorch, tensorflow, mxnet",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed93,d2l-ai/d2l-en,1152931073,1043179000,AnirudhDagar,23621655,2022-12-08T10:17:34+00:00,2022-12-08T10:17:34+00:00,What is redundant?,true,chapter_recurrent-modern/seq2seq.md,NaN,"@@ -453,11 +527,35 @@ where the last dimension of the tensor stores the predicted token distribution.\n ```{.python .input}\n %%tab all\n decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n-state = decoder.init_state(encoder(X))\n-outputs, state = decoder(X, state)\n+if tab.selected('mxnet', 'pytorch', 'tensorflow'):\n+    state = decoder.init_state(encoder(X))\n+    outputs, state = decoder(X, state)\n+if tab.selected('jax'):\n+    state = decoder.init_state(encoder.init_with_output(d2l.get_key(), X)[0])\n+    (outputs, state), _ = decoder.init_with_output(d2l.get_key(), X, state)\n+\n \n d2l.check_shape(outputs, (batch_size, num_steps, vocab_size))\n-if tab.selected('mxnet', 'pytorch'):\n+if tab.selected('mxnet', 'pytorch', 'jax'):\n+    d2l.check_shape(state, (num_layers, batch_size, num_hiddens))\n+if tab.selected('tensorflow'):\n+    d2l.check_len(state, num_layers)\n+    d2l.check_shape(state[0], (batch_size, num_hiddens))\n+```\n+\n+```{.python .input}\n+%%tab all\n+decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n+if tab.selected('mxnet', 'pytorch', 'tensorflow'):\n+    state = decoder.init_state(encoder(X))\n+    outputs, state = decoder(X, state)\n+if tab.selected('jax'):\n+    state = decoder.init_state(encoder.init_with_output(d2l.get_key(), X)[0])\n+    (outputs, state), _ = decoder.init_with_output(d2l.get_key(), X, state)\n+\n+\n+d2l.check_shape(outputs, (batch_size, num_steps, vocab_size))\n+if tab.selected('mxnet', 'pytorch', 'jax'):",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed94,d2l-ai/d2l-en,1152931073,1043185724,AnirudhDagar,23621655,2022-12-08T10:21:22+00:00,2022-12-08T10:21:22+00:00,"Ohh, i see, somehow a couple of tabs were copied. Turns out the final commit doesn't include the final state of the saved notebook on my local machine. Sorry my bad.",true,chapter_recurrent-modern/seq2seq.md,NaN,"@@ -453,11 +527,35 @@ where the last dimension of the tensor stores the predicted token distribution.\n ```{.python .input}\n %%tab all\n decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n-state = decoder.init_state(encoder(X))\n-outputs, state = decoder(X, state)\n+if tab.selected('mxnet', 'pytorch', 'tensorflow'):\n+    state = decoder.init_state(encoder(X))\n+    outputs, state = decoder(X, state)\n+if tab.selected('jax'):\n+    state = decoder.init_state(encoder.init_with_output(d2l.get_key(), X)[0])\n+    (outputs, state), _ = decoder.init_with_output(d2l.get_key(), X, state)\n+\n \n d2l.check_shape(outputs, (batch_size, num_steps, vocab_size))\n-if tab.selected('mxnet', 'pytorch'):\n+if tab.selected('mxnet', 'pytorch', 'jax'):\n+    d2l.check_shape(state, (num_layers, batch_size, num_hiddens))\n+if tab.selected('tensorflow'):\n+    d2l.check_len(state, num_layers)\n+    d2l.check_shape(state[0], (batch_size, num_hiddens))\n+```\n+\n+```{.python .input}\n+%%tab all\n+decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\n+if tab.selected('mxnet', 'pytorch', 'tensorflow'):\n+    state = decoder.init_state(encoder(X))\n+    outputs, state = decoder(X, state)\n+if tab.selected('jax'):\n+    state = decoder.init_state(encoder.init_with_output(d2l.get_key(), X)[0])\n+    (outputs, state), _ = decoder.init_with_output(d2l.get_key(), X, state)\n+\n+\n+d2l.check_shape(outputs, (batch_size, num_steps, vocab_size))\n+if tab.selected('mxnet', 'pytorch', 'jax'):",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed95,d2l-ai/d2l-en,1151162586,1042851725,astonzhang,22279212,2022-12-08T02:05:00+00:00,2022-12-08T02:05:20+00:00,"```suggestion\r\n**Pratik Chaudhari** (*University of Pennsylvania and Amazon*), **Rasool Fakoor** (*Amazon*), and **Kavosh Asadi** (*Amazon*)\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed96,d2l-ai/d2l-en,1151162586,1042851725,astonzhang,22279212,2022-12-08T02:05:00+00:00,2022-12-08T02:05:20+00:00,"```suggestion\r\n**Pratik Chaudhari** (*University of Pennsylvania and Amazon*), **Rasool Fakoor** (*Amazon*), and **Kavosh Asadi** (*Amazon*)\r\n```",true,chapter_reinforcement-learning/index.md,NaN,"@@ -1,7 +1,14 @@\n # Reinforcement Learning\n :label:`chap_reinforcement_learning`\n \n-Reinforcement learning (RL) refers to techniques by which an agent learns a policy that optimizes a given performance metric from a sequence of interactions with an environment.  Each of the agent's actions effects the state of the environment which the agent observes. After executing an action, the agent will also receive some scalar value which is called reward. Using this feedback, the agent has to determine which action to take in order to maximize the rewards obtained through the interaction with the environment. By maximizing rewards, the agent can determine the optimal policy. It is worth noting that in contrast to other classes of machine learning methods, like Supervised learning in which a training set has been provided, an RL agent should act in the environment in order to explore its actions' effects and maximize the reward. More importantly, in interactive problems, it is often infeasible to collect data that represent all the cases in which the agent has to act.\n+\n+**Pratik Chaudhari** (*University of Pennsylvania and Amazon*), **Rasool Fakoor** (*Amazon*), and **‪Kavosh Asadi‬** (*Amazon*)",0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed97,d2l-ai/d2l-en,1147230754,1041056101,AnirudhDagar,23621655,2022-12-06T14:35:11+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed98,d2l-ai/d2l-en,1147230754,1041056377,AnirudhDagar,23621655,2022-12-06T14:35:24+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed99,d2l-ai/d2l-en,1147230754,1041056682,AnirudhDagar,23621655,2022-12-06T14:35:38+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed9a,d2l-ai/d2l-en,1147230754,1041057721,AnirudhDagar,23621655,2022-12-06T14:36:29+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed9b,d2l-ai/d2l-en,1147230754,1041057949,AnirudhDagar,23621655,2022-12-06T14:36:41+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed9c,d2l-ai/d2l-en,1147230754,1041058205,AnirudhDagar,23621655,2022-12-06T14:36:52+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed9d,d2l-ai/d2l-en,1147230754,1041058472,AnirudhDagar,23621655,2022-12-06T14:37:06+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed9e,d2l-ai/d2l-en,1147230754,1041058939,AnirudhDagar,23621655,2022-12-06T14:37:31+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842ed9f,d2l-ai/d2l-en,1147230754,1041062608,AnirudhDagar,23621655,2022-12-06T14:40:32+00:00,2022-12-06T15:21:20+00:00,"Instead of implementing and saving AlexNet here, let's reuse the implementation from conv modern alexnet chapter.\r\nYou can mark this line with the save annotation:\r\nhttps://github.com/d2l-ai/d2l-en/blob/29ad7cc8ce5e47f13a031207d80ee1fc94c3cf12/chapter_convolutional-modern/alexnet.md?plain=1#L319",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda0,d2l-ai/d2l-en,1147230754,1041066013,AnirudhDagar,23621655,2022-12-06T14:43:15+00:00,2022-12-06T15:21:20+00:00,"Also, I see that the CI is [timing out](http://ci.d2l.ai/blue/organizations/jenkins/d2l-en/detail/PR-2372/9/pipeline#step-12-log-1270) on for hyperopt-intro section. That is because we have a upper limit of 20 minutes for evaluation of a notebook. If the notebook takes longer than 20 minutes, I'd suggest using a simpler model instead of AlexNet for the same purpose. That should reduce the time.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda1,d2l-ai/d2l-en,1147230754,1041068998,AnirudhDagar,23621655,2022-12-06T14:45:45+00:00,2022-12-06T15:21:20+00:00,Even for hyperopt-api section you can [see](http://ci.d2l.ai/blue/organizations/jenkins/d2l-en/detail/PR-2372/9/pipeline#step-12-log-1023) that it takes 15minutes which is probably the longest compared to other notebooks. So if possible it would be useful to consider a simpler model.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda2,d2l-ai/d2l-en,1147230754,1041070363,AnirudhDagar,23621655,2022-12-06T14:46:47+00:00,2022-12-06T15:21:20+00:00,"```suggestion\r\n%%tab pytorch\r\ndef objective(batch_size, learning_rate, max_epochs=8):  #@save\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda3,d2l-ai/d2l-en,1147230754,1041071503,AnirudhDagar,23621655,2022-12-06T14:47:44+00:00,2022-12-06T15:21:20+00:00,"All code blocks should be annotated with the framework name or all if it is generic, this might apply to other places below as well.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda4,d2l-ai/d2l-en,1147230754,1041072320,AnirudhDagar,23621655,2022-12-06T14:48:24+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda5,d2l-ai/d2l-en,1147230754,1041072675,AnirudhDagar,23621655,2022-12-06T14:48:41+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda6,d2l-ai/d2l-en,1147230754,1041074633,AnirudhDagar,23621655,2022-12-06T14:50:15+00:00,2022-12-06T15:21:20+00:00,We can reuse this from SoftmaxRegression concise section. You can add save annotation in https://github.com/d2l-ai/d2l-en/blob/29ad7cc8ce5e47f13a031207d80ee1fc94c3cf12/chapter_linear-classification/softmax-regression-concise.md?plain=1#L66-L82,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda7,d2l-ai/d2l-en,1147230754,1041079456,AnirudhDagar,23621655,2022-12-06T14:54:07+00:00,2022-12-06T15:21:20+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842eda8,d2l-ai/d2l-en,1147230754,1041081533,AnirudhDagar,23621655,2022-12-06T14:55:41+00:00,2022-12-06T15:21:20+00:00,Not a big deal but might be useful to consider naming this to something else because we already have a validation_step function which calculates val loss but we can better name this to validation_error or something like that to be more explicit for the readers.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,d2l-ai_d2l-en
6824cd8f0ce5dcec7842f3d7,eugeneyan/applied-ml,1238049635,1111266807,eugeneyan,6831355,2023-02-19T15:58:12+00:00,2023-02-19T15:58:13+00:00,Could you order this paper chronologically please?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3d8,eugeneyan/applied-ml,1238049635,1111266833,eugeneyan,6831355,2023-02-19T15:58:22+00:00,2023-02-19T15:58:22+00:00,Ditto.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3d9,eugeneyan/applied-ml,1238049635,1111267111,eugeneyan,6831355,2023-02-19T16:00:03+00:00,2023-02-19T16:00:03+00:00,"We don't need separate links if they're they same link. Also, could you fix the spelling error please?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3da,eugeneyan/applied-ml,1238049635,1111268804,eugeneyan,6831355,2023-02-19T16:00:42+00:00,2023-02-19T16:00:42+00:00,Ditto.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3db,eugeneyan/applied-ml,1238049635,1111276771,R-Mahmoudi,108347344,2023-02-19T16:48:22+00:00,2023-02-19T16:48:22+00:00,"@eugeneyan  Yes, Sure. Thank you, I will do it.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3dc,eugeneyan/applied-ml,1238049635,1111266807,eugeneyan,6831355,2023-02-19T15:58:12+00:00,2023-02-19T15:58:13+00:00,Could you order this paper chronologically please?,true,README.md,4,"@@ -602,6 +602,11 @@ P.P.S, Looking for guides and interviews on applying ML? 👉[`applyingML`](http\n 25. [Didact AI: The anatomy of an ML-powered stock picking engine](https://principiamundi.com/posts/didact-anatomy/?utm_campaign=Data_Elixir&utm_source=Data_Elixir_407/) `Didact AI` `2022`\n 26. [Deployment for Free - A Machine Learning Platform for Stitch Fix's Data Scientists](https://multithreaded.stitchfix.com/blog/2022/07/14/deployment-for-free/) `Stitch Fix` `2022`\n 27. [Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/abs/2205.02302) ([Paper](https://arxiv.org/ftp/arxiv/papers/2205/2205.02302.pdf)) `IBM` `2022`\n+28. [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://research.google/pubs/pub46555/) ([Paper](https://static.googleusercontent.com/media/research.google.com/cs//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf)) `Google` `2017`",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3dd,eugeneyan/applied-ml,1238049635,1111266833,eugeneyan,6831355,2023-02-19T15:58:22+00:00,2023-02-19T15:58:22+00:00,Ditto.,true,README.md,5,"@@ -602,6 +602,11 @@ P.P.S, Looking for guides and interviews on applying ML? 👉[`applyingML`](http\n 25. [Didact AI: The anatomy of an ML-powered stock picking engine](https://principiamundi.com/posts/didact-anatomy/?utm_campaign=Data_Elixir&utm_source=Data_Elixir_407/) `Didact AI` `2022`\n 26. [Deployment for Free - A Machine Learning Platform for Stitch Fix's Data Scientists](https://multithreaded.stitchfix.com/blog/2022/07/14/deployment-for-free/) `Stitch Fix` `2022`\n 27. [Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/abs/2205.02302) ([Paper](https://arxiv.org/ftp/arxiv/papers/2205/2205.02302.pdf)) `IBM` `2022`\n+28. [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://research.google/pubs/pub46555/) ([Paper](https://static.googleusercontent.com/media/research.google.com/cs//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf)) `Google` `2017`\n+29. [hidden technical debt in machine learning systems](https://arxiv.org/abs/2103.10510) ([Paper](https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)) `Google` `2015`",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3de,eugeneyan/applied-ml,1238049635,1111267111,eugeneyan,6831355,2023-02-19T16:00:03+00:00,2023-02-19T16:00:03+00:00,"We don't need separate links if they're they same link. Also, could you fix the spelling error please?",true,README.md,7,"@@ -602,6 +602,11 @@ P.P.S, Looking for guides and interviews on applying ML? 👉[`applyingML`](http\n 25. [Didact AI: The anatomy of an ML-powered stock picking engine](https://principiamundi.com/posts/didact-anatomy/?utm_campaign=Data_Elixir&utm_source=Data_Elixir_407/) `Didact AI` `2022`\n 26. [Deployment for Free - A Machine Learning Platform for Stitch Fix's Data Scientists](https://multithreaded.stitchfix.com/blog/2022/07/14/deployment-for-free/) `Stitch Fix` `2022`\n 27. [Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/abs/2205.02302) ([Paper](https://arxiv.org/ftp/arxiv/papers/2205/2205.02302.pdf)) `IBM` `2022`\n+28. [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://research.google/pubs/pub46555/) ([Paper](https://static.googleusercontent.com/media/research.google.com/cs//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf)) `Google` `2017`\n+29. [hidden technical debt in machine learning systems](https://arxiv.org/abs/2103.10510) ([Paper](https://proceedings.neurips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)) `Google` `2015`\n+30. [MLOps: Continuous delivery and automation pipelines in machine learning](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) ([Blog](https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)) `Google` `2021`\n+31. [Design Patterns in Machine Learning for MLOps](https://towardsdatascience.com/design-patterns-in-machine-learning-for-mlops-a3f63f745ce4) ([Blog](https://towardsdatascience.com/design-patterns-in-machine-learning-for-mlops-a3f63f745ce4)) `Meduim` `2022`",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3df,eugeneyan/applied-ml,1238049635,1111268804,eugeneyan,6831355,2023-02-19T16:00:42+00:00,2023-02-19T16:00:42+00:00,Ditto.,true,README.md,17,"@@ -625,7 +630,7 @@ P.P.S, Looking for guides and interviews on applying ML? 👉[`applyingML`](http\n 19. [Automatic Retraining for Machine Learning Models: Tips and Lessons Learned](https://building.nubank.com.br/automatic-retraining-for-machine-learning-models/) `Nubank` `2022`\n 20. [RecSysOps: Best Practices for Operating a Large-Scale Recommender System](https://netflixtechblog.medium.com/recsysops-best-practices-for-operating-a-large-scale-recommender-system-95bbe195a841) `Netflix` `2022`\n 21. [ML Education at Uber: Frameworks Inspired by Engineering Principles](https://www.uber.com/en-PL/blog/ml-education-at-uber/) `Uber` `2022`\n-\n+22. [MLOps Best Practices for Data Scientists](https://towardsdatascience.com/mlops-practices-for-data-scientists-dbb01be45dd8)`Meduim` `2021`",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e0,eugeneyan/applied-ml,1238049635,1111276771,R-Mahmoudi,108347344,2023-02-19T16:48:22+00:00,2023-02-19T16:48:22+00:00,"@eugeneyan  Yes, Sure. Thank you, I will do it.",true,README.md,4,"@@ -602,6 +602,11 @@ P.P.S, Looking for guides and interviews on applying ML? 👉[`applyingML`](http\n 25. [Didact AI: The anatomy of an ML-powered stock picking engine](https://principiamundi.com/posts/didact-anatomy/?utm_campaign=Data_Elixir&utm_source=Data_Elixir_407/) `Didact AI` `2022`\n 26. [Deployment for Free - A Machine Learning Platform for Stitch Fix's Data Scientists](https://multithreaded.stitchfix.com/blog/2022/07/14/deployment-for-free/) `Stitch Fix` `2022`\n 27. [Machine Learning Operations (MLOps): Overview, Definition, and Architecture](https://arxiv.org/abs/2205.02302) ([Paper](https://arxiv.org/ftp/arxiv/papers/2205/2205.02302.pdf)) `IBM` `2022`\n+28. [The ML Test Score: A Rubric for ML Production Readiness and Technical Debt Reduction](https://research.google/pubs/pub46555/) ([Paper](https://static.googleusercontent.com/media/research.google.com/cs//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf)) `Google` `2017`",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e1,eugeneyan/applied-ml,1237700657,1111269567,eugeneyan,6831355,2023-02-19T16:04:40+00:00,2023-02-19T16:04:40+00:00,This is more of a general resource than an example of ML applied in industry. Could you exclude this please?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e2,eugeneyan/applied-ml,1237700657,1111269676,eugeneyan,6831355,2023-02-19T16:05:11+00:00,2023-02-19T16:05:11+00:00,This is more of an overview than an example of ML applied in industry. Could you exclude this please?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e3,eugeneyan/applied-ml,1237700657,1111269871,eugeneyan,6831355,2023-02-19T16:06:19+00:00,2023-02-19T16:06:20+00:00,This is more of a general resource than an example of ML applied in industry. Could you exclude this please?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e4,eugeneyan/applied-ml,1237700657,1111269567,eugeneyan,6831355,2023-02-19T16:04:40+00:00,2023-02-19T16:04:40+00:00,This is more of a general resource than an example of ML applied in industry. Could you exclude this please?,true,README.md,NaN,"@@ -56,6 +56,7 @@ P.P.S, Looking for guides and interviews on applying ML? 👉[`applyingML`](http\n 6. [Data Validation for Machine Learning](https://research.google/pubs/pub47967/) ([Paper](https://mlsys.org/Conferences/2019/doc/2019/167.pdf)) `Google` `2019`\n 6. [An Approach to Data Quality for Netflix Personalization Systems](https://www.youtube.com/watch?v=t7vHpA39TXM) `Netflix` `2020`\n 7. [Improving Accuracy By Certainty Estimation of Human Decisions, Labels, and Raters](https://research.fb.com/blog/2020/08/improving-the-accuracy-of-community-standards-enforcement-by-certainty-estimation-of-human-decisions/) ([Paper](https://research.fb.com/wp-content/uploads/2020/08/CLARA-Confidence-of-Labels-and-Raters.pdf)) `Facebook` `2020`\n+8. [How to test categorical data drift (article and code)](https://www.giskard.ai/knowledge/how-to-test-ml-models-2-n-categorical-data-drift) `Giskard` `2022`",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e5,eugeneyan/applied-ml,1237700657,1111269676,eugeneyan,6831355,2023-02-19T16:05:11+00:00,2023-02-19T16:05:11+00:00,This is more of an overview than an example of ML applied in industry. Could you exclude this please?,true,README.md,NaN,"@@ -546,6 +547,7 @@ P.P.S, Looking for guides and interviews on applying ML? 👉[`applyingML`](http\n 47. [Comparing quantiles at scale in online A/B-testing](https://engineering.atspotify.com/2022/03/comparing-quantiles-at-scale-in-online-a-b-testing) `Spotify` `2022`\n 48. [Accelerating our A/B experiments with machine learning](https://dropbox.tech/machine-learning/accelerating-our-a-b-experiments-with-machine-learning-xr) `Dropbox` `2023`\n 49. [Supercharging A/B Testing at Uber](https://www.uber.com/blog/supercharging-a-b-testing-at-uber/) `Uber` \n+50. [How to test ML models? Metamorphic testing (article and code)](https://www.giskard.ai/knowledge/how-to-test-ml-models-4-metamorphic-testing) `Giskard` `2022`",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e6,eugeneyan/applied-ml,1237700657,1111269871,eugeneyan,6831355,2023-02-19T16:06:19+00:00,2023-02-19T16:06:20+00:00,This is more of a general resource than an example of ML applied in industry. Could you exclude this please?,true,README.md,NaN,"@@ -564,8 +566,9 @@ P.P.S, Looking for guides and interviews on applying ML? 👉[`applyingML`](http\n 1. [Building Inclusive Products Through A/B Testing](https://engineering.linkedin.com/blog/2020/building-inclusive-products-through-a-b-testing) ([Paper](https://arxiv.org/pdf/2002.05819.pdf)) `LinkedIn` `2020`\n 2. [LiFT: A Scalable Framework for Measuring Fairness in ML Applications](https://engineering.linkedin.com/blog/2020/lift-addressing-bias-in-large-scale-ai-applications) ([Paper](https://arxiv.org/pdf/2008.07433.pdf)) `LinkedIn` `2020`\n 3. [Introducing Twitter’s first algorithmic bias bounty challenge](https://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge) `Twitter` `2021`\n-3. [Examining algorithmic amplification of political content on Twitter](https://blog.twitter.com/en_us/topics/company/2021/rml-politicalcontent) `Twitter` `2021`\n-3. [A closer look at how LinkedIn integrates fairness into its AI products](https://engineering.linkedin.com/blog/2022/a-closer-look-at-how-linkedin-integrates-fairness-into-its-ai-pr) `LinkedIn` `2022`\n+4. [Examining algorithmic amplification of political content on Twitter](https://blog.twitter.com/en_us/topics/company/2021/rml-politicalcontent) `Twitter` `2021`\n+5. [A closer look at how LinkedIn integrates fairness into its AI products](https://engineering.linkedin.com/blog/2022/a-closer-look-at-how-linkedin-integrates-fairness-into-its-ai-pr) `LinkedIn` `2022`\n+6. [How to test the fairness of ML models? The 80% rule to measure the disparate impact](https://www.giskard.ai/knowledge/how-to-test-ml-models-5-the-80-rule-to-measure-disparity) `Giskard` `2023`",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e7,eugeneyan/applied-ml,444433988,449898490,eugeneyan,6831355,2020-07-05T17:07:13+00:00,2020-07-05T17:13:56+00:00,Could you remove the newline introduced please?\r\n```suggestion\r\n- [E-commerce in Your Inbox: Product Recommendations at Scale](https://arxiv.org/abs/1606.07154) `Yahoo`\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f3e8,eugeneyan/applied-ml,444433988,449898490,eugeneyan,6831355,2020-07-05T17:07:13+00:00,2020-07-05T17:13:56+00:00,Could you remove the newline introduced please?\r\n```suggestion\r\n- [E-commerce in Your Inbox: Product Recommendations at Scale](https://arxiv.org/abs/1606.07154) `Yahoo`\r\n```,true,README.md,NaN,"@@ -52,7 +52,8 @@ Have a favourite piece you're not seeing here? Want to contribute? Make a [pull\n - [Session-based Recommendations with Recurrent Neural Networks](https://arxiv.org/abs/1511.06939) `Telefonica`\n - [Deep Neural Networks for YouTube Recommendations](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf) `YouTube`\n - [Personalized Recommendations for Experiences Using Deep Learning](https://www.tripadvisor.com/engineering/personalized-recommendations-for-experiences-using-deep-learning/) `TripAdvisor`\n-- [E-commerce in Your Inbox:\rProduct Recommendations at Scale](https://arxiv.org/abs/1606.07154) `Yahoo`\n+- [E-commerce in Your Inbox:",0,0,0,0,0,0,0,eugeneyan_applied-ml
6824cd8f0ce5dcec7842f515,explosion/spaCy,2305294132,1934447930,ljvmiranda921,12949683,2025-01-29T19:17:02+00:00,2025-01-29T19:17:03+00:00,```suggestion\r\n        }\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f516,explosion/spaCy,2305294132,1934447930,ljvmiranda921,12949683,2025-01-29T19:17:02+00:00,2025-01-29T19:17:03+00:00,```suggestion\r\n        }\r\n```,true,website/meta/universe.json,NaN,"@@ -5669,7 +5669,46 @@\n                 ""nlp"",\n                 ""spacy""\n             ]\n-        }\n+        },\n+        {\n+            ""id"": ""calamanCy"",\n+            ""slogan"": ""NLP Pipelines for Tagalog"",\n+            ""description"": ""calamanCy is a Tagalog natural language preprocessing framework made with spaCy. Its goal is to provide pipelines and datasets for downstream NLP tasks."",\n+            ""github"": ""ljvmiranda921/calamanCy"",\n+            ""pip"": ""calamanCy"",\n+            ""code_example"": [\n+                ""import calamancy"",\n+                ""nlp = calamancy.load('tl_calamancy_md-0.1.0')"",\n+                ""doc = nlp('Ako si Juan de la Cruz')"",\n+                """",\n+                ""for ent in doc.ents:"",\n+                ""    print(f'Text: {ent.text} -> Entity: {ent.label_}')"",\n+            ],\n+            ""code_language"": ""python"",\n+            ""url"": ""https://github.com/ljvmiranda921/calamanCy"",\n+            ""thumb"": ""https://github.com/ljvmiranda921/calamanCy/raw/master/logo.png?raw=True"",\n+            ""image"": ""https://github.com/ljvmiranda921/calamanCy/raw/master/logo.png?raw=True"",\n+            ""author"": ""Lester James V. Miranda"",\n+            ""author_links"": {\n+                ""twitter"": ""ljvmiranda"",\n+                ""github"": ""ljvmiranda921"",\n+                ""website"": ""https://ljvmiranda921.github.io"",\n+            },\n+            ""category"": [\n+              ""pipeline"",\n+              ""training"",\n+              ""research"",\n+              ""standalone"",\n+              ""models""\n+            ],\n+            ""tags"": [\n+              ""nlp"",\n+              ""spacy"",\n+              ""multilingual"",\n+              ""filipino"",\n+              ""tagalog""\n+            ]\n+        },",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f517,explosion/spaCy,1898282365,1751838510,ines,13643239,2024-09-10T12:18:02+00:00,2024-09-10T12:18:02+00:00,"```suggestion\r\n            ""slogan"": ""Normalizer for contemporary French"",\r\n            ""description"": ""Normalizer for French with focus on online and informal communication, _peùUUUt-èTRE_ becomes _peut-être_, _voilaaaa_ becomes _voilà_. it also harmonizes inclusive language (the user can chose how): by default, _auteur-rice-s-x et relecteur.xrices_ becomes _auteur·ricexs et relecteur·ricexs_."",\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f518,explosion/spaCy,1898282365,1751838510,ines,13643239,2024-09-10T12:18:02+00:00,2024-09-10T12:18:02+00:00,"```suggestion\r\n            ""slogan"": ""Normalizer for contemporary French"",\r\n            ""description"": ""Normalizer for French with focus on online and informal communication, _peùUUUt-èTRE_ becomes _peut-être_, _voilaaaa_ becomes _voilà_. it also harmonizes inclusive language (the user can chose how): by default, _auteur-rice-s-x et relecteur.xrices_ becomes _auteur·ricexs et relecteur·ricexs_."",\r\n```",true,website/meta/universe.json,NaN,"@@ -4517,6 +4517,31 @@\n                 ""website"": ""https://redfield.ai""\n             },\n             ""category"": [""standalone""]\n+        },\n+        {\n+            ""id"": ""presque"",\n+            ""title"": ""presque"",\n+            ""slogan"": ""normalizer for contemporary french."",\n+            ""description"": ""normalizer for french with focus on online and informal communication, _peùUUUt-èTRE_ becomes _peut-être_, _voilaaaa_ becomes _voilà_. it also harmonizes inclusive language (the user can chose how): by default, _auteur-rice-s-x et relecteur.xrices_ becomes _auteur·ricexs et relecteur·ricexs_."",",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f519,explosion/spaCy,1898209962,1751835219,ines,13643239,2024-09-10T12:15:41+00:00,2024-09-10T12:15:41+00:00,"```suggestion\r\n            ""slogan"": ""Tokenizer for contemporary French"",\r\n            ""description"": ""A tokenizer for French that handles inword parentheses like in _(b)rouille_, inclusive language (won't split _relecteur.rice.s_,but will split _mais.maintenant_), hyphens (split _peut-on_, or _pouvons-vous_ but not _tubulu-pimpant_), apostrophes (split _j'arrive_ or _j'arrivons_, but not _aujourd'hui_ or _r'garder_), emoticons, text-emoji (_:happy:_), urls, mails and more."",\r\n            ""github"": ""thjbdvlt/quelquhui"",\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f51a,explosion/spaCy,1898209962,1751835219,ines,13643239,2024-09-10T12:15:41+00:00,2024-09-10T12:15:41+00:00,"```suggestion\r\n            ""slogan"": ""Tokenizer for contemporary French"",\r\n            ""description"": ""A tokenizer for French that handles inword parentheses like in _(b)rouille_, inclusive language (won't split _relecteur.rice.s_,but will split _mais.maintenant_), hyphens (split _peut-on_, or _pouvons-vous_ but not _tubulu-pimpant_), apostrophes (split _j'arrive_ or _j'arrivons_, but not _aujourd'hui_ or _r'garder_), emoticons, text-emoji (_:happy:_), urls, mails and more."",\r\n            ""github"": ""thjbdvlt/quelquhui"",\r\n```",true,website/meta/universe.json,NaN,"@@ -4517,6 +4517,26 @@\n                 ""website"": ""https://redfield.ai""\n             },\n             ""category"": [""standalone""]\n+        },\n+        {\n+            ""id"": ""quelquhui"",\n+            ""title"": ""quelquhui"",\n+            ""slogan"": ""tokenizer for contemporary french"",\n+            ""description"": ""A tokenizer for french that handles inword parentheses like in _(b)rouille_, inclusive language (won't split _relecteur.rice.s_,but will split _mais.maintenant_), hyphens (split _peut-on_, or _pouvons-vous_ but not _tubulu-pimpant_), apostrophes (split _j'arrive_ or _j'arrivons_, but not _aujourd'hui_ or _r'garder_), emoticons, text-emoji (_:happy:_), urls, mails, ..."",\n+            ""github"": ""thjbdvlt/quelquhui"",",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f51b,explosion/spaCy,1843858239,1626310806,svlandeg,8796347,2024-06-04T16:34:53+00:00,2024-06-04T16:37:58+00:00,"```suggestion\r\nThe `sentiment` attribute is removed from the `Token`, `Span`, `Doc` and `Lexeme`\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f51c,explosion/spaCy,1843858239,1626311639,svlandeg,8796347,2024-06-04T16:35:32+00:00,2024-06-04T16:37:58+00:00,```suggestion\r\ncandidates. The batching is by doc since the [`Span`](/api/span) objects in a\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f51d,explosion/spaCy,1843858239,1626310806,svlandeg,8796347,2024-06-04T16:34:53+00:00,2024-06-04T16:37:58+00:00,"```suggestion\r\nThe `sentiment` attribute is removed from the `Token`, `Span`, `Doc` and `Lexeme`\r\n```",true,website/docs/usage/v4.mdx,NaN,"@@ -0,0 +1,191 @@\n+---\n+title: What's New in v4.0\n+teaser: New features and how to upgrade\n+menu:\n+  - ['New Features', 'features']\n+  - ['Upgrading Notes', 'upgrading']\n+---\n+\n+## New features {id=""features"",hidden=""true""}\n+\n+spaCy v4.0 supports more flexible learning rates and adds experimental support\n+for model distillation. This release also fixes some long-standing issues that\n+require minor API changes.\n+\n+spaCy v4.0 drops support for Python 3.7 and 3.8.\n+\n+### Flexible learning rates {id=""learn-rate""}\n+\n+Thinc 9 adds support for more flexible learning rates that can use the step,\n+parameter names, and results from prior evaluations. spaCy v4 makes use of these\n+flexible learning rates by passing the aggregate score of the most recent\n+evaluation to the learning rate schedule. This makes it possible for schedules\n+like [`plateau`](https://thinc.ai/docs/api-schedules#plateau) to adjust the\n+learning rate when training is stagnant.\n+\n+### Experimental support for model distillation {id=""distillation""}\n+\n+spaCy v4 lays the groundwork for model distillation. Distillation trains a\n+_student_ model on the predictions of a _teacher_ model using an unannotated\n+corpus. One of the more exciting applications of distillation is extracting\n+small, task-focused models from large, pretrained transformer models.\n+\n+Support for distillation support consists of several parts:\n+\n+- [`TrainablePipe`](/api/pipe) now provides a [`distill`](/api/pipe#distill)\n+  method. This can be used to perform a distillation step, where a student is\n+  updated to mimick the outputs of the teacher.\n+- A configuration section called `distilation` for configuring various\n+  distillation settings.\n+- The distillation loop.\n+- The [`distill`](/api/cli#distill) subcommand to run distillation from the\n+  command-line.\n+\n+Most of the trainable pipeline components are updated to support distillation.\n+\n+### Saving activations {id=""save-activation""}\n+\n+Trainable pipes can now save the pipe's model activations for a document in the\n+[`Doc.activations`](/api/doc#attributes) dictionary. You can use this\n+functionality to get programmatic access to e.g. the probability distibution of\n+a pipe's classifier.\n+\n+The following activations are currently available:\n+\n+- `EditTreeLemmatizer`: `probabilities` and `tree_ids`\n+- `EntityLinker`: `ents` and `scores`\n+- `Morphologizer`: `probabilities` and `label_ids`\n+- `SentenceRecognizer`: `probabilities` and `label_ids`\n+- `SpanCategorizer`: `indices` and `scores`\n+- `Tagger`: `probabilities` and `label_ids`\n+- `TextCategorizer`: `probabilities`\n+\n+> #### Example\n+>\n+> ```python\n+> import spacy\n+> nlp = spacy.load(""de_core_news_lg"")\n+> nlp.get_pipe(""tagger"").save_activations = True\n+> doc = nlp(""Hallo Welt!"")\n+> assert ""tagger"" in doc.activations\n+> assert ""probabilities"" in doc.activations[""tagger""]\n+> ```\n+\n+### Additional features and improvements {id=""additional-features-and-improvements""}\n+\n+- The `--code` option that is used by several CLI subcommands now accepts\n+  multiple files to load by separating them with a comma.\n+- `spacy download` does not redownload models that are already installed.\n+- When modifying a `Span` that was retrieved through a `SpanGroup`, the change\n+  is now reflected in the `SpanGroup`.\n+- Lookups can now be downloaded from a URL using\n+  `spacy.LookupsDataLoaderFromURL.v1`.\n+\n+## Notes about upgrading from v3.7 {id=""upgrading""}\n+\n+This release drops support for Python 3.7 and 3.8. Most configuration files from\n+spaCy 3.7 can be used with spaCy 4.0 without any modifications (excepting\n+configurations that use `EntityLinker.v1`, see below). However, spaCy 4.0\n+introduces some (minor) API changes that are discussed in the remainder of this\n+section.\n+\n+### Removal of the `EntityRuler` class\n+\n+The `EntityRuler` class is removed. The entity ruler is implemented as a special\n+case of the `SpanRuler` component.\n+\n+See the [migration guide](/api/entityruler#migrating) for differences between\n+the v3 `EntityRuler` and v4 `SpanRuler` implementations of the `entity_ruler`\n+component.\n+\n+### Renamed language codes: `is` -> `isl` and `xx` to `mul`\n+\n+The language code for Icelandic has been changed from `is` to `isl` to avoid\n+incompatibilities with the Python `is` keyword. The language code for\n+multilingual models has been changed from `xx` to `mul`. Existing code that uses\n+these language codes should be adjusted accordingly.\n+\n+### Removal of the `sentiment` attribute\n+\n+The `sentiment` attribute is removed the `Token`, `Span`, `Doc` and `Lexeme`",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f51e,explosion/spaCy,1843858239,1626311639,svlandeg,8796347,2024-06-04T16:35:32+00:00,2024-06-04T16:37:58+00:00,```suggestion\r\ncandidates. The batching is by doc since the [`Span`](/api/span) objects in a\r\n```,true,website/docs/usage/v4.mdx,NaN,"@@ -0,0 +1,191 @@\n+---\n+title: What's New in v4.0\n+teaser: New features and how to upgrade\n+menu:\n+  - ['New Features', 'features']\n+  - ['Upgrading Notes', 'upgrading']\n+---\n+\n+## New features {id=""features"",hidden=""true""}\n+\n+spaCy v4.0 supports more flexible learning rates and adds experimental support\n+for model distillation. This release also fixes some long-standing issues that\n+require minor API changes.\n+\n+spaCy v4.0 drops support for Python 3.7 and 3.8.\n+\n+### Flexible learning rates {id=""learn-rate""}\n+\n+Thinc 9 adds support for more flexible learning rates that can use the step,\n+parameter names, and results from prior evaluations. spaCy v4 makes use of these\n+flexible learning rates by passing the aggregate score of the most recent\n+evaluation to the learning rate schedule. This makes it possible for schedules\n+like [`plateau`](https://thinc.ai/docs/api-schedules#plateau) to adjust the\n+learning rate when training is stagnant.\n+\n+### Experimental support for model distillation {id=""distillation""}\n+\n+spaCy v4 lays the groundwork for model distillation. Distillation trains a\n+_student_ model on the predictions of a _teacher_ model using an unannotated\n+corpus. One of the more exciting applications of distillation is extracting\n+small, task-focused models from large, pretrained transformer models.\n+\n+Support for distillation support consists of several parts:\n+\n+- [`TrainablePipe`](/api/pipe) now provides a [`distill`](/api/pipe#distill)\n+  method. This can be used to perform a distillation step, where a student is\n+  updated to mimick the outputs of the teacher.\n+- A configuration section called `distilation` for configuring various\n+  distillation settings.\n+- The distillation loop.\n+- The [`distill`](/api/cli#distill) subcommand to run distillation from the\n+  command-line.\n+\n+Most of the trainable pipeline components are updated to support distillation.\n+\n+### Saving activations {id=""save-activation""}\n+\n+Trainable pipes can now save the pipe's model activations for a document in the\n+[`Doc.activations`](/api/doc#attributes) dictionary. You can use this\n+functionality to get programmatic access to e.g. the probability distibution of\n+a pipe's classifier.\n+\n+The following activations are currently available:\n+\n+- `EditTreeLemmatizer`: `probabilities` and `tree_ids`\n+- `EntityLinker`: `ents` and `scores`\n+- `Morphologizer`: `probabilities` and `label_ids`\n+- `SentenceRecognizer`: `probabilities` and `label_ids`\n+- `SpanCategorizer`: `indices` and `scores`\n+- `Tagger`: `probabilities` and `label_ids`\n+- `TextCategorizer`: `probabilities`\n+\n+> #### Example\n+>\n+> ```python\n+> import spacy\n+> nlp = spacy.load(""de_core_news_lg"")\n+> nlp.get_pipe(""tagger"").save_activations = True\n+> doc = nlp(""Hallo Welt!"")\n+> assert ""tagger"" in doc.activations\n+> assert ""probabilities"" in doc.activations[""tagger""]\n+> ```\n+\n+### Additional features and improvements {id=""additional-features-and-improvements""}\n+\n+- The `--code` option that is used by several CLI subcommands now accepts\n+  multiple files to load by separating them with a comma.\n+- `spacy download` does not redownload models that are already installed.\n+- When modifying a `Span` that was retrieved through a `SpanGroup`, the change\n+  is now reflected in the `SpanGroup`.\n+- Lookups can now be downloaded from a URL using\n+  `spacy.LookupsDataLoaderFromURL.v1`.\n+\n+## Notes about upgrading from v3.7 {id=""upgrading""}\n+\n+This release drops support for Python 3.7 and 3.8. Most configuration files from\n+spaCy 3.7 can be used with spaCy 4.0 without any modifications (excepting\n+configurations that use `EntityLinker.v1`, see below). However, spaCy 4.0\n+introduces some (minor) API changes that are discussed in the remainder of this\n+section.\n+\n+### Removal of the `EntityRuler` class\n+\n+The `EntityRuler` class is removed. The entity ruler is implemented as a special\n+case of the `SpanRuler` component.\n+\n+See the [migration guide](/api/entityruler#migrating) for differences between\n+the v3 `EntityRuler` and v4 `SpanRuler` implementations of the `entity_ruler`\n+component.\n+\n+### Renamed language codes: `is` -> `isl` and `xx` to `mul`\n+\n+The language code for Icelandic has been changed from `is` to `isl` to avoid\n+incompatibilities with the Python `is` keyword. The language code for\n+multilingual models has been changed from `xx` to `mul`. Existing code that uses\n+these language codes should be adjusted accordingly.\n+\n+### Removal of the `sentiment` attribute\n+\n+The `sentiment` attribute is removed the `Token`, `Span`, `Doc` and `Lexeme`\n+classes. If you used this attribute in a `sentiment` analysis component, we\n+recommend you to store the sentiment analysis in an\n+[extension attribute](/usage/processing-pipelines#custom-components-attributes)\n+instead.\n+\n+### Removal of `get_candidates_batch`\n+\n+Prior to spaCy v4, `get_candidates()` returned an `Iterable` of candidates for a\n+specific mention. spaCy >= 3.5 provides `get_candidates_batch()` for looking up\n+multiple mentions — given an `Iterable[Span]` of mentions, it returns for each\n+mention the candidates.\n+\n+spaCy v4 replaces both functions by a single function\n+[`get_candidates`](/api/entitylinker#config) that does doc-wise batching. For an\n+`Iterator[SpanGroup]` it returns for each mention in the spangroup the\n+candidates. The batching is by doc since the [`Span`](/api/span)s in a",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f51f,explosion/spaCy,1834419349,1575350718,svlandeg,8796347,2024-04-22T20:57:17+00:00,2024-04-22T20:58:56+00:00,```suggestion\r\n[`set_annotations`](/api/pipe#set_annotations) is called.\r\n```\r\n(just to be consistent though I think it'll resolve to the same thing),false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f520,explosion/spaCy,1834419349,1575352102,svlandeg,8796347,2024-04-22T20:58:51+00:00,2024-04-22T20:58:56+00:00,```suggestion\r\n| **RETURNS** | Whether to save activations in the `Doc` object in [`set_annotations`](/api/pipe#set_annotations). ~~bool~~ |\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f521,explosion/spaCy,1834419349,1575350718,svlandeg,8796347,2024-04-22T20:57:17+00:00,2024-04-22T20:58:56+00:00,```suggestion\r\n[`set_annotations`](/api/pipe#set_annotations) is called.\r\n```\r\n(just to be consistent though I think it'll resolve to the same thing),true,website/docs/api/pipe.mdx,NaN,"@@ -476,6 +476,17 @@ as an attribute to the component's model.\n | ----------- | ---------------------------------------------------------------------------------------------- |\n | **RETURNS** | Whether or not the output dimension of the model can be changed after initialization. ~~bool~~ |\n \n+## TrainablePipe.save_activations {id=""save_activations"",tag=""property"",version=""4""}\n+\n+Check/set whether this component stores activations. When enabled, the\n+activations of the pipe's model are stored in the\n+[`Doc.activations`](/api/doc#attributes) dictionary when\n+[`set_annotations`](set_annotations) is called.",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f522,explosion/spaCy,1834419349,1575352102,svlandeg,8796347,2024-04-22T20:58:51+00:00,2024-04-22T20:58:56+00:00,```suggestion\r\n| **RETURNS** | Whether to save activations in the `Doc` object in [`set_annotations`](/api/pipe#set_annotations). ~~bool~~ |\r\n```,true,website/docs/api/pipe.mdx,NaN,"@@ -476,6 +476,17 @@ as an attribute to the component's model.\n | ----------- | ---------------------------------------------------------------------------------------------- |\n | **RETURNS** | Whether or not the output dimension of the model can be changed after initialization. ~~bool~~ |\n \n+## TrainablePipe.save_activations {id=""save_activations"",tag=""property"",version=""4""}\n+\n+Check/set whether this component stores activations. When enabled, the\n+activations of the pipe's model are stored in the\n+[`Doc.activations`](/api/doc#attributes) dictionary when\n+[`set_annotations`](set_annotations) is called.\n+\n+| Name        | Description                                                                                       |\n+| ----------- | ------------------------------------------------------------------------------------------------- |\n+| **RETURNS** | Whether to save activations in the `Doc` object in [`set_annotations`](set_annotations). ~~bool~~ |",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f523,explosion/spaCy,1815880622,1560695445,svlandeg,8796347,2024-04-11T09:14:06+00:00,2024-04-11T09:27:27+00:00,"We can type this as `Union[str, Path]` which is also what `util.load_model()` supports.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f524,explosion/spaCy,1815880622,1560696818,svlandeg,8796347,2024-04-11T09:15:11+00:00,2024-04-11T09:27:27+00:00,```suggestion\r\n    # Make sure all files and paths exist if they are needed\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f525,explosion/spaCy,1815880622,1560698761,svlandeg,8796347,2024-04-11T09:16:42+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\n    msg.divider(""Initializing student pipeline"")\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f526,explosion/spaCy,1815880622,1560699240,svlandeg,8796347,2024-04-11T09:17:05+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\n    msg.good(""Initialized student pipeline"")\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f527,explosion/spaCy,1815880622,1560701753,svlandeg,8796347,2024-04-11T09:19:03+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\n    msg.divider(""Distilling student pipeline from teacher"")\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f528,explosion/spaCy,1815880622,1560704210,svlandeg,8796347,2024-04-11T09:20:55+00:00,2024-04-11T09:27:27+00:00,```suggestion\r\nfrom a larger high-accuracy model. Since distillation uses the activations of the\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f529,explosion/spaCy,1815880622,1560704811,svlandeg,8796347,2024-04-11T09:21:25+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\nteacher, distillation can be performed on a corpus of raw text without (gold standard)\r\n```\r\n(just to make it abundantly clear)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f52a,explosion/spaCy,1815880622,1560705769,svlandeg,8796347,2024-04-11T09:22:12+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\n`distill` will save out the best performing pipeline across all epochs, as well as the final\r\n```\r\nMostly to distinguish ""model"" from ""pipeline"" - the eval is holistic across all models in the one pipeline",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f52b,explosion/spaCy,1815880622,1560707524,svlandeg,8796347,2024-04-11T09:23:27+00:00,2024-04-11T09:27:27+00:00,"Refer to Weasel instead: https://github.com/explosion/weasel (we should update this in other places as well, but that's outside of the scope of this PR)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f52c,explosion/spaCy,1815880622,1560710016,svlandeg,8796347,2024-04-11T09:25:18+00:00,2024-04-11T09:27:27+00:00,Should we add a comment that the loop still needs a dev set of gold annotations to evaluate on? To make it more clear why the example CLI statement 👇  has a `--paths.dev ./dev` overwrite part.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f52d,explosion/spaCy,1815880622,1560711164,svlandeg,8796347,2024-04-11T09:26:12+00:00,2024-04-11T09:27:27+00:00,"This should mention that it can also be a `str` to refer to an installed pipeline, right?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f52e,explosion/spaCy,1815880622,1560711656,svlandeg,8796347,2024-04-11T09:26:33+00:00,2024-04-11T09:27:27+00:00,And what happens when it's `None`?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f52f,explosion/spaCy,1815880622,1560712400,svlandeg,8796347,2024-04-11T09:27:12+00:00,2024-04-11T09:27:27+00:00,Does it? 🤭 ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f530,explosion/spaCy,1815880622,1561057055,danieldk,49398,2024-04-11T13:49:53+00:00,2024-04-11T13:49:53+00:00,Fixed.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f531,explosion/spaCy,1815880622,1561057454,danieldk,49398,2024-04-11T13:50:08+00:00,2024-04-11T13:50:09+00:00,Done.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f532,explosion/spaCy,1815880622,1561059912,danieldk,49398,2024-04-11T13:51:46+00:00,2024-04-11T13:51:47+00:00,"Yeah, good idea! Added.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f533,explosion/spaCy,1815880622,1561075527,danieldk,49398,2024-04-11T14:02:10+00:00,2024-04-11T14:02:11+00:00,Done.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f534,explosion/spaCy,1815880622,1561080112,danieldk,49398,2024-04-11T14:04:36+00:00,2024-04-11T14:04:36+00:00,Added a remark that no pipeline is saved when this option is absent.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f535,explosion/spaCy,1815880622,1561080684,danieldk,49398,2024-04-11T14:04:49+00:00,2024-04-11T14:04:50+00:00,Oops 🤦 . Fixed.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f536,explosion/spaCy,1815880622,1561113110,svlandeg,8796347,2024-04-11T14:22:53+00:00,2024-04-11T14:22:53+00:00,"```suggestion\r\nIf you need to manage complex multi-step training workflows, check out \r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f537,explosion/spaCy,1815880622,1560695445,svlandeg,8796347,2024-04-11T09:14:06+00:00,2024-04-11T09:27:27+00:00,"We can type this as `Union[str, Path]` which is also what `util.load_model()` supports.",true,spacy/cli/distill.py,NaN,"@@ -0,0 +1,98 @@\n+import logging\n+import sys\n+from pathlib import Path\n+from typing import Any, Dict, Optional, Union\n+\n+import typer\n+from wasabi import msg\n+\n+from .. import util\n+from ..pipeline.trainable_pipe import TrainablePipe\n+from ..schemas import ConfigSchemaDistill\n+from ..training.initialize import init_nlp_student\n+from ..training.loop import distill as distill_nlp\n+from ._util import (\n+    Arg,\n+    Opt,\n+    app,\n+    import_code_paths,\n+    parse_config_overrides,\n+    setup_gpu,\n+    show_validation_error,\n+)\n+\n+\n+@app.command(\n+    ""distill"",\n+    context_settings={""allow_extra_args"": True, ""ignore_unknown_options"": True},\n+)\n+def distill_cli(\n+    # fmt: off\n+    ctx: typer.Context,  # This is only used to read additional arguments\n+    teacher_model: str = Arg(..., help=""Teacher model name or path""),\n+    student_config_path: Path = Arg(..., help=""Path to config file"", exists=True, allow_dash=True),\n+    output_path: Optional[Path] = Opt(None, ""--output"", ""--output-path"", ""-o"", help=""Output directory to store trained pipeline in""),\n+    code_path: str = Opt("""", ""--code"", ""-c"", help=""Comma-separated paths to Python files with additional code (registered functions) to be imported""),\n+    verbose: bool = Opt(False, ""--verbose"", ""-V"", ""-VV"", help=""Display more information for debugging purposes""),\n+    use_gpu: int = Opt(-1, ""--gpu-id"", ""-g"", help=""GPU ID or -1 for CPU"")\n+    # fmt: on\n+):\n+    """"""\n+    Distill a spaCy pipeline from a teacher model.\n+\n+    DOCS: https://spacy.io/api/cli#distill\n+    """"""\n+    util.logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n+    overrides = parse_config_overrides(ctx.args)\n+    import_code_paths(code_path)\n+    distill(\n+        teacher_model,\n+        student_config_path,\n+        output_path,\n+        use_gpu=use_gpu,\n+        overrides=overrides,\n+    )\n+\n+\n+def distill(\n+    teacher_model: str,",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f538,explosion/spaCy,1815880622,1560696818,svlandeg,8796347,2024-04-11T09:15:11+00:00,2024-04-11T09:27:27+00:00,```suggestion\r\n    # Make sure all files and paths exist if they are needed\r\n```,true,spacy/cli/distill.py,NaN,"@@ -0,0 +1,98 @@\n+import logging\n+import sys\n+from pathlib import Path\n+from typing import Any, Dict, Optional, Union\n+\n+import typer\n+from wasabi import msg\n+\n+from .. import util\n+from ..pipeline.trainable_pipe import TrainablePipe\n+from ..schemas import ConfigSchemaDistill\n+from ..training.initialize import init_nlp_student\n+from ..training.loop import distill as distill_nlp\n+from ._util import (\n+    Arg,\n+    Opt,\n+    app,\n+    import_code_paths,\n+    parse_config_overrides,\n+    setup_gpu,\n+    show_validation_error,\n+)\n+\n+\n+@app.command(\n+    ""distill"",\n+    context_settings={""allow_extra_args"": True, ""ignore_unknown_options"": True},\n+)\n+def distill_cli(\n+    # fmt: off\n+    ctx: typer.Context,  # This is only used to read additional arguments\n+    teacher_model: str = Arg(..., help=""Teacher model name or path""),\n+    student_config_path: Path = Arg(..., help=""Path to config file"", exists=True, allow_dash=True),\n+    output_path: Optional[Path] = Opt(None, ""--output"", ""--output-path"", ""-o"", help=""Output directory to store trained pipeline in""),\n+    code_path: str = Opt("""", ""--code"", ""-c"", help=""Comma-separated paths to Python files with additional code (registered functions) to be imported""),\n+    verbose: bool = Opt(False, ""--verbose"", ""-V"", ""-VV"", help=""Display more information for debugging purposes""),\n+    use_gpu: int = Opt(-1, ""--gpu-id"", ""-g"", help=""GPU ID or -1 for CPU"")\n+    # fmt: on\n+):\n+    """"""\n+    Distill a spaCy pipeline from a teacher model.\n+\n+    DOCS: https://spacy.io/api/cli#distill\n+    """"""\n+    util.logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n+    overrides = parse_config_overrides(ctx.args)\n+    import_code_paths(code_path)\n+    distill(\n+        teacher_model,\n+        student_config_path,\n+        output_path,\n+        use_gpu=use_gpu,\n+        overrides=overrides,\n+    )\n+\n+\n+def distill(\n+    teacher_model: str,\n+    student_config_path: Union[str, Path],\n+    output_path: Optional[Union[str, Path]] = None,\n+    *,\n+    use_gpu: int = -1,\n+    overrides: Dict[str, Any] = util.SimpleFrozenDict(),\n+):\n+    student_config_path = util.ensure_path(student_config_path)\n+    output_path = util.ensure_path(output_path)\n+    # Make sure all files and paths exists if they are needed",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f539,explosion/spaCy,1815880622,1560698761,svlandeg,8796347,2024-04-11T09:16:42+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\n    msg.divider(""Initializing student pipeline"")\r\n```",true,spacy/cli/distill.py,NaN,"@@ -0,0 +1,98 @@\n+import logging\n+import sys\n+from pathlib import Path\n+from typing import Any, Dict, Optional, Union\n+\n+import typer\n+from wasabi import msg\n+\n+from .. import util\n+from ..pipeline.trainable_pipe import TrainablePipe\n+from ..schemas import ConfigSchemaDistill\n+from ..training.initialize import init_nlp_student\n+from ..training.loop import distill as distill_nlp\n+from ._util import (\n+    Arg,\n+    Opt,\n+    app,\n+    import_code_paths,\n+    parse_config_overrides,\n+    setup_gpu,\n+    show_validation_error,\n+)\n+\n+\n+@app.command(\n+    ""distill"",\n+    context_settings={""allow_extra_args"": True, ""ignore_unknown_options"": True},\n+)\n+def distill_cli(\n+    # fmt: off\n+    ctx: typer.Context,  # This is only used to read additional arguments\n+    teacher_model: str = Arg(..., help=""Teacher model name or path""),\n+    student_config_path: Path = Arg(..., help=""Path to config file"", exists=True, allow_dash=True),\n+    output_path: Optional[Path] = Opt(None, ""--output"", ""--output-path"", ""-o"", help=""Output directory to store trained pipeline in""),\n+    code_path: str = Opt("""", ""--code"", ""-c"", help=""Comma-separated paths to Python files with additional code (registered functions) to be imported""),\n+    verbose: bool = Opt(False, ""--verbose"", ""-V"", ""-VV"", help=""Display more information for debugging purposes""),\n+    use_gpu: int = Opt(-1, ""--gpu-id"", ""-g"", help=""GPU ID or -1 for CPU"")\n+    # fmt: on\n+):\n+    """"""\n+    Distill a spaCy pipeline from a teacher model.\n+\n+    DOCS: https://spacy.io/api/cli#distill\n+    """"""\n+    util.logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n+    overrides = parse_config_overrides(ctx.args)\n+    import_code_paths(code_path)\n+    distill(\n+        teacher_model,\n+        student_config_path,\n+        output_path,\n+        use_gpu=use_gpu,\n+        overrides=overrides,\n+    )\n+\n+\n+def distill(\n+    teacher_model: str,\n+    student_config_path: Union[str, Path],\n+    output_path: Optional[Union[str, Path]] = None,\n+    *,\n+    use_gpu: int = -1,\n+    overrides: Dict[str, Any] = util.SimpleFrozenDict(),\n+):\n+    student_config_path = util.ensure_path(student_config_path)\n+    output_path = util.ensure_path(output_path)\n+    # Make sure all files and paths exists if they are needed\n+    if not student_config_path or (\n+        str(student_config_path) != ""-"" and not student_config_path.exists()\n+    ):\n+        msg.fail(""Student config file not found"", student_config_path, exits=1)\n+    if not output_path:\n+        msg.info(""No output directory provided"")\n+    else:\n+        if not output_path.exists():\n+            output_path.mkdir(parents=True)\n+            msg.good(f""Created output directory: {output_path}"")\n+        msg.info(f""Saving to output directory: {output_path}"")\n+    setup_gpu(use_gpu)\n+    teacher = util.load_model(teacher_model)\n+    with show_validation_error(student_config_path):\n+        config = util.load_config(\n+            student_config_path, overrides=overrides, interpolate=False\n+        )\n+    msg.divider(""Initializing pipeline"")",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f53a,explosion/spaCy,1815880622,1560699240,svlandeg,8796347,2024-04-11T09:17:05+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\n    msg.good(""Initialized student pipeline"")\r\n```",true,spacy/cli/distill.py,NaN,"@@ -0,0 +1,98 @@\n+import logging\n+import sys\n+from pathlib import Path\n+from typing import Any, Dict, Optional, Union\n+\n+import typer\n+from wasabi import msg\n+\n+from .. import util\n+from ..pipeline.trainable_pipe import TrainablePipe\n+from ..schemas import ConfigSchemaDistill\n+from ..training.initialize import init_nlp_student\n+from ..training.loop import distill as distill_nlp\n+from ._util import (\n+    Arg,\n+    Opt,\n+    app,\n+    import_code_paths,\n+    parse_config_overrides,\n+    setup_gpu,\n+    show_validation_error,\n+)\n+\n+\n+@app.command(\n+    ""distill"",\n+    context_settings={""allow_extra_args"": True, ""ignore_unknown_options"": True},\n+)\n+def distill_cli(\n+    # fmt: off\n+    ctx: typer.Context,  # This is only used to read additional arguments\n+    teacher_model: str = Arg(..., help=""Teacher model name or path""),\n+    student_config_path: Path = Arg(..., help=""Path to config file"", exists=True, allow_dash=True),\n+    output_path: Optional[Path] = Opt(None, ""--output"", ""--output-path"", ""-o"", help=""Output directory to store trained pipeline in""),\n+    code_path: str = Opt("""", ""--code"", ""-c"", help=""Comma-separated paths to Python files with additional code (registered functions) to be imported""),\n+    verbose: bool = Opt(False, ""--verbose"", ""-V"", ""-VV"", help=""Display more information for debugging purposes""),\n+    use_gpu: int = Opt(-1, ""--gpu-id"", ""-g"", help=""GPU ID or -1 for CPU"")\n+    # fmt: on\n+):\n+    """"""\n+    Distill a spaCy pipeline from a teacher model.\n+\n+    DOCS: https://spacy.io/api/cli#distill\n+    """"""\n+    util.logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n+    overrides = parse_config_overrides(ctx.args)\n+    import_code_paths(code_path)\n+    distill(\n+        teacher_model,\n+        student_config_path,\n+        output_path,\n+        use_gpu=use_gpu,\n+        overrides=overrides,\n+    )\n+\n+\n+def distill(\n+    teacher_model: str,\n+    student_config_path: Union[str, Path],\n+    output_path: Optional[Union[str, Path]] = None,\n+    *,\n+    use_gpu: int = -1,\n+    overrides: Dict[str, Any] = util.SimpleFrozenDict(),\n+):\n+    student_config_path = util.ensure_path(student_config_path)\n+    output_path = util.ensure_path(output_path)\n+    # Make sure all files and paths exists if they are needed\n+    if not student_config_path or (\n+        str(student_config_path) != ""-"" and not student_config_path.exists()\n+    ):\n+        msg.fail(""Student config file not found"", student_config_path, exits=1)\n+    if not output_path:\n+        msg.info(""No output directory provided"")\n+    else:\n+        if not output_path.exists():\n+            output_path.mkdir(parents=True)\n+            msg.good(f""Created output directory: {output_path}"")\n+        msg.info(f""Saving to output directory: {output_path}"")\n+    setup_gpu(use_gpu)\n+    teacher = util.load_model(teacher_model)\n+    with show_validation_error(student_config_path):\n+        config = util.load_config(\n+            student_config_path, overrides=overrides, interpolate=False\n+        )\n+    msg.divider(""Initializing pipeline"")\n+    with show_validation_error(student_config_path, hint_fill=False):\n+        student = init_nlp_student(config, teacher, use_gpu=use_gpu)\n+\n+    msg.good(""Initialized pipeline"")",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f53b,explosion/spaCy,1815880622,1560701753,svlandeg,8796347,2024-04-11T09:19:03+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\n    msg.divider(""Distilling student pipeline from teacher"")\r\n```",true,spacy/cli/distill.py,NaN,"@@ -0,0 +1,98 @@\n+import logging\n+import sys\n+from pathlib import Path\n+from typing import Any, Dict, Optional, Union\n+\n+import typer\n+from wasabi import msg\n+\n+from .. import util\n+from ..pipeline.trainable_pipe import TrainablePipe\n+from ..schemas import ConfigSchemaDistill\n+from ..training.initialize import init_nlp_student\n+from ..training.loop import distill as distill_nlp\n+from ._util import (\n+    Arg,\n+    Opt,\n+    app,\n+    import_code_paths,\n+    parse_config_overrides,\n+    setup_gpu,\n+    show_validation_error,\n+)\n+\n+\n+@app.command(\n+    ""distill"",\n+    context_settings={""allow_extra_args"": True, ""ignore_unknown_options"": True},\n+)\n+def distill_cli(\n+    # fmt: off\n+    ctx: typer.Context,  # This is only used to read additional arguments\n+    teacher_model: str = Arg(..., help=""Teacher model name or path""),\n+    student_config_path: Path = Arg(..., help=""Path to config file"", exists=True, allow_dash=True),\n+    output_path: Optional[Path] = Opt(None, ""--output"", ""--output-path"", ""-o"", help=""Output directory to store trained pipeline in""),\n+    code_path: str = Opt("""", ""--code"", ""-c"", help=""Comma-separated paths to Python files with additional code (registered functions) to be imported""),\n+    verbose: bool = Opt(False, ""--verbose"", ""-V"", ""-VV"", help=""Display more information for debugging purposes""),\n+    use_gpu: int = Opt(-1, ""--gpu-id"", ""-g"", help=""GPU ID or -1 for CPU"")\n+    # fmt: on\n+):\n+    """"""\n+    Distill a spaCy pipeline from a teacher model.\n+\n+    DOCS: https://spacy.io/api/cli#distill\n+    """"""\n+    util.logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n+    overrides = parse_config_overrides(ctx.args)\n+    import_code_paths(code_path)\n+    distill(\n+        teacher_model,\n+        student_config_path,\n+        output_path,\n+        use_gpu=use_gpu,\n+        overrides=overrides,\n+    )\n+\n+\n+def distill(\n+    teacher_model: str,\n+    student_config_path: Union[str, Path],\n+    output_path: Optional[Union[str, Path]] = None,\n+    *,\n+    use_gpu: int = -1,\n+    overrides: Dict[str, Any] = util.SimpleFrozenDict(),\n+):\n+    student_config_path = util.ensure_path(student_config_path)\n+    output_path = util.ensure_path(output_path)\n+    # Make sure all files and paths exists if they are needed\n+    if not student_config_path or (\n+        str(student_config_path) != ""-"" and not student_config_path.exists()\n+    ):\n+        msg.fail(""Student config file not found"", student_config_path, exits=1)\n+    if not output_path:\n+        msg.info(""No output directory provided"")\n+    else:\n+        if not output_path.exists():\n+            output_path.mkdir(parents=True)\n+            msg.good(f""Created output directory: {output_path}"")\n+        msg.info(f""Saving to output directory: {output_path}"")\n+    setup_gpu(use_gpu)\n+    teacher = util.load_model(teacher_model)\n+    with show_validation_error(student_config_path):\n+        config = util.load_config(\n+            student_config_path, overrides=overrides, interpolate=False\n+        )\n+    msg.divider(""Initializing pipeline"")\n+    with show_validation_error(student_config_path, hint_fill=False):\n+        student = init_nlp_student(config, teacher, use_gpu=use_gpu)\n+\n+    msg.good(""Initialized pipeline"")\n+    msg.divider(""Distilling pipeline"")",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f53c,explosion/spaCy,1815880622,1560704210,svlandeg,8796347,2024-04-11T09:20:55+00:00,2024-04-11T09:27:27+00:00,```suggestion\r\nfrom a larger high-accuracy model. Since distillation uses the activations of the\r\n```,true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f53d,explosion/spaCy,1815880622,1560704811,svlandeg,8796347,2024-04-11T09:21:25+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\nteacher, distillation can be performed on a corpus of raw text without (gold standard)\r\n```\r\n(just to make it abundantly clear)",true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f53e,explosion/spaCy,1815880622,1560705769,svlandeg,8796347,2024-04-11T09:22:12+00:00,2024-04-11T09:27:27+00:00,"```suggestion\r\n`distill` will save out the best performing pipeline across all epochs, as well as the final\r\n```\r\nMostly to distinguish ""model"" from ""pipeline"" - the eval is holistic across all models in the one pipeline",true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f53f,explosion/spaCy,1815880622,1560707524,svlandeg,8796347,2024-04-11T09:23:27+00:00,2024-04-11T09:27:27+00:00,"Refer to Weasel instead: https://github.com/explosion/weasel (we should update this in other places as well, but that's outside of the scope of this PR)",true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final\n+pipeline. The `--code` argument can be used to provide a Python file that's\n+imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the new\n+[spaCy projects](/usage/projects).",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f540,explosion/spaCy,1815880622,1560710016,svlandeg,8796347,2024-04-11T09:25:18+00:00,2024-04-11T09:27:27+00:00,Should we add a comment that the loop still needs a dev set of gold annotations to evaluate on? To make it more clear why the example CLI statement 👇  has a `--paths.dev ./dev` overwrite part.,true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f541,explosion/spaCy,1815880622,1560711164,svlandeg,8796347,2024-04-11T09:26:12+00:00,2024-04-11T09:27:27+00:00,"This should mention that it can also be a `str` to refer to an installed pipeline, right?",true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final\n+pipeline. The `--code` argument can be used to provide a Python file that's\n+imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the new\n+[spaCy projects](/usage/projects).\n+\n+> #### Example\n+>\n+> ```bash\n+> $ python -m spacy distill teacher-pipeline student.cfg --output ./output --paths.distill ./distill --paths.dev ./dev\n+> ```\n+\n+```bash\n+$ python -m spacy distill [teacher_model] [student_config_path] [--output] [--code] [--verbose] [--gpu-id] [overrides]\n+```\n+\n+| Name                  | Description                                                                                                                                                                                            |\n+| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n+| `teacher_model`       | The teacher pipeline to distill the student from. ~~Path (positional)~~                                                                                                                                |",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f542,explosion/spaCy,1815880622,1560711656,svlandeg,8796347,2024-04-11T09:26:33+00:00,2024-04-11T09:27:27+00:00,And what happens when it's `None`?,true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final\n+pipeline. The `--code` argument can be used to provide a Python file that's\n+imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the new\n+[spaCy projects](/usage/projects).\n+\n+> #### Example\n+>\n+> ```bash\n+> $ python -m spacy distill teacher-pipeline student.cfg --output ./output --paths.distill ./distill --paths.dev ./dev\n+> ```\n+\n+```bash\n+$ python -m spacy distill [teacher_model] [student_config_path] [--output] [--code] [--verbose] [--gpu-id] [overrides]\n+```\n+\n+| Name                  | Description                                                                                                                                                                                            |\n+| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n+| `teacher_model`       | The teacher pipeline to distill the student from. ~~Path (positional)~~                                                                                                                                |\n+| `student_config_path` | The configuration of the student pipeline. ~~Path (positional)~~                                                                                                                                       |\n+| `--output`, `-o`      | Directory to store the distilled pipeline in. Will be created if it doesn't exist. ~~Optional[Path] \(option)~~                                                                                        |",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f543,explosion/spaCy,1815880622,1560712400,svlandeg,8796347,2024-04-11T09:27:12+00:00,2024-04-11T09:27:27+00:00,Does it? 🤭 ,true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final\n+pipeline. The `--code` argument can be used to provide a Python file that's\n+imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the new\n+[spaCy projects](/usage/projects).\n+\n+> #### Example\n+>\n+> ```bash\n+> $ python -m spacy distill teacher-pipeline student.cfg --output ./output --paths.distill ./distill --paths.dev ./dev\n+> ```\n+\n+```bash\n+$ python -m spacy distill [teacher_model] [student_config_path] [--output] [--code] [--verbose] [--gpu-id] [overrides]\n+```\n+\n+| Name                  | Description                                                                                                                                                                                            |\n+| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n+| `teacher_model`       | The teacher pipeline to distill the student from. ~~Path (positional)~~                                                                                                                                |\n+| `student_config_path` | The configuration of the student pipeline. ~~Path (positional)~~                                                                                                                                       |\n+| `--output`, `-o`      | Directory to store the distilled pipeline in. Will be created if it doesn't exist. ~~Optional[Path] \(option)~~                                                                                        |\n+| `--code`, `-c`        | Comma-separated paths to Python files with additional code to be imported. Allows [registering custom functions](/usage/training#custom-functions) for new architectures. ~~Optional[Path] \(option)~~ |\n+| `--verbose`, `-V`     | Show more detailed messages during distillation. ~~bool (flag)~~                                                                                                                                       |\n+| `--gpu-id`, `-g`      | GPU ID or `-1` for CPU. Defaults to `-1`. ~~int (option)~~                                                                                                                                             |\n+| `--help`, `-h`        | Show help message and available arguments. ~~bool (flag)~~                                                                                                                                             |\n+| **CREATES**           | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                                                                                                          |",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f544,explosion/spaCy,1815880622,1561057055,danieldk,49398,2024-04-11T13:49:53+00:00,2024-04-11T13:49:53+00:00,Fixed.,true,spacy/cli/distill.py,NaN,"@@ -0,0 +1,98 @@\n+import logging\n+import sys\n+from pathlib import Path\n+from typing import Any, Dict, Optional, Union\n+\n+import typer\n+from wasabi import msg\n+\n+from .. import util\n+from ..pipeline.trainable_pipe import TrainablePipe\n+from ..schemas import ConfigSchemaDistill\n+from ..training.initialize import init_nlp_student\n+from ..training.loop import distill as distill_nlp\n+from ._util import (\n+    Arg,\n+    Opt,\n+    app,\n+    import_code_paths,\n+    parse_config_overrides,\n+    setup_gpu,\n+    show_validation_error,\n+)\n+\n+\n+@app.command(\n+    ""distill"",\n+    context_settings={""allow_extra_args"": True, ""ignore_unknown_options"": True},\n+)\n+def distill_cli(\n+    # fmt: off\n+    ctx: typer.Context,  # This is only used to read additional arguments\n+    teacher_model: str = Arg(..., help=""Teacher model name or path""),\n+    student_config_path: Path = Arg(..., help=""Path to config file"", exists=True, allow_dash=True),\n+    output_path: Optional[Path] = Opt(None, ""--output"", ""--output-path"", ""-o"", help=""Output directory to store trained pipeline in""),\n+    code_path: str = Opt("""", ""--code"", ""-c"", help=""Comma-separated paths to Python files with additional code (registered functions) to be imported""),\n+    verbose: bool = Opt(False, ""--verbose"", ""-V"", ""-VV"", help=""Display more information for debugging purposes""),\n+    use_gpu: int = Opt(-1, ""--gpu-id"", ""-g"", help=""GPU ID or -1 for CPU"")\n+    # fmt: on\n+):\n+    """"""\n+    Distill a spaCy pipeline from a teacher model.\n+\n+    DOCS: https://spacy.io/api/cli#distill\n+    """"""\n+    util.logger.setLevel(logging.DEBUG if verbose else logging.INFO)\n+    overrides = parse_config_overrides(ctx.args)\n+    import_code_paths(code_path)\n+    distill(\n+        teacher_model,\n+        student_config_path,\n+        output_path,\n+        use_gpu=use_gpu,\n+        overrides=overrides,\n+    )\n+\n+\n+def distill(\n+    teacher_model: str,",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f545,explosion/spaCy,1815880622,1561057454,danieldk,49398,2024-04-11T13:50:08+00:00,2024-04-11T13:50:09+00:00,Done.,true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final\n+pipeline. The `--code` argument can be used to provide a Python file that's\n+imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the new\n+[spaCy projects](/usage/projects).",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f546,explosion/spaCy,1815880622,1561059912,danieldk,49398,2024-04-11T13:51:46+00:00,2024-04-11T13:51:47+00:00,"Yeah, good idea! Added.",true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f547,explosion/spaCy,1815880622,1561075527,danieldk,49398,2024-04-11T14:02:10+00:00,2024-04-11T14:02:11+00:00,Done.,true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final\n+pipeline. The `--code` argument can be used to provide a Python file that's\n+imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the new\n+[spaCy projects](/usage/projects).\n+\n+> #### Example\n+>\n+> ```bash\n+> $ python -m spacy distill teacher-pipeline student.cfg --output ./output --paths.distill ./distill --paths.dev ./dev\n+> ```\n+\n+```bash\n+$ python -m spacy distill [teacher_model] [student_config_path] [--output] [--code] [--verbose] [--gpu-id] [overrides]\n+```\n+\n+| Name                  | Description                                                                                                                                                                                            |\n+| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n+| `teacher_model`       | The teacher pipeline to distill the student from. ~~Path (positional)~~                                                                                                                                |",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f548,explosion/spaCy,1815880622,1561080112,danieldk,49398,2024-04-11T14:04:36+00:00,2024-04-11T14:04:36+00:00,Added a remark that no pipeline is saved when this option is absent.,true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final\n+pipeline. The `--code` argument can be used to provide a Python file that's\n+imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the new\n+[spaCy projects](/usage/projects).\n+\n+> #### Example\n+>\n+> ```bash\n+> $ python -m spacy distill teacher-pipeline student.cfg --output ./output --paths.distill ./distill --paths.dev ./dev\n+> ```\n+\n+```bash\n+$ python -m spacy distill [teacher_model] [student_config_path] [--output] [--code] [--verbose] [--gpu-id] [overrides]\n+```\n+\n+| Name                  | Description                                                                                                                                                                                            |\n+| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n+| `teacher_model`       | The teacher pipeline to distill the student from. ~~Path (positional)~~                                                                                                                                |\n+| `student_config_path` | The configuration of the student pipeline. ~~Path (positional)~~                                                                                                                                       |\n+| `--output`, `-o`      | Directory to store the distilled pipeline in. Will be created if it doesn't exist. ~~Optional[Path] \(option)~~                                                                                        |",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f549,explosion/spaCy,1815880622,1561080684,danieldk,49398,2024-04-11T14:04:49+00:00,2024-04-11T14:04:50+00:00,Oops 🤦 . Fixed.,true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,44 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from large high-accuracy model. Since distillation uses the activations of the\n+teacher, distillation can be performed on a corpus without (gold standard)\n+annotations.\n+\n+`distill` will save out the best model from all epochs, as well as the final\n+pipeline. The `--code` argument can be used to provide a Python file that's\n+imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the new\n+[spaCy projects](/usage/projects).\n+\n+> #### Example\n+>\n+> ```bash\n+> $ python -m spacy distill teacher-pipeline student.cfg --output ./output --paths.distill ./distill --paths.dev ./dev\n+> ```\n+\n+```bash\n+$ python -m spacy distill [teacher_model] [student_config_path] [--output] [--code] [--verbose] [--gpu-id] [overrides]\n+```\n+\n+| Name                  | Description                                                                                                                                                                                            |\n+| --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n+| `teacher_model`       | The teacher pipeline to distill the student from. ~~Path (positional)~~                                                                                                                                |\n+| `student_config_path` | The configuration of the student pipeline. ~~Path (positional)~~                                                                                                                                       |\n+| `--output`, `-o`      | Directory to store the distilled pipeline in. Will be created if it doesn't exist. ~~Optional[Path] \(option)~~                                                                                        |\n+| `--code`, `-c`        | Comma-separated paths to Python files with additional code to be imported. Allows [registering custom functions](/usage/training#custom-functions) for new architectures. ~~Optional[Path] \(option)~~ |\n+| `--verbose`, `-V`     | Show more detailed messages during distillation. ~~bool (flag)~~                                                                                                                                       |\n+| `--gpu-id`, `-g`      | GPU ID or `-1` for CPU. Defaults to `-1`. ~~int (option)~~                                                                                                                                             |\n+| `--help`, `-h`        | Show help message and available arguments. ~~bool (flag)~~                                                                                                                                             |\n+| **CREATES**           | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                                                                                                          |",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f54a,explosion/spaCy,1815880622,1561113110,svlandeg,8796347,2024-04-11T14:22:53+00:00,2024-04-11T14:22:53+00:00,"```suggestion\r\nIf you need to manage complex multi-step training workflows, check out \r\n```",true,website/docs/api/cli.mdx,NaN,"@@ -1699,6 +1702,45 @@ $ python -m spacy project dvc [project_dir] [workflow] [--force] [--verbose] [--\n | `--help`, `-h`    | Show help message and available arguments. ~~bool (flag)~~                                                    |\n | **CREATES**       | A `dvc.yaml` file in the project directory, based on the steps defined in the given workflow.                 |\n \n+## distill {id=""distill"", tag=""experimental"", version=""4""}\n+\n+Distill a _student_ pipeline from a _teacher_ pipeline. Distillation trains the\n+models in the student pipeline on the activations of the teacher's models. A\n+typical use case for distillation is to extract a smaller, more performant model\n+from a larger high-accuracy model. Since distillation uses the activations of\n+the teacher, distillation can be performed on a corpus of raw text without (gold\n+standard) annotations. A development set of gold annotations _is_ needed to\n+evaluate the student pipeline on during distillation.\n+\n+`distill` will save out the best performing pipeline across all epochs, as well\n+as the final pipeline. The `--code` argument can be used to provide a Python\n+file that's imported before the training process starts. This lets you register\n+[custom functions](/usage/training#custom-functions) and architectures and refer\n+to them in your config, all while still using spaCy's built-in `train` workflow.\n+If you need to manage complex multi-step training workflows, check out the",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f54b,explosion/spaCy,1812290811,1668580828,svlandeg,8796347,2024-07-08T12:55:14+00:00,2024-07-08T13:13:20+00:00,"```suggestion\r\n            ""title"": ""GLiNER spaCy Wrapper"",\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f54c,explosion/spaCy,1812290811,1668582510,svlandeg,8796347,2024-07-08T12:56:32+00:00,2024-07-08T13:13:20+00:00,"```suggestion\r\n            ""slogan"": ""Integrating GLiNER's Advanced NER with spaCy"",\r\n            ""description"": ""GLiNER SpaCy Wrapper is a project that brings together GLiNER, a zero-shot Named Entity Recognition (NER) model, with spaCy's NLP capabilities. It provides an easy way to integrate GLiNER within the spaCy environment, thus enhancing NER tasks with GLiNER's features."",\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f54d,explosion/spaCy,1812290811,1668588944,svlandeg,8796347,2024-07-08T13:00:42+00:00,2024-07-08T13:13:20+00:00,"Removing too generic and too specific tags, to be more in line with other entries in the universe:\r\n\r\n```suggestion\r\n            ""tags"": [""NER""]\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f54e,explosion/spaCy,1812290811,1668606981,svlandeg,8796347,2024-07-08T13:12:47+00:00,2024-07-08T13:13:20+00:00,This import doesn't seem necessary:\r\n```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f54f,explosion/spaCy,1812290811,1668580828,svlandeg,8796347,2024-07-08T12:55:14+00:00,2024-07-08T13:13:20+00:00,"```suggestion\r\n            ""title"": ""GLiNER spaCy Wrapper"",\r\n```",true,website/meta/universe.json,NaN,"@@ -4517,7 +4517,36 @@\n                 ""website"": ""https://redfield.ai""\n             },\n             ""category"": [""standalone""]\n+        },\n+        {\n+            ""id"": ""gliner-spacy"",\n+            ""title"": ""GLiNER SpaCy Wrapper"",",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f550,explosion/spaCy,1812290811,1668582510,svlandeg,8796347,2024-07-08T12:56:32+00:00,2024-07-08T13:13:20+00:00,"```suggestion\r\n            ""slogan"": ""Integrating GLiNER's Advanced NER with spaCy"",\r\n            ""description"": ""GLiNER SpaCy Wrapper is a project that brings together GLiNER, a zero-shot Named Entity Recognition (NER) model, with spaCy's NLP capabilities. It provides an easy way to integrate GLiNER within the spaCy environment, thus enhancing NER tasks with GLiNER's features."",\r\n```",true,website/meta/universe.json,NaN,"@@ -4517,7 +4517,36 @@\n                 ""website"": ""https://redfield.ai""\n             },\n             ""category"": [""standalone""]\n+        },\n+        {\n+            ""id"": ""gliner-spacy"",\n+            ""title"": ""GLiNER SpaCy Wrapper"",\n+            ""slogan"": ""Integrating GLiNER's Advanced NER with SpaCy"",\n+            ""description"": ""GLiNER SpaCy Wrapper is a project that brings together GLiNER, a zero-shot Named Entity Recognition (NER) model, with SpaCy's NLP capabilities. It provides an easy way to integrate GLiNER within the SpaCy environment, thus enhancing NER tasks with GLiNER's features."",",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f551,explosion/spaCy,1812290811,1668588944,svlandeg,8796347,2024-07-08T13:00:42+00:00,2024-07-08T13:13:20+00:00,"Removing too generic and too specific tags, to be more in line with other entries in the universe:\r\n\r\n```suggestion\r\n            ""tags"": [""NER""]\r\n```",true,website/meta/universe.json,NaN,"@@ -4517,7 +4517,36 @@\n                 ""website"": ""https://redfield.ai""\n             },\n             ""category"": [""standalone""]\n+        },\n+        {\n+            ""id"": ""gliner-spacy"",\n+            ""title"": ""GLiNER SpaCy Wrapper"",\n+            ""slogan"": ""Integrating GLiNER's Advanced NER with SpaCy"",\n+            ""description"": ""GLiNER SpaCy Wrapper is a project that brings together GLiNER, a zero-shot Named Entity Recognition (NER) model, with SpaCy's NLP capabilities. It provides an easy way to integrate GLiNER within the SpaCy environment, thus enhancing NER tasks with GLiNER's features."",\n+            ""github"": ""theirstory/gliner-spacy"",\n+            ""pip"": ""gliner-spacy"",\n+            ""code_example"": [\n+                ""import spacy"",\n+                ""from gliner_spacy.pipeline import GlinerSpacy"",\n+                """",\n+                ""nlp = spacy.blank('en')"",\n+                ""nlp.add_pipe('gliner_spacy')"",\n+                ""text = 'This is a text about Bill Gates and Microsoft.'"",\n+                ""doc = nlp(text)"",\n+                """",\n+                ""for ent in doc.ents:"",\n+                ""    print(ent.text, ent.label_)""\n+            ],\n+            ""code_language"": ""python"",\n+            ""url"": ""https://github.com/theirstory/gliner-spacy"",\n+            ""author"": ""TheirStory"",\n+            ""author_links"": {\n+                ""website"": ""https://theirstory.io""\n+            },\n+            ""category"": [""pipeline""],\n+            ""tags"": [""gliner"", ""spacy"", ""nlp"", ""ner""]",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f552,explosion/spaCy,1812290811,1668606981,svlandeg,8796347,2024-07-08T13:12:47+00:00,2024-07-08T13:13:20+00:00,This import doesn't seem necessary:\r\n```suggestion\r\n```,true,website/meta/universe.json,NaN,"@@ -4517,7 +4517,36 @@\n                 ""website"": ""https://redfield.ai""\n             },\n             ""category"": [""standalone""]\n+        },\n+        {\n+            ""id"": ""gliner-spacy"",\n+            ""title"": ""GLiNER SpaCy Wrapper"",\n+            ""slogan"": ""Integrating GLiNER's Advanced NER with SpaCy"",\n+            ""description"": ""GLiNER SpaCy Wrapper is a project that brings together GLiNER, a zero-shot Named Entity Recognition (NER) model, with SpaCy's NLP capabilities. It provides an easy way to integrate GLiNER within the SpaCy environment, thus enhancing NER tasks with GLiNER's features."",\n+            ""github"": ""theirstory/gliner-spacy"",\n+            ""pip"": ""gliner-spacy"",\n+            ""code_example"": [\n+                ""import spacy"",\n+                ""from gliner_spacy.pipeline import GlinerSpacy"",",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f553,explosion/spaCy,1794491429,1541576964,svlandeg,8796347,2024-03-27T17:47:53+00:00,2024-03-27T17:47:53+00:00,"This whole bit is surely pretty hacky, but considering bug 3 as explained in the PR, I don't see a better option other than changing the entire mechanism how evaluation/scoring of a pipeline works...",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f554,explosion/spaCy,1794491429,1541578270,svlandeg,8796347,2024-03-27T17:48:45+00:00,2024-03-27T17:48:45+00:00,"Making a copy here feels safest? Not 100% about all the possible interactions with all other components in the pipeline, before or after, annotated or not, and frozen or not...",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f555,explosion/spaCy,1794491429,1545262110,rmitsch,7152225,2024-03-30T09:42:51+00:00,2024-03-30T09:56:47+00:00,"```suggestion\r\n        def _score_augmented(examples: Iterable[Example], **kwargs):\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f556,explosion/spaCy,1794491429,1545262191,rmitsch,7152225,2024-03-30T09:43:25+00:00,2024-03-30T09:56:47+00:00,"Agreed, this is not really satisfying. The workaround makes sense in this context though.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f557,explosion/spaCy,1794491429,1545262324,rmitsch,7152225,2024-03-30T09:44:37+00:00,2024-03-30T09:56:47+00:00,"Hm, do we manipulate examples in other components? I'm also unsure about this. Either way :+1: for copying it.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f558,explosion/spaCy,1794491429,1545262711,rmitsch,7152225,2024-03-30T09:47:36+00:00,2024-03-30T09:56:47+00:00,"I'm not very happy with the method name - this doesn't ""augment"" in the usual ML ""data augmentation"" sense, so this may be confusing. Maybe something like `_adopt_[predicted_]examples` or `_set_gold_ents`?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f559,explosion/spaCy,1794491429,1545262770,rmitsch,7152225,2024-03-30T09:48:21+00:00,2024-03-30T09:56:47+00:00,Why exactly 10 here? I don't remember the specifics about this anymore.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f55a,explosion/spaCy,1794491429,1545263004,rmitsch,7152225,2024-03-30T09:50:06+00:00,2024-03-30T09:56:47+00:00,Params in docstring would be great for public methods :grin: ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f55b,explosion/spaCy,1794491429,1545263279,rmitsch,7152225,2024-03-30T09:52:53+00:00,2024-03-30T09:56:47+00:00,"Formatting in suggestion TBD. Ignore if you separated on purpose for easier debugging.\r\n```suggestion\r\n    assert all([metric in eval for metric in (""nel_macro_p"", ""nel_macro_r"", ""nel_macro_f"", ""nel_micro_p"", ""nel_micro_r"", ""nel_micro_f"", ""nel_f_per_type"")]\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f55c,explosion/spaCy,1794491429,1545263455,rmitsch,7152225,2024-03-30T09:54:40+00:00,2024-03-30T09:56:47+00:00,"Why `""use_gold_ents"": False` here?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f55d,explosion/spaCy,1794491429,1545263542,rmitsch,7152225,2024-03-30T09:55:28+00:00,2024-03-30T09:56:47+00:00,TBD in this PR?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f55e,explosion/spaCy,1794491429,1545263606,rmitsch,7152225,2024-03-30T09:55:57+00:00,2024-03-30T09:56:47+00:00,"Formatting in suggestion TBD. Ignore if you separated on purpose for easier debugging.\r\n```suggestion\r\n    assert all([metric in eval for metric in (""nel_macro_p"", ""nel_macro_r"", ""nel_macro_f"", ""nel_micro_p"", ""nel_micro_r"", ""nel_micro_f"", ""nel_f_per_type"")]\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f55f,explosion/spaCy,1794491429,1547281271,svlandeg,8796347,2024-04-02T07:21:52+00:00,2024-04-02T07:21:52+00:00,In unit tests I prefer to test one thing per line so it's clear what fails when it does ;-),false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f560,explosion/spaCy,1794491429,1547282347,svlandeg,8796347,2024-04-02T07:22:47+00:00,2024-04-02T07:22:47+00:00,This is a different issue so a separate PR would be prefered,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f561,explosion/spaCy,1794491429,1547284550,svlandeg,8796347,2024-04-02T07:24:15+00:00,2024-04-02T07:25:05+00:00,Because there is an `ner` component in this pipeline - the point is to try and overfit when there is both an NER and an EL component.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f562,explosion/spaCy,1794491429,1547289505,svlandeg,8796347,2024-04-02T07:28:33+00:00,2024-04-02T07:28:34+00:00,We do this everywhere just to have some data for `model.initialize()` so that Thinc can infer the correct `nI`/`nO` dimensions.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f563,explosion/spaCy,1794491429,1547301769,svlandeg,8796347,2024-04-02T07:32:36+00:00,2024-04-02T07:32:36+00:00,True! I renamed it to `_ensure_ents` as the function doesn't set any ents if `use_gold_ents` is `False`.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f564,explosion/spaCy,1794491429,1541576964,svlandeg,8796347,2024-03-27T17:47:53+00:00,2024-03-27T17:47:53+00:00,"This whole bit is surely pretty hacky, but considering bug 3 as explained in the PR, I don't see a better option other than changing the entire mechanism how evaluation/scoring of a pipeline works...",true,spacy/pipeline/entity_linker.py,NaN,"@@ -235,14 +234,42 @@ def __init__(\n         self.cfg: Dict[str, Any] = {""overwrite"": overwrite}\n         self.distance = CosineDistance(normalize=False)\n         self.kb = generate_empty_kb(self.vocab, entity_vector_length)\n-        self.scorer = scorer\n         self.use_gold_ents = use_gold_ents\n         self.candidates_batch_size = candidates_batch_size\n         self.threshold = threshold\n \n         if candidates_batch_size < 1:\n             raise ValueError(Errors.E1044)\n \n+        def _score_augmented(examples, **kwargs):\n+            # Because of how spaCy works, we can't just score immediately, because Language.evaluate\n+            # calls pipe() on the predicted docs, which won't have entities if there is no NER in the pipeline.\n+            if not self.use_gold_ents:\n+                return scorer(examples, **kwargs)\n+            else:\n+                examples = self._augment_examples(examples)\n+                docs = self.pipe(\n+                    (eg.predicted for eg in examples),\n+                )\n+                for eg, doc in zip(examples, docs):\n+                    eg.predicted = doc\n+                return scorer(examples, **kwargs)\n+\n+        self.scorer = _score_augmented",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f565,explosion/spaCy,1794491429,1541578270,svlandeg,8796347,2024-03-27T17:48:45+00:00,2024-03-27T17:48:45+00:00,"Making a copy here feels safest? Not 100% about all the possible interactions with all other components in the pipeline, before or after, annotated or not, and frozen or not...",true,spacy/pipeline/entity_linker.py,55,"@@ -235,14 +234,42 @@ def __init__(\n         self.cfg: Dict[str, Any] = {""overwrite"": overwrite}\n         self.distance = CosineDistance(normalize=False)\n         self.kb = generate_empty_kb(self.vocab, entity_vector_length)\n-        self.scorer = scorer\n         self.use_gold_ents = use_gold_ents\n         self.candidates_batch_size = candidates_batch_size\n         self.threshold = threshold\n \n         if candidates_batch_size < 1:\n             raise ValueError(Errors.E1044)\n \n+        def _score_augmented(examples, **kwargs):\n+            # Because of how spaCy works, we can't just score immediately, because Language.evaluate\n+            # calls pipe() on the predicted docs, which won't have entities if there is no NER in the pipeline.\n+            if not self.use_gold_ents:\n+                return scorer(examples, **kwargs)\n+            else:\n+                examples = self._augment_examples(examples)\n+                docs = self.pipe(\n+                    (eg.predicted for eg in examples),\n+                )\n+                for eg, doc in zip(examples, docs):\n+                    eg.predicted = doc\n+                return scorer(examples, **kwargs)\n+\n+        self.scorer = _score_augmented\n+\n+    def _augment_examples(self, examples: Iterable[Example]) -> Iterable[Example]:\n+        """"""If use_gold_ents is true, set the gold entities to (a copy of) eg.predicted.""""""\n+        if not self.use_gold_ents:\n+            return examples\n+\n+        new_examples = []\n+        for eg in examples:\n+            ents, _ = eg.get_aligned_ents_and_ner()\n+            new_eg = eg.copy()",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f566,explosion/spaCy,1794491429,1545262110,rmitsch,7152225,2024-03-30T09:42:51+00:00,2024-03-30T09:56:47+00:00,"```suggestion\r\n        def _score_augmented(examples: Iterable[Example], **kwargs):\r\n```",true,spacy/pipeline/entity_linker.py,NaN,"@@ -235,14 +234,42 @@ def __init__(\n         self.cfg: Dict[str, Any] = {""overwrite"": overwrite}\n         self.distance = CosineDistance(normalize=False)\n         self.kb = generate_empty_kb(self.vocab, entity_vector_length)\n-        self.scorer = scorer\n         self.use_gold_ents = use_gold_ents\n         self.candidates_batch_size = candidates_batch_size\n         self.threshold = threshold\n \n         if candidates_batch_size < 1:\n             raise ValueError(Errors.E1044)\n \n+        def _score_augmented(examples, **kwargs):",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f567,explosion/spaCy,1794491429,1545262191,rmitsch,7152225,2024-03-30T09:43:25+00:00,2024-03-30T09:56:47+00:00,"Agreed, this is not really satisfying. The workaround makes sense in this context though.",true,spacy/pipeline/entity_linker.py,NaN,"@@ -235,14 +234,42 @@ def __init__(\n         self.cfg: Dict[str, Any] = {""overwrite"": overwrite}\n         self.distance = CosineDistance(normalize=False)\n         self.kb = generate_empty_kb(self.vocab, entity_vector_length)\n-        self.scorer = scorer\n         self.use_gold_ents = use_gold_ents\n         self.candidates_batch_size = candidates_batch_size\n         self.threshold = threshold\n \n         if candidates_batch_size < 1:\n             raise ValueError(Errors.E1044)\n \n+        def _score_augmented(examples, **kwargs):\n+            # Because of how spaCy works, we can't just score immediately, because Language.evaluate\n+            # calls pipe() on the predicted docs, which won't have entities if there is no NER in the pipeline.\n+            if not self.use_gold_ents:\n+                return scorer(examples, **kwargs)\n+            else:\n+                examples = self._augment_examples(examples)\n+                docs = self.pipe(\n+                    (eg.predicted for eg in examples),\n+                )\n+                for eg, doc in zip(examples, docs):\n+                    eg.predicted = doc\n+                return scorer(examples, **kwargs)\n+\n+        self.scorer = _score_augmented",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f568,explosion/spaCy,1794491429,1545262324,rmitsch,7152225,2024-03-30T09:44:37+00:00,2024-03-30T09:56:47+00:00,"Hm, do we manipulate examples in other components? I'm also unsure about this. Either way :+1: for copying it.",true,spacy/pipeline/entity_linker.py,55,"@@ -235,14 +234,42 @@ def __init__(\n         self.cfg: Dict[str, Any] = {""overwrite"": overwrite}\n         self.distance = CosineDistance(normalize=False)\n         self.kb = generate_empty_kb(self.vocab, entity_vector_length)\n-        self.scorer = scorer\n         self.use_gold_ents = use_gold_ents\n         self.candidates_batch_size = candidates_batch_size\n         self.threshold = threshold\n \n         if candidates_batch_size < 1:\n             raise ValueError(Errors.E1044)\n \n+        def _score_augmented(examples, **kwargs):\n+            # Because of how spaCy works, we can't just score immediately, because Language.evaluate\n+            # calls pipe() on the predicted docs, which won't have entities if there is no NER in the pipeline.\n+            if not self.use_gold_ents:\n+                return scorer(examples, **kwargs)\n+            else:\n+                examples = self._augment_examples(examples)\n+                docs = self.pipe(\n+                    (eg.predicted for eg in examples),\n+                )\n+                for eg, doc in zip(examples, docs):\n+                    eg.predicted = doc\n+                return scorer(examples, **kwargs)\n+\n+        self.scorer = _score_augmented\n+\n+    def _augment_examples(self, examples: Iterable[Example]) -> Iterable[Example]:\n+        """"""If use_gold_ents is true, set the gold entities to (a copy of) eg.predicted.""""""\n+        if not self.use_gold_ents:\n+            return examples\n+\n+        new_examples = []\n+        for eg in examples:\n+            ents, _ = eg.get_aligned_ents_and_ner()\n+            new_eg = eg.copy()",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f569,explosion/spaCy,1794491429,1545262711,rmitsch,7152225,2024-03-30T09:47:36+00:00,2024-03-30T09:56:47+00:00,"I'm not very happy with the method name - this doesn't ""augment"" in the usual ML ""data augmentation"" sense, so this may be confusing. Maybe something like `_adopt_[predicted_]examples` or `_set_gold_ents`?",true,spacy/pipeline/entity_linker.py,NaN,"@@ -235,14 +234,42 @@ def __init__(\n         self.cfg: Dict[str, Any] = {""overwrite"": overwrite}\n         self.distance = CosineDistance(normalize=False)\n         self.kb = generate_empty_kb(self.vocab, entity_vector_length)\n-        self.scorer = scorer\n         self.use_gold_ents = use_gold_ents\n         self.candidates_batch_size = candidates_batch_size\n         self.threshold = threshold\n \n         if candidates_batch_size < 1:\n             raise ValueError(Errors.E1044)\n \n+        def _score_augmented(examples, **kwargs):\n+            # Because of how spaCy works, we can't just score immediately, because Language.evaluate\n+            # calls pipe() on the predicted docs, which won't have entities if there is no NER in the pipeline.\n+            if not self.use_gold_ents:\n+                return scorer(examples, **kwargs)\n+            else:\n+                examples = self._augment_examples(examples)\n+                docs = self.pipe(\n+                    (eg.predicted for eg in examples),\n+                )\n+                for eg, doc in zip(examples, docs):\n+                    eg.predicted = doc\n+                return scorer(examples, **kwargs)\n+\n+        self.scorer = _score_augmented\n+\n+    def _augment_examples(self, examples: Iterable[Example]) -> Iterable[Example]:",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f56a,explosion/spaCy,1794491429,1545262770,rmitsch,7152225,2024-03-30T09:48:21+00:00,2024-03-30T09:56:47+00:00,Why exactly 10 here? I don't remember the specifics about this anymore.,true,spacy/pipeline/entity_linker.py,NaN,"@@ -284,11 +311,9 @@ def initialize(\n         nO = self.kb.entity_vector_length\n         doc_sample = []\n         vector_sample = []\n-        for eg in islice(get_examples(), 10):\n+        examples = self._augment_examples(islice(get_examples(), 10))",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f56b,explosion/spaCy,1794491429,1545263004,rmitsch,7152225,2024-03-30T09:50:06+00:00,2024-03-30T09:56:47+00:00,Params in docstring would be great for public methods :grin: ,true,spacy/pipeline/entity_linker.py,NaN,"@@ -354,41 +379,29 @@ def update(\n         losses.setdefault(self.name, 0.0)\n         if not examples:\n             return losses\n+        examples = self._augment_examples(examples)\n         validate_examples(examples, ""EntityLinker.update"")\n \n-        set_dropout_rate(self.model, drop)\n-        docs = [eg.predicted for eg in examples]\n-        # save to restore later\n-        old_ents = [doc.ents for doc in docs]\n-\n-        for doc, ex in zip(docs, examples):\n-            if self.use_gold_ents:\n-                ents, _ = ex.get_aligned_ents_and_ner()\n-                doc.ents = ents\n-            else:\n-                # only keep matching ents\n-                doc.ents = ex.get_matching_ents()\n-\n         # make sure we have something to learn from, if not, short-circuit\n         if not self.batch_has_learnable_example(examples):\n             return losses\n \n+        set_dropout_rate(self.model, drop)\n+        docs = [eg.predicted for eg in examples]\n         sentence_encodings, bp_context = self.model.begin_update(docs)\n \n-        # now restore the ents\n-        for doc, old in zip(docs, old_ents):\n-            doc.ents = old\n-\n         loss, d_scores = self.get_loss(\n             sentence_encodings=sentence_encodings, examples=examples\n         )\n         bp_context(d_scores)\n         if sgd is not None:\n             self.finish_update(sgd)\n         losses[self.name] += loss\n+\n         return losses\n \n     def get_loss(self, examples: Iterable[Example], sentence_encodings: Floats2d):\n+        """"""Here, we assume that get_loss is called with augmented examples if need be""""""",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f56c,explosion/spaCy,1794491429,1545263279,rmitsch,7152225,2024-03-30T09:52:53+00:00,2024-03-30T09:56:47+00:00,"Formatting in suggestion TBD. Ignore if you separated on purpose for easier debugging.\r\n```suggestion\r\n    assert all([metric in eval for metric in (""nel_macro_p"", ""nel_macro_r"", ""nel_macro_f"", ""nel_micro_p"", ""nel_micro_r"", ""nel_micro_f"", ""nel_f_per_type"")]\r\n```",true,spacy/tests/pipeline/test_entity_linker.py,31,"@@ -807,6 +809,107 @@ def create_kb(vocab):\n     assert_equal(batch_deps_1, batch_deps_2)\n     assert_equal(batch_deps_1, no_batch_deps)\n \n+    eval = nlp.evaluate(train_examples)\n+    assert ""nel_macro_p"" in eval\n+    assert ""nel_macro_r"" in eval\n+    assert ""nel_macro_f"" in eval\n+    assert ""nel_micro_p"" in eval\n+    assert ""nel_micro_r"" in eval\n+    assert ""nel_micro_f"" in eval\n+    assert ""nel_f_per_type"" in eval",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f56d,explosion/spaCy,1794491429,1545263455,rmitsch,7152225,2024-03-30T09:54:40+00:00,2024-03-30T09:56:47+00:00,"Why `""use_gold_ents"": False` here?",true,spacy/tests/pipeline/test_entity_linker.py,67,"@@ -807,6 +809,107 @@ def create_kb(vocab):\n     assert_equal(batch_deps_1, batch_deps_2)\n     assert_equal(batch_deps_1, no_batch_deps)\n \n+    eval = nlp.evaluate(train_examples)\n+    assert ""nel_macro_p"" in eval\n+    assert ""nel_macro_r"" in eval\n+    assert ""nel_macro_f"" in eval\n+    assert ""nel_micro_p"" in eval\n+    assert ""nel_micro_r"" in eval\n+    assert ""nel_micro_f"" in eval\n+    assert ""nel_f_per_type"" in eval\n+    assert ""PERSON"" in eval[""nel_f_per_type""]\n+\n+    assert eval[""nel_macro_f""] > 0\n+    assert eval[""nel_micro_f""] > 0\n+\n+\n+def test_overfitting_IO_with_ner():\n+    # Simple test to try and overfit the NER and NEL component in combination - ensuring the ML models work correctly\n+    nlp = English()\n+    vector_length = 3\n+    assert ""Q2146908"" not in nlp.vocab.strings\n+\n+    # Convert the texts to docs to make sure we have doc.ents set for the training examples\n+    train_examples = []\n+    for text, annotation in TRAIN_DATA:\n+        doc = nlp(text)\n+        train_examples.append(Example.from_dict(doc, annotation))\n+\n+    def create_kb(vocab):\n+        # create artificial KB - assign same prior weight to the two russ cochran's\n+        # Q2146908 (Russ Cochran): American golfer\n+        # Q7381115 (Russ Cochran): publisher\n+        mykb = InMemoryLookupKB(vocab, entity_vector_length=vector_length)\n+        mykb.add_entity(entity=""Q2146908"", freq=12, entity_vector=[6, -4, 3])\n+        mykb.add_entity(entity=""Q7381115"", freq=12, entity_vector=[9, 1, -7])\n+        mykb.add_alias(\n+            alias=""Russ Cochran"",\n+            entities=[""Q2146908"", ""Q7381115""],\n+            probabilities=[0.5, 0.5],\n+        )\n+        return mykb\n+\n+    # Create the NER and EL components and add them to the pipeline\n+    ner = nlp.add_pipe(""ner"", first=True)\n+    entity_linker = nlp.add_pipe(\n+        ""entity_linker"", last=True, config={""use_gold_ents"": False}",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f56e,explosion/spaCy,1794491429,1545263542,rmitsch,7152225,2024-03-30T09:55:28+00:00,2024-03-30T09:56:47+00:00,TBD in this PR?,true,spacy/tests/pipeline/test_entity_linker.py,98,"@@ -807,6 +809,107 @@ def create_kb(vocab):\n     assert_equal(batch_deps_1, batch_deps_2)\n     assert_equal(batch_deps_1, no_batch_deps)\n \n+    eval = nlp.evaluate(train_examples)\n+    assert ""nel_macro_p"" in eval\n+    assert ""nel_macro_r"" in eval\n+    assert ""nel_macro_f"" in eval\n+    assert ""nel_micro_p"" in eval\n+    assert ""nel_micro_r"" in eval\n+    assert ""nel_micro_f"" in eval\n+    assert ""nel_f_per_type"" in eval\n+    assert ""PERSON"" in eval[""nel_f_per_type""]\n+\n+    assert eval[""nel_macro_f""] > 0\n+    assert eval[""nel_micro_f""] > 0\n+\n+\n+def test_overfitting_IO_with_ner():\n+    # Simple test to try and overfit the NER and NEL component in combination - ensuring the ML models work correctly\n+    nlp = English()\n+    vector_length = 3\n+    assert ""Q2146908"" not in nlp.vocab.strings\n+\n+    # Convert the texts to docs to make sure we have doc.ents set for the training examples\n+    train_examples = []\n+    for text, annotation in TRAIN_DATA:\n+        doc = nlp(text)\n+        train_examples.append(Example.from_dict(doc, annotation))\n+\n+    def create_kb(vocab):\n+        # create artificial KB - assign same prior weight to the two russ cochran's\n+        # Q2146908 (Russ Cochran): American golfer\n+        # Q7381115 (Russ Cochran): publisher\n+        mykb = InMemoryLookupKB(vocab, entity_vector_length=vector_length)\n+        mykb.add_entity(entity=""Q2146908"", freq=12, entity_vector=[6, -4, 3])\n+        mykb.add_entity(entity=""Q7381115"", freq=12, entity_vector=[9, 1, -7])\n+        mykb.add_alias(\n+            alias=""Russ Cochran"",\n+            entities=[""Q2146908"", ""Q7381115""],\n+            probabilities=[0.5, 0.5],\n+        )\n+        return mykb\n+\n+    # Create the NER and EL components and add them to the pipeline\n+    ner = nlp.add_pipe(""ner"", first=True)\n+    entity_linker = nlp.add_pipe(\n+        ""entity_linker"", last=True, config={""use_gold_ents"": False}\n+    )\n+    entity_linker.set_kb(create_kb)\n+\n+    train_examples = []\n+    for text, annotations in TRAIN_DATA:\n+        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))\n+        for ent in annotations.get(""entities""):\n+            ner.add_label(ent[2])\n+    optimizer = nlp.initialize()\n+\n+    # train the NER and NEL pipes\n+    for i in range(50):\n+        losses = {}\n+        nlp.update(train_examples, sgd=optimizer, losses=losses)\n+    assert losses[""ner""] < 0.001\n+    assert losses[""entity_linker""] < 0.001\n+\n+    # adding additional components that are required for the entity_linker\n+    nlp.add_pipe(""sentencizer"", first=True)\n+\n+    # test the trained model\n+    test_text = ""Russ Cochran captured his first major title with his son as caddie.""\n+    doc = nlp(test_text)\n+    ents = doc.ents\n+    assert len(ents) == 1\n+    assert ents[0].text == ""Russ Cochran""\n+    assert ents[0].label_ == ""PERSON""\n+    assert ents[0].kb_id_ != ""NIL""\n+\n+    # TODO: below assert is still flaky - EL doesn't properly overfit quite yet\n+    # assert ents[0].kb_id_ == ""Q2146908""",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f56f,explosion/spaCy,1794491429,1545263606,rmitsch,7152225,2024-03-30T09:55:57+00:00,2024-03-30T09:56:47+00:00,"Formatting in suggestion TBD. Ignore if you separated on purpose for easier debugging.\r\n```suggestion\r\n    assert all([metric in eval for metric in (""nel_macro_p"", ""nel_macro_r"", ""nel_macro_f"", ""nel_micro_p"", ""nel_micro_r"", ""nel_micro_f"", ""nel_f_per_type"")]\r\n```",true,spacy/tests/pipeline/test_entity_linker.py,119,"@@ -807,6 +809,107 @@ def create_kb(vocab):\n     assert_equal(batch_deps_1, batch_deps_2)\n     assert_equal(batch_deps_1, no_batch_deps)\n \n+    eval = nlp.evaluate(train_examples)\n+    assert ""nel_macro_p"" in eval\n+    assert ""nel_macro_r"" in eval\n+    assert ""nel_macro_f"" in eval\n+    assert ""nel_micro_p"" in eval\n+    assert ""nel_micro_r"" in eval\n+    assert ""nel_micro_f"" in eval\n+    assert ""nel_f_per_type"" in eval\n+    assert ""PERSON"" in eval[""nel_f_per_type""]\n+\n+    assert eval[""nel_macro_f""] > 0\n+    assert eval[""nel_micro_f""] > 0\n+\n+\n+def test_overfitting_IO_with_ner():\n+    # Simple test to try and overfit the NER and NEL component in combination - ensuring the ML models work correctly\n+    nlp = English()\n+    vector_length = 3\n+    assert ""Q2146908"" not in nlp.vocab.strings\n+\n+    # Convert the texts to docs to make sure we have doc.ents set for the training examples\n+    train_examples = []\n+    for text, annotation in TRAIN_DATA:\n+        doc = nlp(text)\n+        train_examples.append(Example.from_dict(doc, annotation))\n+\n+    def create_kb(vocab):\n+        # create artificial KB - assign same prior weight to the two russ cochran's\n+        # Q2146908 (Russ Cochran): American golfer\n+        # Q7381115 (Russ Cochran): publisher\n+        mykb = InMemoryLookupKB(vocab, entity_vector_length=vector_length)\n+        mykb.add_entity(entity=""Q2146908"", freq=12, entity_vector=[6, -4, 3])\n+        mykb.add_entity(entity=""Q7381115"", freq=12, entity_vector=[9, 1, -7])\n+        mykb.add_alias(\n+            alias=""Russ Cochran"",\n+            entities=[""Q2146908"", ""Q7381115""],\n+            probabilities=[0.5, 0.5],\n+        )\n+        return mykb\n+\n+    # Create the NER and EL components and add them to the pipeline\n+    ner = nlp.add_pipe(""ner"", first=True)\n+    entity_linker = nlp.add_pipe(\n+        ""entity_linker"", last=True, config={""use_gold_ents"": False}\n+    )\n+    entity_linker.set_kb(create_kb)\n+\n+    train_examples = []\n+    for text, annotations in TRAIN_DATA:\n+        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))\n+        for ent in annotations.get(""entities""):\n+            ner.add_label(ent[2])\n+    optimizer = nlp.initialize()\n+\n+    # train the NER and NEL pipes\n+    for i in range(50):\n+        losses = {}\n+        nlp.update(train_examples, sgd=optimizer, losses=losses)\n+    assert losses[""ner""] < 0.001\n+    assert losses[""entity_linker""] < 0.001\n+\n+    # adding additional components that are required for the entity_linker\n+    nlp.add_pipe(""sentencizer"", first=True)\n+\n+    # test the trained model\n+    test_text = ""Russ Cochran captured his first major title with his son as caddie.""\n+    doc = nlp(test_text)\n+    ents = doc.ents\n+    assert len(ents) == 1\n+    assert ents[0].text == ""Russ Cochran""\n+    assert ents[0].label_ == ""PERSON""\n+    assert ents[0].kb_id_ != ""NIL""\n+\n+    # TODO: below assert is still flaky - EL doesn't properly overfit quite yet\n+    # assert ents[0].kb_id_ == ""Q2146908""\n+\n+    # Also test the results are still the same after IO\n+    with make_tempdir() as tmp_dir:\n+        nlp.to_disk(tmp_dir)\n+        nlp2 = util.load_model_from_path(tmp_dir)\n+        assert nlp2.pipe_names == nlp.pipe_names\n+        doc2 = nlp2(test_text)\n+        ents2 = doc2.ents\n+        assert len(ents2) == 1\n+        assert ents2[0].text == ""Russ Cochran""\n+        assert ents2[0].label_ == ""PERSON""\n+        assert ents2[0].kb_id_ != ""NIL""\n+\n+    eval = nlp.evaluate(train_examples)\n+    assert ""nel_macro_f"" in eval\n+    assert ""nel_micro_f"" in eval\n+    assert ""ents_f"" in eval\n+    assert ""nel_f_per_type"" in eval\n+    assert ""ents_per_type"" in eval\n+    assert ""PERSON"" in eval[""nel_f_per_type""]\n+    assert ""PERSON"" in eval[""ents_per_type""]",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f570,explosion/spaCy,1794491429,1547281271,svlandeg,8796347,2024-04-02T07:21:52+00:00,2024-04-02T07:21:52+00:00,In unit tests I prefer to test one thing per line so it's clear what fails when it does ;-),true,spacy/tests/pipeline/test_entity_linker.py,31,"@@ -807,6 +809,107 @@ def create_kb(vocab):\n     assert_equal(batch_deps_1, batch_deps_2)\n     assert_equal(batch_deps_1, no_batch_deps)\n \n+    eval = nlp.evaluate(train_examples)\n+    assert ""nel_macro_p"" in eval\n+    assert ""nel_macro_r"" in eval\n+    assert ""nel_macro_f"" in eval\n+    assert ""nel_micro_p"" in eval\n+    assert ""nel_micro_r"" in eval\n+    assert ""nel_micro_f"" in eval\n+    assert ""nel_f_per_type"" in eval",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f571,explosion/spaCy,1794491429,1547282347,svlandeg,8796347,2024-04-02T07:22:47+00:00,2024-04-02T07:22:47+00:00,This is a different issue so a separate PR would be prefered,true,spacy/tests/pipeline/test_entity_linker.py,98,"@@ -807,6 +809,107 @@ def create_kb(vocab):\n     assert_equal(batch_deps_1, batch_deps_2)\n     assert_equal(batch_deps_1, no_batch_deps)\n \n+    eval = nlp.evaluate(train_examples)\n+    assert ""nel_macro_p"" in eval\n+    assert ""nel_macro_r"" in eval\n+    assert ""nel_macro_f"" in eval\n+    assert ""nel_micro_p"" in eval\n+    assert ""nel_micro_r"" in eval\n+    assert ""nel_micro_f"" in eval\n+    assert ""nel_f_per_type"" in eval\n+    assert ""PERSON"" in eval[""nel_f_per_type""]\n+\n+    assert eval[""nel_macro_f""] > 0\n+    assert eval[""nel_micro_f""] > 0\n+\n+\n+def test_overfitting_IO_with_ner():\n+    # Simple test to try and overfit the NER and NEL component in combination - ensuring the ML models work correctly\n+    nlp = English()\n+    vector_length = 3\n+    assert ""Q2146908"" not in nlp.vocab.strings\n+\n+    # Convert the texts to docs to make sure we have doc.ents set for the training examples\n+    train_examples = []\n+    for text, annotation in TRAIN_DATA:\n+        doc = nlp(text)\n+        train_examples.append(Example.from_dict(doc, annotation))\n+\n+    def create_kb(vocab):\n+        # create artificial KB - assign same prior weight to the two russ cochran's\n+        # Q2146908 (Russ Cochran): American golfer\n+        # Q7381115 (Russ Cochran): publisher\n+        mykb = InMemoryLookupKB(vocab, entity_vector_length=vector_length)\n+        mykb.add_entity(entity=""Q2146908"", freq=12, entity_vector=[6, -4, 3])\n+        mykb.add_entity(entity=""Q7381115"", freq=12, entity_vector=[9, 1, -7])\n+        mykb.add_alias(\n+            alias=""Russ Cochran"",\n+            entities=[""Q2146908"", ""Q7381115""],\n+            probabilities=[0.5, 0.5],\n+        )\n+        return mykb\n+\n+    # Create the NER and EL components and add them to the pipeline\n+    ner = nlp.add_pipe(""ner"", first=True)\n+    entity_linker = nlp.add_pipe(\n+        ""entity_linker"", last=True, config={""use_gold_ents"": False}\n+    )\n+    entity_linker.set_kb(create_kb)\n+\n+    train_examples = []\n+    for text, annotations in TRAIN_DATA:\n+        train_examples.append(Example.from_dict(nlp.make_doc(text), annotations))\n+        for ent in annotations.get(""entities""):\n+            ner.add_label(ent[2])\n+    optimizer = nlp.initialize()\n+\n+    # train the NER and NEL pipes\n+    for i in range(50):\n+        losses = {}\n+        nlp.update(train_examples, sgd=optimizer, losses=losses)\n+    assert losses[""ner""] < 0.001\n+    assert losses[""entity_linker""] < 0.001\n+\n+    # adding additional components that are required for the entity_linker\n+    nlp.add_pipe(""sentencizer"", first=True)\n+\n+    # test the trained model\n+    test_text = ""Russ Cochran captured his first major title with his son as caddie.""\n+    doc = nlp(test_text)\n+    ents = doc.ents\n+    assert len(ents) == 1\n+    assert ents[0].text == ""Russ Cochran""\n+    assert ents[0].label_ == ""PERSON""\n+    assert ents[0].kb_id_ != ""NIL""\n+\n+    # TODO: below assert is still flaky - EL doesn't properly overfit quite yet\n+    # assert ents[0].kb_id_ == ""Q2146908""",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f572,explosion/spaCy,1794491429,1547284550,svlandeg,8796347,2024-04-02T07:24:15+00:00,2024-04-02T07:25:05+00:00,Because there is an `ner` component in this pipeline - the point is to try and overfit when there is both an NER and an EL component.,true,spacy/tests/pipeline/test_entity_linker.py,67,"@@ -807,6 +809,107 @@ def create_kb(vocab):\n     assert_equal(batch_deps_1, batch_deps_2)\n     assert_equal(batch_deps_1, no_batch_deps)\n \n+    eval = nlp.evaluate(train_examples)\n+    assert ""nel_macro_p"" in eval\n+    assert ""nel_macro_r"" in eval\n+    assert ""nel_macro_f"" in eval\n+    assert ""nel_micro_p"" in eval\n+    assert ""nel_micro_r"" in eval\n+    assert ""nel_micro_f"" in eval\n+    assert ""nel_f_per_type"" in eval\n+    assert ""PERSON"" in eval[""nel_f_per_type""]\n+\n+    assert eval[""nel_macro_f""] > 0\n+    assert eval[""nel_micro_f""] > 0\n+\n+\n+def test_overfitting_IO_with_ner():\n+    # Simple test to try and overfit the NER and NEL component in combination - ensuring the ML models work correctly\n+    nlp = English()\n+    vector_length = 3\n+    assert ""Q2146908"" not in nlp.vocab.strings\n+\n+    # Convert the texts to docs to make sure we have doc.ents set for the training examples\n+    train_examples = []\n+    for text, annotation in TRAIN_DATA:\n+        doc = nlp(text)\n+        train_examples.append(Example.from_dict(doc, annotation))\n+\n+    def create_kb(vocab):\n+        # create artificial KB - assign same prior weight to the two russ cochran's\n+        # Q2146908 (Russ Cochran): American golfer\n+        # Q7381115 (Russ Cochran): publisher\n+        mykb = InMemoryLookupKB(vocab, entity_vector_length=vector_length)\n+        mykb.add_entity(entity=""Q2146908"", freq=12, entity_vector=[6, -4, 3])\n+        mykb.add_entity(entity=""Q7381115"", freq=12, entity_vector=[9, 1, -7])\n+        mykb.add_alias(\n+            alias=""Russ Cochran"",\n+            entities=[""Q2146908"", ""Q7381115""],\n+            probabilities=[0.5, 0.5],\n+        )\n+        return mykb\n+\n+    # Create the NER and EL components and add them to the pipeline\n+    ner = nlp.add_pipe(""ner"", first=True)\n+    entity_linker = nlp.add_pipe(\n+        ""entity_linker"", last=True, config={""use_gold_ents"": False}",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f573,explosion/spaCy,1794491429,1547289505,svlandeg,8796347,2024-04-02T07:28:33+00:00,2024-04-02T07:28:34+00:00,We do this everywhere just to have some data for `model.initialize()` so that Thinc can infer the correct `nI`/`nO` dimensions.,true,spacy/pipeline/entity_linker.py,NaN,"@@ -284,11 +311,9 @@ def initialize(\n         nO = self.kb.entity_vector_length\n         doc_sample = []\n         vector_sample = []\n-        for eg in islice(get_examples(), 10):\n+        examples = self._augment_examples(islice(get_examples(), 10))",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f574,explosion/spaCy,1794491429,1547301769,svlandeg,8796347,2024-04-02T07:32:36+00:00,2024-04-02T07:32:36+00:00,True! I renamed it to `_ensure_ents` as the function doesn't set any ents if `use_gold_ents` is `False`.,true,spacy/pipeline/entity_linker.py,NaN,"@@ -235,14 +234,42 @@ def __init__(\n         self.cfg: Dict[str, Any] = {""overwrite"": overwrite}\n         self.distance = CosineDistance(normalize=False)\n         self.kb = generate_empty_kb(self.vocab, entity_vector_length)\n-        self.scorer = scorer\n         self.use_gold_ents = use_gold_ents\n         self.candidates_batch_size = candidates_batch_size\n         self.threshold = threshold\n \n         if candidates_batch_size < 1:\n             raise ValueError(Errors.E1044)\n \n+        def _score_augmented(examples, **kwargs):\n+            # Because of how spaCy works, we can't just score immediately, because Language.evaluate\n+            # calls pipe() on the predicted docs, which won't have entities if there is no NER in the pipeline.\n+            if not self.use_gold_ents:\n+                return scorer(examples, **kwargs)\n+            else:\n+                examples = self._augment_examples(examples)\n+                docs = self.pipe(\n+                    (eg.predicted for eg in examples),\n+                )\n+                for eg, doc in zip(examples, docs):\n+                    eg.predicted = doc\n+                return scorer(examples, **kwargs)\n+\n+        self.scorer = _score_augmented\n+\n+    def _augment_examples(self, examples: Iterable[Example]) -> Iterable[Example]:",0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f575,explosion/spaCy,1732813366,1502298868,svlandeg,8796347,2024-02-26T09:37:20+00:00,2024-02-26T09:37:21+00:00,"We'll want to find another solution for this, because we don't want to enforce all users to have exactly this model in their environment",false,NaN,NaN,NaN,1,1,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f576,explosion/spaCy,1732813366,1502299802,svlandeg,8796347,2024-02-26T09:38:05+00:00,2024-02-26T09:38:05+00:00,Could you run `isort` on all files? (the test suite will fail otherwise),false,NaN,NaN,NaN,1,1,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f577,explosion/spaCy,1732813366,1507990078,india-kerle,46863334,2024-02-29T18:11:06+00:00,2024-02-29T18:11:06+00:00,FYI @honnibal ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f578,explosion/spaCy,1732813366,1511133141,india-kerle,46863334,2024-03-04T13:04:34+00:00,2024-03-04T13:04:34+00:00,"doesn't account for i.e. the ""water and power meters and electrical sockets"" ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,explosion_spaCy
6824cd8f0ce5dcec7842f97c,gradio-app/gradio,2514139619,2085168635,abidlabs,1778297,2025-05-12T17:51:00+00:00,2025-05-12T17:51:00+00:00,Use the `|` syntax instead of `Union`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f97d,gradio-app/gradio,2514139619,2085171151,abidlabs,1778297,2025-05-12T17:52:52+00:00,2025-05-12T17:52:52+00:00,Can you explain what this if/else is handling?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f97e,gradio-app/gradio,2514139619,2085332826,dawoodkhan82,12725292,2025-05-12T19:41:39+00:00,2025-05-12T19:41:39+00:00,"checks for value prop of the component. But the else is redundant, will fix.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f97f,gradio-app/gradio,2514139619,2085407750,abidlabs,1778297,2025-05-12T20:31:11+00:00,2025-05-12T20:31:11+00:00,This can be a static method,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f980,gradio-app/gradio,2514139619,2085409090,abidlabs,1778297,2025-05-12T20:31:40+00:00,2025-05-12T20:31:40+00:00,"Instead of checking if it has a ""get_config"" attribute, why not just check if it's an instance of Component?",false,NaN,NaN,NaN,1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f981,gradio-app/gradio,2514139619,2085409751,abidlabs,1778297,2025-05-12T20:31:55+00:00,2025-05-12T20:31:55+00:00,What is this try/except designed to handle? ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f982,gradio-app/gradio,2514139619,2085412686,abidlabs,1778297,2025-05-12T20:32:53+00:00,2025-05-12T20:32:53+00:00,"This casting is not ideal as it'll mask any typing issues, would be better if we ensured that make_serializable returned a list[MessageDict].",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f983,gradio-app/gradio,2514139619,2085562395,dawoodkhan82,12725292,2025-05-12T21:34:19+00:00,2025-05-12T21:34:19+00:00,"To fix this, changed the `make_serializable` method to return `any`. Since it's recursive and has to to return other the other types (messagedict, component) as well",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f984,gradio-app/gradio,2514139619,2085563700,dawoodkhan82,12725292,2025-05-12T21:34:59+00:00,2025-05-12T21:34:59+00:00,"Lmk if this is better, than `cast`",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f985,gradio-app/gradio,2514139619,2085567221,dawoodkhan82,12725292,2025-05-12T21:36:47+00:00,2025-05-12T21:37:11+00:00,I can make it static but since its recursive I would have to call itself using `ChatInterface._make_serializable` instead of using self. opted not to since we don't do that anywhere else. ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f986,gradio-app/gradio,2514139619,2085574774,abidlabs,1778297,2025-05-12T21:40:12+00:00,2025-05-12T21:40:18+00:00,"you could just call it with its name `make_serializable()` from within the function, no?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f987,gradio-app/gradio,2514139619,2085580162,abidlabs,1778297,2025-05-12T21:42:21+00:00,2025-05-12T21:42:21+00:00,"This is ok but I think the better approach would be for `make_serializable` to have the correct return type, but inside `make_serializable`, you have an inner function that is called recursively. That way, you still get the guarantees of type checking.",false,NaN,NaN,NaN,1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f988,gradio-app/gradio,2514139619,2085582151,dawoodkhan82,12725292,2025-05-12T21:43:19+00:00,2025-05-12T21:43:19+00:00,"No, it's not defined",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f989,gradio-app/gradio,2514139619,2085586724,abidlabs,1778297,2025-05-12T21:47:22+00:00,2025-05-12T21:47:22+00:00,"Ok but if you do the inner recursive function approach suggested below, that should definitely work, right? ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f98a,gradio-app/gradio,2514139619,2085595630,dawoodkhan82,12725292,2025-05-12T21:56:32+00:00,2025-05-12T21:56:32+00:00,yeah fixed it,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f98b,gradio-app/gradio,2514139619,2085632301,abidlabs,1778297,2025-05-12T22:40:19+00:00,2025-05-12T22:40:28+00:00,I think:\r\n```suggestion\r\n    def serialize_conversation(conversation: list[MessageDict]) -> list[MessageDict]:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f98c,gradio-app/gradio,2514139619,2085633037,abidlabs,1778297,2025-05-12T22:41:14+00:00,2025-05-12T22:41:15+00:00,nit: might be clearer name:\r\n```suggestion\r\n    def serialize_cocomponents(conversation: Any) -> list[MessageDict]:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f98d,gradio-app/gradio,2514139619,2085168635,abidlabs,1778297,2025-05-12T17:51:00+00:00,2025-05-12T17:51:00+00:00,Use the `|` syntax instead of `Union`,true,gradio/components/browser_state.py,NaN,"@@ -4,7 +4,7 @@\n \n import secrets\n import string\n-from typing import Any\n+from typing import Any, Union",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f98e,gradio-app/gradio,2514139619,2085171151,abidlabs,1778297,2025-05-12T17:52:52+00:00,2025-05-12T17:52:52+00:00,Can you explain what this if/else is handling?,true,gradio/components/browser_state.py,NaN,"@@ -61,9 +61,24 @@ def postprocess(self, value: Any) -> Any:\n         Parameters:\n             value: Value to store in local storage\n         Returns:\n-            Passes value through unchanged\n+            Value converted to JSON-serializable format\n         """"""\n-        return value\n+        return self.make_serializable(value)\n+\n+    def make_serializable(self, obj: Any) -> Union[dict, list, str, Any]:\n+        if isinstance(obj, list):\n+            return [self.make_serializable(item) for item in obj]\n+        elif isinstance(obj, dict):\n+            return {k: self.make_serializable(v) for k, v in obj.items()}\n+        elif hasattr(obj, ""get_config""):\n+            try:\n+                if hasattr(obj, ""value""):\n+                    return obj.value\n+                else:\n+                    return str(obj)",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f98f,gradio-app/gradio,2514139619,2085332826,dawoodkhan82,12725292,2025-05-12T19:41:39+00:00,2025-05-12T19:41:39+00:00,"checks for value prop of the component. But the else is redundant, will fix.",true,gradio/components/browser_state.py,NaN,"@@ -61,9 +61,24 @@ def postprocess(self, value: Any) -> Any:\n         Parameters:\n             value: Value to store in local storage\n         Returns:\n-            Passes value through unchanged\n+            Value converted to JSON-serializable format\n         """"""\n-        return value\n+        return self.make_serializable(value)\n+\n+    def make_serializable(self, obj: Any) -> Union[dict, list, str, Any]:\n+        if isinstance(obj, list):\n+            return [self.make_serializable(item) for item in obj]\n+        elif isinstance(obj, dict):\n+            return {k: self.make_serializable(v) for k, v in obj.items()}\n+        elif hasattr(obj, ""get_config""):\n+            try:\n+                if hasattr(obj, ""value""):\n+                    return obj.value\n+                else:\n+                    return str(obj)",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f990,gradio-app/gradio,2514139619,2085407750,abidlabs,1778297,2025-05-12T20:31:11+00:00,2025-05-12T20:31:11+00:00,This can be a static method,true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f991,gradio-app/gradio,2514139619,2085409090,abidlabs,1778297,2025-05-12T20:31:40+00:00,2025-05-12T20:31:40+00:00,"Instead of checking if it has a ""get_config"" attribute, why not just check if it's an instance of Component?",true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:\n+        if isinstance(obj, list):\n+            return [self.make_serializable(item) for item in obj]\n+        elif isinstance(obj, dict):\n+            return {k: self.make_serializable(v) for k, v in obj.items()}\n+        elif hasattr(obj, ""get_config""):",1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f992,gradio-app/gradio,2514139619,2085409751,abidlabs,1778297,2025-05-12T20:31:55+00:00,2025-05-12T20:31:55+00:00,What is this try/except designed to handle? ,true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:\n+        if isinstance(obj, list):\n+            return [self.make_serializable(item) for item in obj]\n+        elif isinstance(obj, dict):\n+            return {k: self.make_serializable(v) for k, v in obj.items()}\n+        elif hasattr(obj, ""get_config""):\n+            try:\n+                if hasattr(obj, ""value""):\n+                    return obj.value\n+            except Exception:\n+                return str(obj)",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f993,gradio-app/gradio,2514139619,2085412686,abidlabs,1778297,2025-05-12T20:32:53+00:00,2025-05-12T20:32:53+00:00,"This casting is not ideal as it'll mask any typing issues, would be better if we ensured that make_serializable returned a list[MessageDict].",true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:\n+        if isinstance(obj, list):\n+            return [self.make_serializable(item) for item in obj]\n+        elif isinstance(obj, dict):\n+            return {k: self.make_serializable(v) for k, v in obj.items()}\n+        elif hasattr(obj, ""get_config""):\n+            try:\n+                if hasattr(obj, ""value""):\n+                    return obj.value\n+            except Exception:\n+                return str(obj)\n+        return obj\n+\n     def _save_conversation(\n         self,\n         index: int | None,\n         conversation: list[MessageDict],\n         saved_conversations: list[list[MessageDict]],\n     ):\n         if self.save_history:\n+            serializable_conversation = self.make_serializable(conversation)\n             if index is not None:\n-                saved_conversations[index] = conversation\n+                saved_conversations[index] = cast(",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f994,gradio-app/gradio,2514139619,2085562395,dawoodkhan82,12725292,2025-05-12T21:34:19+00:00,2025-05-12T21:34:19+00:00,"To fix this, changed the `make_serializable` method to return `any`. Since it's recursive and has to to return other the other types (messagedict, component) as well",true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:\n+        if isinstance(obj, list):\n+            return [self.make_serializable(item) for item in obj]\n+        elif isinstance(obj, dict):\n+            return {k: self.make_serializable(v) for k, v in obj.items()}\n+        elif hasattr(obj, ""get_config""):\n+            try:\n+                if hasattr(obj, ""value""):\n+                    return obj.value\n+            except Exception:\n+                return str(obj)\n+        return obj\n+\n     def _save_conversation(\n         self,\n         index: int | None,\n         conversation: list[MessageDict],\n         saved_conversations: list[list[MessageDict]],\n     ):\n         if self.save_history:\n+            serializable_conversation = self.make_serializable(conversation)\n             if index is not None:\n-                saved_conversations[index] = conversation\n+                saved_conversations[index] = cast(",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f995,gradio-app/gradio,2514139619,2085563700,dawoodkhan82,12725292,2025-05-12T21:34:59+00:00,2025-05-12T21:34:59+00:00,"Lmk if this is better, than `cast`",true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:\n+        if isinstance(obj, list):\n+            return [self.make_serializable(item) for item in obj]\n+        elif isinstance(obj, dict):\n+            return {k: self.make_serializable(v) for k, v in obj.items()}\n+        elif hasattr(obj, ""get_config""):\n+            try:\n+                if hasattr(obj, ""value""):\n+                    return obj.value\n+            except Exception:\n+                return str(obj)\n+        return obj\n+\n     def _save_conversation(\n         self,\n         index: int | None,\n         conversation: list[MessageDict],\n         saved_conversations: list[list[MessageDict]],\n     ):\n         if self.save_history:\n+            serializable_conversation = self.make_serializable(conversation)\n             if index is not None:\n-                saved_conversations[index] = conversation\n+                saved_conversations[index] = cast(",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f996,gradio-app/gradio,2514139619,2085567221,dawoodkhan82,12725292,2025-05-12T21:36:47+00:00,2025-05-12T21:37:11+00:00,I can make it static but since its recursive I would have to call itself using `ChatInterface._make_serializable` instead of using self. opted not to since we don't do that anywhere else. ,true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f997,gradio-app/gradio,2514139619,2085574774,abidlabs,1778297,2025-05-12T21:40:12+00:00,2025-05-12T21:40:18+00:00,"you could just call it with its name `make_serializable()` from within the function, no?",true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f998,gradio-app/gradio,2514139619,2085580162,abidlabs,1778297,2025-05-12T21:42:21+00:00,2025-05-12T21:42:21+00:00,"This is ok but I think the better approach would be for `make_serializable` to have the correct return type, but inside `make_serializable`, you have an inner function that is called recursively. That way, you still get the guarantees of type checking.",true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:\n+        if isinstance(obj, list):\n+            return [self.make_serializable(item) for item in obj]\n+        elif isinstance(obj, dict):\n+            return {k: self.make_serializable(v) for k, v in obj.items()}\n+        elif hasattr(obj, ""get_config""):\n+            try:\n+                if hasattr(obj, ""value""):\n+                    return obj.value\n+            except Exception:\n+                return str(obj)\n+        return obj\n+\n     def _save_conversation(\n         self,\n         index: int | None,\n         conversation: list[MessageDict],\n         saved_conversations: list[list[MessageDict]],\n     ):\n         if self.save_history:\n+            serializable_conversation = self.make_serializable(conversation)\n             if index is not None:\n-                saved_conversations[index] = conversation\n+                saved_conversations[index] = cast(",1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f999,gradio-app/gradio,2514139619,2085582151,dawoodkhan82,12725292,2025-05-12T21:43:19+00:00,2025-05-12T21:43:19+00:00,"No, it's not defined",true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f99a,gradio-app/gradio,2514139619,2085586724,abidlabs,1778297,2025-05-12T21:47:22+00:00,2025-05-12T21:47:22+00:00,"Ok but if you do the inner recursive function approach suggested below, that should definitely work, right? ",true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f99b,gradio-app/gradio,2514139619,2085595630,dawoodkhan82,12725292,2025-05-12T21:56:32+00:00,2025-05-12T21:56:32+00:00,yeah fixed it,true,gradio/chat_interface.py,NaN,"@@ -461,18 +461,36 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    def make_serializable(self, obj: Any) -> dict | list | str:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f99c,gradio-app/gradio,2514139619,2085632301,abidlabs,1778297,2025-05-12T22:40:19+00:00,2025-05-12T22:40:28+00:00,I think:\r\n```suggestion\r\n    def serialize_conversation(conversation: list[MessageDict]) -> list[MessageDict]:\r\n```,true,gradio/chat_interface.py,14,"@@ -461,18 +461,32 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    @staticmethod\n+    def serialize_conversation(conversation: Any) -> list[MessageDict]:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f99d,gradio-app/gradio,2514139619,2085633037,abidlabs,1778297,2025-05-12T22:41:14+00:00,2025-05-12T22:41:15+00:00,nit: might be clearer name:\r\n```suggestion\r\n    def serialize_cocomponents(conversation: Any) -> list[MessageDict]:\r\n```,true,gradio/chat_interface.py,14,"@@ -461,18 +461,32 @@ def _generate_chat_title(self, conversation: list[MessageDict]) -> str:\n             title = title[:40] + ""...""\n         return title or ""Conversation""\n \n+    @staticmethod\n+    def serialize_conversation(conversation: Any) -> list[MessageDict]:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f99e,gradio-app/gradio,2498689027,2085212572,abidlabs,1778297,2025-05-12T18:21:48+00:00,2025-05-12T18:21:48+00:00,I think we can be more specific:\r\n```suggestion\r\n    except ValueError:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f99f,gradio-app/gradio,2498689027,2085212572,abidlabs,1778297,2025-05-12T18:21:48+00:00,2025-05-12T18:21:48+00:00,I think we can be more specific:\r\n```suggestion\r\n    except ValueError:\r\n```,true,gradio/utils.py,NaN,"@@ -1308,7 +1308,10 @@ def get_function_params(func: Callable) -> list[tuple[str, bool, Any, Any]]:\n     Excludes *args and **kwargs, as well as args that are Gradio-specific, such as gr.Request, gr.EventData, gr.OAuthProfile, and gr.OAuthToken.\n     """"""\n     params_info = []\n-    signature = inspect.signature(func)\n+    try:\n+        signature = inspect.signature(func)\n+    except Exception:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a0,gradio-app/gradio,2498639141,2073446019,freddyaboulton,41651716,2025-05-05T13:25:32+00:00,2025-05-05T13:25:32+00:00,switch from tuples to lists. Tuples are not json-able so they get converted to lists downstream anyways.,false,NaN,NaN,NaN,1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a1,gradio-app/gradio,2498639141,2073446019,freddyaboulton,41651716,2025-05-05T13:25:32+00:00,2025-05-05T13:25:32+00:00,switch from tuples to lists. Tuples are not json-able so they get converted to lists downstream anyways.,true,gradio/utils.py,5,"@@ -1262,35 +1262,42 @@ def compare_objects(obj1, obj2, path=None):\n             return edits\n \n         if type(obj1) is not type(obj2):\n-            edits.append((""replace"", path, obj2))\n+            edits.append([""replace"", path, obj2])",1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a2,gradio-app/gradio,2494205689,2070971194,abidlabs,1778297,2025-05-02T00:54:10+00:00,2025-05-02T00:54:10+00:00,unrelated follow up from https://github.com/gradio-app/gradio/pull/11103,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a3,gradio-app/gradio,2494205689,2070971194,abidlabs,1778297,2025-05-02T00:54:10+00:00,2025-05-02T00:54:10+00:00,unrelated follow up from https://github.com/gradio-app/gradio/pull/11103,true,guides/04_additional-features/14_view-api-page.md,4,"@@ -56,4 +56,6 @@ The API page also includes instructions on how to use the Gradio app as an Model\n \n For the MCP sever, each tool, its description, and its parameters are listed, along with instructions on how to integrate with popular MCP Clients. Read more about Gradio's [MCP integration here](https://www.gradio.app/guides/building-mcp-server-with-gradio).\n \n+## OpenAPI Specification",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a4,gradio-app/gradio,2491479718,2068769838,freddyaboulton,41651716,2025-04-30T14:20:41+00:00,2025-04-30T14:22:22+00:00,Can we build a version of this and ship with gradio? e.g. `mcp/inspect` ?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a5,gradio-app/gradio,2491479718,2068774629,freddyaboulton,41651716,2025-04-30T14:23:13+00:00,2025-04-30T14:23:13+00:00,Maybe we should do this at the gradio mcp-server level so that developers don't have to code with mcp in mind from the start.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a6,gradio-app/gradio,2491479718,2068790545,abidlabs,1778297,2025-04-30T14:31:37+00:00,2025-04-30T14:31:38+00:00,"I think it's better to point users to the official debugger. I don't think we should be maintaining a rapidly-evolving inspector tool ourselves (and if there are issues with the inspector tool, then that could complicate debugging -- is the issue with our inspector tool or with the developer's code? What we could do instead is mention the `gradio_api/mcp/schema` route as an intermediate way to inspect your endpoint's schema",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a7,gradio-app/gradio,2491479718,2068822001,abidlabs,1778297,2025-04-30T14:44:47+00:00,2025-04-30T14:44:47+00:00,"Yeah I was thinking that as well, but I think this is a temporary issue with some MCP clients. Coercing to a str and then back to a specific type might have some side effects",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a8,gradio-app/gradio,2491479718,2068769838,freddyaboulton,41651716,2025-04-30T14:20:41+00:00,2025-04-30T14:22:22+00:00,Can we build a version of this and ship with gradio? e.g. `mcp/inspect` ?,true,guides/10_other-tutorials/building-mcp-server-with-gradio.md,72,"@@ -209,3 +206,82 @@ To use this MCP Server with Claude Desktop (as MCP Client):\n Now, when you ask Claude about generating an image or transcribing audio, it can use your Gradio-powered tools to accomplish these tasks.\n \n \n+## Troubleshooting your MCP Servers\n+\n+The MCP protocol is still in its infancy and you might see issues connecting to an MCP Server that you've built. We generally recommend using the [MCP Inspector Tool](https://github.com/modelcontextprotocol/inspector) to try connecting and debugging your MCP Server.",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9a9,gradio-app/gradio,2491479718,2068774629,freddyaboulton,41651716,2025-04-30T14:23:13+00:00,2025-04-30T14:23:13+00:00,Maybe we should do this at the gradio mcp-server level so that developers don't have to code with mcp in mind from the start.,true,guides/10_other-tutorials/building-mcp-server-with-gradio.md,93,"@@ -209,3 +206,82 @@ To use this MCP Server with Claude Desktop (as MCP Client):\n Now, when you ask Claude about generating an image or transcribing audio, it can use your Gradio-powered tools to accomplish these tasks.\n \n \n+## Troubleshooting your MCP Servers\n+\n+The MCP protocol is still in its infancy and you might see issues connecting to an MCP Server that you've built. We generally recommend using the [MCP Inspector Tool](https://github.com/modelcontextprotocol/inspector) to try connecting and debugging your MCP Server.\n+\n+Here are some things that may help:\n+\n+**1. Ensure that you've provided type hints and valid docstrings for your functions**\n+\n+As mentioned earlier, Gradio reads the docstrings for your functions and the type hints of input arguments to generate the description of the tool and parameters. A valid function and docstring looks like this:\n+\n+```py\n+def image_orientation(image: Image.Image) -> str:\n+    """"""\n+    Returns whether image is portrait or landscape.\n+\n+    Args:\n+        image (Image.Image): The image to check.\n+    """"""\n+    return ""Portrait"" if image.height > image.width else ""Landscape""\n+```\n+\n+Note the ""Args:"" block with indented parameter names underneath.\n+\n+**2. Try accepting input arguments as `str`**",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9aa,gradio-app/gradio,2491479718,2068790545,abidlabs,1778297,2025-04-30T14:31:37+00:00,2025-04-30T14:31:38+00:00,"I think it's better to point users to the official debugger. I don't think we should be maintaining a rapidly-evolving inspector tool ourselves (and if there are issues with the inspector tool, then that could complicate debugging -- is the issue with our inspector tool or with the developer's code? What we could do instead is mention the `gradio_api/mcp/schema` route as an intermediate way to inspect your endpoint's schema",true,guides/10_other-tutorials/building-mcp-server-with-gradio.md,72,"@@ -209,3 +206,82 @@ To use this MCP Server with Claude Desktop (as MCP Client):\n Now, when you ask Claude about generating an image or transcribing audio, it can use your Gradio-powered tools to accomplish these tasks.\n \n \n+## Troubleshooting your MCP Servers\n+\n+The MCP protocol is still in its infancy and you might see issues connecting to an MCP Server that you've built. We generally recommend using the [MCP Inspector Tool](https://github.com/modelcontextprotocol/inspector) to try connecting and debugging your MCP Server.",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9ab,gradio-app/gradio,2491479718,2068822001,abidlabs,1778297,2025-04-30T14:44:47+00:00,2025-04-30T14:44:47+00:00,"Yeah I was thinking that as well, but I think this is a temporary issue with some MCP clients. Coercing to a str and then back to a specific type might have some side effects",true,guides/10_other-tutorials/building-mcp-server-with-gradio.md,93,"@@ -209,3 +206,82 @@ To use this MCP Server with Claude Desktop (as MCP Client):\n Now, when you ask Claude about generating an image or transcribing audio, it can use your Gradio-powered tools to accomplish these tasks.\n \n \n+## Troubleshooting your MCP Servers\n+\n+The MCP protocol is still in its infancy and you might see issues connecting to an MCP Server that you've built. We generally recommend using the [MCP Inspector Tool](https://github.com/modelcontextprotocol/inspector) to try connecting and debugging your MCP Server.\n+\n+Here are some things that may help:\n+\n+**1. Ensure that you've provided type hints and valid docstrings for your functions**\n+\n+As mentioned earlier, Gradio reads the docstrings for your functions and the type hints of input arguments to generate the description of the tool and parameters. A valid function and docstring looks like this:\n+\n+```py\n+def image_orientation(image: Image.Image) -> str:\n+    """"""\n+    Returns whether image is portrait or landscape.\n+\n+    Args:\n+        image (Image.Image): The image to check.\n+    """"""\n+    return ""Portrait"" if image.height > image.width else ""Landscape""\n+```\n+\n+Note the ""Args:"" block with indented parameter names underneath.\n+\n+**2. Try accepting input arguments as `str`**",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9ac,gradio-app/gradio,2490252348,2069288878,abidlabs,1778297,2025-04-30T19:04:27+00:00,2025-04-30T19:04:27+00:00,I don't think there was any reference to the API Recorder anywhere in our docs before this!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9ad,gradio-app/gradio,2490252348,2069731854,aliabd,9021060,2025-05-01T01:10:12+00:00,2025-05-01T01:16:18+00:00,fixes the broken link\r\n```suggestion\r\nThis will add and document the endpoint `/addition/` to the automatically generated API page. Read more about the [API page here](./view-api-page).\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9ae,gradio-app/gradio,2490252348,2069733141,aliabd,9021060,2025-05-01T01:11:24+00:00,2025-05-01T01:16:18+00:00,"```suggestion\r\nYou can use almost any Gradio app programmatically via the built-in API! In the footer of any Gradio app, you'll see a ""Use via API"" link. Clicking on the link opens up a detailed documentation page for the API that Gradio generates based on the function signatures in your Gradio app.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9af,gradio-app/gradio,2490252348,2069736891,aliabd,9021060,2025-05-01T01:15:37+00:00,2025-05-01T01:16:18+00:00,"its mentioned in the python/js client pages, but not in detail ",false,NaN,NaN,NaN,1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b0,gradio-app/gradio,2490252348,2069288878,abidlabs,1778297,2025-04-30T19:04:27+00:00,2025-04-30T19:04:27+00:00,I don't think there was any reference to the API Recorder anywhere in our docs before this!,true,guides/04_additional-features/14_view-api-page.md,45,"@@ -0,0 +1,59 @@\n+# API Page\n+\n+You can use almost any Gradio app programmatically via the built-in an API! In the footer of any Gradio app, you'll see a ""Use via API"" link. Clicking on the link opens up a detailed documentation page for the API that Gradio generates based on the function signatures in your Gradio app.\n+\n+![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-animated.gif)\n+\n+## Configuring the API Page\n+\n+**API endpoint names**\n+\n+When you create a Gradio application, the API endpoint names are automatically generated based on the function names. You can change this by using the `api_name` parameter in `gr.Interface` or `gr.ChatInterface`. If you are using Gradio `Blocks`, you can name each event listener, like this:\n+\n+```python\n+btn.click(add, [num1, num2], output, api_name=""addition"")\n+```\n+\n+**Hiding API endpoints**\n+\n+When building a complex Gradio app, you might want to hide certain API endpoints from appearing on the view API page, e.g. if they correspond to functions that simply update the UI. You can set the  `show_api` parameter to `False` in any `Blocks` event listener to achieve this, e.g. \n+\n+```python\n+btn.click(add, [num1, num2], output, show_api=False)\n+```\n+\n+**Disabling API endpoints**\n+\n+Hiding the API endpoint doesn't disable it. A user can still programmatically call the API endpoint if they know the name. If you want to disable an API endpoint altogether, set `api_name=False`, e.g. \n+\n+```python\n+btn.click(add, [num1, num2], output, api_name=False)\n+```\n+\n+Note: setting an `api_name=False` also means that downstream apps will not be able to load your Gradio app using `gr.load()` as this function uses the Gradio API under the hood.\n+\n+\n+## The Clients\n+\n+This API page not only lists all of the endpoints that can be used to query the Gradio app, but also shows the usage of both [the Gradio Python client](https://gradio.app/guides/getting-started-with-the-python-client/), and [the Gradio JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/). \n+\n+For each endpoint, Gradio automatically generates a complete code snippet with the parameters and their types, as well as example inputs, allowing you to immediately test an endpoint. Here's an example showing an image file input and `str` output:\n+\n+![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-snippet.png)\n+\n+\n+## The API Recorder 🪄",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b1,gradio-app/gradio,2490252348,2069731854,aliabd,9021060,2025-05-01T01:10:12+00:00,2025-05-01T01:16:18+00:00,fixes the broken link\r\n```suggestion\r\nThis will add and document the endpoint `/addition/` to the automatically generated API page. Read more about the [API page here](./view-api-page).\r\n```,true,guides/04_additional-features/07_sharing-your-app.md,NaN,"@@ -176,13 +176,13 @@ This is a page that lists the endpoints that can be used to query the Gradio app\n \n ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api.png)\n \n-The endpoints are automatically created when you launch a Gradio `Interface`. If you are using Gradio `Blocks`, you can also set up a Gradio API page, though we recommend that you explicitly name each event listener, such as\n+The endpoints are automatically created when you launch a Gradio application. If you are using Gradio `Blocks`, you can also name each event listener, such as\n \n ```python\n btn.click(add, [num1, num2], output, api_name=""addition"")\n ```\n \n-This will add and document the endpoint `/api/addition/` to the automatically generated API page. Otherwise, your API endpoints will appear as ""unnamed"" endpoints.\n+This will add and document the endpoint `/addition/` to the automatically generated API page. Read more about the [API page here](/guides/additional-features/view-api-page).",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b2,gradio-app/gradio,2490252348,2069733141,aliabd,9021060,2025-05-01T01:11:24+00:00,2025-05-01T01:16:18+00:00,"```suggestion\r\nYou can use almost any Gradio app programmatically via the built-in API! In the footer of any Gradio app, you'll see a ""Use via API"" link. Clicking on the link opens up a detailed documentation page for the API that Gradio generates based on the function signatures in your Gradio app.\r\n```",true,guides/04_additional-features/14_view-api-page.md,NaN,"@@ -0,0 +1,59 @@\n+# API Page\n+\n+You can use almost any Gradio app programmatically via the built-in an API! In the footer of any Gradio app, you'll see a ""Use via API"" link. Clicking on the link opens up a detailed documentation page for the API that Gradio generates based on the function signatures in your Gradio app.",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b3,gradio-app/gradio,2490252348,2069736891,aliabd,9021060,2025-05-01T01:15:37+00:00,2025-05-01T01:16:18+00:00,"its mentioned in the python/js client pages, but not in detail ",true,guides/04_additional-features/14_view-api-page.md,45,"@@ -0,0 +1,59 @@\n+# API Page\n+\n+You can use almost any Gradio app programmatically via the built-in an API! In the footer of any Gradio app, you'll see a ""Use via API"" link. Clicking on the link opens up a detailed documentation page for the API that Gradio generates based on the function signatures in your Gradio app.\n+\n+![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-animated.gif)\n+\n+## Configuring the API Page\n+\n+**API endpoint names**\n+\n+When you create a Gradio application, the API endpoint names are automatically generated based on the function names. You can change this by using the `api_name` parameter in `gr.Interface` or `gr.ChatInterface`. If you are using Gradio `Blocks`, you can name each event listener, like this:\n+\n+```python\n+btn.click(add, [num1, num2], output, api_name=""addition"")\n+```\n+\n+**Hiding API endpoints**\n+\n+When building a complex Gradio app, you might want to hide certain API endpoints from appearing on the view API page, e.g. if they correspond to functions that simply update the UI. You can set the  `show_api` parameter to `False` in any `Blocks` event listener to achieve this, e.g. \n+\n+```python\n+btn.click(add, [num1, num2], output, show_api=False)\n+```\n+\n+**Disabling API endpoints**\n+\n+Hiding the API endpoint doesn't disable it. A user can still programmatically call the API endpoint if they know the name. If you want to disable an API endpoint altogether, set `api_name=False`, e.g. \n+\n+```python\n+btn.click(add, [num1, num2], output, api_name=False)\n+```\n+\n+Note: setting an `api_name=False` also means that downstream apps will not be able to load your Gradio app using `gr.load()` as this function uses the Gradio API under the hood.\n+\n+\n+## The Clients\n+\n+This API page not only lists all of the endpoints that can be used to query the Gradio app, but also shows the usage of both [the Gradio Python client](https://gradio.app/guides/getting-started-with-the-python-client/), and [the Gradio JavaScript client](https://gradio.app/guides/getting-started-with-the-js-client/). \n+\n+For each endpoint, Gradio automatically generates a complete code snippet with the parameters and their types, as well as example inputs, allowing you to immediately test an endpoint. Here's an example showing an image file input and `str` output:\n+\n+![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/view-api-snippet.png)\n+\n+\n+## The API Recorder 🪄",1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b4,gradio-app/gradio,2489896162,2069128922,abidlabs,1778297,2025-04-30T17:10:33+00:00,2025-04-30T17:10:33+00:00,```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b5,gradio-app/gradio,2489896162,2069128922,abidlabs,1778297,2025-04-30T17:10:33+00:00,2025-04-30T17:10:33+00:00,```suggestion\r\n```,true,gradio/routes.py,NaN,"@@ -1425,6 +1425,7 @@ async def sse_stream(request: fastapi.Request):\n                             message = {""msg"": ""done""}\n                             is_done = True\n                         else:\n+                            print(""not done"")",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b6,gradio-app/gradio,2488808612,2066932706,freddyaboulton,41651716,2025-04-29T16:29:07+00:00,2025-04-29T16:33:35+00:00,Use camel case for the function name to be consistent,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b7,gradio-app/gradio,2488808612,2066938959,freddyaboulton,41651716,2025-04-29T16:33:19+00:00,2025-04-29T16:33:35+00:00,You can just use `is_valid_file` instead of implementing a new filter function\r\n\r\n```js\r\n		const files_to_load = Array.from(e.dataTransfer.files).filter(is_valid_file);\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b8,gradio-app/gradio,2488808612,2067182692,pngwn,12937446,2025-04-29T19:17:09+00:00,2025-04-29T19:17:09+00:00,Did you mean snake?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9b9,gradio-app/gradio,2488808612,2067374783,freddyaboulton,41651716,2025-04-29T20:29:44+00:00,2025-04-29T20:29:44+00:00,Yea lol,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9ba,gradio-app/gradio,2488808612,2068555444,Martim-Rito,134000281,2025-04-30T12:24:33+00:00,2025-04-30T12:24:33+00:00,I think it is done.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9bb,gradio-app/gradio,2488808612,2068556237,Martim-Rito,134000281,2025-04-30T12:24:59+00:00,2025-04-30T12:24:59+00:00,I think it is done.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9bc,gradio-app/gradio,2488808612,2066932706,freddyaboulton,41651716,2025-04-29T16:29:07+00:00,2025-04-29T16:33:35+00:00,Use camel case for the function name to be consistent,true,js/upload/src/Upload.svelte,NaN,"@@ -240,6 +240,39 @@\n 			dispatch(""load"", files_to_load);\n 		}\n 	}\n+\n+	export async function loadFilesFromDrop(e: DragEvent): Promise<void> {",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9bd,gradio-app/gradio,2488808612,2066938959,freddyaboulton,41651716,2025-04-29T16:33:19+00:00,2025-04-29T16:33:35+00:00,You can just use `is_valid_file` instead of implementing a new filter function\r\n\r\n```js\r\n		const files_to_load = Array.from(e.dataTransfer.files).filter(is_valid_file);\r\n```,true,js/upload/src/Upload.svelte,NaN,"@@ -240,6 +240,39 @@\n 			dispatch(""load"", files_to_load);\n 		}\n 	}\n+\n+	export async function loadFilesFromDrop(e: DragEvent): Promise<void> {\n+		dragging = false;\n+		if (!e.dataTransfer?.files) return;\n+		const files_to_load = Array.from(e.dataTransfer.files).filter((file) => {",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9be,gradio-app/gradio,2488808612,2067182692,pngwn,12937446,2025-04-29T19:17:09+00:00,2025-04-29T19:17:09+00:00,Did you mean snake?,true,js/upload/src/Upload.svelte,NaN,"@@ -240,6 +240,39 @@\n 			dispatch(""load"", files_to_load);\n 		}\n 	}\n+\n+	export async function loadFilesFromDrop(e: DragEvent): Promise<void> {",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9bf,gradio-app/gradio,2488808612,2067374783,freddyaboulton,41651716,2025-04-29T20:29:44+00:00,2025-04-29T20:29:44+00:00,Yea lol,true,js/upload/src/Upload.svelte,NaN,"@@ -240,6 +240,39 @@\n 			dispatch(""load"", files_to_load);\n 		}\n 	}\n+\n+	export async function loadFilesFromDrop(e: DragEvent): Promise<void> {",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c0,gradio-app/gradio,2488808612,2068555444,Martim-Rito,134000281,2025-04-30T12:24:33+00:00,2025-04-30T12:24:33+00:00,I think it is done.,true,js/upload/src/Upload.svelte,NaN,"@@ -240,6 +240,39 @@\n 			dispatch(""load"", files_to_load);\n 		}\n 	}\n+\n+	export async function loadFilesFromDrop(e: DragEvent): Promise<void> {",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c1,gradio-app/gradio,2488808612,2068556237,Martim-Rito,134000281,2025-04-30T12:24:59+00:00,2025-04-30T12:24:59+00:00,I think it is done.,true,js/upload/src/Upload.svelte,NaN,"@@ -240,6 +240,39 @@\n 			dispatch(""load"", files_to_load);\n 		}\n 	}\n+\n+	export async function loadFilesFromDrop(e: DragEvent): Promise<void> {\n+		dragging = false;\n+		if (!e.dataTransfer?.files) return;\n+		const files_to_load = Array.from(e.dataTransfer.files).filter((file) => {",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c2,gradio-app/gradio,2470598604,2054491354,abidlabs,1778297,2025-04-22T16:51:05+00:00,2025-04-22T16:51:06+00:00,We don't need to make these changes if we are only testing a single version of Python. Would you mind reverting?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c3,gradio-app/gradio,2470598604,2055398131,abidlabs,1778297,2025-04-23T07:12:31+00:00,2025-04-23T07:12:31+00:00,why did you remove the line about the license @reneleonhardt? Gradio is Apache 2 licensed: https://github.com/gradio-app/gradio/blob/main/LICENSE,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c4,gradio-app/gradio,2470598604,2055502795,reneleonhardt,65483435,2025-04-23T08:14:28+00:00,2025-04-23T08:14:28+00:00,"Did you double-check your classifiers?\r\nhttps://github.com/gradio-app/gradio/blob/main/pyproject.toml#L13\r\n```\r\nlicense = ""Apache-2.0""\r\n```\r\nhttps://packaging.python.org/en/latest/guides/licensing-examples-and-user-scenarios/\r\n\r\nYour license is `Apache License, Version 2.0` not the old `Apache Software License, version 1.1`\r\nhttps://opensource.org/licenses?ls=apache\r\n",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c5,gradio-app/gradio,2470598604,2056699929,abidlabs,1778297,2025-04-23T18:49:19+00:00,2025-04-23T18:49:19+00:00,got it thanks,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c6,gradio-app/gradio,2470598604,2054491354,abidlabs,1778297,2025-04-22T16:51:05+00:00,2025-04-22T16:51:06+00:00,We don't need to make these changes if we are only testing a single version of Python. Would you mind reverting?,true,.github/workflows/test-python.yml,1,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c7,gradio-app/gradio,2470598604,2055398131,abidlabs,1778297,2025-04-23T07:12:31+00:00,2025-04-23T07:12:31+00:00,why did you remove the line about the license @reneleonhardt? Gradio is Apache 2 licensed: https://github.com/gradio-app/gradio/blob/main/LICENSE,true,pyproject.toml,4,"@@ -27,14 +27,13 @@ keywords = [""machine learning"", ""reproducibility"", ""visualization""]\n \n classifiers = [\n   'Development Status :: 5 - Production/Stable',\n-  'License :: OSI Approved :: Apache Software License',",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c8,gradio-app/gradio,2470598604,2055502795,reneleonhardt,65483435,2025-04-23T08:14:28+00:00,2025-04-23T08:14:28+00:00,"Did you double-check your classifiers?\r\nhttps://github.com/gradio-app/gradio/blob/main/pyproject.toml#L13\r\n```\r\nlicense = ""Apache-2.0""\r\n```\r\nhttps://packaging.python.org/en/latest/guides/licensing-examples-and-user-scenarios/\r\n\r\nYour license is `Apache License, Version 2.0` not the old `Apache Software License, version 1.1`\r\nhttps://opensource.org/licenses?ls=apache\r\n",true,pyproject.toml,4,"@@ -27,14 +27,13 @@ keywords = [""machine learning"", ""reproducibility"", ""visualization""]\n \n classifiers = [\n   'Development Status :: 5 - Production/Stable',\n-  'License :: OSI Approved :: Apache Software License',",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9c9,gradio-app/gradio,2470598604,2056699929,abidlabs,1778297,2025-04-23T18:49:19+00:00,2025-04-23T18:49:19+00:00,got it thanks,true,pyproject.toml,4,"@@ -27,14 +27,13 @@ keywords = [""machine learning"", ""reproducibility"", ""visualization""]\n \n classifiers = [\n   'Development Status :: 5 - Production/Stable',\n-  'License :: OSI Approved :: Apache Software License',",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9ca,gradio-app/gradio,2468673836,2051177226,pngwn,12937446,2025-04-18T22:15:25+00:00,2025-04-18T22:15:29+00:00,I think we can just dispatch clear in the `remove_image` event handler without adding the other clear event to the Controls component. ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9cb,gradio-app/gradio,2468673836,2054531639,freddyaboulton,41651716,2025-04-22T17:18:15+00:00,2025-04-22T17:18:16+00:00,Will modify!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9cc,gradio-app/gradio,2468673836,2051177226,pngwn,12937446,2025-04-18T22:15:25+00:00,2025-04-18T22:15:29+00:00,I think we can just dispatch clear in the `remove_image` event handler without adding the other clear event to the Controls component. ,true,js/imageeditor/shared/ImageEditor.svelte,NaN,"@@ -605,6 +605,7 @@\n 				on:set_zoom={(e) => handle_zoom_change(e.detail)}\n 				on:zoom_in={() => zoom_in_out(""in"")}\n 				on:zoom_out={() => zoom_in_out(""out"")}\n+				on:clear={() => dispatch(""clear"")}",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9cd,gradio-app/gradio,2468673836,2054531639,freddyaboulton,41651716,2025-04-22T17:18:15+00:00,2025-04-22T17:18:16+00:00,Will modify!,true,js/imageeditor/shared/ImageEditor.svelte,NaN,"@@ -605,6 +605,7 @@\n 				on:set_zoom={(e) => handle_zoom_change(e.detail)}\n 				on:zoom_in={() => zoom_in_out(""in"")}\n 				on:zoom_out={() => zoom_in_out(""out"")}\n+				on:clear={() => dispatch(""clear"")}",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9ce,gradio-app/gradio,2465940157,2049179814,hannahblair,15165908,2025-04-17T15:17:46+00:00,2025-04-17T15:17:46+00:00,"wasn't causing an issue, just adding this for type safety",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9cf,gradio-app/gradio,2465940157,2049179814,hannahblair,15165908,2025-04-17T15:17:46+00:00,2025-04-17T15:17:46+00:00,"wasn't causing an issue, just adding this for type safety",true,js/utils/src/utils.ts,NaN,"@@ -221,7 +221,7 @@ type component_loader = (args: Args) => {\n \n const is_browser = typeof window !== ""undefined"";\n \n-export type I18nFormatter = any;\n+export type I18nFormatter = (x: string) => string;",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d0,gradio-app/gradio,2465784474,2054523098,hannahblair,15165908,2025-04-22T17:12:10+00:00,2025-04-22T17:12:10+00:00,we need to audit other components that need this syntax fixed,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d1,gradio-app/gradio,2465784474,2056312476,hannahblair,15165908,2025-04-23T15:26:05+00:00,2025-04-23T15:26:05+00:00,is values the right word? 🤔 ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d2,gradio-app/gradio,2465784474,2056842699,abidlabs,1778297,2025-04-23T20:40:17+00:00,2025-04-23T20:40:17+00:00,"This approach of using a factory method to create an internal class works, but a more standard approach would be to just have `i18n` as a class that a user instantiates directly, like this:\r\n```\r\nclass i18n:\r\n    def __init__(self, **translations: dict[str, str]):\r\n```\r\n\r\nBenefit of this, besides being simpler, is that we'll want to update all of the typing for all the parameters for all of components that can also accept i18n objects, and we can reference this class directly. So for example, in `Textbox`, the parameter\r\n\r\n```\r\nlabel: str | None = None,\r\n```\r\n\r\nshould now be:\r\n\r\n```\r\nlabel: str | i18n | None = None,\r\n```",false,NaN,NaN,NaN,1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d3,gradio-app/gradio,2465784474,2056843516,abidlabs,1778297,2025-04-23T20:40:55+00:00,2025-04-23T20:40:56+00:00,"""strings""?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d4,gradio-app/gradio,2465784474,2056844110,abidlabs,1778297,2025-04-23T20:41:29+00:00,2025-04-23T20:41:29+00:00,Is  __call__ actually used? My understanding is that the .translations attribute is just passed into the frontend directly,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d5,gradio-app/gradio,2465784474,2085900404,abidlabs,1778297,2025-05-13T04:33:24+00:00,2025-05-13T04:33:25+00:00,Could we give this class a name like `I18nData` instead of `TranslationMetadata`? Only because this is visible to users in the docs and I think we should choose a name that they assiociate with `gr.I18n()`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d6,gradio-app/gradio,2465784474,2085901540,abidlabs,1778297,2025-05-13T04:34:58+00:00,2025-05-13T04:34:59+00:00,"For Markdown, `value` can also be i18n-ed?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d7,gradio-app/gradio,2465784474,2054523098,hannahblair,15165908,2025-04-22T17:12:10+00:00,2025-04-22T17:12:10+00:00,we need to audit other components that need this syntax fixed,true,js/number/Index.svelte,NaN,"@@ -71,7 +72,7 @@\n 		on:clear_status={() => gradio.dispatch(""clear_status"", loading_status)}\n 	/>\n 	<label class=""block"" class:container>\n-		<BlockTitle {root} {show_label} {info}>{label}</BlockTitle>\n+		<BlockTitle {root} {show_label} {info}>{$_(label)}</BlockTitle>",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d8,gradio-app/gradio,2465784474,2056312476,hannahblair,15165908,2025-04-23T15:26:05+00:00,2025-04-23T15:26:05+00:00,is values the right word? 🤔 ,true,gradio/blocks.py,NaN,"@@ -2471,6 +2473,7 @@ def launch(\n             strict_cors: If True, prevents external domains from making requests to a Gradio server running on localhost. If False, allows requests to localhost that originate from localhost but also, crucially, from ""null"". This parameter should normally be True to prevent CSRF attacks but may need to be False when embedding a *locally-running Gradio app* using web components.\n             ssr_mode: If True, the Gradio app will be rendered using server-side rendering mode, which is typically more performant and provides better SEO, but this requires Node 20+ to be installed on the system. If False, the app will be rendered using client-side rendering mode. If None, will use GRADIO_SSR_MODE environment variable or default to False.\n             pwa: If True, the Gradio app will be set up as an installable PWA (Progressive Web App). If set to None (default behavior), then the PWA feature will be enabled if this Gradio app is launched on Spaces, but not otherwise.\n+            i18n: An I18n instance containing custom translations, which are used to translate values in our components. This feature can only be used to translate static text in the frontend, not values in the backend.",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9d9,gradio-app/gradio,2465784474,2056842699,abidlabs,1778297,2025-04-23T20:40:17+00:00,2025-04-23T20:40:17+00:00,"This approach of using a factory method to create an internal class works, but a more standard approach would be to just have `i18n` as a class that a user instantiates directly, like this:\r\n```\r\nclass i18n:\r\n    def __init__(self, **translations: dict[str, str]):\r\n```\r\n\r\nBenefit of this, besides being simpler, is that we'll want to update all of the typing for all the parameters for all of components that can also accept i18n objects, and we can reference this class directly. So for example, in `Textbox`, the parameter\r\n\r\n```\r\nlabel: str | None = None,\r\n```\r\n\r\nshould now be:\r\n\r\n```\r\nlabel: str | i18n | None = None,\r\n```",true,gradio/__init__.py,NaN,"@@ -240,3 +241,27 @@\n     ""update"",\n     ""DeepLinkButton"",\n ]\n+\n+\n+def i18n(**kwargs) -> _I18n:\n+    """"""Create an i18n instance with translations. Use this function to define\n+    translation dictionaries for different locales.\n+\n+    Args:\n+        **kwargs: Each keyword argument should be a locale code (e.g., ""en"", ""fr"") with a\n+        dictionary value, which maps translation keys to translated strings.\n+\n+    Returns:\n+        An I18n instance with the specified translations.\n+\n+    Example:\n+        i18n = gr.i18n(\n+            en={""greeting"": ""Hello""},\n+            es={""greeting"": ""Hola""}\n+        )\n+        with gr.Blocks() as demo:\n+            gr.Textbox(label=i18n(""greeting""))\n+\n+        demo.launch(i18n=i18n)\n+    """"""\n+    return _I18n(translations=kwargs)",1,1,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9da,gradio-app/gradio,2465784474,2056843516,abidlabs,1778297,2025-04-23T20:40:55+00:00,2025-04-23T20:40:56+00:00,"""strings""?",true,gradio/blocks.py,NaN,"@@ -2471,6 +2473,7 @@ def launch(\n             strict_cors: If True, prevents external domains from making requests to a Gradio server running on localhost. If False, allows requests to localhost that originate from localhost but also, crucially, from ""null"". This parameter should normally be True to prevent CSRF attacks but may need to be False when embedding a *locally-running Gradio app* using web components.\n             ssr_mode: If True, the Gradio app will be rendered using server-side rendering mode, which is typically more performant and provides better SEO, but this requires Node 20+ to be installed on the system. If False, the app will be rendered using client-side rendering mode. If None, will use GRADIO_SSR_MODE environment variable or default to False.\n             pwa: If True, the Gradio app will be set up as an installable PWA (Progressive Web App). If set to None (default behavior), then the PWA feature will be enabled if this Gradio app is launched on Spaces, but not otherwise.\n+            i18n: An I18n instance containing custom translations, which are used to translate values in our components. This feature can only be used to translate static text in the frontend, not values in the backend.",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9db,gradio-app/gradio,2465784474,2056844110,abidlabs,1778297,2025-04-23T20:41:29+00:00,2025-04-23T20:41:29+00:00,Is  __call__ actually used? My understanding is that the .translations attribute is just passed into the frontend directly,true,gradio/i18n.py,NaN,"@@ -0,0 +1,44 @@\n+from __future__ import annotations\n+\n+\n+class _I18n:\n+    """"""\n+    Handles internationalization (i18n) for Gradio applications.\n+\n+    Stores translation dictionaries and provides a method to retrieve translation keys.\n+    The translation lookup happens on the frontend based on the browser's locale\n+    and the provided translation dictionaries.\n+    """"""\n+\n+    def __init__(self, translations: dict[str, dict[str, str]] | None = None):\n+        """"""\n+        Initializes the I18n class.\n+\n+        Args:\n+            translations: A dictionary where keys are locale codes (e.g., ""en"", ""es"")\n+                          and values are dictionaries mapping translation keys to translated strings.\n+                          Example: {""en"": {""greeting"": ""Hello""}, ""es"": {""greeting"": ""Hola""}}\n+        """"""\n+        self.translations = translations or {}\n+\n+    def __call__(self, key: str) -> str:",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9dc,gradio-app/gradio,2465784474,2085900404,abidlabs,1778297,2025-05-13T04:33:24+00:00,2025-05-13T04:33:25+00:00,Could we give this class a name like `I18nData` instead of `TranslationMetadata`? Only because this is visible to users in the docs and I think we should choose a name that they assiociate with `gr.I18n()`,true,gradio/_simple_templates/simpledropdown.py,14,"@@ -23,8 +24,8 @@ def __init__(\n         choices: list[str | int | float | tuple[str, str | int | float]] | None = None,\n         *,\n         value: str | int | float | Callable | None = None,\n-        label: str | None = None,\n-        info: str | None = None,\n+        label: str | TranslationMetadata | None = None,",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9dd,gradio-app/gradio,2465784474,2085901540,abidlabs,1778297,2025-05-13T04:34:58+00:00,2025-05-13T04:34:59+00:00,"For Markdown, `value` can also be i18n-ed?",true,gradio/components/markdown.py,10,"@@ -34,7 +35,7 @@ def __init__(\n         self,\n         value: str | Callable | None = None,",0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9de,gradio-app/gradio,2464277033,2049431075,freddyaboulton,41651716,2025-04-17T18:03:45+00:00,2025-04-17T18:09:28+00:00,Nit but I think AppError should subclass `Exception` instead of `ValueError` ? ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842f9df,gradio-app/gradio,2464277033,2049431550,freddyaboulton,41651716,2025-04-17T18:04:11+00:00,2025-04-17T18:09:28+00:00,Shouldn't it just be AppError if Error inherits from AppError?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,gradio-app_gradio
6824cd8f0ce5dcec7842fe90,keras-team/keras,2513074374,2084163209,JyotinderSingh,33001894,2025-05-12T08:47:22+00:00,2025-05-12T08:48:15+00:00,"This may be a cleaner way to write this\r\n\r\n```suggestion\r\n        # Base case: Unbatched data\r\n        batch_size = 1\r\n        if len(images.shape) == 4:\r\n            # This is a batch of images (4D input) \r\n            batch_size = self.backend.core.shape(images)[0]\r\n\r\n        random_values = self.backend.random.uniform(\r\n                shape=(batch_size,),\r\n                minval=0,\r\n                maxval=1,\r\n                seed=seed,\r\n            )\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe91,keras-team/keras,2513074374,2085999177,JyotinderSingh,33001894,2025-05-13T06:16:51+00:00,2025-05-13T06:18:17+00:00,"Nit: The conditional logic is too deeply nested and reduces clarity. Maybe we can rewrite this as follows:\r\n\r\n```py\r\n            # Determine if the input was batched\r\n            is_batched = (len(xs.shape) == 4)\r\n\r\n            # If batched, select the first image from the batch for inspection.\r\n            # Otherwise, use the transformed image directly.\r\n            # `image_to_inspect` will always be a 3D tensor.\r\n            if is_batched:\r\n                image_to_inspect = transformed[0]\r\n            else:\r\n                image_to_inspect = transformed\r\n\r\n            if data_format == ""channels_last"":\r\n                # image_to_inspect has shape (H, W, C), get the first channel [:, :, 0]\r\n                channel_data = image_to_inspect[:, :, 0]\r\n            else: # data_format == ""channels_first""\r\n                # image_to_inspect has shape (C, H, W), get the first channel [0, :, :]\r\n                channel_data = image_to_inspect[0, :, :]\r\n\r\n            unique_vals = np.unique(channel_data)\r\n            self.assertEqual(len(unique_vals), 1)\r\n\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe92,keras-team/keras,2513074374,2085999792,JyotinderSingh,33001894,2025-05-13T06:17:22+00:00,2025-05-13T06:18:17+00:00,"Nit\r\n\r\n```suggestion\r\n            # batched inputs\r\n            (np.full((1, 4, 4, 3), 128, dtype=np.float32), ""channels_last""),\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe93,keras-team/keras,2513074374,2084163209,JyotinderSingh,33001894,2025-05-12T08:47:22+00:00,2025-05-12T08:48:15+00:00,"This may be a cleaner way to write this\r\n\r\n```suggestion\r\n        # Base case: Unbatched data\r\n        batch_size = 1\r\n        if len(images.shape) == 4:\r\n            # This is a batch of images (4D input) \r\n            batch_size = self.backend.core.shape(images)[0]\r\n\r\n        random_values = self.backend.random.uniform(\r\n                shape=(batch_size,),\r\n                minval=0,\r\n                maxval=1,\r\n                seed=seed,\r\n            )\r\n```",true,keras/src/layers/preprocessing/image_preprocessing/random_grayscale.py,NaN,"@@ -59,12 +59,20 @@ def __init__(self, factor=0.5, data_format=None, seed=None, **kwargs):\n     def get_random_transformation(self, images, training=True, seed=None):\n         if seed is None:\n             seed = self._get_seed_generator(self.backend._backend)\n-        random_values = self.backend.random.uniform(\n-            shape=(self.backend.core.shape(images)[0],),\n-            minval=0,\n-            maxval=1,\n-            seed=seed,\n-        )\n+        if len(images.shape) == 4:\n+            random_values = self.backend.random.uniform(\n+                shape=(self.backend.core.shape(images)[0],),\n+                minval=0,\n+                maxval=1,\n+                seed=seed,\n+            )\n+        else:\n+            random_values = self.backend.random.uniform(\n+                shape=(1,),\n+                minval=0,\n+                maxval=1,\n+                seed=seed,\n+            )",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe94,keras-team/keras,2513074374,2085999177,JyotinderSingh,33001894,2025-05-13T06:16:51+00:00,2025-05-13T06:18:17+00:00,"Nit: The conditional logic is too deeply nested and reduces clarity. Maybe we can rewrite this as follows:\r\n\r\n```py\r\n            # Determine if the input was batched\r\n            is_batched = (len(xs.shape) == 4)\r\n\r\n            # If batched, select the first image from the batch for inspection.\r\n            # Otherwise, use the transformed image directly.\r\n            # `image_to_inspect` will always be a 3D tensor.\r\n            if is_batched:\r\n                image_to_inspect = transformed[0]\r\n            else:\r\n                image_to_inspect = transformed\r\n\r\n            if data_format == ""channels_last"":\r\n                # image_to_inspect has shape (H, W, C), get the first channel [:, :, 0]\r\n                channel_data = image_to_inspect[:, :, 0]\r\n            else: # data_format == ""channels_first""\r\n                # image_to_inspect has shape (C, H, W), get the first channel [0, :, :]\r\n                channel_data = image_to_inspect[0, :, :]\r\n\r\n            unique_vals = np.unique(channel_data)\r\n            self.assertEqual(len(unique_vals), 1)\r\n\r\n```",true,keras/src/layers/preprocessing/image_preprocessing/random_grayscale_test.py,NaN,"@@ -80,15 +80,28 @@ def test_grayscale_with_single_color_image(self):\n         test_cases = [\n             (np.full((1, 4, 4, 3), 128, dtype=np.float32), ""channels_last""),\n             (np.full((1, 3, 4, 4), 128, dtype=np.float32), ""channels_first""),\n+            # unbatched inputs\n+            (np.full((4, 4, 3), 128, dtype=np.float32), ""channels_last""),\n+            (np.full((3, 4, 4), 128, dtype=np.float32), ""channels_first""),\n         ]\n \n         for xs, data_format in test_cases:\n             layer = layers.RandomGrayscale(factor=1.0, data_format=data_format)\n             transformed = ops.convert_to_numpy(layer(xs))\n-\n-            if data_format == ""channels_last"":\n-                unique_vals = np.unique(transformed[0, :, :, 0])\n-                self.assertEqual(len(unique_vals), 1)\n+            \n+            if len(xs.shape)==4:\n+                # batched inputs\n+                if data_format == ""channels_last"":\n+                    unique_vals = np.unique(transformed[0, :, :, 0])\n+                    self.assertEqual(len(unique_vals), 1)\n+                else:\n+                    unique_vals = np.unique(transformed[0, 0, :, :])\n+                    self.assertEqual(len(unique_vals), 1)\n             else:\n-                unique_vals = np.unique(transformed[0, 0, :, :])\n-                self.assertEqual(len(unique_vals), 1)\n+                # unbatched inputs\n+                if data_format == ""channels_last"":\n+                    unique_vals = np.unique(transformed[ :, :, 0])\n+                    self.assertEqual(len(unique_vals), 1)\n+                else:\n+                    unique_vals = np.unique(transformed[ 0, :, :])\n+                    self.assertEqual(len(unique_vals), 1)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe95,keras-team/keras,2513074374,2085999792,JyotinderSingh,33001894,2025-05-13T06:17:22+00:00,2025-05-13T06:18:17+00:00,"Nit\r\n\r\n```suggestion\r\n            # batched inputs\r\n            (np.full((1, 4, 4, 3), 128, dtype=np.float32), ""channels_last""),\r\n```",true,keras/src/layers/preprocessing/image_preprocessing/random_grayscale_test.py,5,"@@ -80,15 +80,28 @@ def test_grayscale_with_single_color_image(self):\n         test_cases = [\n             (np.full((1, 4, 4, 3), 128, dtype=np.float32), ""channels_last""),",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe96,keras-team/keras,2512272091,2083690235,fchollet,710255,2025-05-12T01:24:41+00:00,2025-05-12T01:24:47+00:00,Can we refactor this logic into a standalone function that we could reuse across all callbacks that need this functionality?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe97,keras-team/keras,2512272091,2083690235,fchollet,710255,2025-05-12T01:24:41+00:00,2025-05-12T01:24:47+00:00,Can we refactor this logic into a standalone function that we could reuse across all callbacks that need this functionality?,true,keras/src/callbacks/model_checkpoint.py,53,"@@ -197,6 +183,44 @@ def __init__(\n                     f""filepath={self.filepath}""\n                 )\n \n+    def _set_monitor_op(self):",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe98,keras-team/keras,2510619007,2085959552,rkazants,35459624,2025-05-13T05:42:46+00:00,2025-05-13T05:42:47+00:00,"do we really need this code? Why `x1, x2 = _align_operand_types(x1, x2, ""select()"")` does not help? The main logic of this helper to align arguments",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe99,keras-team/keras,2510619007,2085960831,rkazants,35459624,2025-05-13T05:44:09+00:00,2025-05-13T05:44:09+00:00,why it is needed?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe9a,keras-team/keras,2510619007,2086063453,Mohamed-Ashraf273,117025882,2025-05-13T06:57:53+00:00,2025-05-13T07:39:53+00:00,"Yeah, we need this code,  ```_align_operand_types``` alone isn't sufficient to handle cases where xi is an integer or float. Without this fix, related tests fail.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe9b,keras-team/keras,2510619007,2086074560,Mohamed-Ashraf273,117025882,2025-05-13T07:05:19+00:00,2025-05-13T07:29:39+00:00,"This change handles cases where a list contains ```OpenVINOKerasTensor``` elements. I first encountered the issue while running ```split``` tests, the tests took an unusually long time and never completed. After investigating, I found that the issue was caused by split returning a list of ```OpenVINOKerasTensor``` objects, and the issue was at line where ```np.array(x)``` was trying to convert them. Although I’m not sure whether it would have eventually finished, it was taking far too long, I solved it by converting each element in the list to a numpy first if it is an ```OpenVINOKerasTensor``` object.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe9c,keras-team/keras,2510619007,2086131884,Mohamed-Ashraf273,117025882,2025-05-13T07:35:53+00:00,2025-05-13T07:35:53+00:00,![image](https://github.com/user-attachments/assets/14d71922-74cb-4a23-929e-0f4722cc46cb)\r\nLook tests stop here and take too long and never finish.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe9d,keras-team/keras,2510619007,2086138559,Mohamed-Ashraf273,117025882,2025-05-13T07:39:36+00:00,2025-05-13T07:39:36+00:00,![image](https://github.com/user-attachments/assets/32d7cd7b-d6e9-403c-b62a-86ab6a7beefb)\r\n![image](https://github.com/user-attachments/assets/38238501-eb27-4dd6-96f7-c94237f7c022)\r\n,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe9e,keras-team/keras,2510619007,2085959552,rkazants,35459624,2025-05-13T05:42:46+00:00,2025-05-13T05:42:47+00:00,"do we really need this code? Why `x1, x2 = _align_operand_types(x1, x2, ""select()"")` does not help? The main logic of this helper to align arguments",true,keras/src/backend/openvino/numpy.py,130,"@@ -1426,7 +1478,44 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n \n \n def where(condition, x1, x2):\n-    raise NotImplementedError(""`where` is not supported with openvino backend"")\n+    condition = get_ov_output(condition)\n+    if x1 is None and x2 is None:\n+        nonzero_indices = ov_opset.non_zero(condition)\n+        return OpenVINOKerasTensor(nonzero_indices.output(0))\n+    if x1 is None:\n+        return OpenVINOKerasTensor(condition)\n+    if x2 is None:\n+        raise ValueError(""x2 must be provided if x1 is specified."")\n+\n+    x1_type = get_ov_output(x1).get_element_type()\n+    x2_type = get_ov_output(x2).get_element_type()\n+\n+    if x1_type == Type.boolean or x2_type == Type.boolean:\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2)\n+    elif isinstance(x1, float):\n+        x2 = get_ov_output(x2)\n+        if x2_type.is_integral():\n+            x1 = get_ov_output(x1)\n+        else:\n+            x1 = get_ov_output(x1, x2_type)\n+    elif isinstance(x2, float):\n+        x1 = get_ov_output(x1)\n+        if x1_type.is_integral():\n+            x2 = get_ov_output(x2)\n+        else:\n+            x2 = get_ov_output(x2, x1_type)\n+    elif isinstance(x2, int):\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2, x1_type)\n+    elif isinstance(x1, int):\n+        x2 = get_ov_output(x2)\n+        x1 = get_ov_output(x1, x2_type)\n+    else:\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fe9f,keras-team/keras,2510619007,2085960831,rkazants,35459624,2025-05-13T05:44:09+00:00,2025-05-13T05:44:09+00:00,why it is needed?,true,keras/src/backend/openvino/core.py,14,"@@ -450,8 +450,16 @@ def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):\n def convert_to_numpy(x):\n     if isinstance(x, np.ndarray):\n         return x\n-    elif isinstance(x, (int, float, list, tuple)):\n+    elif isinstance(x, (int, float)):\n         return np.array(x)\n+    elif isinstance(x, (list, tuple)):\n+        x_new = []\n+        for elem in x:\n+            if isinstance(elem, OpenVINOKerasTensor):\n+                x_new.append(convert_to_numpy(elem))\n+            else:\n+                x_new.append(elem)\n+        return np.array(x_new)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea0,keras-team/keras,2510619007,2086063453,Mohamed-Ashraf273,117025882,2025-05-13T06:57:53+00:00,2025-05-13T07:39:53+00:00,"Yeah, we need this code,  ```_align_operand_types``` alone isn't sufficient to handle cases where xi is an integer or float. Without this fix, related tests fail.",true,keras/src/backend/openvino/numpy.py,130,"@@ -1426,7 +1478,44 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n \n \n def where(condition, x1, x2):\n-    raise NotImplementedError(""`where` is not supported with openvino backend"")\n+    condition = get_ov_output(condition)\n+    if x1 is None and x2 is None:\n+        nonzero_indices = ov_opset.non_zero(condition)\n+        return OpenVINOKerasTensor(nonzero_indices.output(0))\n+    if x1 is None:\n+        return OpenVINOKerasTensor(condition)\n+    if x2 is None:\n+        raise ValueError(""x2 must be provided if x1 is specified."")\n+\n+    x1_type = get_ov_output(x1).get_element_type()\n+    x2_type = get_ov_output(x2).get_element_type()\n+\n+    if x1_type == Type.boolean or x2_type == Type.boolean:\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2)\n+    elif isinstance(x1, float):\n+        x2 = get_ov_output(x2)\n+        if x2_type.is_integral():\n+            x1 = get_ov_output(x1)\n+        else:\n+            x1 = get_ov_output(x1, x2_type)\n+    elif isinstance(x2, float):\n+        x1 = get_ov_output(x1)\n+        if x1_type.is_integral():\n+            x2 = get_ov_output(x2)\n+        else:\n+            x2 = get_ov_output(x2, x1_type)\n+    elif isinstance(x2, int):\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2, x1_type)\n+    elif isinstance(x1, int):\n+        x2 = get_ov_output(x2)\n+        x1 = get_ov_output(x1, x2_type)\n+    else:\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea1,keras-team/keras,2510619007,2086074560,Mohamed-Ashraf273,117025882,2025-05-13T07:05:19+00:00,2025-05-13T07:29:39+00:00,"This change handles cases where a list contains ```OpenVINOKerasTensor``` elements. I first encountered the issue while running ```split``` tests, the tests took an unusually long time and never completed. After investigating, I found that the issue was caused by split returning a list of ```OpenVINOKerasTensor``` objects, and the issue was at line where ```np.array(x)``` was trying to convert them. Although I’m not sure whether it would have eventually finished, it was taking far too long, I solved it by converting each element in the list to a numpy first if it is an ```OpenVINOKerasTensor``` object.",true,keras/src/backend/openvino/core.py,14,"@@ -450,8 +450,16 @@ def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):\n def convert_to_numpy(x):\n     if isinstance(x, np.ndarray):\n         return x\n-    elif isinstance(x, (int, float, list, tuple)):\n+    elif isinstance(x, (int, float)):\n         return np.array(x)\n+    elif isinstance(x, (list, tuple)):\n+        x_new = []\n+        for elem in x:\n+            if isinstance(elem, OpenVINOKerasTensor):\n+                x_new.append(convert_to_numpy(elem))\n+            else:\n+                x_new.append(elem)\n+        return np.array(x_new)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea2,keras-team/keras,2510619007,2086131884,Mohamed-Ashraf273,117025882,2025-05-13T07:35:53+00:00,2025-05-13T07:35:53+00:00,![image](https://github.com/user-attachments/assets/14d71922-74cb-4a23-929e-0f4722cc46cb)\r\nLook tests stop here and take too long and never finish.,true,keras/src/backend/openvino/core.py,14,"@@ -450,8 +450,16 @@ def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):\n def convert_to_numpy(x):\n     if isinstance(x, np.ndarray):\n         return x\n-    elif isinstance(x, (int, float, list, tuple)):\n+    elif isinstance(x, (int, float)):\n         return np.array(x)\n+    elif isinstance(x, (list, tuple)):\n+        x_new = []\n+        for elem in x:\n+            if isinstance(elem, OpenVINOKerasTensor):\n+                x_new.append(convert_to_numpy(elem))\n+            else:\n+                x_new.append(elem)\n+        return np.array(x_new)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea3,keras-team/keras,2510619007,2086138559,Mohamed-Ashraf273,117025882,2025-05-13T07:39:36+00:00,2025-05-13T07:39:36+00:00,![image](https://github.com/user-attachments/assets/32d7cd7b-d6e9-403c-b62a-86ab6a7beefb)\r\n![image](https://github.com/user-attachments/assets/38238501-eb27-4dd6-96f7-c94237f7c022)\r\n,true,keras/src/backend/openvino/numpy.py,130,"@@ -1426,7 +1478,44 @@ def vectorize(pyfunc, *, excluded=None, signature=None):\n \n \n def where(condition, x1, x2):\n-    raise NotImplementedError(""`where` is not supported with openvino backend"")\n+    condition = get_ov_output(condition)\n+    if x1 is None and x2 is None:\n+        nonzero_indices = ov_opset.non_zero(condition)\n+        return OpenVINOKerasTensor(nonzero_indices.output(0))\n+    if x1 is None:\n+        return OpenVINOKerasTensor(condition)\n+    if x2 is None:\n+        raise ValueError(""x2 must be provided if x1 is specified."")\n+\n+    x1_type = get_ov_output(x1).get_element_type()\n+    x2_type = get_ov_output(x2).get_element_type()\n+\n+    if x1_type == Type.boolean or x2_type == Type.boolean:\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2)\n+    elif isinstance(x1, float):\n+        x2 = get_ov_output(x2)\n+        if x2_type.is_integral():\n+            x1 = get_ov_output(x1)\n+        else:\n+            x1 = get_ov_output(x1, x2_type)\n+    elif isinstance(x2, float):\n+        x1 = get_ov_output(x1)\n+        if x1_type.is_integral():\n+            x2 = get_ov_output(x2)\n+        else:\n+            x2 = get_ov_output(x2, x1_type)\n+    elif isinstance(x2, int):\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2, x1_type)\n+    elif isinstance(x1, int):\n+        x2 = get_ov_output(x2)\n+        x1 = get_ov_output(x1, x2_type)\n+    else:\n+        x1 = get_ov_output(x1)\n+        x2 = get_ov_output(x2)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea4,keras-team/keras,2509728912,2085961638,rkazants,35459624,2025-05-13T05:44:43+00:00,2025-05-13T05:44:43+00:00,please remove this file. It is needed only for developer purpose,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea5,keras-team/keras,2509728912,2085963589,rkazants,35459624,2025-05-13T05:46:23+00:00,2025-05-13T05:46:23+00:00,these two reshapes are not needed. please remove it.\r\nreshapes below are sufficient.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea6,keras-team/keras,2509728912,2085961638,rkazants,35459624,2025-05-13T05:44:43+00:00,2025-05-13T05:44:43+00:00,please remove this file. It is needed only for developer purpose,true,pytest.ini,NaN,"@@ -0,0 +1,3 @@\n+[pytest]",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea7,keras-team/keras,2509728912,2085963589,rkazants,35459624,2025-05-13T05:46:23+00:00,2025-05-13T05:46:23+00:00,these two reshapes are not needed. please remove it.\r\nreshapes below are sufficient.,true,keras/src/backend/openvino/numpy.py,NaN,"@@ -1191,7 +1191,27 @@ def ones_like(x, dtype=None):\n \n \n def outer(x1, x2):\n-    raise NotImplementedError(""`outer` is not supported with openvino backend"")\n+    x1 = get_ov_output(x1)\n+    x2 = get_ov_output(x2)\n+\n+    x1, x2 = _align_operand_types(x1, x2, ""outer()"")\n+\n+    x1_flatten = ov_opset.reshape(\n+        x1, ov_opset.constant([-1], Type.i32), False\n+    ).output(0)\n+    x2_flatten = ov_opset.reshape(\n+        x2, ov_opset.constant([-1], Type.i32), False\n+    ).output(0)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea8,keras-team/keras,2509278252,2085968066,rkazants,35459624,2025-05-13T05:50:15+00:00,2025-05-13T05:50:15+00:00,remove these changes,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fea9,keras-team/keras,2509278252,2085968376,rkazants,35459624,2025-05-13T05:50:30+00:00,2025-05-13T05:50:31+00:00,remove it. it is needed only for developer purpose,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feaa,keras-team/keras,2509278252,2085968066,rkazants,35459624,2025-05-13T05:50:15+00:00,2025-05-13T05:50:15+00:00,remove these changes,true,.gitignore,NaN,"@@ -19,4 +19,5 @@ examples/**/*.jpg\n .python-version\n .coverage\n *coverage.xml\n-.ruff_cache\n\ No newline at end of file\n+.ruff_cache\n+venv/",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feab,keras-team/keras,2509278252,2085968376,rkazants,35459624,2025-05-13T05:50:30+00:00,2025-05-13T05:50:31+00:00,remove it. it is needed only for developer purpose,true,pytest.ini,NaN,"@@ -0,0 +1,3 @@\n+[pytest]",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feac,keras-team/keras,2506591990,2085970378,rkazants,35459624,2025-05-13T05:52:22+00:00,2025-05-13T05:52:22+00:00,please remove it,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fead,keras-team/keras,2506591990,2085970536,rkazants,35459624,2025-05-13T05:52:31+00:00,2025-05-13T05:52:31+00:00,please remove it,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feae,keras-team/keras,2506591990,2085970378,rkazants,35459624,2025-05-13T05:52:22+00:00,2025-05-13T05:52:22+00:00,please remove it,true,.gitignore,6,"@@ -19,4 +19,5 @@ examples/**/*.jpg\n .python-version\n .coverage\n *coverage.xml\n-.ruff_cache\n\ No newline at end of file\n+.ruff_cache",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feaf,keras-team/keras,2506591990,2085970536,rkazants,35459624,2025-05-13T05:52:31+00:00,2025-05-13T05:52:31+00:00,please remove it,true,pytest.ini,1,"@@ -0,0 +1,3 @@\n+[pytest]",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb0,keras-team/keras,2503377193,2077570293,rkazants,35459624,2025-05-07T12:56:51+00:00,2025-05-07T12:56:51+00:00,"no need, pls remove this file. it is only for developer purpose.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb1,keras-team/keras,2503377193,2077570293,rkazants,35459624,2025-05-07T12:56:51+00:00,2025-05-07T12:56:51+00:00,"no need, pls remove this file. it is only for developer purpose.",true,pytest.ini,NaN,"@@ -0,0 +1,3 @@\n+[pytest]",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb2,keras-team/keras,2500554851,2076783463,fchollet,710255,2025-05-07T04:52:02+00:00,2025-05-07T04:52:35+00:00,Please add indent,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb3,keras-team/keras,2500554851,2076783557,fchollet,710255,2025-05-07T04:52:09+00:00,2025-05-07T04:52:35+00:00,TPU,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb4,keras-team/keras,2500554851,2076783660,fchollet,710255,2025-05-07T04:52:18+00:00,2025-05-07T04:52:35+00:00,Please use 4-space indent,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb5,keras-team/keras,2500554851,2076783463,fchollet,710255,2025-05-07T04:52:02+00:00,2025-05-07T04:52:35+00:00,Please add indent,true,keras/src/backend/jax/nn.py,NaN,"@@ -205,9 +205,9 @@ def _pool(\n         initial_value: the initial value for the reduction.\n         reduce_fn: a reduce function of the form `(T, T) -> T`.\n         pool_size: a sequence of `N` integers, representing the window size to\n-            reduce over.\n+          reduce over.\n         strides: a sequence of `N` integers, representing the inter-window\n-            strides (default: `(1, ..., 1)`).\n+        strides (default: `(1, ..., 1)`).",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb6,keras-team/keras,2500554851,2076783557,fchollet,710255,2025-05-07T04:52:09+00:00,2025-05-07T04:52:35+00:00,TPU,true,keras/src/backend/jax/nn.py,NaN,"@@ -1126,16 +1126,17 @@ def wrap_flash_attention(\n     decoder_segment_ids,\n     custom_mask=None,\n     attn_logits_soft_cap=None,\n+    head_shards=1,\n+    q_seq_shards=1,\n ):\n     if decoder_segment_ids is not None:\n         assert query.shape[2] == decoder_segment_ids.q.shape[1], (\n-            ""Sharding along sequence dimension not allowed in tpu kernel ""\n-            ""attention""\n+            ""Sharding along sequence dimension not allowed""\n+            "" in tpu kernel attention""",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb7,keras-team/keras,2500554851,2076783660,fchollet,710255,2025-05-07T04:52:18+00:00,2025-05-07T04:52:35+00:00,Please use 4-space indent,true,keras/src/backend/jax/nn.py,NaN,"@@ -1168,6 +1169,37 @@ def dot_product_attention(\n     flash_attention=None,\n     attn_logits_soft_cap=None,\n ):\n+    """"""Computes dot-product attention given query, key, and value.\n+\n+    This is the core computation of attention that is used in transformers.\n+    For TPU platforms, flash attention optimizations are automatically applied\n+    when possible, and sharding parameters are inferred from the layout map\n+    in the current distribution context.\n+\n+    Args:\n+        query: Queries with shape `[batch, time, heads,\n+          depth_k]`.",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb8,keras-team/keras,2499990569,2083303285,fchollet,710255,2025-05-10T22:46:02+00:00,2025-05-10T22:46:02+00:00,Would the integration prevent the use of `jax.jit` with Keras layers?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feb9,keras-team/keras,2499990569,2083303285,fchollet,710255,2025-05-10T22:46:02+00:00,2025-05-10T22:46:02+00:00,Would the integration prevent the use of `jax.jit` with Keras layers?,true,keras/src/random/random_test.py,18,"@@ -380,12 +381,11 @@ def test_uniform_dtype_validation(self):\n         reason=""This test requires `jax` as the backend."",\n     )\n     def test_dropout_jax_jit_stateless(self):\n-        import jax\n         import jax.numpy as jnp\n \n         x = ops.ones(3)\n \n-        @jax.jit\n+        @nnx.jit",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feba,keras-team/keras,2498396029,2073811296,divyashreepathihalli,78194266,2025-05-05T16:54:17+00:00,2025-05-05T16:59:21+00:00,I am very confused about the logic here.\r\n\r\n- why is FA disabled if inputs are sharded?\r\n,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842febb,keras-team/keras,2498396029,2073812589,divyashreepathihalli,78194266,2025-05-05T16:55:11+00:00,2025-05-05T17:03:41+00:00,this condition is weird\r\nif FA is enabled and inputs are sharded and is not running on TPU - you are disabling FA? why? can you please explain?\r\nfollowing this you are checking if running on TPU and FA is enabled - this will never be true if inputs are sharded - whats the point?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842febc,keras-team/keras,2498396029,2073816560,divyashreepathihalli,78194266,2025-05-05T16:58:06+00:00,2025-05-05T16:59:05+00:00,lets verify numerics remain consistent with this updated code to mask,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842febd,keras-team/keras,2498396029,2073811296,divyashreepathihalli,78194266,2025-05-05T16:54:17+00:00,2025-05-05T16:59:21+00:00,I am very confused about the logic here.\r\n\r\n- why is FA disabled if inputs are sharded?\r\n,true,keras/src/backend/jax/nn.py,NaN,"@@ -1177,54 +1178,111 @@ def dot_product_attention(\n             f""Received: query.shape={query.shape}, key.shape={key.shape}, ""\n             f""value.shape={value.shape}.""\n         )\n-    if flash_attention is None:\n-        flash_attention = _can_use_flash_attention(query, key, value, bias)\n-    elif flash_attention is True:\n-        # Use `raise_error=True` to provide more details if the inputs failed to\n-        # use flash attention\n-        _can_use_flash_attention(query, key, value, bias, raise_error=True)\n \n-    if jax.devices()[0].platform == ""tpu"":\n-        # Transpose to ('batch', 'heads', 'length', 'kv')\n-        query = jnp.transpose(query, axes=(0, 2, 1, 3))\n-        key = jnp.transpose(key, axes=(0, 2, 1, 3))\n-        value = jnp.transpose(value, axes=(0, 2, 1, 3))\n-        B, H, S, KV = query.shape\n-\n-        segment_ids = jnp.ones([B, S])\n-        # {token_ids, padding_mask, segment_ids} enable packing\n-        out = wrap_flash_attention(\n-            query,\n-            key,\n-            value,\n-            decoder_segment_ids=splash_attention_kernel.SegmentIds(\n-                segment_ids, segment_ids\n-            ),\n-            custom_mask=mask,\n-            attn_logits_soft_cap=attn_logits_soft_cap,\n+    # Check if inputs are sharded\n+    inputs_sharded = any(\n+        hasattr(t, ""sharding"") and not t.sharding.is_fully_replicated\n+        for t in (query, key, value)\n+    )\n+    is_tpu = jax.devices()[0].platform == ""tpu""\n+\n+    # Determine flash attention compatibility",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842febe,keras-team/keras,2498396029,2073812589,divyashreepathihalli,78194266,2025-05-05T16:55:11+00:00,2025-05-05T17:03:41+00:00,this condition is weird\r\nif FA is enabled and inputs are sharded and is not running on TPU - you are disabling FA? why? can you please explain?\r\nfollowing this you are checking if running on TPU and FA is enabled - this will never be true if inputs are sharded - whats the point?,true,keras/src/backend/jax/nn.py,NaN,"@@ -1177,54 +1178,111 @@ def dot_product_attention(\n             f""Received: query.shape={query.shape}, key.shape={key.shape}, ""\n             f""value.shape={value.shape}.""\n         )\n-    if flash_attention is None:\n-        flash_attention = _can_use_flash_attention(query, key, value, bias)\n-    elif flash_attention is True:\n-        # Use `raise_error=True` to provide more details if the inputs failed to\n-        # use flash attention\n-        _can_use_flash_attention(query, key, value, bias, raise_error=True)\n \n-    if jax.devices()[0].platform == ""tpu"":\n-        # Transpose to ('batch', 'heads', 'length', 'kv')\n-        query = jnp.transpose(query, axes=(0, 2, 1, 3))\n-        key = jnp.transpose(key, axes=(0, 2, 1, 3))\n-        value = jnp.transpose(value, axes=(0, 2, 1, 3))\n-        B, H, S, KV = query.shape\n-\n-        segment_ids = jnp.ones([B, S])\n-        # {token_ids, padding_mask, segment_ids} enable packing\n-        out = wrap_flash_attention(\n-            query,\n-            key,\n-            value,\n-            decoder_segment_ids=splash_attention_kernel.SegmentIds(\n-                segment_ids, segment_ids\n-            ),\n-            custom_mask=mask,\n-            attn_logits_soft_cap=attn_logits_soft_cap,\n+    # Check if inputs are sharded\n+    inputs_sharded = any(\n+        hasattr(t, ""sharding"") and not t.sharding.is_fully_replicated\n+        for t in (query, key, value)\n+    )\n+    is_tpu = jax.devices()[0].platform == ""tpu""\n+\n+    # Determine flash attention compatibility\n+    if flash_attention is None:\n+        flash_attention = (\n+            not inputs_sharded or is_tpu\n+        ) and _can_use_flash_attention(query, key, value, bias)\n+    elif flash_attention and inputs_sharded and not is_tpu:",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842febf,keras-team/keras,2498396029,2073816560,divyashreepathihalli,78194266,2025-05-05T16:58:06+00:00,2025-05-05T16:59:05+00:00,lets verify numerics remain consistent with this updated code to mask,true,keras/src/backend/jax/nn.py,NaN,"@@ -1177,54 +1178,111 @@ def dot_product_attention(\n             f""Received: query.shape={query.shape}, key.shape={key.shape}, ""\n             f""value.shape={value.shape}.""\n         )\n-    if flash_attention is None:\n-        flash_attention = _can_use_flash_attention(query, key, value, bias)\n-    elif flash_attention is True:\n-        # Use `raise_error=True` to provide more details if the inputs failed to\n-        # use flash attention\n-        _can_use_flash_attention(query, key, value, bias, raise_error=True)\n \n-    if jax.devices()[0].platform == ""tpu"":\n-        # Transpose to ('batch', 'heads', 'length', 'kv')\n-        query = jnp.transpose(query, axes=(0, 2, 1, 3))\n-        key = jnp.transpose(key, axes=(0, 2, 1, 3))\n-        value = jnp.transpose(value, axes=(0, 2, 1, 3))\n-        B, H, S, KV = query.shape\n-\n-        segment_ids = jnp.ones([B, S])\n-        # {token_ids, padding_mask, segment_ids} enable packing\n-        out = wrap_flash_attention(\n-            query,\n-            key,\n-            value,\n-            decoder_segment_ids=splash_attention_kernel.SegmentIds(\n-                segment_ids, segment_ids\n-            ),\n-            custom_mask=mask,\n-            attn_logits_soft_cap=attn_logits_soft_cap,\n+    # Check if inputs are sharded\n+    inputs_sharded = any(\n+        hasattr(t, ""sharding"") and not t.sharding.is_fully_replicated\n+        for t in (query, key, value)\n+    )\n+    is_tpu = jax.devices()[0].platform == ""tpu""\n+\n+    # Determine flash attention compatibility\n+    if flash_attention is None:\n+        flash_attention = (\n+            not inputs_sharded or is_tpu\n+        ) and _can_use_flash_attention(query, key, value, bias)\n+    elif flash_attention and inputs_sharded and not is_tpu:\n+        flash_attention = False\n+\n+    # TPU-specific flash attention path\n+    if is_tpu and flash_attention:\n+        # Transpose to ('batch', 'heads', 'length', 'head_dim')\n+        query_tpu_layout = jnp.transpose(query, axes=(0, 2, 1, 3))\n+        key_tpu_layout = jnp.transpose(key, axes=(0, 2, 1, 3))\n+        value_tpu_layout = jnp.transpose(value, axes=(0, 2, 1, 3))\n+\n+        bs, num_heads, q_len, head_dim = query_tpu_layout.shape\n+\n+        # Apply scale to query if provided\n+        if scale is not None:\n+            # TPU kernel applies 1/sqrt(head_dim) internally, to achieve\n+            # overall QK^T * scale, scale query by (scale * sqrt(head_dim))\n+            query_tpu_layout = query_tpu_layout * (scale * math.sqrt(head_dim))\n+\n+        # Create segment IDs for Splash Attention (for packing/batching)\n+        segment_ids = jnp.zeros([bs, q_len], dtype=jnp.int32)\n+        decoder_segment_ids = splash_attention_kernel.SegmentIds(\n+            q=segment_ids, kv=segment_ids\n         )\n-        out = jnp.transpose(out, axes=(0, 2, 1, 3))\n-        return out\n \n-    # `dot_product_attention` is only available in jax>=0.4.31\n+        # Process mask for Splash Attention\n+        custom_mask = None",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec0,keras-team/keras,2493538526,2070847236,fchollet,710255,2025-05-01T21:45:23+00:00,2025-05-01T21:46:04+00:00,Please also add the function as raising `NotImplementedError` in the OpenVino `numpy.py`,false,NaN,NaN,NaN,1,1,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec1,keras-team/keras,2493538526,2070847729,fchollet,710255,2025-05-01T21:45:58+00:00,2025-05-01T21:46:04+00:00,Is floatx the right dtype to cast to or should be infer from the input dtype?,false,NaN,NaN,NaN,1,1,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec2,keras-team/keras,2493538526,2070847236,fchollet,710255,2025-05-01T21:45:23+00:00,2025-05-01T21:46:04+00:00,Please also add the function as raising `NotImplementedError` in the OpenVino `numpy.py`,true,keras/src/backend/openvino/excluded_concrete_tests.txt,4,"@@ -9,6 +9,7 @@ NumpyDtypeTest::test_any\n NumpyDtypeTest::test_argpartition\n NumpyDtypeTest::test_array\n NumpyDtypeTest::test_bartlett\n+NumpyDtypeTest::test_blackman",1,1,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec3,keras-team/keras,2493538526,2070847729,fchollet,710255,2025-05-01T21:45:58+00:00,2025-05-01T21:46:04+00:00,Is floatx the right dtype to cast to or should be infer from the input dtype?,true,keras/src/backend/numpy/numpy.py,6,"@@ -390,6 +390,11 @@ def right_shift(x, y):\n     return bitwise_right_shift(x, y)\n \n \n+def blackman(x):\n+    x = convert_to_tensor(x)\n+    return np.blackman(x).astype(config.floatx())",1,1,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec4,keras-team/keras,2491474991,2070641353,rkazants,35459624,2025-05-01T18:40:17+00:00,2025-05-01T18:40:17+00:00,"also, please remove `NumpyDtypeTest::test_flip` above",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec5,keras-team/keras,2491474991,2070641782,rkazants,35459624,2025-05-01T18:40:39+00:00,2025-05-01T18:40:40+00:00,not needed. use `ov_opset`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec6,keras-team/keras,2491474991,2070641353,rkazants,35459624,2025-05-01T18:40:17+00:00,2025-05-01T18:40:17+00:00,"also, please remove `NumpyDtypeTest::test_flip` above",true,keras/src/backend/openvino/excluded_concrete_tests.txt,9,"@@ -82,7 +82,6 @@ NumpyOneInputOpsCorrectnessTest::test_cumprod\n NumpyOneInputOpsCorrectnessTest::test_diag",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec7,keras-team/keras,2491474991,2070641782,rkazants,35459624,2025-05-01T18:40:39+00:00,2025-05-01T18:40:40+00:00,not needed. use `ov_opset`,true,keras/src/backend/openvino/numpy.py,NaN,"@@ -1,4 +1,5 @@\n import numpy as np\n+import openvino.runtime.opset1 as ov1",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec8,keras-team/keras,2489509461,2069583181,fchollet,710255,2025-04-30T22:36:24+00:00,2025-04-30T22:36:30+00:00,If this is intended to be public in the API then we should provide an explanation and a usage example in the docstring,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fec9,keras-team/keras,2489509461,2069593117,cantonios,2538739,2025-04-30T22:40:01+00:00,2025-04-30T22:40:01+00:00,"Great!  Yes, I'd like it to be public so I can inherit from it without needing to import internal keras APIs.\r\n\r\nIs this location (within `optimizer.py`) okay, or would you prefer it somewhere else, (e.g. in its own file)?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feca,keras-team/keras,2489509461,2069583181,fchollet,710255,2025-04-30T22:36:24+00:00,2025-04-30T22:36:30+00:00,If this is intended to be public in the API then we should provide an explanation and a usage example in the docstring,true,keras/src/optimizers/optimizer.py,12,"@@ -23,5 +24,90 @@ class Optimizer(BackendOptimizer, base_optimizer.BaseOptimizer):\n     pass\n \n \n+@keras_export(""keras.optimizers.VariableUpdater"")\n+class VariableUpdater:\n+    """"""Allows special handling of variable updates.""""""",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fecb,keras-team/keras,2489509461,2069593117,cantonios,2538739,2025-04-30T22:40:01+00:00,2025-04-30T22:40:01+00:00,"Great!  Yes, I'd like it to be public so I can inherit from it without needing to import internal keras APIs.\r\n\r\nIs this location (within `optimizer.py`) okay, or would you prefer it somewhere else, (e.g. in its own file)?",true,keras/src/optimizers/optimizer.py,12,"@@ -23,5 +24,90 @@ class Optimizer(BackendOptimizer, base_optimizer.BaseOptimizer):\n     pass\n \n \n+@keras_export(""keras.optimizers.VariableUpdater"")\n+class VariableUpdater:\n+    """"""Allows special handling of variable updates.""""""",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fecc,keras-team/keras,2489024825,2066727329,rkazants,35459624,2025-04-29T14:40:43+00:00,2025-04-29T14:40:44+00:00,please also remove `NumpyOneInputOpsCorrectnessTest::test_ravel` below,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fecd,keras-team/keras,2489024825,2066727329,rkazants,35459624,2025-04-29T14:40:43+00:00,2025-04-29T14:40:44+00:00,please also remove `NumpyOneInputOpsCorrectnessTest::test_ravel` below,true,keras/src/backend/openvino/excluded_concrete_tests.txt,6,"@@ -42,7 +42,6 @@ NumpyDtypeTest::test_outer_\n NumpyDtypeTest::test_power\n NumpyDtypeTest::test_prod\n NumpyDtypeTest::test_quantile\n-NumpyDtypeTest::test_ravel\n NumpyDtypeTest::test_repeat\n NumpyDtypeTest::test_roll",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fece,keras-team/keras,2488968726,2067094465,fchollet,710255,2025-04-29T18:15:54+00:00,2025-04-29T18:15:55+00:00,"This API is basically single-user, so please keep it underscore-private",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fecf,keras-team/keras,2488968726,2067094932,fchollet,710255,2025-04-29T18:16:04+00:00,2025-04-29T18:16:04+00:00,Likewise here,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed0,keras-team/keras,2488968726,2067698125,JyotinderSingh,33001894,2025-04-30T01:08:13+00:00,2025-04-30T01:08:13+00:00,Done,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed1,keras-team/keras,2488968726,2067698204,JyotinderSingh,33001894,2025-04-30T01:08:21+00:00,2025-04-30T01:08:22+00:00,Done,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed2,keras-team/keras,2488968726,2067725044,hertschuh,1091026,2025-04-30T01:50:02+00:00,2025-04-30T01:50:02+00:00,"Wait, this one does not have an underscore.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed3,keras-team/keras,2488968726,2067842789,JyotinderSingh,33001894,2025-04-30T04:41:49+00:00,2025-04-30T04:41:49+00:00,Had forgotten to upload that change. Fixed now!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed4,keras-team/keras,2488968726,2069023143,hertschuh,1091026,2025-04-30T16:15:30+00:00,2025-04-30T16:15:30+00:00,"Wait, this needs to move to `__init__`. Otherwise it will overwrite your registrations.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed5,keras-team/keras,2488968726,2069035015,hertschuh,1091026,2025-04-30T16:20:48+00:00,2025-04-30T16:20:48+00:00,"Never mind, this is `__init__`.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed6,keras-team/keras,2488968726,2067094465,fchollet,710255,2025-04-29T18:15:54+00:00,2025-04-29T18:15:55+00:00,"This API is basically single-user, so please keep it underscore-private",true,keras/src/layers/layer.py,NaN,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):\n                 return rematerialized_activation_call_wrapper\n         return layer_call\n \n+    def register_call_context_args(self, *names):",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed7,keras-team/keras,2488968726,2067094932,fchollet,710255,2025-04-29T18:16:04+00:00,2025-04-29T18:16:04+00:00,Likewise here,true,keras/src/layers/layer.py,NaN,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):\n                 return rematerialized_activation_call_wrapper\n         return layer_call\n \n+    def register_call_context_args(self, *names):\n+        """"""Register call-context args to be propagated by this layer.\n+\n+        This is useful in registering custom context-args with predefined\n+        layer and model classes so that they know which arguments to\n+        propagate down their call-stack.\n+\n+        Can be invoked any time *before* the layer is called.\n+\n+        Example:\n+        ```\n+        class Inner(layers.Layer):\n+            call_context_args = (""foo_mode"",)\n+\n+            def call(self, x, foo_mode=False):\n+                # If foo_mode=True add 1, otherwise add 0\n+                add_val = ops.where(foo_mode, 1.0, 0.0)\n+                return x + add_val\n+\n+        class Outer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                # We don’t explicitly pass foo_mode here—Base Layer.__call__\n+                # should inject it into `self.inner`\n+                return self.inner(x)\n+\n+        sample_input = np.array([[1.0], [2.0]])\n+\n+        # Sequential model\n+        seq = models.Sequential([Outer()])\n+\n+        # Tell the Sequential model to propagate foo_mode down\n+        # the call-stack\n+        seq.register_call_context_args(""foo_mode"")\n+\n+        # foo_mode=True -> input + 1\n+        out_true = seq(sample_input, foo_mode=True)\n+        """"""\n+        if self._called:\n+            raise RuntimeError(\n+                ""Cannot add call-context args after the layer has been called.""\n+            )\n+        self._call_context_args = self._call_context_args | set(names)\n+\n+    @property\n+    def call_context_args(self):",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed8,keras-team/keras,2488968726,2067698125,JyotinderSingh,33001894,2025-04-30T01:08:13+00:00,2025-04-30T01:08:13+00:00,Done,true,keras/src/layers/layer.py,NaN,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):\n                 return rematerialized_activation_call_wrapper\n         return layer_call\n \n+    def register_call_context_args(self, *names):",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fed9,keras-team/keras,2488968726,2067698204,JyotinderSingh,33001894,2025-04-30T01:08:21+00:00,2025-04-30T01:08:22+00:00,Done,true,keras/src/layers/layer.py,NaN,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):\n                 return rematerialized_activation_call_wrapper\n         return layer_call\n \n+    def register_call_context_args(self, *names):\n+        """"""Register call-context args to be propagated by this layer.\n+\n+        This is useful in registering custom context-args with predefined\n+        layer and model classes so that they know which arguments to\n+        propagate down their call-stack.\n+\n+        Can be invoked any time *before* the layer is called.\n+\n+        Example:\n+        ```\n+        class Inner(layers.Layer):\n+            call_context_args = (""foo_mode"",)\n+\n+            def call(self, x, foo_mode=False):\n+                # If foo_mode=True add 1, otherwise add 0\n+                add_val = ops.where(foo_mode, 1.0, 0.0)\n+                return x + add_val\n+\n+        class Outer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                # We don’t explicitly pass foo_mode here—Base Layer.__call__\n+                # should inject it into `self.inner`\n+                return self.inner(x)\n+\n+        sample_input = np.array([[1.0], [2.0]])\n+\n+        # Sequential model\n+        seq = models.Sequential([Outer()])\n+\n+        # Tell the Sequential model to propagate foo_mode down\n+        # the call-stack\n+        seq.register_call_context_args(""foo_mode"")\n+\n+        # foo_mode=True -> input + 1\n+        out_true = seq(sample_input, foo_mode=True)\n+        """"""\n+        if self._called:\n+            raise RuntimeError(\n+                ""Cannot add call-context args after the layer has been called.""\n+            )\n+        self._call_context_args = self._call_context_args | set(names)\n+\n+    @property\n+    def call_context_args(self):",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feda,keras-team/keras,2488968726,2067725044,hertschuh,1091026,2025-04-30T01:50:02+00:00,2025-04-30T01:50:02+00:00,"Wait, this one does not have an underscore.",true,keras/src/layers/layer.py,NaN,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):\n                 return rematerialized_activation_call_wrapper\n         return layer_call\n \n+    def register_call_context_args(self, *names):\n+        """"""Register call-context args to be propagated by this layer.\n+\n+        This is useful in registering custom context-args with predefined\n+        layer and model classes so that they know which arguments to\n+        propagate down their call-stack.\n+\n+        Can be invoked any time *before* the layer is called.\n+\n+        Example:\n+        ```\n+        class Inner(layers.Layer):\n+            call_context_args = (""foo_mode"",)\n+\n+            def call(self, x, foo_mode=False):\n+                # If foo_mode=True add 1, otherwise add 0\n+                add_val = ops.where(foo_mode, 1.0, 0.0)\n+                return x + add_val\n+\n+        class Outer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                # We don’t explicitly pass foo_mode here—Base Layer.__call__\n+                # should inject it into `self.inner`\n+                return self.inner(x)\n+\n+        sample_input = np.array([[1.0], [2.0]])\n+\n+        # Sequential model\n+        seq = models.Sequential([Outer()])\n+\n+        # Tell the Sequential model to propagate foo_mode down\n+        # the call-stack\n+        seq.register_call_context_args(""foo_mode"")\n+\n+        # foo_mode=True -> input + 1\n+        out_true = seq(sample_input, foo_mode=True)\n+        """"""\n+        if self._called:\n+            raise RuntimeError(\n+                ""Cannot add call-context args after the layer has been called.""\n+            )\n+        self._call_context_args = self._call_context_args | set(names)\n+\n+    @property\n+    def call_context_args(self):",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fedb,keras-team/keras,2488968726,2067842789,JyotinderSingh,33001894,2025-04-30T04:41:49+00:00,2025-04-30T04:41:49+00:00,Had forgotten to upload that change. Fixed now!,true,keras/src/layers/layer.py,NaN,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):\n                 return rematerialized_activation_call_wrapper\n         return layer_call\n \n+    def register_call_context_args(self, *names):\n+        """"""Register call-context args to be propagated by this layer.\n+\n+        This is useful in registering custom context-args with predefined\n+        layer and model classes so that they know which arguments to\n+        propagate down their call-stack.\n+\n+        Can be invoked any time *before* the layer is called.\n+\n+        Example:\n+        ```\n+        class Inner(layers.Layer):\n+            call_context_args = (""foo_mode"",)\n+\n+            def call(self, x, foo_mode=False):\n+                # If foo_mode=True add 1, otherwise add 0\n+                add_val = ops.where(foo_mode, 1.0, 0.0)\n+                return x + add_val\n+\n+        class Outer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.inner = Inner()\n+\n+            def call(self, x):\n+                # We don’t explicitly pass foo_mode here—Base Layer.__call__\n+                # should inject it into `self.inner`\n+                return self.inner(x)\n+\n+        sample_input = np.array([[1.0], [2.0]])\n+\n+        # Sequential model\n+        seq = models.Sequential([Outer()])\n+\n+        # Tell the Sequential model to propagate foo_mode down\n+        # the call-stack\n+        seq.register_call_context_args(""foo_mode"")\n+\n+        # foo_mode=True -> input + 1\n+        out_true = seq(sample_input, foo_mode=True)\n+        """"""\n+        if self._called:\n+            raise RuntimeError(\n+                ""Cannot add call-context args after the layer has been called.""\n+            )\n+        self._call_context_args = self._call_context_args | set(names)\n+\n+    @property\n+    def call_context_args(self):",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fedc,keras-team/keras,2488968726,2069023143,hertschuh,1091026,2025-04-30T16:15:30+00:00,2025-04-30T16:15:30+00:00,"Wait, this needs to move to `__init__`. Otherwise it will overwrite your registrations.",true,keras/src/layers/layer.py,19,"@@ -306,21 +306,21 @@ def __init__(\n         self._losses_override = []\n \n         self._call_signature = inspect.signature(self.call)\n-        call_signature_parameters = [\n+        self.call_signature_parameters = [\n             p.name for p in self._call_signature.parameters.values()\n         ]\n-        self._call_has_training_arg = ""training"" in call_signature_parameters\n-        self._call_has_mask_arg = ""mask"" in call_signature_parameters\n+        self._call_has_training_arg = (\n+            ""training"" in self.call_signature_parameters\n+        )\n+        self._call_has_mask_arg = ""mask"" in self.call_signature_parameters\n \n         # 1. collect names that should be auto‑propagated\n-        builtin_context_args = {""training""}\n-        custom_context_args = set(getattr(self, ""call_context_args"", ()))\n-        self._call_ctx_args = builtin_context_args | custom_context_args\n+        self._call_context_args = {""training""}",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fedd,keras-team/keras,2488968726,2069035015,hertschuh,1091026,2025-04-30T16:20:48+00:00,2025-04-30T16:20:48+00:00,"Never mind, this is `__init__`.",true,keras/src/layers/layer.py,19,"@@ -306,21 +306,21 @@ def __init__(\n         self._losses_override = []\n \n         self._call_signature = inspect.signature(self.call)\n-        call_signature_parameters = [\n+        self.call_signature_parameters = [\n             p.name for p in self._call_signature.parameters.values()\n         ]\n-        self._call_has_training_arg = ""training"" in call_signature_parameters\n-        self._call_has_mask_arg = ""mask"" in call_signature_parameters\n+        self._call_has_training_arg = (\n+            ""training"" in self.call_signature_parameters\n+        )\n+        self._call_has_mask_arg = ""mask"" in self.call_signature_parameters\n \n         # 1. collect names that should be auto‑propagated\n-        builtin_context_args = {""training""}\n-        custom_context_args = set(getattr(self, ""call_context_args"", ()))\n-        self._call_ctx_args = builtin_context_args | custom_context_args\n+        self._call_context_args = {""training""}",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fede,keras-team/keras,2488390099,2072497061,fchollet,710255,2025-05-04T01:34:50+00:00,2025-05-04T01:34:50+00:00,There is not `math` namespace -- just export to `ops`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fedf,keras-team/keras,2488390099,2072636142,DavidLandup0,60978046,2025-05-04T14:44:56+00:00,2025-05-04T14:44:56+00:00,Whoops - good point. Accidentally left it in due to the original `keras.ops.numpy` location.\r\nRemoved!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee0,keras-team/keras,2488390099,2072497061,fchollet,710255,2025-05-04T01:34:50+00:00,2025-05-04T01:34:50+00:00,There is not `math` namespace -- just export to `ops`,true,keras/src/ops/math.py,NaN,"@@ -1044,3 +1044,101 @@ def logdet(x):\n     if any_symbolic_tensors((x,)):\n         return Logdet().symbolic_call(x)\n     return backend.math.logdet(x)\n+\n+\n+class ViewAsComplex(Operation):\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        if len(x.shape) < 1 or x.shape[-1] != 2:\n+            raise ValueError(\n+                ""Input tensor's last dimension must be 2 (real and imaginary).""\n+            )\n+        return x[..., 0] + 1j * x[..., 1]\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape[:-1], dtype=""complex64"")\n+\n+\n+class ViewAsReal(Operation):\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        real_part = backend.numpy.real(x)\n+        imag_part = backend.numpy.imag(x)\n+        return backend.numpy.stack((real_part, imag_part), axis=-1)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape + (2,), dtype=""float32"")\n+\n+\n+@keras_export([""keras.ops.view_as_complex"", ""keras.ops.math.view_as_complex""])\n+def view_as_complex(x):\n+    """"""Converts a real tensor with shape `(..., 2)` to a complex tensor,\n+    where the last dimension represents the real and imaginary components\n+    of a complex tensor.\n+\n+    Args:\n+        x: A real tensor with last dimension of size 2.\n+\n+    Returns:\n+        A complex tensor with shape `x.shape[:-1]`.\n+\n+    Example:\n+\n+    ```\n+    >>> import numpy as np\n+    >>> from keras import ops\n+\n+    >>> real_imag = np.array([[1.0, 2.0], [3.0, 4.0]])\n+    >>> complex_tensor = ops.view_as_complex(real_imag)\n+    >>> complex_tensor\n+    array([1.+2.j, 3.+4.j])\n+    ```\n+    """"""\n+    if any_symbolic_tensors((x,)):\n+        return ViewAsComplex().symbolic_call(x)\n+\n+    x = backend.convert_to_tensor(x)\n+    if len(x.shape) < 1 or x.shape[-1] != 2:\n+        raise ValueError(\n+            ""Last dimension of input must be size 2 (real and imaginary). ""\n+            f""Received shape: {x.shape}""\n+        )\n+    real_part = x[..., 0]\n+    imag_part = x[..., 1]\n+\n+    return backend.cast(real_part, dtype=""complex64"") + 1j * backend.cast(\n+        imag_part, dtype=""complex64""\n+    )\n+\n+\n+@keras_export([""keras.ops.view_as_real"", ""keras.ops.math.view_as_real""])",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee1,keras-team/keras,2488390099,2072636142,DavidLandup0,60978046,2025-05-04T14:44:56+00:00,2025-05-04T14:44:56+00:00,Whoops - good point. Accidentally left it in due to the original `keras.ops.numpy` location.\r\nRemoved!,true,keras/src/ops/math.py,NaN,"@@ -1044,3 +1044,101 @@ def logdet(x):\n     if any_symbolic_tensors((x,)):\n         return Logdet().symbolic_call(x)\n     return backend.math.logdet(x)\n+\n+\n+class ViewAsComplex(Operation):\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        if len(x.shape) < 1 or x.shape[-1] != 2:\n+            raise ValueError(\n+                ""Input tensor's last dimension must be 2 (real and imaginary).""\n+            )\n+        return x[..., 0] + 1j * x[..., 1]\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape[:-1], dtype=""complex64"")\n+\n+\n+class ViewAsReal(Operation):\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        real_part = backend.numpy.real(x)\n+        imag_part = backend.numpy.imag(x)\n+        return backend.numpy.stack((real_part, imag_part), axis=-1)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(shape=x.shape + (2,), dtype=""float32"")\n+\n+\n+@keras_export([""keras.ops.view_as_complex"", ""keras.ops.math.view_as_complex""])\n+def view_as_complex(x):\n+    """"""Converts a real tensor with shape `(..., 2)` to a complex tensor,\n+    where the last dimension represents the real and imaginary components\n+    of a complex tensor.\n+\n+    Args:\n+        x: A real tensor with last dimension of size 2.\n+\n+    Returns:\n+        A complex tensor with shape `x.shape[:-1]`.\n+\n+    Example:\n+\n+    ```\n+    >>> import numpy as np\n+    >>> from keras import ops\n+\n+    >>> real_imag = np.array([[1.0, 2.0], [3.0, 4.0]])\n+    >>> complex_tensor = ops.view_as_complex(real_imag)\n+    >>> complex_tensor\n+    array([1.+2.j, 3.+4.j])\n+    ```\n+    """"""\n+    if any_symbolic_tensors((x,)):\n+        return ViewAsComplex().symbolic_call(x)\n+\n+    x = backend.convert_to_tensor(x)\n+    if len(x.shape) < 1 or x.shape[-1] != 2:\n+        raise ValueError(\n+            ""Last dimension of input must be size 2 (real and imaginary). ""\n+            f""Received shape: {x.shape}""\n+        )\n+    real_part = x[..., 0]\n+    imag_part = x[..., 1]\n+\n+    return backend.cast(real_part, dtype=""complex64"") + 1j * backend.cast(\n+        imag_part, dtype=""complex64""\n+    )\n+\n+\n+@keras_export([""keras.ops.view_as_real"", ""keras.ops.math.view_as_real""])",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee2,keras-team/keras,2484638697,2064087939,mattdangerw,1389937,2025-04-28T16:50:09+00:00,2025-04-28T16:50:09+00:00,"This controls the epoch iterator, so it sets `steps` for `predict` and `evaluate` and `steps_per_epoch` for `fit`. I'm not sure if `KERAS_MAX_STEPS` or `KERAS_MAX_STEPS_PER_EPOCH` is a better name.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee3,keras-team/keras,2484638697,2064260494,mattdangerw,1389937,2025-04-28T18:29:25+00:00,2025-04-28T18:29:25+00:00,switching to max steps per epoch,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee4,keras-team/keras,2484638697,2064087939,mattdangerw,1389937,2025-04-28T16:50:09+00:00,2025-04-28T16:50:09+00:00,"This controls the epoch iterator, so it sets `steps` for `predict` and `evaluate` and `steps_per_epoch` for `fit`. I'm not sure if `KERAS_MAX_STEPS` or `KERAS_MAX_STEPS_PER_EPOCH` is a better name.",true,keras/src/backend/config.py,NaN,"@@ -304,7 +308,10 @@ def keras_home():\n     _backend = os.environ[""KERAS_BACKEND""]\n     if _backend:\n         _BACKEND = _backend\n-\n+if ""KERAS_MAX_EPOCHS"" in os.environ:\n+    _MAX_EPOCHS = int(os.environ[""KERAS_MAX_EPOCHS""])\n+if ""KERAS_MAX_STEPS"" in os.environ:\n+    _MAX_STEPS = int(os.environ[""KERAS_MAX_STEPS""])",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee5,keras-team/keras,2484638697,2064260494,mattdangerw,1389937,2025-04-28T18:29:25+00:00,2025-04-28T18:29:25+00:00,switching to max steps per epoch,true,keras/src/backend/config.py,NaN,"@@ -304,7 +308,10 @@ def keras_home():\n     _backend = os.environ[""KERAS_BACKEND""]\n     if _backend:\n         _BACKEND = _backend\n-\n+if ""KERAS_MAX_EPOCHS"" in os.environ:\n+    _MAX_EPOCHS = int(os.environ[""KERAS_MAX_EPOCHS""])\n+if ""KERAS_MAX_STEPS"" in os.environ:\n+    _MAX_STEPS = int(os.environ[""KERAS_MAX_STEPS""])",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee6,keras-team/keras,2484403588,2062986573,rkazants,35459624,2025-04-28T06:44:35+00:00,2025-04-28T06:44:36+00:00,not correct implementation. See other PRs as examples,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee7,keras-team/keras,2484403588,2062986573,rkazants,35459624,2025-04-28T06:44:35+00:00,2025-04-28T06:44:36+00:00,not correct implementation. See other PRs as examples,true,keras/src/backend/openvino/numpy.py,8,"@@ -965,9 +965,8 @@ def log2(x):\n \n \n def logaddexp(x1, x2):\n-    raise NotImplementedError(\n-        ""`logaddexp` is not supported with openvino backend""\n-    )\n+    m = maximum(x1, x2)\n+    return add(m, log(add(exp(subtract(x1, m)), exp(subtract(x2, m)))))",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee8,keras-team/keras,2480610563,2073222818,rkazants,35459624,2025-05-05T10:45:50+00:00,2025-05-05T10:45:51+00:00,not needed because `get_ov_output` already performs `convert`. So please remove,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fee9,keras-team/keras,2480610563,2073223590,rkazants,35459624,2025-05-05T10:46:31+00:00,2025-05-05T10:46:31+00:00,not needed,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feea,keras-team/keras,2480610563,2073224007,rkazants,35459624,2025-05-05T10:46:52+00:00,2025-05-05T10:46:52+00:00,"I think `num = get_ov_output(num, Type.i32)` should be enought for all cases",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feeb,keras-team/keras,2480610563,2073228478,rkazants,35459624,2025-05-05T10:50:52+00:00,2025-05-05T10:50:52+00:00,create `return_step` variable and assign. `return_step` should be wrapped with `OpenVINOKerasTensor`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feec,keras-team/keras,2480610563,2073908247,vi-shruti,41509646,2025-05-05T17:52:39+00:00,2025-05-05T17:52:39+00:00,"Thanks for your review, @rkazants.\r\nI've incorporated the rest of the changes but removing this seems to cause type errors.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feed,keras-team/keras,2480610563,2073222818,rkazants,35459624,2025-05-05T10:45:50+00:00,2025-05-05T10:45:51+00:00,not needed because `get_ov_output` already performs `convert`. So please remove,true,keras/src/backend/openvino/numpy.py,61,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        ""`linspace` is not supported with openvino backend""\n-    )\n+    """"""\n+    Return evenly spaced numbers over a specified interval.\n+\n+    Returns `num` evenly spaced samples, calculated over the\n+    interval [`start`, `stop`].\n+\n+    The endpoint of the interval can optionally be excluded.\n+\n+    Parameters\n+    ----------\n+    start : array_like\n+        The starting value of the sequence.\n+    stop : array_like\n+        The end value of the sequence, unless `endpoint` is set to False.\n+        In that case, the sequence consists of all but the last of ``num + 1``\n+        evenly spaced samples, so that `stop` is excluded.  Note that the step\n+        size changes when `endpoint` is False.\n+    num : int, optional\n+        Number of samples to generate. Default is 50. Must be non-negative.\n+    endpoint : bool, optional\n+        If True, `stop` is the last sample. Otherwise, it is not included.\n+        Default is True.\n+    retstep : bool, optional\n+        If True, return (`samples`, `step`), where `step` is the spacing\n+        between samples.\n+    dtype : dtype, optional\n+        The type of the output array.  If `dtype` is not given, the data type\n+        is inferred from `start` and `stop`. The inferred dtype will never be\n+        an integer; `float` is chosen even if the arguments would produce an\n+        array of integers.\n+    axis : int, optional\n+        The axis in the result to store the samples.  Relevant only if start\n+        or stop are array-like.  By default (0), the samples will be along a\n+        new axis inserted at the beginning. Use -1 to get an axis at the end.\n+\n+    Returns\n+    -------\n+    samples : ndarray\n+        There are `num` equally spaced samples in the closed interval\n+        ``[start, stop]`` or the half-open interval ``[start, stop)``\n+        (depending on whether `endpoint` is True or False).\n+    step : float, optional\n+        Only returned if `retstep` is True\n+\n+        Size of spacing between samples.\n+    """"""\n+\n+    dtype = standardize_dtype(dtype) or config.floatx()\n+    out_dtype = OPENVINO_DTYPES[dtype]\n+    dtype = OPENVINO_DTYPES[config.floatx()]\n+\n+    start = get_ov_output(start, dtype)\n+    stop = get_ov_output(stop, dtype)\n+    start = ov_opset.convert(start, dtype).output(0)\n+    stop = ov_opset.convert(stop, dtype).output(0)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feee,keras-team/keras,2480610563,2073223590,rkazants,35459624,2025-05-05T10:46:31+00:00,2025-05-05T10:46:31+00:00,not needed,true,keras/src/backend/openvino/numpy.py,64,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        ""`linspace` is not supported with openvino backend""\n-    )\n+    """"""\n+    Return evenly spaced numbers over a specified interval.\n+\n+    Returns `num` evenly spaced samples, calculated over the\n+    interval [`start`, `stop`].\n+\n+    The endpoint of the interval can optionally be excluded.\n+\n+    Parameters\n+    ----------\n+    start : array_like\n+        The starting value of the sequence.\n+    stop : array_like\n+        The end value of the sequence, unless `endpoint` is set to False.\n+        In that case, the sequence consists of all but the last of ``num + 1``\n+        evenly spaced samples, so that `stop` is excluded.  Note that the step\n+        size changes when `endpoint` is False.\n+    num : int, optional\n+        Number of samples to generate. Default is 50. Must be non-negative.\n+    endpoint : bool, optional\n+        If True, `stop` is the last sample. Otherwise, it is not included.\n+        Default is True.\n+    retstep : bool, optional\n+        If True, return (`samples`, `step`), where `step` is the spacing\n+        between samples.\n+    dtype : dtype, optional\n+        The type of the output array.  If `dtype` is not given, the data type\n+        is inferred from `start` and `stop`. The inferred dtype will never be\n+        an integer; `float` is chosen even if the arguments would produce an\n+        array of integers.\n+    axis : int, optional\n+        The axis in the result to store the samples.  Relevant only if start\n+        or stop are array-like.  By default (0), the samples will be along a\n+        new axis inserted at the beginning. Use -1 to get an axis at the end.\n+\n+    Returns\n+    -------\n+    samples : ndarray\n+        There are `num` equally spaced samples in the closed interval\n+        ``[start, stop]`` or the half-open interval ``[start, stop)``\n+        (depending on whether `endpoint` is True or False).\n+    step : float, optional\n+        Only returned if `retstep` is True\n+\n+        Size of spacing between samples.\n+    """"""\n+\n+    dtype = standardize_dtype(dtype) or config.floatx()\n+    out_dtype = OPENVINO_DTYPES[dtype]\n+    dtype = OPENVINO_DTYPES[config.floatx()]\n+\n+    start = get_ov_output(start, dtype)\n+    stop = get_ov_output(stop, dtype)\n+    start = ov_opset.convert(start, dtype).output(0)\n+    stop = ov_opset.convert(stop, dtype).output(0)\n+\n+    if isinstance(num, OpenVINOKerasTensor):\n+        num = get_ov_output(num, Type.i32)\n+    elif isinstance(num, int):\n+        num = ov_opset.constant(num, Type.i32).output(0)\n+    else:\n+        raise TypeError(""`num` must be an int or OpenVINOKerasTensor."")\n+    num = ov_opset.convert(num, Type.i32).output(0)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842feef,keras-team/keras,2480610563,2073224007,rkazants,35459624,2025-05-05T10:46:52+00:00,2025-05-05T10:46:52+00:00,"I think `num = get_ov_output(num, Type.i32)` should be enought for all cases",true,keras/src/backend/openvino/numpy.py,NaN,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        ""`linspace` is not supported with openvino backend""\n-    )\n+    """"""\n+    Return evenly spaced numbers over a specified interval.\n+\n+    Returns `num` evenly spaced samples, calculated over the\n+    interval [`start`, `stop`].\n+\n+    The endpoint of the interval can optionally be excluded.\n+\n+    Parameters\n+    ----------\n+    start : array_like\n+        The starting value of the sequence.\n+    stop : array_like\n+        The end value of the sequence, unless `endpoint` is set to False.\n+        In that case, the sequence consists of all but the last of ``num + 1``\n+        evenly spaced samples, so that `stop` is excluded.  Note that the step\n+        size changes when `endpoint` is False.\n+    num : int, optional\n+        Number of samples to generate. Default is 50. Must be non-negative.\n+    endpoint : bool, optional\n+        If True, `stop` is the last sample. Otherwise, it is not included.\n+        Default is True.\n+    retstep : bool, optional\n+        If True, return (`samples`, `step`), where `step` is the spacing\n+        between samples.\n+    dtype : dtype, optional\n+        The type of the output array.  If `dtype` is not given, the data type\n+        is inferred from `start` and `stop`. The inferred dtype will never be\n+        an integer; `float` is chosen even if the arguments would produce an\n+        array of integers.\n+    axis : int, optional\n+        The axis in the result to store the samples.  Relevant only if start\n+        or stop are array-like.  By default (0), the samples will be along a\n+        new axis inserted at the beginning. Use -1 to get an axis at the end.\n+\n+    Returns\n+    -------\n+    samples : ndarray\n+        There are `num` equally spaced samples in the closed interval\n+        ``[start, stop]`` or the half-open interval ``[start, stop)``\n+        (depending on whether `endpoint` is True or False).\n+    step : float, optional\n+        Only returned if `retstep` is True\n+\n+        Size of spacing between samples.\n+    """"""\n+\n+    dtype = standardize_dtype(dtype) or config.floatx()\n+    out_dtype = OPENVINO_DTYPES[dtype]\n+    dtype = OPENVINO_DTYPES[config.floatx()]\n+\n+    start = get_ov_output(start, dtype)\n+    stop = get_ov_output(stop, dtype)\n+    start = ov_opset.convert(start, dtype).output(0)\n+    stop = ov_opset.convert(stop, dtype).output(0)\n+\n+    if isinstance(num, OpenVINOKerasTensor):\n+        num = get_ov_output(num, Type.i32)\n+    elif isinstance(num, int):\n+        num = ov_opset.constant(num, Type.i32).output(0)\n+    else:\n+        raise TypeError(""`num` must be an int or OpenVINOKerasTensor."")",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fef0,keras-team/keras,2480610563,2073228478,rkazants,35459624,2025-05-05T10:50:52+00:00,2025-05-05T10:50:52+00:00,create `return_step` variable and assign. `return_step` should be wrapped with `OpenVINOKerasTensor`,true,keras/src/backend/openvino/numpy.py,NaN,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        ""`linspace` is not supported with openvino backend""\n-    )\n+    """"""\n+    Return evenly spaced numbers over a specified interval.\n+\n+    Returns `num` evenly spaced samples, calculated over the\n+    interval [`start`, `stop`].\n+\n+    The endpoint of the interval can optionally be excluded.\n+\n+    Parameters\n+    ----------\n+    start : array_like\n+        The starting value of the sequence.\n+    stop : array_like\n+        The end value of the sequence, unless `endpoint` is set to False.\n+        In that case, the sequence consists of all but the last of ``num + 1``\n+        evenly spaced samples, so that `stop` is excluded.  Note that the step\n+        size changes when `endpoint` is False.\n+    num : int, optional\n+        Number of samples to generate. Default is 50. Must be non-negative.\n+    endpoint : bool, optional\n+        If True, `stop` is the last sample. Otherwise, it is not included.\n+        Default is True.\n+    retstep : bool, optional\n+        If True, return (`samples`, `step`), where `step` is the spacing\n+        between samples.\n+    dtype : dtype, optional\n+        The type of the output array.  If `dtype` is not given, the data type\n+        is inferred from `start` and `stop`. The inferred dtype will never be\n+        an integer; `float` is chosen even if the arguments would produce an\n+        array of integers.\n+    axis : int, optional\n+        The axis in the result to store the samples.  Relevant only if start\n+        or stop are array-like.  By default (0), the samples will be along a\n+        new axis inserted at the beginning. Use -1 to get an axis at the end.\n+\n+    Returns\n+    -------\n+    samples : ndarray\n+        There are `num` equally spaced samples in the closed interval\n+        ``[start, stop]`` or the half-open interval ``[start, stop)``\n+        (depending on whether `endpoint` is True or False).\n+    step : float, optional\n+        Only returned if `retstep` is True\n+\n+        Size of spacing between samples.\n+    """"""\n+\n+    dtype = standardize_dtype(dtype) or config.floatx()\n+    out_dtype = OPENVINO_DTYPES[dtype]\n+    dtype = OPENVINO_DTYPES[config.floatx()]\n+\n+    start = get_ov_output(start, dtype)\n+    stop = get_ov_output(stop, dtype)\n+    start = ov_opset.convert(start, dtype).output(0)\n+    stop = ov_opset.convert(stop, dtype).output(0)\n+\n+    if isinstance(num, OpenVINOKerasTensor):\n+        num = get_ov_output(num, Type.i32)\n+    elif isinstance(num, int):\n+        num = ov_opset.constant(num, Type.i32).output(0)\n+    else:\n+        raise TypeError(""`num` must be an int or OpenVINOKerasTensor."")\n+    num = ov_opset.convert(num, Type.i32).output(0)\n+\n+    zero_i = ov_opset.constant(0, Type.i32).output(0)\n+    one_i = ov_opset.constant(1, Type.i32).output(0)\n+    axis_i = ov_opset.constant(axis, Type.i32).output(0)\n+\n+    div = ov_opset.subtract(num, one_i).output(0) if endpoint else num\n+    div = ov_opset.convert(div, dtype).output(0)\n+\n+    zero = ov_opset.constant(0.0, dtype).output(0)\n+    one = ov_opset.constant(1.0, dtype).output(0)\n+    num_f = ov_opset.convert(num, dtype).output(0)\n+    seq = ov_opset.range(zero, num_f, one, dtype).output(0)\n+\n+    ndim = len(start.shape)\n+    dims = ov_opset.concat(\n+        [\n+            ov_opset.constant([-1], Type.i32),\n+            ov_opset.constant([1] * ndim, Type.i32),\n+        ],\n+        0,\n+    ).output(0)\n+    seq = ov_opset.reshape(seq, dims, False).output(0)\n+\n+    delta = ov_opset.subtract(stop, start).output(0)\n+\n+    cond = ov_opset.greater(div, zero).output(0)\n+    nan_const = ov_opset.constant(float(""nan""), dtype).output(0)\n+    step_val = ov_opset.divide(delta, div).output(0)\n+    step = ov_opset.select(cond, step_val, nan_const).output(0)\n+\n+    target_shape = ov_opset.concat(\n+        [\n+            ov_opset.constant([1], Type.i64),\n+            ov_opset.shape_of(start).output(0),\n+        ],\n+        0,\n+    ).output(0)\n+    step = ov_opset.reshape(step, target_shape, False).output(0)\n+\n+    eq_zero = ov_opset.equal(step, zero).output(0)\n+    any_zero = ov_opset.reduce_logical_or(\n+        eq_zero, ov_opset.constant([0], Type.i32), False\n+    ).output(0)\n+\n+    y_norm = ov_opset.multiply(seq, step).output(0)\n+    y_denorm = ov_opset.multiply(\n+        ov_opset.divide(seq, div).output(0),\n+        delta,\n+    ).output(0)\n+    y_pos = ov_opset.convert(\n+        ov_opset.select(any_zero, y_denorm, y_norm).output(0), dtype\n+    ).output(0)\n+\n+    y_zero = ov_opset.convert(\n+        ov_opset.multiply(seq, delta).output(0), dtype\n+    ).output(0)\n+    y = ov_opset.add(\n+        ov_opset.convert(ov_opset.select(cond, y_pos, y_zero).output(0), dtype),\n+        start,\n+    ).output(0)\n+\n+    if endpoint:\n+        idx = ov_opset.subtract(num, one_i).output(0)\n+        idx = ov_opset.convert(idx, Type.i32).output(0)\n+        idx_tensor = ov_opset.broadcast(idx, target_shape).output(0)\n+        stop_tensor = ov_opset.broadcast(stop, target_shape).output(0)\n+        y = ov_opset.scatter_elements_update(\n+            y, idx_tensor, stop_tensor, 0\n+        ).output(0)\n+\n+    if axis != 0:\n+        rank = ov_opset.rank(y).output(0)\n+        axis_p1 = ov_opset.add(axis_i, one_i).output(0)\n+        pre = ov_opset.range(one_i, axis_p1, one_i).output(0)\n+        post = ov_opset.range(axis_p1, rank, one_i).output(0)\n+        zero_i = ov_opset.reshape(\n+            zero_i, ov_opset.constant([1], Type.i32), False\n+        ).output(0)\n+        perm = ov_opset.concat([pre, zero_i, post], 0).output(0)\n+        y = ov_opset.transpose(y, perm).output(0)\n+\n+    y = ov_opset.convert(y, out_dtype).output(0)\n+    if retstep:\n+        return OpenVINOKerasTensor(y), ov_opset.convert(step, out_dtype).output(",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fef1,keras-team/keras,2480610563,2073908247,vi-shruti,41509646,2025-05-05T17:52:39+00:00,2025-05-05T17:52:39+00:00,"Thanks for your review, @rkazants.\r\nI've incorporated the rest of the changes but removing this seems to cause type errors.",true,keras/src/backend/openvino/numpy.py,61,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n-    raise NotImplementedError(\n-        ""`linspace` is not supported with openvino backend""\n-    )\n+    """"""\n+    Return evenly spaced numbers over a specified interval.\n+\n+    Returns `num` evenly spaced samples, calculated over the\n+    interval [`start`, `stop`].\n+\n+    The endpoint of the interval can optionally be excluded.\n+\n+    Parameters\n+    ----------\n+    start : array_like\n+        The starting value of the sequence.\n+    stop : array_like\n+        The end value of the sequence, unless `endpoint` is set to False.\n+        In that case, the sequence consists of all but the last of ``num + 1``\n+        evenly spaced samples, so that `stop` is excluded.  Note that the step\n+        size changes when `endpoint` is False.\n+    num : int, optional\n+        Number of samples to generate. Default is 50. Must be non-negative.\n+    endpoint : bool, optional\n+        If True, `stop` is the last sample. Otherwise, it is not included.\n+        Default is True.\n+    retstep : bool, optional\n+        If True, return (`samples`, `step`), where `step` is the spacing\n+        between samples.\n+    dtype : dtype, optional\n+        The type of the output array.  If `dtype` is not given, the data type\n+        is inferred from `start` and `stop`. The inferred dtype will never be\n+        an integer; `float` is chosen even if the arguments would produce an\n+        array of integers.\n+    axis : int, optional\n+        The axis in the result to store the samples.  Relevant only if start\n+        or stop are array-like.  By default (0), the samples will be along a\n+        new axis inserted at the beginning. Use -1 to get an axis at the end.\n+\n+    Returns\n+    -------\n+    samples : ndarray\n+        There are `num` equally spaced samples in the closed interval\n+        ``[start, stop]`` or the half-open interval ``[start, stop)``\n+        (depending on whether `endpoint` is True or False).\n+    step : float, optional\n+        Only returned if `retstep` is True\n+\n+        Size of spacing between samples.\n+    """"""\n+\n+    dtype = standardize_dtype(dtype) or config.floatx()\n+    out_dtype = OPENVINO_DTYPES[dtype]\n+    dtype = OPENVINO_DTYPES[config.floatx()]\n+\n+    start = get_ov_output(start, dtype)\n+    stop = get_ov_output(stop, dtype)\n+    start = ov_opset.convert(start, dtype).output(0)\n+    stop = ov_opset.convert(stop, dtype).output(0)",0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fef2,keras-team/keras,2475055982,2058904903,hertschuh,1091026,2025-04-24T17:15:54+00:00,2025-04-24T18:18:23+00:00,let's rename to\r\n- `buildin_context_args`\r\n- `custom_context_args`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd8f0ce5dcec7842fef3,keras-team/keras,2475055982,2058905941,hertschuh,1091026,2025-04-24T17:16:17+00:00,2025-04-24T18:18:23+00:00,Let's rename to `_call_has_context_arg`.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,keras-team_keras
6824cd900ce5dcec78430022,Lightning-AI/pytorch-lightning,2490518542,2069915446,mauvilsa,5780272,2025-05-01T06:57:04+00:00,2025-05-01T06:57:05+00:00,I think the tests that fail is because the newer version of jsonargparse is not installed.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430023,Lightning-AI/pytorch-lightning,2490518542,2069915446,mauvilsa,5780272,2025-05-01T06:57:04+00:00,2025-05-01T06:57:05+00:00,I think the tests that fail is because the newer version of jsonargparse is not installed.,true,requirements/pytorch/extra.txt,5,"@@ -5,7 +5,7 @@\n matplotlib>3.1, <3.9.0\n omegaconf >=2.2.3, <2.4.0\n hydra-core >=1.2.0, <1.4.0\n-jsonargparse[signatures] >=4.27.7, <=4.35.0\n+jsonargparse[signatures] >=4.39.0, <4.40.0",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430024,Lightning-AI/pytorch-lightning,2489671943,2067881717,Borda,6035284,2025-04-30T05:33:49+00:00,2025-04-30T05:33:49+00:00,```suggestion\r\nfrom lightning.pytorch import Trainer\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430025,Lightning-AI/pytorch-lightning,2489671943,2084121741,Borda,6035284,2025-05-12T08:29:00+00:00,2025-05-12T08:29:01+00:00,```suggestion\r\n        # Only call `iter()` if all following cases:\r\n        # 1. Not restarting\r\n        # 2. Not resuming from checkpoint (not _is_resuming)\r\n        # 3. Past first epoch (current_epoch > 0)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430026,Lightning-AI/pytorch-lightning,2489671943,2084127187,Borda,6035284,2025-05-12T08:31:49+00:00,2025-05-12T08:31:49+00:00,how or when it would return trainer as None?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430027,Lightning-AI/pytorch-lightning,2489671943,2067881717,Borda,6035284,2025-04-30T05:33:49+00:00,2025-04-30T05:33:49+00:00,```suggestion\r\nfrom lightning.pytorch import Trainer\r\n```,true,tests/tests_pytorch/loops/test_double_iter_in_iterable_dataset.py,NaN,"@@ -0,0 +1,81 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# This test tests the resuming of training from a checkpoint file using an IterableDataset.\n+# And contains code mentioned in the issue: #19427.\n+# Ref: https://github.com/Lightning-AI/pytorch-lightning/issues/19427\n+import multiprocessing as mp\n+import os\n+from collections.abc import Iterator\n+from pathlib import Path\n+from queue import Queue\n+\n+import numpy as np\n+from torch.utils.data import DataLoader, IterableDataset\n+\n+from lightning import Trainer",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430028,Lightning-AI/pytorch-lightning,2489671943,2084121741,Borda,6035284,2025-05-12T08:29:00+00:00,2025-05-12T08:29:01+00:00,```suggestion\r\n        # Only call `iter()` if all following cases:\r\n        # 1. Not restarting\r\n        # 2. Not resuming from checkpoint (not _is_resuming)\r\n        # 3. Past first epoch (current_epoch > 0)\r\n```,true,src/lightning/pytorch/loops/training_epoch_loop.py,8,"@@ -235,7 +235,11 @@ def reset(self) -> None:\n \n     def on_run_start(self, data_fetcher: _DataFetcher) -> None:\n         # `iter()` was called once in `FitLoop.setup_data()` already\n-        if self.trainer.current_epoch > 0 and not self.restarting:\n+        # Only call iter() if:\n+        # 1. Not restarting AND\n+        # 2. Not resuming from checkpoint (not _is_resuming) AND\n+        # 3. Past first epoch (current_epoch > 0)",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430029,Lightning-AI/pytorch-lightning,2489671943,2084127187,Borda,6035284,2025-05-12T08:31:49+00:00,2025-05-12T08:31:49+00:00,how or when it would return trainer as None?,true,tests/tests_pytorch/loops/test_double_iter_in_iterable_dataset.py,73,"@@ -0,0 +1,81 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+# This test tests the resuming of training from a checkpoint file using an IterableDataset.\n+# And contains code mentioned in the issue: #19427.\n+# Ref: https://github.com/Lightning-AI/pytorch-lightning/issues/19427\n+import multiprocessing as mp\n+import os\n+from collections.abc import Iterator\n+from pathlib import Path\n+from queue import Queue\n+\n+import numpy as np\n+from torch.utils.data import DataLoader, IterableDataset\n+\n+from lightning.pytorch import Trainer\n+from lightning.pytorch.demos.boring_classes import BoringModel\n+\n+\n+class QueueDataset(IterableDataset):\n+    def __init__(self, queue: Queue) -> None:\n+        super().__init__()\n+        self.queue = queue\n+\n+    def __iter__(self) -> Iterator:\n+        for _ in range(5):\n+            tensor, _ = self.queue.get(timeout=5)\n+            yield tensor\n+\n+\n+def create_queue() -> Queue:\n+    q = mp.Queue()\n+    arr = np.random.random([1, 32]).astype(np.float32)\n+    for ind in range(20):\n+        q.put((arr, ind))\n+    return q\n+\n+\n+def train_model(queue: Queue, max_epochs: int, ckpt_path: Path) -> Trainer:\n+    dataloader = DataLoader(QueueDataset(queue), num_workers=1, batch_size=None, persistent_workers=True)\n+    trainer = Trainer(\n+        max_epochs=max_epochs,\n+        enable_progress_bar=False,\n+        enable_checkpointing=False,\n+        devices=1,\n+        logger=False,\n+    )\n+    if ckpt_path.exists():\n+        trainer.fit(BoringModel(), dataloader, ckpt_path=str(ckpt_path))\n+    else:\n+        trainer.fit(BoringModel(), dataloader)\n+        trainer.save_checkpoint(str(ckpt_path))\n+    return trainer\n+\n+\n+def test_resume_training_with(tmp_path):\n+    """"""Test resuming training from checkpoint file using a IterableDataset.""""""\n+    queue = create_queue()\n+    max_epoch = 2\n+    ckpt_path = tmp_path / ""model.ckpt""\n+    trainer = train_model(queue, max_epoch, ckpt_path)\n+    assert trainer is not None",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843002a,Lightning-AI/pytorch-lightning,2470057042,2053788264,Borda,6035284,2025-04-22T10:01:38+00:00,2025-04-22T10:01:39+00:00,"```suggestion\r\n            dl = DataLoader(\r\n                dataset=dl.dataset,\r\n                batch_size=dl.batch_size,\r\n                sampler=sampler,\r\n                num_workers=dl.num_workers,\r\n                collate_fn=dl.collate_fn,\r\n                pin_memory=dl.pin_memory,\r\n                drop_last=dl.drop_last,\r\n                timeout=dl.timeout,\r\n                worker_init_fn=dl.worker_init_fn,\r\n                multiprocessing_context=dl.multiprocessing_context,\r\n                generator=dl.generator,\r\n                prefetch_factor=dl.prefetch_factor,\r\n                persistent_workers=dl.persistent_workers,\r\n            )\r\n        updated.append(dl)\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843002b,Lightning-AI/pytorch-lightning,2470057042,2063881262,adosar,110358278,2025-04-28T15:10:27+00:00,2025-04-28T15:10:49+00:00,"I still insist that the case `overfit_batches=1` doesn't need any special handling. Model debugging, overfitting and sanity checks can all be achieved with the current implementation, i.e. training and validation use different batches even for `overfit_batches=1`. ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843002c,Lightning-AI/pytorch-lightning,2470057042,2063889129,adosar,110358278,2025-04-28T15:13:10+00:00,2025-04-28T15:13:10+00:00,"LGTM since it makes explicit that ""same"" refers to training and validation, respectively.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843002d,Lightning-AI/pytorch-lightning,2470057042,2063890058,adosar,110358278,2025-04-28T15:13:32+00:00,2025-04-28T15:13:33+00:00,Same as https://github.com/Lightning-AI/pytorch-lightning/pull/20731/files/01ce1c130a52b6bcf7983fc7d7ca432ee26f421a#r2063881262.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843002e,Lightning-AI/pytorch-lightning,2470057042,2053788264,Borda,6035284,2025-04-22T10:01:38+00:00,2025-04-22T10:01:39+00:00,"```suggestion\r\n            dl = DataLoader(\r\n                dataset=dl.dataset,\r\n                batch_size=dl.batch_size,\r\n                sampler=sampler,\r\n                num_workers=dl.num_workers,\r\n                collate_fn=dl.collate_fn,\r\n                pin_memory=dl.pin_memory,\r\n                drop_last=dl.drop_last,\r\n                timeout=dl.timeout,\r\n                worker_init_fn=dl.worker_init_fn,\r\n                multiprocessing_context=dl.multiprocessing_context,\r\n                generator=dl.generator,\r\n                prefetch_factor=dl.prefetch_factor,\r\n                persistent_workers=dl.persistent_workers,\r\n            )\r\n        updated.append(dl)\r\n```",true,src/lightning/pytorch/trainer/connectors/data_connector.py,NaN,"@@ -244,19 +244,66 @@ def _get_distributed_sampler(\n \n \n def _resolve_overfit_batches(combined_loader: CombinedLoader, mode: RunningStage) -> None:\n+    """"""Resolve overfit batches by ensuring the same batch is used for both training and validation.""""""\n     all_have_sequential_sampler = all(\n         isinstance(dl.sampler, SequentialSampler) for dl in combined_loader.flattened if hasattr(dl, ""sampler"")\n     )\n     if all_have_sequential_sampler:\n         return\n+\n     rank_zero_warn(\n         f""You requested to overfit but enabled {mode.dataloader_prefix} dataloader shuffling.""\n         f"" We are turning off the {mode.dataloader_prefix} dataloader shuffling for you.""\n     )\n-    updated = [\n-        _update_dataloader(dl, sampler=SequentialSampler(dl.dataset), mode=mode) if hasattr(dl, ""dataset"") else dl\n-        for dl in combined_loader.flattened\n-    ]\n+\n+    # Get the first batch from the training dataloader\n+    first_batch = None\n+    if mode == RunningStage.TRAINING:\n+        for dl in combined_loader.flattened:\n+            if hasattr(dl, ""dataset""):\n+                first_batch = next(iter(dl))\n+                break\n+\n+    # Create new dataloaders with SequentialSampler\n+    updated = []\n+    for dl in combined_loader.flattened:\n+        if hasattr(dl, ""dataset""):\n+            if mode == RunningStage.VALIDATING and first_batch is not None:\n+                # For validation, create a custom sampler that always returns the first batch\n+                class SingleBatchSampler(Sampler):\n+                    def __init__(self, batch):\n+                        self.batch = batch\n+\n+                    def __iter__(self):\n+                        yield self.batch\n+\n+                    def __len__(self):\n+                        return 1\n+\n+                sampler = SingleBatchSampler(first_batch)\n+            else:\n+                sampler = SequentialSampler(dl.dataset)\n+\n+            # Create a new dataloader with the new sampler\n+            new_dl = DataLoader(\n+                dataset=dl.dataset,\n+                batch_size=dl.batch_size,\n+                sampler=sampler,\n+                num_workers=dl.num_workers,\n+                collate_fn=dl.collate_fn,\n+                pin_memory=dl.pin_memory,\n+                drop_last=dl.drop_last,\n+                timeout=dl.timeout,\n+                worker_init_fn=dl.worker_init_fn,\n+                multiprocessing_context=dl.multiprocessing_context,\n+                generator=dl.generator,\n+                prefetch_factor=dl.prefetch_factor,\n+                persistent_workers=dl.persistent_workers,\n+            )\n+            updated.append(new_dl)\n+        else:\n+            updated.append(dl)",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843002f,Lightning-AI/pytorch-lightning,2470057042,2063881262,adosar,110358278,2025-04-28T15:10:27+00:00,2025-04-28T15:10:49+00:00,"I still insist that the case `overfit_batches=1` doesn't need any special handling. Model debugging, overfitting and sanity checks can all be achieved with the current implementation, i.e. training and validation use different batches even for `overfit_batches=1`. ",true,docs/source-pytorch/common/trainer.rst,NaN,"@@ -759,6 +759,9 @@ overfit_batches\n Uses this much data of the training & validation set.\n If the training & validation dataloaders have ``shuffle=True``, Lightning will automatically disable it.\n \n+* When set to exactly 1, the same batch is used for both training and validation steps, which is useful for debugging model implementation\n+* For other values, sequential sampling (no shuffling) is used\n+",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430030,Lightning-AI/pytorch-lightning,2470057042,2063889129,adosar,110358278,2025-04-28T15:13:10+00:00,2025-04-28T15:13:10+00:00,"LGTM since it makes explicit that ""same"" refers to training and validation, respectively.",true,docs/source-pytorch/common/trainer.rst,NaN,"@@ -769,9 +772,13 @@ Useful for quickly debugging or trying to overfit on purpose.\n     # use only 1% of the train & val set\n     trainer = Trainer(overfit_batches=0.01)\n \n-    # overfit on 10 of the same batches\n+    # overfit on 10 (same) train batches & 10 (same) val batches",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430031,Lightning-AI/pytorch-lightning,2470057042,2063890058,adosar,110358278,2025-04-28T15:13:32+00:00,2025-04-28T15:13:33+00:00,Same as https://github.com/Lightning-AI/pytorch-lightning/pull/20731/files/01ce1c130a52b6bcf7983fc7d7ca432ee26f421a#r2063881262.,true,docs/source-pytorch/common/trainer.rst,NaN,"@@ -769,9 +772,13 @@ Useful for quickly debugging or trying to overfit on purpose.\n     # use only 1% of the train & val set\n     trainer = Trainer(overfit_batches=0.01)\n \n-    # overfit on 10 of the same batches\n+    # overfit on 10 (same) train batches & 10 (same) val batches\n     trainer = Trainer(overfit_batches=10)\n \n+    # debug by training and validating on exactly the same single batch\n+    # (useful for verifying model implementation)\n+    trainer = Trainer(overfit_batches=1)\n+",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430032,Lightning-AI/pytorch-lightning,2468991395,2055648850,Borda,6035284,2025-04-23T09:29:58+00:00,2025-04-23T09:29:58+00:00,convert this tests to validate the warning,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430033,Lightning-AI/pytorch-lightning,2468991395,2055648850,Borda,6035284,2025-04-23T09:29:58+00:00,2025-04-23T09:29:58+00:00,convert this tests to validate the warning,true,tests/tests_pytorch/loops/optimization/test_automatic_loop.py,14,"@@ -84,27 +83,3 @@ def training_step(self, batch, batch_idx):\n \n     with pytest.raises(MisconfigurationException, match=match):\n         trainer.fit(model)\n-\n-\n-@pytest.mark.parametrize(""world_size"", [1, 2])",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430034,Lightning-AI/pytorch-lightning,2463171759,2046642186,ali-alshaar7,45029495,2025-04-16T10:34:43+00:00,2025-04-16T10:36:02+00:00,```suggestion\r\n        A simple guide on how to create such a studio can be found [here](https://www.loom.com/share/4150ea7650ba4df191894cfe30ff9b3c).\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430035,Lightning-AI/pytorch-lightning,2463171759,2046643865,ali-alshaar7,45029495,2025-04-16T10:35:51+00:00,2025-04-16T10:36:02+00:00,"```suggestion\r\n        In the special case when the issue can't be reproduced in a studio, provide steps and example code here.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430036,Lightning-AI/pytorch-lightning,2463171759,2060322969,Borda,6035284,2025-04-25T14:10:25+00:00,2025-04-25T14:10:25+00:00,```suggestion\r\n        A simple guide on how to create such a studio can be found [here](https://www.loom.com/share/fc4761c2fcd04cff896ea3d630fcb419).\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430037,Lightning-AI/pytorch-lightning,2463171759,2078583516,Borda,6035284,2025-05-07T22:34:42+00:00,2025-05-07T22:34:43+00:00,```suggestion\r\n        A simple guide on how to create such a studio can be found [here](https://www.youtube.com/watch?v=YcW-2Zt_bFg&ab_channel=LightningAI).\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430038,Lightning-AI/pytorch-lightning,2463171759,2046642186,ali-alshaar7,45029495,2025-04-16T10:34:43+00:00,2025-04-16T10:36:02+00:00,```suggestion\r\n        A simple guide on how to create such a studio can be found [here](https://www.loom.com/share/4150ea7650ba4df191894cfe30ff9b3c).\r\n```,true,.github/ISSUE_TEMPLATE/1_bug_report.yaml,NaN,"@@ -46,12 +46,22 @@ body:\n     attributes:\n       value: ""**Note: The rest of this form is optional, but filling it out may help us to provide better support.**""\n \n+  - type: input\n+    attributes:\n+      label: Reproduced in studio\n+      description: >\n+        Create a new Lightning Studio with code that reproduces the issue and share the link.\n+        Also include all the relevant files and data required to reproduce shared issue.\n+        In case the code does not crash, please add assert statements to show what is the real and expected output.\n+        Simple guide on how to create such studio can be found [here](https://www.loom.com/share/4150ea7650ba4df191894cfe30ff9b3c).",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430039,Lightning-AI/pytorch-lightning,2463171759,2046643865,ali-alshaar7,45029495,2025-04-16T10:35:51+00:00,2025-04-16T10:36:02+00:00,"```suggestion\r\n        In the special case when the issue can't be reproduced in a studio, provide steps and example code here.\r\n```",true,.github/ISSUE_TEMPLATE/1_bug_report.yaml,NaN,"@@ -46,12 +46,22 @@ body:\n     attributes:\n       value: ""**Note: The rest of this form is optional, but filling it out may help us to provide better support.**""\n \n+  - type: input\n+    attributes:\n+      label: Reproduced in studio\n+      description: >\n+        Create a new Lightning Studio with code that reproduces the issue and share the link.\n+        Also include all the relevant files and data required to reproduce shared issue.\n+        In case the code does not crash, please add assert statements to show what is the real and expected output.\n+        Simple guide on how to create such studio can be found [here](https://www.loom.com/share/4150ea7650ba4df191894cfe30ff9b3c).\n+      placeholder: https://lightning.ai/live-session/...\n+    validations:\n+      required: false\n   - type: textarea\n     attributes:\n       label: How to reproduce the bug\n       description: >\n-        Provide steps and example code here.\n-        You can also paste a link to Google Colab (see our [Colab bug report template](https://colab.research.google.com/github/Lightning-AI/lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb)) or adapt this minimal [snippet](https://github.com/Lightning-AI/lightning/blob/master/examples/pytorch/bug_report/bug_report_model.py).\n+        In a special case when the issues can't be reproduced in a studio, provide steps and example code here.",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843003a,Lightning-AI/pytorch-lightning,2463171759,2060322969,Borda,6035284,2025-04-25T14:10:25+00:00,2025-04-25T14:10:25+00:00,```suggestion\r\n        A simple guide on how to create such a studio can be found [here](https://www.loom.com/share/fc4761c2fcd04cff896ea3d630fcb419).\r\n```,true,.github/ISSUE_TEMPLATE/1_bug_report.yaml,NaN,"@@ -46,12 +46,22 @@ body:\n     attributes:\n       value: ""**Note: The rest of this form is optional, but filling it out may help us to provide better support.**""\n \n+  - type: input\n+    attributes:\n+      label: Reproduced in studio\n+      description: >\n+        Create a new Lightning Studio with code that reproduces the issue and share the link.\n+        Also include all the relevant files and data required to reproduce shared issue.\n+        In case the code does not crash, please add assert statements to show what is the real and expected output.\n+        A simple guide on how to create such a studio can be found [here](https://www.loom.com/share/4150ea7650ba4df191894cfe30ff9b3c).",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843003b,Lightning-AI/pytorch-lightning,2463171759,2078583516,Borda,6035284,2025-05-07T22:34:42+00:00,2025-05-07T22:34:43+00:00,```suggestion\r\n        A simple guide on how to create such a studio can be found [here](https://www.youtube.com/watch?v=YcW-2Zt_bFg&ab_channel=LightningAI).\r\n```,true,.github/ISSUE_TEMPLATE/1_bug_report.yaml,NaN,"@@ -46,12 +46,22 @@ body:\n     attributes:\n       value: ""**Note: The rest of this form is optional, but filling it out may help us to provide better support.**""\n \n+  - type: input\n+    attributes:\n+      label: Reproduced in studio\n+      description: >\n+        Create a new Lightning Studio with code that reproduces the issue and share the link.\n+        Also include all the relevant files and data required to reproduce shared issue.\n+        In case the code does not crash, please add assert statements to show what is the real and expected output.\n+        A simple guide on how to create such a studio can be found [here](https://www.loom.com/share/fc4761c2fcd04cff896ea3d630fcb419).",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843003c,Lightning-AI/pytorch-lightning,2449228413,2038194045,Borda,6035284,2025-04-10T19:41:12+00:00,2025-04-10T19:41:12+00:00,```suggestion\r\n```\r\n\r\nthis is not needed :),false,NaN,NaN,NaN,1,1,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843003d,Lightning-AI/pytorch-lightning,2449228413,2038425369,iyilmaz24,128280383,2025-04-10T22:15:31+00:00,2025-04-10T22:15:31+00:00,Sounds good. I went ahead and made these changes 🙂,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843003e,Lightning-AI/pytorch-lightning,2449228413,2041675405,Borda,6035284,2025-04-14T08:36:45+00:00,2025-04-14T08:36:45+00:00,"```suggestion\r\n    assert os.path.exists(ckpt_path), f""Checkpoint file '{ckpt_path}' wasn't created""\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843003f,Lightning-AI/pytorch-lightning,2449228413,2038194045,Borda,6035284,2025-04-10T19:41:12+00:00,2025-04-10T19:41:12+00:00,```suggestion\r\n```\r\n\r\nthis is not needed :),true,tests/tests_pytorch/loops/test_trainer_iterable_dataset_double_iter.py,NaN,"@@ -0,0 +1,63 @@\n+import multiprocessing as mp\n+import os\n+from collections.abc import Iterator\n+from queue import Queue\n+\n+import numpy as np\n+from torch.utils.data import DataLoader, IterableDataset\n+\n+from lightning import Trainer\n+from lightning.pytorch.demos.boring_classes import BoringModel\n+\n+\n+class QueueDataset(IterableDataset):\n+    def __init__(self, queue: Queue) -> None:\n+        super().__init__()\n+        self.queue = queue\n+\n+    def __iter__(self) -> Iterator:\n+        for _ in range(5):\n+            tensor, _ = self.queue.get(timeout=10)\n+            yield tensor\n+\n+\n+def create_queue():\n+    q = mp.Queue()\n+    arr = np.random.random([1, 32]).astype(np.float32)\n+    for ind in range(10):\n+        q.put((arr, ind))\n+    return q\n+\n+\n+def train_model(queue, maxEpochs, ckptPath):\n+    dataloader = DataLoader(QueueDataset(queue), num_workers=1, batch_size=None, persistent_workers=True)\n+    trainer = Trainer(max_epochs=maxEpochs, enable_progress_bar=False, devices=1)\n+    trainer.fit(BoringModel(), dataloader)\n+    if os.path.exists(ckptPath):\n+        trainer.fit(BoringModel(), dataloader, ckpt_path=ckptPath)\n+    else:\n+        trainer.fit(BoringModel(), dataloader)\n+        trainer.save_checkpoint(ckptPath)\n+    return trainer\n+\n+\n+def test_training():\n+    """"""Test that reproduces issue in calling iter twice on a queue-based IterableDataset leads to Queue Empty errors\n+    when resuming from a checkpoint.""""""\n+    queue = create_queue()\n+\n+    ckpt_path = ""model.ckpt""\n+    trainer = train_model(queue, 1, ckpt_path)\n+    assert trainer is not None\n+\n+    assert os.path.exists(ckpt_path), ""Checkpoint file wasn't created""\n+\n+    ckpt_size = os.path.getsize(ckpt_path)\n+    assert ckpt_size > 0, f""Checkpoint file is empty (size: {ckpt_size} bytes)""\n+\n+    trainer = train_model(queue, 1, ckpt_path)\n+    assert trainer is not None\n+\n+\n+if __name__ == ""__main__"":\n+    test_training()",1,1,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430040,Lightning-AI/pytorch-lightning,2449228413,2038425369,iyilmaz24,128280383,2025-04-10T22:15:31+00:00,2025-04-10T22:15:31+00:00,Sounds good. I went ahead and made these changes 🙂,true,tests/tests_pytorch/loops/test_trainer_iterable_dataset_double_iter.py,NaN,"@@ -0,0 +1,63 @@\n+import multiprocessing as mp\n+import os\n+from collections.abc import Iterator\n+from queue import Queue\n+\n+import numpy as np\n+from torch.utils.data import DataLoader, IterableDataset\n+\n+from lightning import Trainer\n+from lightning.pytorch.demos.boring_classes import BoringModel\n+\n+\n+class QueueDataset(IterableDataset):\n+    def __init__(self, queue: Queue) -> None:\n+        super().__init__()\n+        self.queue = queue\n+\n+    def __iter__(self) -> Iterator:\n+        for _ in range(5):\n+            tensor, _ = self.queue.get(timeout=10)\n+            yield tensor\n+\n+\n+def create_queue():\n+    q = mp.Queue()\n+    arr = np.random.random([1, 32]).astype(np.float32)\n+    for ind in range(10):\n+        q.put((arr, ind))\n+    return q\n+\n+\n+def train_model(queue, maxEpochs, ckptPath):\n+    dataloader = DataLoader(QueueDataset(queue), num_workers=1, batch_size=None, persistent_workers=True)\n+    trainer = Trainer(max_epochs=maxEpochs, enable_progress_bar=False, devices=1)\n+    trainer.fit(BoringModel(), dataloader)\n+    if os.path.exists(ckptPath):\n+        trainer.fit(BoringModel(), dataloader, ckpt_path=ckptPath)\n+    else:\n+        trainer.fit(BoringModel(), dataloader)\n+        trainer.save_checkpoint(ckptPath)\n+    return trainer\n+\n+\n+def test_training():\n+    """"""Test that reproduces issue in calling iter twice on a queue-based IterableDataset leads to Queue Empty errors\n+    when resuming from a checkpoint.""""""\n+    queue = create_queue()\n+\n+    ckpt_path = ""model.ckpt""\n+    trainer = train_model(queue, 1, ckpt_path)\n+    assert trainer is not None\n+\n+    assert os.path.exists(ckpt_path), ""Checkpoint file wasn't created""\n+\n+    ckpt_size = os.path.getsize(ckpt_path)\n+    assert ckpt_size > 0, f""Checkpoint file is empty (size: {ckpt_size} bytes)""\n+\n+    trainer = train_model(queue, 1, ckpt_path)\n+    assert trainer is not None\n+\n+\n+if __name__ == ""__main__"":\n+    test_training()",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430041,Lightning-AI/pytorch-lightning,2449228413,2041675405,Borda,6035284,2025-04-14T08:36:45+00:00,2025-04-14T08:36:45+00:00,"```suggestion\r\n    assert os.path.exists(ckpt_path), f""Checkpoint file '{ckpt_path}' wasn't created""\r\n```",true,tests/tests_pytorch/loops/test_trainer_iterable_dataset_double_iter.py,NaN,"@@ -0,0 +1,58 @@\n+import multiprocessing as mp\n+import os\n+from collections.abc import Iterator\n+from queue import Queue\n+\n+import numpy as np\n+from torch.utils.data import DataLoader, IterableDataset\n+\n+from lightning import Trainer\n+from lightning.pytorch.demos.boring_classes import BoringModel\n+\n+\n+class QueueDataset(IterableDataset):\n+    def __init__(self, queue: Queue) -> None:\n+        super().__init__()\n+        self.queue = queue\n+\n+    def __iter__(self) -> Iterator:\n+        for _ in range(5):\n+            tensor, _ = self.queue.get(timeout=5)\n+            yield tensor\n+\n+\n+def create_queue():\n+    q = mp.Queue()\n+    arr = np.random.random([1, 32]).astype(np.float32)\n+    for ind in range(10):\n+        q.put((arr, ind))\n+    return q\n+\n+\n+def train_model(queue, maxEpochs, ckptPath):\n+    dataloader = DataLoader(QueueDataset(queue), num_workers=1, batch_size=None, persistent_workers=True)\n+    trainer = Trainer(max_epochs=maxEpochs, enable_progress_bar=False, devices=1)\n+    if os.path.exists(ckptPath):\n+        trainer.fit(BoringModel(), dataloader, ckpt_path=ckptPath)\n+    else:\n+        trainer.fit(BoringModel(), dataloader)\n+        trainer.save_checkpoint(ckptPath)\n+    return trainer\n+\n+\n+def test_training():\n+    """"""Test that reproduces issue in calling iter twice on a queue-based IterableDataset leads to Queue Empty errors\n+    when resuming from a checkpoint.""""""\n+    queue = create_queue()\n+\n+    ckpt_path = ""model.ckpt""\n+    trainer = train_model(queue, 1, ckpt_path)\n+    assert trainer is not None\n+\n+    assert os.path.exists(ckpt_path), ""Checkpoint file wasn't created""",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430042,Lightning-AI/pytorch-lightning,2444273779,2032851868,Borda,6035284,2025-04-08T10:05:27+00:00,2025-04-08T10:05:27+00:00,```suggestion\r\n    rev: 06907d0267368b49b9180eed423fae5697c1e909 # todo: fix for docformatter after last 1.7.5\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430043,Lightning-AI/pytorch-lightning,2444273779,2032852434,Borda,6035284,2025-04-08T10:05:49+00:00,2025-04-08T10:05:49+00:00,```suggestion\r\n    rev: v3.1.0\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430044,Lightning-AI/pytorch-lightning,2444273779,2032851868,Borda,6035284,2025-04-08T10:05:27+00:00,2025-04-08T10:05:27+00:00,```suggestion\r\n    rev: 06907d0267368b49b9180eed423fae5697c1e909 # todo: fix for docformatter after last 1.7.5\r\n```,true,.pre-commit-config.yaml,NaN,"@@ -51,14 +51,14 @@ repos:\n       - id: detect-private-key\n \n   - repo: https://github.com/codespell-project/codespell\n-    rev: v2.3.0\n+    rev: v2.4.1\n     hooks:\n       - id: codespell\n         additional_dependencies: [tomli]\n         #args: [""--write-changes""] # uncomment if you want to get automatic fixing\n \n   - repo: https://github.com/PyCQA/docformatter\n-    rev: 06907d0267368b49b9180eed423fae5697c1e909 # todo: fix for docformatter after last 1.7.5\n+    rev: v1.7.5 # todo: fix for docformatter after last 1.7.5",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430045,Lightning-AI/pytorch-lightning,2444273779,2032852434,Borda,6035284,2025-04-08T10:05:49+00:00,2025-04-08T10:05:49+00:00,```suggestion\r\n    rev: v3.1.0\r\n```,true,.pre-commit-config.yaml,NaN,"@@ -96,7 +96,7 @@ repos:\n           )$\n \n   - repo: https://github.com/pre-commit/mirrors-prettier\n-    rev: v3.1.0\n+    rev: v4.0.0-alpha.8",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430046,Lightning-AI/pytorch-lightning,2442036655,2031015296,Borda,6035284,2025-04-07T11:11:36+00:00,2025-04-07T11:11:37+00:00,```suggestion\r\n      scripts-ref: v0.14.3\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430047,Lightning-AI/pytorch-lightning,2442036655,2031015431,Borda,6035284,2025-04-07T11:11:42+00:00,2025-04-07T11:11:43+00:00,```suggestion\r\n      scripts-ref: v0.14.3\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430048,Lightning-AI/pytorch-lightning,2442036655,2031015296,Borda,6035284,2025-04-07T11:11:36+00:00,2025-04-07T11:11:37+00:00,```suggestion\r\n      scripts-ref: v0.14.3\r\n```,true,.github/workflows/call-clear-cache.yml,NaN,"@@ -23,7 +23,7 @@ on:\n jobs:\n   cron-clear:\n     if: github.event_name == 'schedule' || github.event_name == 'pull_request'\n-    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.2\n+    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.3\n     with:\n       scripts-ref: v0.14.2",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430049,Lightning-AI/pytorch-lightning,2442036655,2031015431,Borda,6035284,2025-04-07T11:11:42+00:00,2025-04-07T11:11:43+00:00,```suggestion\r\n      scripts-ref: v0.14.3\r\n```,true,.github/workflows/call-clear-cache.yml,NaN,"@@ -32,7 +32,7 @@ jobs:\n \n   direct-clear:\n     if: github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request'\n-    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.2\n+    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.3\n     with:\n       scripts-ref: v0.14.2",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843004a,Lightning-AI/pytorch-lightning,2421550122,2016270191,ethanwharris,9592806,2025-03-27T11:13:39+00:00,2025-03-27T11:13:41+00:00,Seems like this only removes it from the test requirements?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843004b,Lightning-AI/pytorch-lightning,2421550122,2016302917,Borda,6035284,2025-03-27T11:24:23+00:00,2025-03-27T11:24:23+00:00,"yes, I was thinking about removing it also from PT but that would be a breaking change so we would need to do it later...\r\n@lantiga or is fine to drop it also from PL where we had it for a long time as a compatibility patch since we cut out Tm to the separate package?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843004c,Lightning-AI/pytorch-lightning,2421550122,2016270191,ethanwharris,9592806,2025-03-27T11:13:39+00:00,2025-03-27T11:13:41+00:00,Seems like this only removes it from the test requirements?,true,requirements/fabric/test.txt,4,"@@ -7,4 +7,3 @@ pytest-rerunfailures ==12.0\n pytest-random-order ==1.1.0\n click ==8.1.7\n tensorboardX >=2.2, <2.7.0  # min version is set by torch.onnx missing attribute\n-torchmetrics >=0.7.0, <1.5.0 # needed for using fixed compare_version",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843004d,Lightning-AI/pytorch-lightning,2421550122,2016302917,Borda,6035284,2025-03-27T11:24:23+00:00,2025-03-27T11:24:23+00:00,"yes, I was thinking about removing it also from PT but that would be a breaking change so we would need to do it later...\r\n@lantiga or is fine to drop it also from PL where we had it for a long time as a compatibility patch since we cut out Tm to the separate package?",true,requirements/fabric/test.txt,4,"@@ -7,4 +7,3 @@ pytest-rerunfailures ==12.0\n pytest-random-order ==1.1.0\n click ==8.1.7\n tensorboardX >=2.2, <2.7.0  # min version is set by torch.onnx missing attribute\n-torchmetrics >=0.7.0, <1.5.0 # needed for using fixed compare_version",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843004e,Lightning-AI/pytorch-lightning,2413683199,2010804988,niander,4544122,2025-03-24T19:27:44+00:00,2025-03-24T19:37:13+00:00,"Instead of doing this string conversion, why not call the `as_posix()` function from the `Path` object. This should make this working on Windows as well. Please check this bug I filed: #20664 \r\n\r\n`MLFlowClient` implementation expects paths in the POSIX format.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843004f,Lightning-AI/pytorch-lightning,2413683199,2011045544,yxtay,5795122,2025-03-24T23:13:00+00:00,2025-03-24T23:13:00+00:00,"```suggestion\r\n            artifact_path = Path(self._checkpoint_path_prefix, Path(p).stem).as_posix()\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430050,Lightning-AI/pytorch-lightning,2413683199,2011049013,yxtay,5795122,2025-03-24T23:17:44+00:00,2025-03-24T23:17:44+00:00,"Based on the comment on the issue, does it mean that it is necessary to call `resolve()` on the `artifact_path`?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430051,Lightning-AI/pytorch-lightning,2413683199,2011058487,niander,4544122,2025-03-24T23:30:36+00:00,2025-03-24T23:30:36+00:00,"I believe `resolve()` is for resolving symlinks and special path components. `artifact_path` is holds the path for the artifact in the `MLFlow` tracking system, not the real local filesystem path. I would argue that using `as_posix` is enough and there is no need to use `resolve()`.",false,NaN,NaN,NaN,1,1,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430052,Lightning-AI/pytorch-lightning,2413683199,2010804988,niander,4544122,2025-03-24T19:27:44+00:00,2025-03-24T19:37:13+00:00,"Instead of doing this string conversion, why not call the `as_posix()` function from the `Path` object. This should make this working on Windows as well. Please check this bug I filed: #20664 \r\n\r\n`MLFlowClient` implementation expects paths in the POSIX format.",true,src/lightning/pytorch/loggers/mlflow.py,NaN,"@@ -363,7 +363,7 @@ def _scan_and_log_checkpoints(self, checkpoint_callback: ModelCheckpoint) -> Non\n             aliases = [""latest"", ""best""] if p == checkpoint_callback.best_model_path else [""latest""]\n \n             # Artifact path on mlflow\n-            artifact_path = Path(self._checkpoint_path_prefix) / Path(p).stem\n+            artifact_path = str(Path(self._checkpoint_path_prefix, Path(p).stem))",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430053,Lightning-AI/pytorch-lightning,2413683199,2011045544,yxtay,5795122,2025-03-24T23:13:00+00:00,2025-03-24T23:13:00+00:00,"```suggestion\r\n            artifact_path = Path(self._checkpoint_path_prefix, Path(p).stem).as_posix()\r\n```",true,src/lightning/pytorch/loggers/mlflow.py,NaN,"@@ -363,7 +363,7 @@ def _scan_and_log_checkpoints(self, checkpoint_callback: ModelCheckpoint) -> Non\n             aliases = [""latest"", ""best""] if p == checkpoint_callback.best_model_path else [""latest""]\n \n             # Artifact path on mlflow\n-            artifact_path = Path(self._checkpoint_path_prefix) / Path(p).stem\n+            artifact_path = str(Path(self._checkpoint_path_prefix, Path(p).stem))",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430054,Lightning-AI/pytorch-lightning,2413683199,2011049013,yxtay,5795122,2025-03-24T23:17:44+00:00,2025-03-24T23:17:44+00:00,"Based on the comment on the issue, does it mean that it is necessary to call `resolve()` on the `artifact_path`?",true,src/lightning/pytorch/loggers/mlflow.py,NaN,"@@ -363,7 +363,7 @@ def _scan_and_log_checkpoints(self, checkpoint_callback: ModelCheckpoint) -> Non\n             aliases = [""latest"", ""best""] if p == checkpoint_callback.best_model_path else [""latest""]\n \n             # Artifact path on mlflow\n-            artifact_path = Path(self._checkpoint_path_prefix) / Path(p).stem\n+            artifact_path = str(Path(self._checkpoint_path_prefix, Path(p).stem))",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430055,Lightning-AI/pytorch-lightning,2413683199,2011058487,niander,4544122,2025-03-24T23:30:36+00:00,2025-03-24T23:30:36+00:00,"I believe `resolve()` is for resolving symlinks and special path components. `artifact_path` is holds the path for the artifact in the `MLFlow` tracking system, not the real local filesystem path. I would argue that using `as_posix` is enough and there is no need to use `resolve()`.",true,src/lightning/pytorch/loggers/mlflow.py,NaN,"@@ -363,7 +363,7 @@ def _scan_and_log_checkpoints(self, checkpoint_callback: ModelCheckpoint) -> Non\n             aliases = [""latest"", ""best""] if p == checkpoint_callback.best_model_path else [""latest""]\n \n             # Artifact path on mlflow\n-            artifact_path = Path(self._checkpoint_path_prefix) / Path(p).stem\n+            artifact_path = str(Path(self._checkpoint_path_prefix, Path(p).stem))",1,1,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430056,Lightning-AI/pytorch-lightning,2412409979,2010846036,Borda,6035284,2025-03-24T19:59:49+00:00,2025-03-24T19:59:49+00:00,```suggestion\r\n      scripts-ref: v0.14.2\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430057,Lightning-AI/pytorch-lightning,2412409979,2010846170,Borda,6035284,2025-03-24T19:59:56+00:00,2025-03-24T19:59:56+00:00,```suggestion\r\n      scripts-ref: v0.14.2\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430058,Lightning-AI/pytorch-lightning,2412409979,2010846036,Borda,6035284,2025-03-24T19:59:49+00:00,2025-03-24T19:59:49+00:00,```suggestion\r\n      scripts-ref: v0.14.2\r\n```,true,.github/workflows/call-clear-cache.yml,NaN,"@@ -23,7 +23,7 @@ on:\n jobs:\n   cron-clear:\n     if: github.event_name == 'schedule' || github.event_name == 'pull_request'\n-    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.1\n+    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.2\n     with:\n       scripts-ref: v0.14.1",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430059,Lightning-AI/pytorch-lightning,2412409979,2010846170,Borda,6035284,2025-03-24T19:59:56+00:00,2025-03-24T19:59:56+00:00,```suggestion\r\n      scripts-ref: v0.14.2\r\n```,true,.github/workflows/call-clear-cache.yml,NaN,"@@ -32,7 +32,7 @@ jobs:\n \n   direct-clear:\n     if: github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request'\n-    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.1\n+    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.2\n     with:\n       scripts-ref: v0.14.1",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843005a,Lightning-AI/pytorch-lightning,2408124057,2006572426,RixhersAjazi,5511749,2025-03-20T22:53:02+00:00,2025-03-20T22:53:02+00:00,Should we add a test to verify naming? ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843005b,Lightning-AI/pytorch-lightning,2408124057,2006971984,Borda,6035284,2025-03-21T07:03:13+00:00,2025-03-21T07:03:14+00:00,there is docstring,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843005c,Lightning-AI/pytorch-lightning,2408124057,2006572426,RixhersAjazi,5511749,2025-03-20T22:53:02+00:00,2025-03-20T22:53:02+00:00,Should we add a test to verify naming? ,true,src/lightning/pytorch/utilities/model_registry.py,5,"@@ -63,24 +63,24 @@ def _parse_registry_model_version(ckpt_path: Optional[_PATH]) -> tuple[str, str]\n     ('model-name', '1.0')\n     >>> _parse_registry_model_version(""registry:model-name"")\n     ('model-name', '')\n-    >>> _parse_registry_model_version(""registry:version:v2"")\n+    >>> _parse_registry_model_version(""registry:VERSION:v2"")",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843005d,Lightning-AI/pytorch-lightning,2408124057,2006971984,Borda,6035284,2025-03-21T07:03:13+00:00,2025-03-21T07:03:14+00:00,there is docstring,true,src/lightning/pytorch/utilities/model_registry.py,5,"@@ -63,24 +63,24 @@ def _parse_registry_model_version(ckpt_path: Optional[_PATH]) -> tuple[str, str]\n     ('model-name', '1.0')\n     >>> _parse_registry_model_version(""registry:model-name"")\n     ('model-name', '')\n-    >>> _parse_registry_model_version(""registry:version:v2"")\n+    >>> _parse_registry_model_version(""registry:VERSION:v2"")",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843005e,Lightning-AI/pytorch-lightning,2396462240,1998544080,Borda,6035284,2025-03-17T11:34:59+00:00,2025-03-17T11:34:59+00:00,```suggestion\r\n      scripts-ref: v0.14.1\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843005f,Lightning-AI/pytorch-lightning,2396462240,1998544294,Borda,6035284,2025-03-17T11:35:08+00:00,2025-03-17T11:35:09+00:00,```suggestion\r\n      scripts-ref: v0.14.1\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430060,Lightning-AI/pytorch-lightning,2396462240,1998544080,Borda,6035284,2025-03-17T11:34:59+00:00,2025-03-17T11:34:59+00:00,```suggestion\r\n      scripts-ref: v0.14.1\r\n```,true,.github/workflows/call-clear-cache.yml,NaN,"@@ -23,7 +23,7 @@ on:\n jobs:\n   cron-clear:\n     if: github.event_name == 'schedule' || github.event_name == 'pull_request'\n-    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.0\n+    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.1\n     with:\n       scripts-ref: v0.14.0",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430061,Lightning-AI/pytorch-lightning,2396462240,1998544294,Borda,6035284,2025-03-17T11:35:08+00:00,2025-03-17T11:35:09+00:00,```suggestion\r\n      scripts-ref: v0.14.1\r\n```,true,.github/workflows/call-clear-cache.yml,NaN,"@@ -32,7 +32,7 @@ jobs:\n \n   direct-clear:\n     if: github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request'\n-    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.0\n+    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.1\n     with:\n       scripts-ref: v0.14.0",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430062,Lightning-AI/pytorch-lightning,2380997165,1987200299,Borda,6035284,2025-03-10T12:35:14+00:00,2025-03-10T12:35:14+00:00,```suggestion\r\n      scripts-ref: v0.14.0\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430063,Lightning-AI/pytorch-lightning,2380997165,1987200432,Borda,6035284,2025-03-10T12:35:20+00:00,2025-03-10T12:35:20+00:00,```suggestion\r\n      scripts-ref: v0.14.0\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430064,Lightning-AI/pytorch-lightning,2380997165,1987200299,Borda,6035284,2025-03-10T12:35:14+00:00,2025-03-10T12:35:14+00:00,```suggestion\r\n      scripts-ref: v0.14.0\r\n```,true,.github/workflows/call-clear-cache.yml,NaN,"@@ -23,7 +23,7 @@ on:\n jobs:\n   cron-clear:\n     if: github.event_name == 'schedule' || github.event_name == 'pull_request'\n-    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.12.0\n+    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.0\n     with:\n       scripts-ref: v0.11.8",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430065,Lightning-AI/pytorch-lightning,2380997165,1987200432,Borda,6035284,2025-03-10T12:35:20+00:00,2025-03-10T12:35:20+00:00,```suggestion\r\n      scripts-ref: v0.14.0\r\n```,true,.github/workflows/call-clear-cache.yml,NaN,"@@ -32,7 +32,7 @@ jobs:\n \n   direct-clear:\n     if: github.event_name == 'workflow_dispatch' || github.event_name == 'pull_request'\n-    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.12.0\n+    uses: Lightning-AI/utilities/.github/workflows/cleanup-caches.yml@v0.14.0\n     with:\n       scripts-ref: v0.11.8",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430066,Lightning-AI/pytorch-lightning,2318655221,1946174315,Borda,6035284,2025-02-07T08:52:51+00:00,2025-02-07T08:52:52+00:00,"this seems like just a filter, but the original code validates that no warning was raised...",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430067,Lightning-AI/pytorch-lightning,2318655221,1946918672,haifeng-jin,5476582,2025-02-07T17:39:18+00:00,2025-02-07T17:39:18+00:00,"Hi Borda,\r\n\r\nAccording to the [doc](https://docs.python.org/3/library/warnings.html#warning-filter), the `simplefilter(""error"")`  would turn any warning into an error.\r\n\r\nSo, when a warning occurs, the test would error out.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430068,Lightning-AI/pytorch-lightning,2318655221,1959350188,lantiga,191033,2025-02-18T09:16:23+00:00,2025-02-18T09:16:28+00:00,"looks good to me, good use of `catch_warnings` to reset behavior back to default outside the context manager",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430069,Lightning-AI/pytorch-lightning,2318655221,1946174315,Borda,6035284,2025-02-07T08:52:51+00:00,2025-02-07T08:52:52+00:00,"this seems like just a filter, but the original code validates that no warning was raised...",true,tests/tests_fabric/utilities/test_seed.py,13,"@@ -30,9 +31,9 @@ def test_seed_stays_same_with_multiple_seed_everything_calls():\n         seed_everything()\n     initial_seed = os.environ.get(""PL_GLOBAL_SEED"")\n \n-    with pytest.warns(None) as record:\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(""error"")",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843006a,Lightning-AI/pytorch-lightning,2318655221,1946918672,haifeng-jin,5476582,2025-02-07T17:39:18+00:00,2025-02-07T17:39:18+00:00,"Hi Borda,\r\n\r\nAccording to the [doc](https://docs.python.org/3/library/warnings.html#warning-filter), the `simplefilter(""error"")`  would turn any warning into an error.\r\n\r\nSo, when a warning occurs, the test would error out.",true,tests/tests_fabric/utilities/test_seed.py,13,"@@ -30,9 +31,9 @@ def test_seed_stays_same_with_multiple_seed_everything_calls():\n         seed_everything()\n     initial_seed = os.environ.get(""PL_GLOBAL_SEED"")\n \n-    with pytest.warns(None) as record:\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(""error"")",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843006b,Lightning-AI/pytorch-lightning,2318655221,1959350188,lantiga,191033,2025-02-18T09:16:23+00:00,2025-02-18T09:16:28+00:00,"looks good to me, good use of `catch_warnings` to reset behavior back to default outside the context manager",true,tests/tests_fabric/utilities/test_seed.py,13,"@@ -30,9 +31,9 @@ def test_seed_stays_same_with_multiple_seed_everything_calls():\n         seed_everything()\n     initial_seed = os.environ.get(""PL_GLOBAL_SEED"")\n \n-    with pytest.warns(None) as record:\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(""error"")",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843006c,Lightning-AI/pytorch-lightning,2276616729,1940243651,lantiga,191033,2025-02-03T23:01:16+00:00,2025-02-03T23:18:15+00:00,"An alternative (or an addition) could be two methods (`self.update_on_step` and `self.update_on_epoch`)  that the user needs to override. This would allow a user to get a hold of the module and the trainer, and condition saving on richer information like the value of the loss.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843006d,Lightning-AI/pytorch-lightning,2276616729,1940247047,lantiga,191033,2025-02-03T23:05:54+00:00,2025-02-03T23:18:15+00:00,"```suggestion\r\n        checkpoint[""averaged_state""] = {\r\n```\r\nI get that it might be still ""averaging"" : ), but it's in fact ""averaged"" up to the current iterations. We can called it ""average"" model if it sounds better.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843006e,Lightning-AI/pytorch-lightning,2276616729,1940251403,lantiga,191033,2025-02-03T23:12:10+00:00,2025-02-03T23:18:15+00:00,"This is hard to understand for a user if they don't know the details of the callback.\r\n\r\n```suggestion\r\n            raise Exception(""Trying to load a checkpoint using the WeightAveraging callback outside the `fit` stage. The WeightAveraging callback can only be used in the `fit` stage."")\r\n```\r\n\r\nI'm wondering: instead of raising we could just load the average model e.g. for predict. This will avoid forcing users to remove the callback from the Trainer.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843006f,Lightning-AI/pytorch-lightning,2276616729,1940253736,lantiga,191033,2025-02-03T23:15:37+00:00,2025-02-03T23:18:15+00:00,"This tests that we can crash and resume, but afaict it doesn't test whether the resulting averaging is equivalent. We can harden this in a subsequent PR, but it is important to know for sure that averaging works if I stop training and resume while averages are being taken, irrespective of where I stop and resume in the lifecycle.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430070,Lightning-AI/pytorch-lightning,2276616729,1940863045,senarvi,2337787,2025-02-04T10:01:14+00:00,2025-02-04T10:01:14+00:00,"@lantiga the name is a bit confusing, but it means the state of the averaging process, not the average model. This includes the state variables of the `AveragedModel` class, excluding the `module` (i.e. `n_averaged`). The average model is saved in `state_dict`, so whatever we'll do with the checkpoint, we'll use the average model. The current model state is saved in `current_model_state`, so that we can continue training with the WeightAveraging callback from the previous state. If you have a less confusing name for the ""averaging state variables, excluding the averaged model parameters"", I can change it.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430071,Lightning-AI/pytorch-lightning,2276616729,1941021555,senarvi,2337787,2025-02-04T11:48:21+00:00,2025-02-04T12:07:02+00:00,"@lantiga I guess I just wasn't sure in which situation this callback would be called outside fit, but yes, if the user calls `Trainer.validate/test/predict(ckpt_path=...)`, I believe this will be called and the best thing to do would be to load the average model. The average model will be loaded if we don't do anything. Maybe just display a warning in that case.\r\n\r\nI guess `on_save_checkpoint()` can also be called outside fit - if the user calls `Trainer.save_checkpoint()` after training. In that case we also don't have to do anything.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430072,Lightning-AI/pytorch-lightning,2276616729,1941350417,senarvi,2337787,2025-02-04T15:02:14+00:00,2025-02-04T15:02:15+00:00,"@lantiga That's true. Overriding some method seems to be a common way of customizing callbacks in Lightning.\r\n\r\nI changed it like this. However, there's just one method for the user to override, `should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None)`. (Otherwise, if the user would override `update_on_epoch()` and not override `update_on_step()`, the callback would still update on every step.)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430073,Lightning-AI/pytorch-lightning,2276616729,1941352829,senarvi,2337787,2025-02-04T15:03:38+00:00,2025-02-04T15:03:38+00:00,@lantiga This is what I did. Please check if you think the messages are clear now.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430074,Lightning-AI/pytorch-lightning,2276616729,1941362705,senarvi,2337787,2025-02-04T15:09:24+00:00,2025-02-04T15:09:24+00:00,I'll try to still improve the test.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430075,Lightning-AI/pytorch-lightning,2276616729,1947854755,senarvi,2337787,2025-02-08T15:54:58+00:00,2025-02-08T15:54:58+00:00,"@lantiga now I test that after stopping and resuming we get the same final model. The parameters are not identical - I have to use `atol=0.001` - but they are close enough so that I think that the difference comes from some random change instead of a bug in restoring the checkpoint. I don't know what could cause the difference, though. I pass `deterministic=True` to Trainer. I'm curious if you have some ideas, or if you think that that's close enough.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430076,Lightning-AI/pytorch-lightning,2276616729,1940243651,lantiga,191033,2025-02-03T23:01:16+00:00,2025-02-03T23:18:15+00:00,"An alternative (or an addition) could be two methods (`self.update_on_step` and `self.update_on_epoch`)  that the user needs to override. This would allow a user to get a hold of the module and the trainer, and condition saving on richer information like the value of the loss.",true,src/lightning/pytorch/callbacks/weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+r""""""\n+Weight Averaging Callback\n+^^^^^^^^^^^^^^^^^^^^^^^^^\n+""""""\n+\n+import itertools\n+from copy import deepcopy\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch import Tensor\n+from torch.optim.swa_utils import AveragedModel\n+\n+import lightning.pytorch as pl\n+from lightning.pytorch.callbacks.callback import Callback\n+from lightning.pytorch.utilities.rank_zero import rank_zero_info, rank_zero_warn\n+from lightning.pytorch.utilities.types import STEP_OUTPUT\n+\n+\n+def _return_true(x: int) -> bool:\n+    return True\n+\n+\n+def _return_false(x: int) -> bool:\n+    return False\n+\n+\n+class WeightAveraging(Callback):\n+    r""""""A callback that updates an averaged model for Stochastic Weight Averaging (SWA) or Exponential Moving Average\n+    (EMA) after each training step.\n+\n+    The user should provide either `update_on_step` or `update_on_epoch`, a function that determines when the average\n+    model should be updated. If neither function is provided, the average model will be updated after every optimizer\n+    step.\n+\n+    During validation and after the training finishes, the current model parameters will be replaced with the averaged\n+    values.\n+\n+    Args:\n+        device: If provided, the :class:`AveragedModel` will be stored on the ``device``. If ``None`` the device will be\n+            inferred from the original model.\n+        avg_fn: The averaging function used to update the parameters. The function must take in an\n+            :class:`AveragedModel` parameter, a current model parameter, and the number of models already averaged. If\n+            ``None``, an equally weighted average will be used.\n+        update_on_step: A function that takes the number of optimizer steps taken, and returns ``True`` if the average",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430077,Lightning-AI/pytorch-lightning,2276616729,1940247047,lantiga,191033,2025-02-03T23:05:54+00:00,2025-02-03T23:18:15+00:00,"```suggestion\r\n        checkpoint[""averaged_state""] = {\r\n```\r\nI get that it might be still ""averaging"" : ), but it's in fact ""averaged"" up to the current iterations. We can called it ""average"" model if it sounds better.",true,src/lightning/pytorch/callbacks/weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+r""""""\n+Weight Averaging Callback\n+^^^^^^^^^^^^^^^^^^^^^^^^^\n+""""""\n+\n+import itertools\n+from copy import deepcopy\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch import Tensor\n+from torch.optim.swa_utils import AveragedModel\n+\n+import lightning.pytorch as pl\n+from lightning.pytorch.callbacks.callback import Callback\n+from lightning.pytorch.utilities.rank_zero import rank_zero_info, rank_zero_warn\n+from lightning.pytorch.utilities.types import STEP_OUTPUT\n+\n+\n+def _return_true(x: int) -> bool:\n+    return True\n+\n+\n+def _return_false(x: int) -> bool:\n+    return False\n+\n+\n+class WeightAveraging(Callback):\n+    r""""""A callback that updates an averaged model for Stochastic Weight Averaging (SWA) or Exponential Moving Average\n+    (EMA) after each training step.\n+\n+    The user should provide either `update_on_step` or `update_on_epoch`, a function that determines when the average\n+    model should be updated. If neither function is provided, the average model will be updated after every optimizer\n+    step.\n+\n+    During validation and after the training finishes, the current model parameters will be replaced with the averaged\n+    values.\n+\n+    Args:\n+        device: If provided, the :class:`AveragedModel` will be stored on the ``device``. If ``None`` the device will be\n+            inferred from the original model.\n+        avg_fn: The averaging function used to update the parameters. The function must take in an\n+            :class:`AveragedModel` parameter, a current model parameter, and the number of models already averaged. If\n+            ``None``, an equally weighted average will be used.\n+        update_on_step: A function that takes the number of optimizer steps taken, and returns ``True`` if the average\n+            model should be updated.\n+        update_on_epoch: A function that takes the zero-based epoch number, and returns ``True`` if the average model\n+            should be updated.\n+\n+    """"""\n+\n+    def __init__(\n+        self,\n+        device: Optional[Union[torch.device, int]] = torch.device(""cpu""),\n+        avg_fn: Optional[Callable[[Tensor, Tensor, Union[Tensor, int]], Tensor]] = None,\n+        update_on_step: Optional[Callable[[int], bool]] = None,\n+        update_on_epoch: Optional[Callable[[int], bool]] = None,\n+    ):\n+        self._device = device\n+        self._avg_fn = avg_fn\n+\n+        if (update_on_step is None) and (update_on_epoch is None):\n+            self._update_on_step: Callable[[int], bool] = _return_true\n+            self._update_on_epoch: Callable[[int], bool] = _return_false\n+        else:\n+            self._update_on_step = _return_false if update_on_step is None else update_on_step\n+            self._update_on_epoch = _return_false if update_on_epoch is None else update_on_epoch\n+\n+        self._average_model: Optional[AveragedModel] = None\n+\n+        # Number of optimizer steps taken, when the average model was last updated. Initializing this with zero ensures\n+        # that the average model will be first updated after the first optimizer step, which takes place after N batches\n+        # when using accumulate_grad_batches=N.\n+        self._latest_update_step = 0\n+        # The epoch after which the average model was last updated. The first epoch is 0, so initializing this to a\n+        # negative value means that if update_on_step(0) returns True, the first update is after the first epoch.\n+        self._latest_update_epoch = -1\n+\n+    def setup(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", stage: str) -> None:\n+        """"""Called when fit, validate, test, predict, or tune begins.\n+\n+        Creates an :class:`AveragedModel` when fit begins.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\n+\n+        """"""\n+        if stage == ""fit"":\n+            device = self._device or pl_module.device\n+            self._average_model = AveragedModel(model=pl_module, device=device, avg_fn=self._avg_fn, use_buffers=True)\n+\n+    def on_train_batch_end(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n+    ) -> None:\n+        """"""Called when a training batch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_step()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            outputs: Outputs from the training batch.\n+            batch: The training batch.\n+            batch_idx: Index of the training batch.\n+\n+        """"""\n+        if self._update_on_step(trainer.global_step) and (trainer.global_step > self._latest_update_step):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_step = trainer.global_step\n+\n+    def on_train_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a training epoch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_epoch()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._update_on_epoch(trainer.current_epoch) and (trainer.current_epoch > self._latest_update_epoch):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_epoch = trainer.current_epoch\n+\n+    def on_train_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when training ends.\n+\n+        Transfers parameters from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        assert self._average_model is not None\n+        self._copy_average_to_current(pl_module)\n+\n+    def on_validation_epoch_start(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch begins.\n+\n+        Transfers parameter values from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Loading the average model parameters for validation."")\n+            self._swap_models(pl_module)\n+\n+    def on_validation_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch ends.\n+\n+        Recovers the current model parameters from the :class:`AveragedModel`.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Recovering the current model parameters after validation."")\n+            self._swap_models(pl_module)\n+\n+    def state_dict(self) -> dict[str, Any]:\n+        """"""Called when saving a checkpoint.\n+\n+        Creates a ``state_dict`` of the callback state.\n+\n+        Returns:\n+            A dictionary containing the callback state.\n+\n+        """"""\n+        return {""latest_update_step"": self._latest_update_step}\n+\n+    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n+        """"""Called when loading a checkpoint.\n+\n+        Reloads the callback state given a ``state_dict``.\n+\n+        Args:\n+            state_dict: A dictionary containing the callback state.\n+\n+        """"""\n+        self._latest_update_step = state_dict[""latest_update_step""]\n+\n+    def on_save_checkpoint(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", checkpoint: dict[str, Any]\n+    ) -> None:\n+        r""""""Called when saving a checkpoint.\n+\n+        Moves the current model state to the key ``current_model_state``, and places the average model state in\n+        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\n+        ``averaging_state``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            checkpoint: The checkpoint dictionary that will be saved.\n+\n+        """"""\n+        if self._average_model is None:\n+            raise Exception(""Trying to save a checkpoint, but no average model (outside fit). Don't know what to do."")\n+\n+        rank_zero_info(""The average model parameters will be saved to the state_dict in the checkpoint."")\n+        average_model_state = self._average_model.state_dict()\n+        checkpoint[""current_model_state""] = checkpoint[""state_dict""]\n+        checkpoint[""state_dict""] = {\n+            name[7:]: value for name, value in average_model_state.items() if name.startswith(""module."")\n+        }\n+        checkpoint[""averaging_state""] = {",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430078,Lightning-AI/pytorch-lightning,2276616729,1940251403,lantiga,191033,2025-02-03T23:12:10+00:00,2025-02-03T23:18:15+00:00,"This is hard to understand for a user if they don't know the details of the callback.\r\n\r\n```suggestion\r\n            raise Exception(""Trying to load a checkpoint using the WeightAveraging callback outside the `fit` stage. The WeightAveraging callback can only be used in the `fit` stage."")\r\n```\r\n\r\nI'm wondering: instead of raising we could just load the average model e.g. for predict. This will avoid forcing users to remove the callback from the Trainer.",true,src/lightning/pytorch/callbacks/weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+r""""""\n+Weight Averaging Callback\n+^^^^^^^^^^^^^^^^^^^^^^^^^\n+""""""\n+\n+import itertools\n+from copy import deepcopy\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch import Tensor\n+from torch.optim.swa_utils import AveragedModel\n+\n+import lightning.pytorch as pl\n+from lightning.pytorch.callbacks.callback import Callback\n+from lightning.pytorch.utilities.rank_zero import rank_zero_info, rank_zero_warn\n+from lightning.pytorch.utilities.types import STEP_OUTPUT\n+\n+\n+def _return_true(x: int) -> bool:\n+    return True\n+\n+\n+def _return_false(x: int) -> bool:\n+    return False\n+\n+\n+class WeightAveraging(Callback):\n+    r""""""A callback that updates an averaged model for Stochastic Weight Averaging (SWA) or Exponential Moving Average\n+    (EMA) after each training step.\n+\n+    The user should provide either `update_on_step` or `update_on_epoch`, a function that determines when the average\n+    model should be updated. If neither function is provided, the average model will be updated after every optimizer\n+    step.\n+\n+    During validation and after the training finishes, the current model parameters will be replaced with the averaged\n+    values.\n+\n+    Args:\n+        device: If provided, the :class:`AveragedModel` will be stored on the ``device``. If ``None`` the device will be\n+            inferred from the original model.\n+        avg_fn: The averaging function used to update the parameters. The function must take in an\n+            :class:`AveragedModel` parameter, a current model parameter, and the number of models already averaged. If\n+            ``None``, an equally weighted average will be used.\n+        update_on_step: A function that takes the number of optimizer steps taken, and returns ``True`` if the average\n+            model should be updated.\n+        update_on_epoch: A function that takes the zero-based epoch number, and returns ``True`` if the average model\n+            should be updated.\n+\n+    """"""\n+\n+    def __init__(\n+        self,\n+        device: Optional[Union[torch.device, int]] = torch.device(""cpu""),\n+        avg_fn: Optional[Callable[[Tensor, Tensor, Union[Tensor, int]], Tensor]] = None,\n+        update_on_step: Optional[Callable[[int], bool]] = None,\n+        update_on_epoch: Optional[Callable[[int], bool]] = None,\n+    ):\n+        self._device = device\n+        self._avg_fn = avg_fn\n+\n+        if (update_on_step is None) and (update_on_epoch is None):\n+            self._update_on_step: Callable[[int], bool] = _return_true\n+            self._update_on_epoch: Callable[[int], bool] = _return_false\n+        else:\n+            self._update_on_step = _return_false if update_on_step is None else update_on_step\n+            self._update_on_epoch = _return_false if update_on_epoch is None else update_on_epoch\n+\n+        self._average_model: Optional[AveragedModel] = None\n+\n+        # Number of optimizer steps taken, when the average model was last updated. Initializing this with zero ensures\n+        # that the average model will be first updated after the first optimizer step, which takes place after N batches\n+        # when using accumulate_grad_batches=N.\n+        self._latest_update_step = 0\n+        # The epoch after which the average model was last updated. The first epoch is 0, so initializing this to a\n+        # negative value means that if update_on_step(0) returns True, the first update is after the first epoch.\n+        self._latest_update_epoch = -1\n+\n+    def setup(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", stage: str) -> None:\n+        """"""Called when fit, validate, test, predict, or tune begins.\n+\n+        Creates an :class:`AveragedModel` when fit begins.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\n+\n+        """"""\n+        if stage == ""fit"":\n+            device = self._device or pl_module.device\n+            self._average_model = AveragedModel(model=pl_module, device=device, avg_fn=self._avg_fn, use_buffers=True)\n+\n+    def on_train_batch_end(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n+    ) -> None:\n+        """"""Called when a training batch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_step()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            outputs: Outputs from the training batch.\n+            batch: The training batch.\n+            batch_idx: Index of the training batch.\n+\n+        """"""\n+        if self._update_on_step(trainer.global_step) and (trainer.global_step > self._latest_update_step):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_step = trainer.global_step\n+\n+    def on_train_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a training epoch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_epoch()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._update_on_epoch(trainer.current_epoch) and (trainer.current_epoch > self._latest_update_epoch):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_epoch = trainer.current_epoch\n+\n+    def on_train_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when training ends.\n+\n+        Transfers parameters from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        assert self._average_model is not None\n+        self._copy_average_to_current(pl_module)\n+\n+    def on_validation_epoch_start(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch begins.\n+\n+        Transfers parameter values from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Loading the average model parameters for validation."")\n+            self._swap_models(pl_module)\n+\n+    def on_validation_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch ends.\n+\n+        Recovers the current model parameters from the :class:`AveragedModel`.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Recovering the current model parameters after validation."")\n+            self._swap_models(pl_module)\n+\n+    def state_dict(self) -> dict[str, Any]:\n+        """"""Called when saving a checkpoint.\n+\n+        Creates a ``state_dict`` of the callback state.\n+\n+        Returns:\n+            A dictionary containing the callback state.\n+\n+        """"""\n+        return {""latest_update_step"": self._latest_update_step}\n+\n+    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n+        """"""Called when loading a checkpoint.\n+\n+        Reloads the callback state given a ``state_dict``.\n+\n+        Args:\n+            state_dict: A dictionary containing the callback state.\n+\n+        """"""\n+        self._latest_update_step = state_dict[""latest_update_step""]\n+\n+    def on_save_checkpoint(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", checkpoint: dict[str, Any]\n+    ) -> None:\n+        r""""""Called when saving a checkpoint.\n+\n+        Moves the current model state to the key ``current_model_state``, and places the average model state in\n+        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\n+        ``averaging_state``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            checkpoint: The checkpoint dictionary that will be saved.\n+\n+        """"""\n+        if self._average_model is None:\n+            raise Exception(""Trying to save a checkpoint, but no average model (outside fit). Don't know what to do."")\n+\n+        rank_zero_info(""The average model parameters will be saved to the state_dict in the checkpoint."")\n+        average_model_state = self._average_model.state_dict()\n+        checkpoint[""current_model_state""] = checkpoint[""state_dict""]\n+        checkpoint[""state_dict""] = {\n+            name[7:]: value for name, value in average_model_state.items() if name.startswith(""module."")\n+        }\n+        checkpoint[""averaging_state""] = {\n+            name: value for name, value in average_model_state.items() if not name.startswith(""module."")\n+        }\n+\n+    def on_load_checkpoint(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", checkpoint: dict[str, Any]\n+    ) -> None:\n+        r""""""Called when loading a model checkpoint.\n+\n+        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\n+\n+        """"""\n+        if self._average_model is None:\n+            raise Exception(""Trying to load a checkpoint, but no average model (outside fit). Don't know what to do."")",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430079,Lightning-AI/pytorch-lightning,2276616729,1940253736,lantiga,191033,2025-02-03T23:15:37+00:00,2025-02-03T23:18:15+00:00,"This tests that we can crash and resume, but afaict it doesn't test whether the resulting averaging is equivalent. We can harden this in a subsequent PR, but it is important to know for sure that averaging works if I stop training and resume while averages are being taken, irrespective of where I stop and resume in the lifecycle.",true,tests/tests_pytorch/callbacks/test_weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import os\n+from pathlib import Path\n+from typing import Any, Optional\n+\n+import pytest\n+import torch\n+from torch import Tensor, nn\n+from torch.optim.swa_utils import get_swa_avg_fn\n+from torch.utils.data import DataLoader\n+\n+from lightning.pytorch import LightningModule, Trainer\n+from lightning.pytorch.callbacks import WeightAveraging\n+from lightning.pytorch.demos.boring_classes import BoringModel, RandomDataset, RandomIterableDataset\n+from tests_pytorch.helpers.runif import RunIf\n+\n+\n+class WeightAveragingTestModel(BoringModel):\n+    def __init__(\n+        self, batch_norm: bool = True, iterable_dataset: bool = False, crash_on_epoch: Optional[int] = None\n+    ) -> None:\n+        super().__init__()\n+        layers = [nn.Linear(32, 32)]\n+        if batch_norm:\n+            layers.append(nn.BatchNorm1d(32))\n+        layers += [nn.ReLU(), nn.Linear(32, 2)]\n+        self.layer = nn.Sequential(*layers)\n+        self.iterable_dataset = iterable_dataset\n+        self.crash_on_epoch = crash_on_epoch\n+\n+    def training_step(self, batch: Tensor, batch_idx: int) -> None:\n+        if self.crash_on_epoch and self.trainer.current_epoch >= self.crash_on_epoch:\n+            raise Exception(""CRASH TEST"")\n+        return super().training_step(batch, batch_idx)\n+\n+    def train_dataloader(self) -> None:\n+        dataset_class = RandomIterableDataset if self.iterable_dataset else RandomDataset\n+        return DataLoader(dataset_class(32, 32), batch_size=4)\n+\n+    def configure_optimizers(self) -> None:\n+        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\n+\n+\n+class EMAAveragingFunction:\n+    """"""EMA averaging function.\n+\n+    Functionally equivalent to the closure that ``get_ema_avg_fn()`` would return. This class is needed because we\n+    cannot use a closure with ddp_spawn. (``Popen(process_obj)`` would fail with\n+    ``Can't get local object 'get_ema_avg_fn.<locals>.ema_update'``).\n+\n+    """"""\n+\n+    def __init__(self, decay: float = 0.999) -> None:\n+        self.decay = decay\n+\n+    @torch.no_grad()\n+    def __call__(self, ema_param: Tensor, current_param: Tensor, num_averaged: Tensor) -> Tensor:\n+        return self.decay * ema_param + (1 - self.decay) * current_param\n+\n+\n+class EMATestCallback(WeightAveraging):\n+    def __init__(self, devices: int = 1, **kwargs: Any) -> None:\n+        super().__init__(avg_fn=EMAAveragingFunction(), **kwargs)\n+        self.devices = devices\n+        self.swap_calls = 0\n+        self.copy_calls = 0\n+        # Record the first epoch, as if we are resuming from a checkpoint this may not be equal to 0.\n+        self.first_epoch: Optional[int] = None\n+\n+    def _swap_models(self, *args: Any, **kwargs: Any):\n+        self.swap_calls += 1\n+        return super()._swap_models(*args, **kwargs)\n+\n+    def _copy_average_to_current(self, *args: Any, **kwargs: Any):\n+        self.copy_calls += 1\n+        return super()._copy_average_to_current(*args, **kwargs)\n+\n+    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_start(trainer, pl_module)\n+        assert self.swap_calls == 0\n+        assert self.copy_calls == 0\n+\n+    def on_train_epoch_start(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_start(trainer, *args)\n+        # Since the checkpoint loaded was saved `on_train_epoch_end`, the first `FitLoop` iteration will not update the\n+        # model and will just call the epoch-level hooks. For that reason, we check that we are not restarting before\n+        # choosing the first epoch.\n+        if self.first_epoch is None and not trainer.fit_loop.restarting:\n+            self.first_epoch = trainer.current_epoch\n+\n+    def on_train_epoch_end(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_end(trainer, *args)\n+        assert self._average_model.n_averaged == trainer.global_step\n+        assert self.swap_calls == (trainer.current_epoch + 1 - self.first_epoch) * 2\n+        assert self.copy_calls == 0\n+\n+    def on_train_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_end(trainer, pl_module)\n+        # length=32, batch_size=4, accumulate_grad_batches=2\n+        # => Using one process we have 4 optimizer steps per epoch.\n+        # => Using two processes we have 2 optimizer steps per epoch.\n+        steps_per_epoch = 4 // self.devices\n+        assert self._average_model.n_averaged == trainer.max_epochs * steps_per_epoch\n+        assert self.swap_calls == (trainer.max_epochs - self.first_epoch) * 2\n+        assert self.copy_calls == 1\n+\n+\n+class SWATestCallback(WeightAveraging):\n+    def __init__(self, **kwargs: Any) -> None:\n+        avg_fn = get_swa_avg_fn()\n+        update_on_epoch = lambda x: x in (3, 5, 7)\n+        super().__init__(avg_fn=avg_fn, update_on_epoch=update_on_epoch, **kwargs)\n+\n+        self.swap_calls = 0\n+        self.copy_calls = 0\n+        # Record the first epoch, as if we are resuming from a checkpoint this may not be equal to 0.\n+        self.first_epoch: Optional[int] = None\n+\n+    def _swap_models(self, *args: Any, **kwargs: Any):\n+        self.swap_calls += 1\n+        return super()._swap_models(*args, **kwargs)\n+\n+    def _copy_average_to_current(self, *args: Any, **kwargs: Any):\n+        self.copy_calls += 1\n+        return super()._copy_average_to_current(*args, **kwargs)\n+\n+    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_start(trainer, pl_module)\n+        assert self.swap_calls == 0\n+        assert self.copy_calls == 0\n+\n+    def on_train_epoch_start(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_start(trainer, *args)\n+        # Since the checkpoint loaded was saved `on_train_epoch_end`, the first `FitLoop` iteration will not update the\n+        # model and will just call the epoch-level hooks. For that reason, we check that we are not restarting before\n+        # choosing the first epoch.\n+        if self.first_epoch is None and not trainer.fit_loop.restarting:\n+            self.first_epoch = trainer.current_epoch\n+\n+    def on_train_epoch_end(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_end(trainer, *args)\n+        if trainer.current_epoch < 3:\n+            assert self._average_model.n_averaged == 0\n+        elif trainer.current_epoch < 5:\n+            assert self._average_model.n_averaged == 1\n+        elif trainer.current_epoch < 7:\n+            assert self._average_model.n_averaged == 2\n+        else:\n+            assert self._average_model.n_averaged == 3\n+        assert self.swap_calls == (trainer.current_epoch + 1 - self.first_epoch) * 2\n+        assert self.copy_calls == 0\n+\n+    def on_train_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_end(trainer, pl_module)\n+        assert self._average_model.n_averaged == 3\n+        assert self.swap_calls == (trainer.max_epochs - self.first_epoch) * 2\n+        assert self.copy_calls == 1\n+\n+\n+def test_weight_averaging_deepcopy(tmp_path):\n+    """"""Ensure that WeightAveraging callback doesn't deepcopy the data loaders or the data module and consume memory\n+    more than necessary.""""""\n+\n+    class TestCallback(WeightAveraging):\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            self.setup_called = False\n+\n+        def setup(self, trainer, pl_module, stage) -> None:\n+            super().setup(trainer, pl_module, stage)\n+            assert self._average_model.module.train_dataloader is not pl_module.train_dataloader\n+            assert self._average_model.module.train_dataloader.__self__ == self._average_model.module\n+            assert self._average_model.module._trainer is None\n+            self.setup_called = True\n+\n+    callback = TestCallback()\n+    trainer = Trainer(default_root_dir=tmp_path, callbacks=callback, fast_dev_run=True)\n+    trainer.fit(BoringModel(), train_dataloaders=DataLoader(RandomDataset(32, 2)))\n+    assert callback.setup_called\n+\n+\n+@pytest.mark.parametrize(""batch_norm"", [True, False])\n+@pytest.mark.parametrize(""iterable_dataset"", [True, False])\n+def test_ema(tmp_path, batch_norm: bool, iterable_dataset: bool):\n+    _train(tmp_path, EMATestCallback(), batch_norm=batch_norm, iterable_dataset=iterable_dataset)\n+\n+\n+@pytest.mark.parametrize(\n+    ""accelerator"", [pytest.param(""gpu"", marks=RunIf(min_cuda_gpus=1)), pytest.param(""mps"", marks=RunIf(mps=True))]\n+)\n+def test_ema_accelerator(tmp_path, accelerator):\n+    _train(tmp_path, EMATestCallback(), accelerator=accelerator, devices=1)\n+\n+\n+@RunIf(min_cuda_gpus=2, standalone=True)\n+def test_ema_ddp(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp"", accelerator=""gpu"", devices=2)\n+\n+\n+@RunIf(min_cuda_gpus=2)\n+def test_ema_ddp_spawn(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp_spawn"", accelerator=""gpu"", devices=2)\n+\n+\n+@RunIf(skip_windows=True)\n+def test_ema_ddp_spawn_cpu(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp_spawn"", accelerator=""cpu"", devices=2)\n+\n+\n+@pytest.mark.parametrize(""crash_on_epoch"", [1, 3])\n+def test_ema_resume(tmp_path, crash_on_epoch):\n+    _train_and_resume(tmp_path, crash_on_epoch=crash_on_epoch)\n+\n+\n+@RunIf(skip_windows=True)\n+def test_ema_resume_ddp(tmp_path):\n+    _train_and_resume(tmp_path, crash_on_epoch=3, use_ddp=True)\n+\n+\n+def test_swa(tmp_path):\n+    _train(tmp_path, SWATestCallback())\n+\n+\n+def _train(\n+    tmp_path: str,\n+    callback: WeightAveraging,\n+    batch_norm: bool = True,\n+    strategy: str = ""auto"",\n+    accelerator: str = ""cpu"",\n+    devices: int = 1,\n+    iterable_dataset: bool = False,\n+    checkpoint_path: Optional[str] = None,\n+    crash_on_epoch: Optional[int] = None,\n+) -> None:\n+    trainer = Trainer(\n+        default_root_dir=tmp_path,\n+        enable_progress_bar=False,\n+        enable_model_summary=False,\n+        logger=False,\n+        max_epochs=8,\n+        num_sanity_val_steps=0,\n+        callbacks=callback,\n+        accumulate_grad_batches=2,\n+        strategy=strategy,\n+        accelerator=accelerator,\n+        devices=devices,\n+    )\n+    model = WeightAveragingTestModel(\n+        batch_norm=batch_norm, iterable_dataset=iterable_dataset, crash_on_epoch=crash_on_epoch\n+    )\n+\n+    if crash_on_epoch is None:\n+        trainer.fit(model, ckpt_path=checkpoint_path)\n+    else:\n+        with pytest.raises(Exception, match=""CRASH TEST""):\n+            trainer.fit(model, ckpt_path=checkpoint_path)\n+\n+    assert trainer.lightning_module == model\n+\n+\n+def _train_and_resume(tmp_path: str, crash_on_epoch: int, use_ddp: bool = False) -> None:",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843007a,Lightning-AI/pytorch-lightning,2276616729,1940863045,senarvi,2337787,2025-02-04T10:01:14+00:00,2025-02-04T10:01:14+00:00,"@lantiga the name is a bit confusing, but it means the state of the averaging process, not the average model. This includes the state variables of the `AveragedModel` class, excluding the `module` (i.e. `n_averaged`). The average model is saved in `state_dict`, so whatever we'll do with the checkpoint, we'll use the average model. The current model state is saved in `current_model_state`, so that we can continue training with the WeightAveraging callback from the previous state. If you have a less confusing name for the ""averaging state variables, excluding the averaged model parameters"", I can change it.",true,src/lightning/pytorch/callbacks/weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+r""""""\n+Weight Averaging Callback\n+^^^^^^^^^^^^^^^^^^^^^^^^^\n+""""""\n+\n+import itertools\n+from copy import deepcopy\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch import Tensor\n+from torch.optim.swa_utils import AveragedModel\n+\n+import lightning.pytorch as pl\n+from lightning.pytorch.callbacks.callback import Callback\n+from lightning.pytorch.utilities.rank_zero import rank_zero_info, rank_zero_warn\n+from lightning.pytorch.utilities.types import STEP_OUTPUT\n+\n+\n+def _return_true(x: int) -> bool:\n+    return True\n+\n+\n+def _return_false(x: int) -> bool:\n+    return False\n+\n+\n+class WeightAveraging(Callback):\n+    r""""""A callback that updates an averaged model for Stochastic Weight Averaging (SWA) or Exponential Moving Average\n+    (EMA) after each training step.\n+\n+    The user should provide either `update_on_step` or `update_on_epoch`, a function that determines when the average\n+    model should be updated. If neither function is provided, the average model will be updated after every optimizer\n+    step.\n+\n+    During validation and after the training finishes, the current model parameters will be replaced with the averaged\n+    values.\n+\n+    Args:\n+        device: If provided, the :class:`AveragedModel` will be stored on the ``device``. If ``None`` the device will be\n+            inferred from the original model.\n+        avg_fn: The averaging function used to update the parameters. The function must take in an\n+            :class:`AveragedModel` parameter, a current model parameter, and the number of models already averaged. If\n+            ``None``, an equally weighted average will be used.\n+        update_on_step: A function that takes the number of optimizer steps taken, and returns ``True`` if the average\n+            model should be updated.\n+        update_on_epoch: A function that takes the zero-based epoch number, and returns ``True`` if the average model\n+            should be updated.\n+\n+    """"""\n+\n+    def __init__(\n+        self,\n+        device: Optional[Union[torch.device, int]] = torch.device(""cpu""),\n+        avg_fn: Optional[Callable[[Tensor, Tensor, Union[Tensor, int]], Tensor]] = None,\n+        update_on_step: Optional[Callable[[int], bool]] = None,\n+        update_on_epoch: Optional[Callable[[int], bool]] = None,\n+    ):\n+        self._device = device\n+        self._avg_fn = avg_fn\n+\n+        if (update_on_step is None) and (update_on_epoch is None):\n+            self._update_on_step: Callable[[int], bool] = _return_true\n+            self._update_on_epoch: Callable[[int], bool] = _return_false\n+        else:\n+            self._update_on_step = _return_false if update_on_step is None else update_on_step\n+            self._update_on_epoch = _return_false if update_on_epoch is None else update_on_epoch\n+\n+        self._average_model: Optional[AveragedModel] = None\n+\n+        # Number of optimizer steps taken, when the average model was last updated. Initializing this with zero ensures\n+        # that the average model will be first updated after the first optimizer step, which takes place after N batches\n+        # when using accumulate_grad_batches=N.\n+        self._latest_update_step = 0\n+        # The epoch after which the average model was last updated. The first epoch is 0, so initializing this to a\n+        # negative value means that if update_on_step(0) returns True, the first update is after the first epoch.\n+        self._latest_update_epoch = -1\n+\n+    def setup(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", stage: str) -> None:\n+        """"""Called when fit, validate, test, predict, or tune begins.\n+\n+        Creates an :class:`AveragedModel` when fit begins.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\n+\n+        """"""\n+        if stage == ""fit"":\n+            device = self._device or pl_module.device\n+            self._average_model = AveragedModel(model=pl_module, device=device, avg_fn=self._avg_fn, use_buffers=True)\n+\n+    def on_train_batch_end(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n+    ) -> None:\n+        """"""Called when a training batch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_step()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            outputs: Outputs from the training batch.\n+            batch: The training batch.\n+            batch_idx: Index of the training batch.\n+\n+        """"""\n+        if self._update_on_step(trainer.global_step) and (trainer.global_step > self._latest_update_step):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_step = trainer.global_step\n+\n+    def on_train_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a training epoch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_epoch()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._update_on_epoch(trainer.current_epoch) and (trainer.current_epoch > self._latest_update_epoch):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_epoch = trainer.current_epoch\n+\n+    def on_train_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when training ends.\n+\n+        Transfers parameters from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        assert self._average_model is not None\n+        self._copy_average_to_current(pl_module)\n+\n+    def on_validation_epoch_start(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch begins.\n+\n+        Transfers parameter values from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Loading the average model parameters for validation."")\n+            self._swap_models(pl_module)\n+\n+    def on_validation_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch ends.\n+\n+        Recovers the current model parameters from the :class:`AveragedModel`.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Recovering the current model parameters after validation."")\n+            self._swap_models(pl_module)\n+\n+    def state_dict(self) -> dict[str, Any]:\n+        """"""Called when saving a checkpoint.\n+\n+        Creates a ``state_dict`` of the callback state.\n+\n+        Returns:\n+            A dictionary containing the callback state.\n+\n+        """"""\n+        return {""latest_update_step"": self._latest_update_step}\n+\n+    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n+        """"""Called when loading a checkpoint.\n+\n+        Reloads the callback state given a ``state_dict``.\n+\n+        Args:\n+            state_dict: A dictionary containing the callback state.\n+\n+        """"""\n+        self._latest_update_step = state_dict[""latest_update_step""]\n+\n+    def on_save_checkpoint(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", checkpoint: dict[str, Any]\n+    ) -> None:\n+        r""""""Called when saving a checkpoint.\n+\n+        Moves the current model state to the key ``current_model_state``, and places the average model state in\n+        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\n+        ``averaging_state``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            checkpoint: The checkpoint dictionary that will be saved.\n+\n+        """"""\n+        if self._average_model is None:\n+            raise Exception(""Trying to save a checkpoint, but no average model (outside fit). Don't know what to do."")\n+\n+        rank_zero_info(""The average model parameters will be saved to the state_dict in the checkpoint."")\n+        average_model_state = self._average_model.state_dict()\n+        checkpoint[""current_model_state""] = checkpoint[""state_dict""]\n+        checkpoint[""state_dict""] = {\n+            name[7:]: value for name, value in average_model_state.items() if name.startswith(""module."")\n+        }\n+        checkpoint[""averaging_state""] = {",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843007b,Lightning-AI/pytorch-lightning,2276616729,1941021555,senarvi,2337787,2025-02-04T11:48:21+00:00,2025-02-04T12:07:02+00:00,"@lantiga I guess I just wasn't sure in which situation this callback would be called outside fit, but yes, if the user calls `Trainer.validate/test/predict(ckpt_path=...)`, I believe this will be called and the best thing to do would be to load the average model. The average model will be loaded if we don't do anything. Maybe just display a warning in that case.\r\n\r\nI guess `on_save_checkpoint()` can also be called outside fit - if the user calls `Trainer.save_checkpoint()` after training. In that case we also don't have to do anything.",true,src/lightning/pytorch/callbacks/weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+r""""""\n+Weight Averaging Callback\n+^^^^^^^^^^^^^^^^^^^^^^^^^\n+""""""\n+\n+import itertools\n+from copy import deepcopy\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch import Tensor\n+from torch.optim.swa_utils import AveragedModel\n+\n+import lightning.pytorch as pl\n+from lightning.pytorch.callbacks.callback import Callback\n+from lightning.pytorch.utilities.rank_zero import rank_zero_info, rank_zero_warn\n+from lightning.pytorch.utilities.types import STEP_OUTPUT\n+\n+\n+def _return_true(x: int) -> bool:\n+    return True\n+\n+\n+def _return_false(x: int) -> bool:\n+    return False\n+\n+\n+class WeightAveraging(Callback):\n+    r""""""A callback that updates an averaged model for Stochastic Weight Averaging (SWA) or Exponential Moving Average\n+    (EMA) after each training step.\n+\n+    The user should provide either `update_on_step` or `update_on_epoch`, a function that determines when the average\n+    model should be updated. If neither function is provided, the average model will be updated after every optimizer\n+    step.\n+\n+    During validation and after the training finishes, the current model parameters will be replaced with the averaged\n+    values.\n+\n+    Args:\n+        device: If provided, the :class:`AveragedModel` will be stored on the ``device``. If ``None`` the device will be\n+            inferred from the original model.\n+        avg_fn: The averaging function used to update the parameters. The function must take in an\n+            :class:`AveragedModel` parameter, a current model parameter, and the number of models already averaged. If\n+            ``None``, an equally weighted average will be used.\n+        update_on_step: A function that takes the number of optimizer steps taken, and returns ``True`` if the average\n+            model should be updated.\n+        update_on_epoch: A function that takes the zero-based epoch number, and returns ``True`` if the average model\n+            should be updated.\n+\n+    """"""\n+\n+    def __init__(\n+        self,\n+        device: Optional[Union[torch.device, int]] = torch.device(""cpu""),\n+        avg_fn: Optional[Callable[[Tensor, Tensor, Union[Tensor, int]], Tensor]] = None,\n+        update_on_step: Optional[Callable[[int], bool]] = None,\n+        update_on_epoch: Optional[Callable[[int], bool]] = None,\n+    ):\n+        self._device = device\n+        self._avg_fn = avg_fn\n+\n+        if (update_on_step is None) and (update_on_epoch is None):\n+            self._update_on_step: Callable[[int], bool] = _return_true\n+            self._update_on_epoch: Callable[[int], bool] = _return_false\n+        else:\n+            self._update_on_step = _return_false if update_on_step is None else update_on_step\n+            self._update_on_epoch = _return_false if update_on_epoch is None else update_on_epoch\n+\n+        self._average_model: Optional[AveragedModel] = None\n+\n+        # Number of optimizer steps taken, when the average model was last updated. Initializing this with zero ensures\n+        # that the average model will be first updated after the first optimizer step, which takes place after N batches\n+        # when using accumulate_grad_batches=N.\n+        self._latest_update_step = 0\n+        # The epoch after which the average model was last updated. The first epoch is 0, so initializing this to a\n+        # negative value means that if update_on_step(0) returns True, the first update is after the first epoch.\n+        self._latest_update_epoch = -1\n+\n+    def setup(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", stage: str) -> None:\n+        """"""Called when fit, validate, test, predict, or tune begins.\n+\n+        Creates an :class:`AveragedModel` when fit begins.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\n+\n+        """"""\n+        if stage == ""fit"":\n+            device = self._device or pl_module.device\n+            self._average_model = AveragedModel(model=pl_module, device=device, avg_fn=self._avg_fn, use_buffers=True)\n+\n+    def on_train_batch_end(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n+    ) -> None:\n+        """"""Called when a training batch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_step()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            outputs: Outputs from the training batch.\n+            batch: The training batch.\n+            batch_idx: Index of the training batch.\n+\n+        """"""\n+        if self._update_on_step(trainer.global_step) and (trainer.global_step > self._latest_update_step):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_step = trainer.global_step\n+\n+    def on_train_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a training epoch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_epoch()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._update_on_epoch(trainer.current_epoch) and (trainer.current_epoch > self._latest_update_epoch):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_epoch = trainer.current_epoch\n+\n+    def on_train_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when training ends.\n+\n+        Transfers parameters from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        assert self._average_model is not None\n+        self._copy_average_to_current(pl_module)\n+\n+    def on_validation_epoch_start(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch begins.\n+\n+        Transfers parameter values from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Loading the average model parameters for validation."")\n+            self._swap_models(pl_module)\n+\n+    def on_validation_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch ends.\n+\n+        Recovers the current model parameters from the :class:`AveragedModel`.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Recovering the current model parameters after validation."")\n+            self._swap_models(pl_module)\n+\n+    def state_dict(self) -> dict[str, Any]:\n+        """"""Called when saving a checkpoint.\n+\n+        Creates a ``state_dict`` of the callback state.\n+\n+        Returns:\n+            A dictionary containing the callback state.\n+\n+        """"""\n+        return {""latest_update_step"": self._latest_update_step}\n+\n+    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n+        """"""Called when loading a checkpoint.\n+\n+        Reloads the callback state given a ``state_dict``.\n+\n+        Args:\n+            state_dict: A dictionary containing the callback state.\n+\n+        """"""\n+        self._latest_update_step = state_dict[""latest_update_step""]\n+\n+    def on_save_checkpoint(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", checkpoint: dict[str, Any]\n+    ) -> None:\n+        r""""""Called when saving a checkpoint.\n+\n+        Moves the current model state to the key ``current_model_state``, and places the average model state in\n+        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\n+        ``averaging_state``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            checkpoint: The checkpoint dictionary that will be saved.\n+\n+        """"""\n+        if self._average_model is None:\n+            raise Exception(""Trying to save a checkpoint, but no average model (outside fit). Don't know what to do."")\n+\n+        rank_zero_info(""The average model parameters will be saved to the state_dict in the checkpoint."")\n+        average_model_state = self._average_model.state_dict()\n+        checkpoint[""current_model_state""] = checkpoint[""state_dict""]\n+        checkpoint[""state_dict""] = {\n+            name[7:]: value for name, value in average_model_state.items() if name.startswith(""module."")\n+        }\n+        checkpoint[""averaging_state""] = {\n+            name: value for name, value in average_model_state.items() if not name.startswith(""module."")\n+        }\n+\n+    def on_load_checkpoint(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", checkpoint: dict[str, Any]\n+    ) -> None:\n+        r""""""Called when loading a model checkpoint.\n+\n+        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\n+\n+        """"""\n+        if self._average_model is None:\n+            raise Exception(""Trying to load a checkpoint, but no average model (outside fit). Don't know what to do."")",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843007c,Lightning-AI/pytorch-lightning,2276616729,1941350417,senarvi,2337787,2025-02-04T15:02:14+00:00,2025-02-04T15:02:15+00:00,"@lantiga That's true. Overriding some method seems to be a common way of customizing callbacks in Lightning.\r\n\r\nI changed it like this. However, there's just one method for the user to override, `should_update(self, step_idx: Optional[int] = None, epoch_idx: Optional[int] = None)`. (Otherwise, if the user would override `update_on_epoch()` and not override `update_on_step()`, the callback would still update on every step.)",true,src/lightning/pytorch/callbacks/weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+r""""""\n+Weight Averaging Callback\n+^^^^^^^^^^^^^^^^^^^^^^^^^\n+""""""\n+\n+import itertools\n+from copy import deepcopy\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch import Tensor\n+from torch.optim.swa_utils import AveragedModel\n+\n+import lightning.pytorch as pl\n+from lightning.pytorch.callbacks.callback import Callback\n+from lightning.pytorch.utilities.rank_zero import rank_zero_info, rank_zero_warn\n+from lightning.pytorch.utilities.types import STEP_OUTPUT\n+\n+\n+def _return_true(x: int) -> bool:\n+    return True\n+\n+\n+def _return_false(x: int) -> bool:\n+    return False\n+\n+\n+class WeightAveraging(Callback):\n+    r""""""A callback that updates an averaged model for Stochastic Weight Averaging (SWA) or Exponential Moving Average\n+    (EMA) after each training step.\n+\n+    The user should provide either `update_on_step` or `update_on_epoch`, a function that determines when the average\n+    model should be updated. If neither function is provided, the average model will be updated after every optimizer\n+    step.\n+\n+    During validation and after the training finishes, the current model parameters will be replaced with the averaged\n+    values.\n+\n+    Args:\n+        device: If provided, the :class:`AveragedModel` will be stored on the ``device``. If ``None`` the device will be\n+            inferred from the original model.\n+        avg_fn: The averaging function used to update the parameters. The function must take in an\n+            :class:`AveragedModel` parameter, a current model parameter, and the number of models already averaged. If\n+            ``None``, an equally weighted average will be used.\n+        update_on_step: A function that takes the number of optimizer steps taken, and returns ``True`` if the average",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843007d,Lightning-AI/pytorch-lightning,2276616729,1941352829,senarvi,2337787,2025-02-04T15:03:38+00:00,2025-02-04T15:03:38+00:00,@lantiga This is what I did. Please check if you think the messages are clear now.,true,src/lightning/pytorch/callbacks/weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+r""""""\n+Weight Averaging Callback\n+^^^^^^^^^^^^^^^^^^^^^^^^^\n+""""""\n+\n+import itertools\n+from copy import deepcopy\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch import Tensor\n+from torch.optim.swa_utils import AveragedModel\n+\n+import lightning.pytorch as pl\n+from lightning.pytorch.callbacks.callback import Callback\n+from lightning.pytorch.utilities.rank_zero import rank_zero_info, rank_zero_warn\n+from lightning.pytorch.utilities.types import STEP_OUTPUT\n+\n+\n+def _return_true(x: int) -> bool:\n+    return True\n+\n+\n+def _return_false(x: int) -> bool:\n+    return False\n+\n+\n+class WeightAveraging(Callback):\n+    r""""""A callback that updates an averaged model for Stochastic Weight Averaging (SWA) or Exponential Moving Average\n+    (EMA) after each training step.\n+\n+    The user should provide either `update_on_step` or `update_on_epoch`, a function that determines when the average\n+    model should be updated. If neither function is provided, the average model will be updated after every optimizer\n+    step.\n+\n+    During validation and after the training finishes, the current model parameters will be replaced with the averaged\n+    values.\n+\n+    Args:\n+        device: If provided, the :class:`AveragedModel` will be stored on the ``device``. If ``None`` the device will be\n+            inferred from the original model.\n+        avg_fn: The averaging function used to update the parameters. The function must take in an\n+            :class:`AveragedModel` parameter, a current model parameter, and the number of models already averaged. If\n+            ``None``, an equally weighted average will be used.\n+        update_on_step: A function that takes the number of optimizer steps taken, and returns ``True`` if the average\n+            model should be updated.\n+        update_on_epoch: A function that takes the zero-based epoch number, and returns ``True`` if the average model\n+            should be updated.\n+\n+    """"""\n+\n+    def __init__(\n+        self,\n+        device: Optional[Union[torch.device, int]] = torch.device(""cpu""),\n+        avg_fn: Optional[Callable[[Tensor, Tensor, Union[Tensor, int]], Tensor]] = None,\n+        update_on_step: Optional[Callable[[int], bool]] = None,\n+        update_on_epoch: Optional[Callable[[int], bool]] = None,\n+    ):\n+        self._device = device\n+        self._avg_fn = avg_fn\n+\n+        if (update_on_step is None) and (update_on_epoch is None):\n+            self._update_on_step: Callable[[int], bool] = _return_true\n+            self._update_on_epoch: Callable[[int], bool] = _return_false\n+        else:\n+            self._update_on_step = _return_false if update_on_step is None else update_on_step\n+            self._update_on_epoch = _return_false if update_on_epoch is None else update_on_epoch\n+\n+        self._average_model: Optional[AveragedModel] = None\n+\n+        # Number of optimizer steps taken, when the average model was last updated. Initializing this with zero ensures\n+        # that the average model will be first updated after the first optimizer step, which takes place after N batches\n+        # when using accumulate_grad_batches=N.\n+        self._latest_update_step = 0\n+        # The epoch after which the average model was last updated. The first epoch is 0, so initializing this to a\n+        # negative value means that if update_on_step(0) returns True, the first update is after the first epoch.\n+        self._latest_update_epoch = -1\n+\n+    def setup(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", stage: str) -> None:\n+        """"""Called when fit, validate, test, predict, or tune begins.\n+\n+        Creates an :class:`AveragedModel` when fit begins.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            stage: The :class:`~lightning.pytorch.trainer.trainer.Trainer` state.\n+\n+        """"""\n+        if stage == ""fit"":\n+            device = self._device or pl_module.device\n+            self._average_model = AveragedModel(model=pl_module, device=device, avg_fn=self._avg_fn, use_buffers=True)\n+\n+    def on_train_batch_end(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", outputs: STEP_OUTPUT, batch: Any, batch_idx: int\n+    ) -> None:\n+        """"""Called when a training batch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_step()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            outputs: Outputs from the training batch.\n+            batch: The training batch.\n+            batch_idx: Index of the training batch.\n+\n+        """"""\n+        if self._update_on_step(trainer.global_step) and (trainer.global_step > self._latest_update_step):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_step = trainer.global_step\n+\n+    def on_train_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a training epoch ends.\n+\n+        Updates the :class:`AveragedModel` parameters, if requested by ``update_on_epoch()``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._update_on_epoch(trainer.current_epoch) and (trainer.current_epoch > self._latest_update_epoch):\n+            assert self._average_model is not None\n+            self._average_model.update_parameters(pl_module)\n+            self._latest_update_epoch = trainer.current_epoch\n+\n+    def on_train_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when training ends.\n+\n+        Transfers parameters from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        assert self._average_model is not None\n+        self._copy_average_to_current(pl_module)\n+\n+    def on_validation_epoch_start(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch begins.\n+\n+        Transfers parameter values from the :class:`AveragedModel` to the current model.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Loading the average model parameters for validation."")\n+            self._swap_models(pl_module)\n+\n+    def on_validation_epoch_end(self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"") -> None:\n+        """"""Called when a validation epoch ends.\n+\n+        Recovers the current model parameters from the :class:`AveragedModel`.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+\n+        """"""\n+        if self._average_model is not None:\n+            rank_zero_info(""Recovering the current model parameters after validation."")\n+            self._swap_models(pl_module)\n+\n+    def state_dict(self) -> dict[str, Any]:\n+        """"""Called when saving a checkpoint.\n+\n+        Creates a ``state_dict`` of the callback state.\n+\n+        Returns:\n+            A dictionary containing the callback state.\n+\n+        """"""\n+        return {""latest_update_step"": self._latest_update_step}\n+\n+    def load_state_dict(self, state_dict: dict[str, Any]) -> None:\n+        """"""Called when loading a checkpoint.\n+\n+        Reloads the callback state given a ``state_dict``.\n+\n+        Args:\n+            state_dict: A dictionary containing the callback state.\n+\n+        """"""\n+        self._latest_update_step = state_dict[""latest_update_step""]\n+\n+    def on_save_checkpoint(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", checkpoint: dict[str, Any]\n+    ) -> None:\n+        r""""""Called when saving a checkpoint.\n+\n+        Moves the current model state to the key ``current_model_state``, and places the average model state in\n+        ``state_dict`` instead. Any other state variables of the ``AveragedModel`` will be saved in\n+        ``averaging_state``.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            checkpoint: The checkpoint dictionary that will be saved.\n+\n+        """"""\n+        if self._average_model is None:\n+            raise Exception(""Trying to save a checkpoint, but no average model (outside fit). Don't know what to do."")\n+\n+        rank_zero_info(""The average model parameters will be saved to the state_dict in the checkpoint."")\n+        average_model_state = self._average_model.state_dict()\n+        checkpoint[""current_model_state""] = checkpoint[""state_dict""]\n+        checkpoint[""state_dict""] = {\n+            name[7:]: value for name, value in average_model_state.items() if name.startswith(""module."")\n+        }\n+        checkpoint[""averaging_state""] = {\n+            name: value for name, value in average_model_state.items() if not name.startswith(""module."")\n+        }\n+\n+    def on_load_checkpoint(\n+        self, trainer: ""pl.Trainer"", pl_module: ""pl.LightningModule"", checkpoint: dict[str, Any]\n+    ) -> None:\n+        r""""""Called when loading a model checkpoint.\n+\n+        Loads the current model and the :class:`AveragedModel` parameters from the checkpoint.\n+\n+        Args:\n+            trainer: The current :class:`~lightning.pytorch.trainer.trainer.Trainer` instance.\n+            pl_module: The current :class:`~lightning.pytorch.core.LightningModule` instance.\n+            checkpoint: The full checkpoint dictionary that got loaded by the Trainer.\n+\n+        """"""\n+        if self._average_model is None:\n+            raise Exception(""Trying to load a checkpoint, but no average model (outside fit). Don't know what to do."")",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843007e,Lightning-AI/pytorch-lightning,2276616729,1941362705,senarvi,2337787,2025-02-04T15:09:24+00:00,2025-02-04T15:09:24+00:00,I'll try to still improve the test.,true,tests/tests_pytorch/callbacks/test_weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import os\n+from pathlib import Path\n+from typing import Any, Optional\n+\n+import pytest\n+import torch\n+from torch import Tensor, nn\n+from torch.optim.swa_utils import get_swa_avg_fn\n+from torch.utils.data import DataLoader\n+\n+from lightning.pytorch import LightningModule, Trainer\n+from lightning.pytorch.callbacks import WeightAveraging\n+from lightning.pytorch.demos.boring_classes import BoringModel, RandomDataset, RandomIterableDataset\n+from tests_pytorch.helpers.runif import RunIf\n+\n+\n+class WeightAveragingTestModel(BoringModel):\n+    def __init__(\n+        self, batch_norm: bool = True, iterable_dataset: bool = False, crash_on_epoch: Optional[int] = None\n+    ) -> None:\n+        super().__init__()\n+        layers = [nn.Linear(32, 32)]\n+        if batch_norm:\n+            layers.append(nn.BatchNorm1d(32))\n+        layers += [nn.ReLU(), nn.Linear(32, 2)]\n+        self.layer = nn.Sequential(*layers)\n+        self.iterable_dataset = iterable_dataset\n+        self.crash_on_epoch = crash_on_epoch\n+\n+    def training_step(self, batch: Tensor, batch_idx: int) -> None:\n+        if self.crash_on_epoch and self.trainer.current_epoch >= self.crash_on_epoch:\n+            raise Exception(""CRASH TEST"")\n+        return super().training_step(batch, batch_idx)\n+\n+    def train_dataloader(self) -> None:\n+        dataset_class = RandomIterableDataset if self.iterable_dataset else RandomDataset\n+        return DataLoader(dataset_class(32, 32), batch_size=4)\n+\n+    def configure_optimizers(self) -> None:\n+        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\n+\n+\n+class EMAAveragingFunction:\n+    """"""EMA averaging function.\n+\n+    Functionally equivalent to the closure that ``get_ema_avg_fn()`` would return. This class is needed because we\n+    cannot use a closure with ddp_spawn. (``Popen(process_obj)`` would fail with\n+    ``Can't get local object 'get_ema_avg_fn.<locals>.ema_update'``).\n+\n+    """"""\n+\n+    def __init__(self, decay: float = 0.999) -> None:\n+        self.decay = decay\n+\n+    @torch.no_grad()\n+    def __call__(self, ema_param: Tensor, current_param: Tensor, num_averaged: Tensor) -> Tensor:\n+        return self.decay * ema_param + (1 - self.decay) * current_param\n+\n+\n+class EMATestCallback(WeightAveraging):\n+    def __init__(self, devices: int = 1, **kwargs: Any) -> None:\n+        super().__init__(avg_fn=EMAAveragingFunction(), **kwargs)\n+        self.devices = devices\n+        self.swap_calls = 0\n+        self.copy_calls = 0\n+        # Record the first epoch, as if we are resuming from a checkpoint this may not be equal to 0.\n+        self.first_epoch: Optional[int] = None\n+\n+    def _swap_models(self, *args: Any, **kwargs: Any):\n+        self.swap_calls += 1\n+        return super()._swap_models(*args, **kwargs)\n+\n+    def _copy_average_to_current(self, *args: Any, **kwargs: Any):\n+        self.copy_calls += 1\n+        return super()._copy_average_to_current(*args, **kwargs)\n+\n+    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_start(trainer, pl_module)\n+        assert self.swap_calls == 0\n+        assert self.copy_calls == 0\n+\n+    def on_train_epoch_start(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_start(trainer, *args)\n+        # Since the checkpoint loaded was saved `on_train_epoch_end`, the first `FitLoop` iteration will not update the\n+        # model and will just call the epoch-level hooks. For that reason, we check that we are not restarting before\n+        # choosing the first epoch.\n+        if self.first_epoch is None and not trainer.fit_loop.restarting:\n+            self.first_epoch = trainer.current_epoch\n+\n+    def on_train_epoch_end(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_end(trainer, *args)\n+        assert self._average_model.n_averaged == trainer.global_step\n+        assert self.swap_calls == (trainer.current_epoch + 1 - self.first_epoch) * 2\n+        assert self.copy_calls == 0\n+\n+    def on_train_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_end(trainer, pl_module)\n+        # length=32, batch_size=4, accumulate_grad_batches=2\n+        # => Using one process we have 4 optimizer steps per epoch.\n+        # => Using two processes we have 2 optimizer steps per epoch.\n+        steps_per_epoch = 4 // self.devices\n+        assert self._average_model.n_averaged == trainer.max_epochs * steps_per_epoch\n+        assert self.swap_calls == (trainer.max_epochs - self.first_epoch) * 2\n+        assert self.copy_calls == 1\n+\n+\n+class SWATestCallback(WeightAveraging):\n+    def __init__(self, **kwargs: Any) -> None:\n+        avg_fn = get_swa_avg_fn()\n+        update_on_epoch = lambda x: x in (3, 5, 7)\n+        super().__init__(avg_fn=avg_fn, update_on_epoch=update_on_epoch, **kwargs)\n+\n+        self.swap_calls = 0\n+        self.copy_calls = 0\n+        # Record the first epoch, as if we are resuming from a checkpoint this may not be equal to 0.\n+        self.first_epoch: Optional[int] = None\n+\n+    def _swap_models(self, *args: Any, **kwargs: Any):\n+        self.swap_calls += 1\n+        return super()._swap_models(*args, **kwargs)\n+\n+    def _copy_average_to_current(self, *args: Any, **kwargs: Any):\n+        self.copy_calls += 1\n+        return super()._copy_average_to_current(*args, **kwargs)\n+\n+    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_start(trainer, pl_module)\n+        assert self.swap_calls == 0\n+        assert self.copy_calls == 0\n+\n+    def on_train_epoch_start(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_start(trainer, *args)\n+        # Since the checkpoint loaded was saved `on_train_epoch_end`, the first `FitLoop` iteration will not update the\n+        # model and will just call the epoch-level hooks. For that reason, we check that we are not restarting before\n+        # choosing the first epoch.\n+        if self.first_epoch is None and not trainer.fit_loop.restarting:\n+            self.first_epoch = trainer.current_epoch\n+\n+    def on_train_epoch_end(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_end(trainer, *args)\n+        if trainer.current_epoch < 3:\n+            assert self._average_model.n_averaged == 0\n+        elif trainer.current_epoch < 5:\n+            assert self._average_model.n_averaged == 1\n+        elif trainer.current_epoch < 7:\n+            assert self._average_model.n_averaged == 2\n+        else:\n+            assert self._average_model.n_averaged == 3\n+        assert self.swap_calls == (trainer.current_epoch + 1 - self.first_epoch) * 2\n+        assert self.copy_calls == 0\n+\n+    def on_train_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_end(trainer, pl_module)\n+        assert self._average_model.n_averaged == 3\n+        assert self.swap_calls == (trainer.max_epochs - self.first_epoch) * 2\n+        assert self.copy_calls == 1\n+\n+\n+def test_weight_averaging_deepcopy(tmp_path):\n+    """"""Ensure that WeightAveraging callback doesn't deepcopy the data loaders or the data module and consume memory\n+    more than necessary.""""""\n+\n+    class TestCallback(WeightAveraging):\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            self.setup_called = False\n+\n+        def setup(self, trainer, pl_module, stage) -> None:\n+            super().setup(trainer, pl_module, stage)\n+            assert self._average_model.module.train_dataloader is not pl_module.train_dataloader\n+            assert self._average_model.module.train_dataloader.__self__ == self._average_model.module\n+            assert self._average_model.module._trainer is None\n+            self.setup_called = True\n+\n+    callback = TestCallback()\n+    trainer = Trainer(default_root_dir=tmp_path, callbacks=callback, fast_dev_run=True)\n+    trainer.fit(BoringModel(), train_dataloaders=DataLoader(RandomDataset(32, 2)))\n+    assert callback.setup_called\n+\n+\n+@pytest.mark.parametrize(""batch_norm"", [True, False])\n+@pytest.mark.parametrize(""iterable_dataset"", [True, False])\n+def test_ema(tmp_path, batch_norm: bool, iterable_dataset: bool):\n+    _train(tmp_path, EMATestCallback(), batch_norm=batch_norm, iterable_dataset=iterable_dataset)\n+\n+\n+@pytest.mark.parametrize(\n+    ""accelerator"", [pytest.param(""gpu"", marks=RunIf(min_cuda_gpus=1)), pytest.param(""mps"", marks=RunIf(mps=True))]\n+)\n+def test_ema_accelerator(tmp_path, accelerator):\n+    _train(tmp_path, EMATestCallback(), accelerator=accelerator, devices=1)\n+\n+\n+@RunIf(min_cuda_gpus=2, standalone=True)\n+def test_ema_ddp(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp"", accelerator=""gpu"", devices=2)\n+\n+\n+@RunIf(min_cuda_gpus=2)\n+def test_ema_ddp_spawn(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp_spawn"", accelerator=""gpu"", devices=2)\n+\n+\n+@RunIf(skip_windows=True)\n+def test_ema_ddp_spawn_cpu(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp_spawn"", accelerator=""cpu"", devices=2)\n+\n+\n+@pytest.mark.parametrize(""crash_on_epoch"", [1, 3])\n+def test_ema_resume(tmp_path, crash_on_epoch):\n+    _train_and_resume(tmp_path, crash_on_epoch=crash_on_epoch)\n+\n+\n+@RunIf(skip_windows=True)\n+def test_ema_resume_ddp(tmp_path):\n+    _train_and_resume(tmp_path, crash_on_epoch=3, use_ddp=True)\n+\n+\n+def test_swa(tmp_path):\n+    _train(tmp_path, SWATestCallback())\n+\n+\n+def _train(\n+    tmp_path: str,\n+    callback: WeightAveraging,\n+    batch_norm: bool = True,\n+    strategy: str = ""auto"",\n+    accelerator: str = ""cpu"",\n+    devices: int = 1,\n+    iterable_dataset: bool = False,\n+    checkpoint_path: Optional[str] = None,\n+    crash_on_epoch: Optional[int] = None,\n+) -> None:\n+    trainer = Trainer(\n+        default_root_dir=tmp_path,\n+        enable_progress_bar=False,\n+        enable_model_summary=False,\n+        logger=False,\n+        max_epochs=8,\n+        num_sanity_val_steps=0,\n+        callbacks=callback,\n+        accumulate_grad_batches=2,\n+        strategy=strategy,\n+        accelerator=accelerator,\n+        devices=devices,\n+    )\n+    model = WeightAveragingTestModel(\n+        batch_norm=batch_norm, iterable_dataset=iterable_dataset, crash_on_epoch=crash_on_epoch\n+    )\n+\n+    if crash_on_epoch is None:\n+        trainer.fit(model, ckpt_path=checkpoint_path)\n+    else:\n+        with pytest.raises(Exception, match=""CRASH TEST""):\n+            trainer.fit(model, ckpt_path=checkpoint_path)\n+\n+    assert trainer.lightning_module == model\n+\n+\n+def _train_and_resume(tmp_path: str, crash_on_epoch: int, use_ddp: bool = False) -> None:",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec7843007f,Lightning-AI/pytorch-lightning,2276616729,1947854755,senarvi,2337787,2025-02-08T15:54:58+00:00,2025-02-08T15:54:58+00:00,"@lantiga now I test that after stopping and resuming we get the same final model. The parameters are not identical - I have to use `atol=0.001` - but they are close enough so that I think that the difference comes from some random change instead of a bug in restoring the checkpoint. I don't know what could cause the difference, though. I pass `deterministic=True` to Trainer. I'm curious if you have some ideas, or if you think that that's close enough.",true,tests/tests_pytorch/callbacks/test_weight_averaging.py,NaN,"@@ -0,0 +1,288 @@\n+# Copyright The Lightning AI team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the ""License"");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an ""AS IS"" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import os\n+from pathlib import Path\n+from typing import Any, Optional\n+\n+import pytest\n+import torch\n+from torch import Tensor, nn\n+from torch.optim.swa_utils import get_swa_avg_fn\n+from torch.utils.data import DataLoader\n+\n+from lightning.pytorch import LightningModule, Trainer\n+from lightning.pytorch.callbacks import WeightAveraging\n+from lightning.pytorch.demos.boring_classes import BoringModel, RandomDataset, RandomIterableDataset\n+from tests_pytorch.helpers.runif import RunIf\n+\n+\n+class WeightAveragingTestModel(BoringModel):\n+    def __init__(\n+        self, batch_norm: bool = True, iterable_dataset: bool = False, crash_on_epoch: Optional[int] = None\n+    ) -> None:\n+        super().__init__()\n+        layers = [nn.Linear(32, 32)]\n+        if batch_norm:\n+            layers.append(nn.BatchNorm1d(32))\n+        layers += [nn.ReLU(), nn.Linear(32, 2)]\n+        self.layer = nn.Sequential(*layers)\n+        self.iterable_dataset = iterable_dataset\n+        self.crash_on_epoch = crash_on_epoch\n+\n+    def training_step(self, batch: Tensor, batch_idx: int) -> None:\n+        if self.crash_on_epoch and self.trainer.current_epoch >= self.crash_on_epoch:\n+            raise Exception(""CRASH TEST"")\n+        return super().training_step(batch, batch_idx)\n+\n+    def train_dataloader(self) -> None:\n+        dataset_class = RandomIterableDataset if self.iterable_dataset else RandomDataset\n+        return DataLoader(dataset_class(32, 32), batch_size=4)\n+\n+    def configure_optimizers(self) -> None:\n+        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\n+\n+\n+class EMAAveragingFunction:\n+    """"""EMA averaging function.\n+\n+    Functionally equivalent to the closure that ``get_ema_avg_fn()`` would return. This class is needed because we\n+    cannot use a closure with ddp_spawn. (``Popen(process_obj)`` would fail with\n+    ``Can't get local object 'get_ema_avg_fn.<locals>.ema_update'``).\n+\n+    """"""\n+\n+    def __init__(self, decay: float = 0.999) -> None:\n+        self.decay = decay\n+\n+    @torch.no_grad()\n+    def __call__(self, ema_param: Tensor, current_param: Tensor, num_averaged: Tensor) -> Tensor:\n+        return self.decay * ema_param + (1 - self.decay) * current_param\n+\n+\n+class EMATestCallback(WeightAveraging):\n+    def __init__(self, devices: int = 1, **kwargs: Any) -> None:\n+        super().__init__(avg_fn=EMAAveragingFunction(), **kwargs)\n+        self.devices = devices\n+        self.swap_calls = 0\n+        self.copy_calls = 0\n+        # Record the first epoch, as if we are resuming from a checkpoint this may not be equal to 0.\n+        self.first_epoch: Optional[int] = None\n+\n+    def _swap_models(self, *args: Any, **kwargs: Any):\n+        self.swap_calls += 1\n+        return super()._swap_models(*args, **kwargs)\n+\n+    def _copy_average_to_current(self, *args: Any, **kwargs: Any):\n+        self.copy_calls += 1\n+        return super()._copy_average_to_current(*args, **kwargs)\n+\n+    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_start(trainer, pl_module)\n+        assert self.swap_calls == 0\n+        assert self.copy_calls == 0\n+\n+    def on_train_epoch_start(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_start(trainer, *args)\n+        # Since the checkpoint loaded was saved `on_train_epoch_end`, the first `FitLoop` iteration will not update the\n+        # model and will just call the epoch-level hooks. For that reason, we check that we are not restarting before\n+        # choosing the first epoch.\n+        if self.first_epoch is None and not trainer.fit_loop.restarting:\n+            self.first_epoch = trainer.current_epoch\n+\n+    def on_train_epoch_end(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_end(trainer, *args)\n+        assert self._average_model.n_averaged == trainer.global_step\n+        assert self.swap_calls == (trainer.current_epoch + 1 - self.first_epoch) * 2\n+        assert self.copy_calls == 0\n+\n+    def on_train_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_end(trainer, pl_module)\n+        # length=32, batch_size=4, accumulate_grad_batches=2\n+        # => Using one process we have 4 optimizer steps per epoch.\n+        # => Using two processes we have 2 optimizer steps per epoch.\n+        steps_per_epoch = 4 // self.devices\n+        assert self._average_model.n_averaged == trainer.max_epochs * steps_per_epoch\n+        assert self.swap_calls == (trainer.max_epochs - self.first_epoch) * 2\n+        assert self.copy_calls == 1\n+\n+\n+class SWATestCallback(WeightAveraging):\n+    def __init__(self, **kwargs: Any) -> None:\n+        avg_fn = get_swa_avg_fn()\n+        update_on_epoch = lambda x: x in (3, 5, 7)\n+        super().__init__(avg_fn=avg_fn, update_on_epoch=update_on_epoch, **kwargs)\n+\n+        self.swap_calls = 0\n+        self.copy_calls = 0\n+        # Record the first epoch, as if we are resuming from a checkpoint this may not be equal to 0.\n+        self.first_epoch: Optional[int] = None\n+\n+    def _swap_models(self, *args: Any, **kwargs: Any):\n+        self.swap_calls += 1\n+        return super()._swap_models(*args, **kwargs)\n+\n+    def _copy_average_to_current(self, *args: Any, **kwargs: Any):\n+        self.copy_calls += 1\n+        return super()._copy_average_to_current(*args, **kwargs)\n+\n+    def on_train_start(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_start(trainer, pl_module)\n+        assert self.swap_calls == 0\n+        assert self.copy_calls == 0\n+\n+    def on_train_epoch_start(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_start(trainer, *args)\n+        # Since the checkpoint loaded was saved `on_train_epoch_end`, the first `FitLoop` iteration will not update the\n+        # model and will just call the epoch-level hooks. For that reason, we check that we are not restarting before\n+        # choosing the first epoch.\n+        if self.first_epoch is None and not trainer.fit_loop.restarting:\n+            self.first_epoch = trainer.current_epoch\n+\n+    def on_train_epoch_end(self, trainer: Trainer, *args: Any) -> None:\n+        super().on_train_epoch_end(trainer, *args)\n+        if trainer.current_epoch < 3:\n+            assert self._average_model.n_averaged == 0\n+        elif trainer.current_epoch < 5:\n+            assert self._average_model.n_averaged == 1\n+        elif trainer.current_epoch < 7:\n+            assert self._average_model.n_averaged == 2\n+        else:\n+            assert self._average_model.n_averaged == 3\n+        assert self.swap_calls == (trainer.current_epoch + 1 - self.first_epoch) * 2\n+        assert self.copy_calls == 0\n+\n+    def on_train_end(self, trainer: Trainer, pl_module: LightningModule) -> None:\n+        super().on_train_end(trainer, pl_module)\n+        assert self._average_model.n_averaged == 3\n+        assert self.swap_calls == (trainer.max_epochs - self.first_epoch) * 2\n+        assert self.copy_calls == 1\n+\n+\n+def test_weight_averaging_deepcopy(tmp_path):\n+    """"""Ensure that WeightAveraging callback doesn't deepcopy the data loaders or the data module and consume memory\n+    more than necessary.""""""\n+\n+    class TestCallback(WeightAveraging):\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            self.setup_called = False\n+\n+        def setup(self, trainer, pl_module, stage) -> None:\n+            super().setup(trainer, pl_module, stage)\n+            assert self._average_model.module.train_dataloader is not pl_module.train_dataloader\n+            assert self._average_model.module.train_dataloader.__self__ == self._average_model.module\n+            assert self._average_model.module._trainer is None\n+            self.setup_called = True\n+\n+    callback = TestCallback()\n+    trainer = Trainer(default_root_dir=tmp_path, callbacks=callback, fast_dev_run=True)\n+    trainer.fit(BoringModel(), train_dataloaders=DataLoader(RandomDataset(32, 2)))\n+    assert callback.setup_called\n+\n+\n+@pytest.mark.parametrize(""batch_norm"", [True, False])\n+@pytest.mark.parametrize(""iterable_dataset"", [True, False])\n+def test_ema(tmp_path, batch_norm: bool, iterable_dataset: bool):\n+    _train(tmp_path, EMATestCallback(), batch_norm=batch_norm, iterable_dataset=iterable_dataset)\n+\n+\n+@pytest.mark.parametrize(\n+    ""accelerator"", [pytest.param(""gpu"", marks=RunIf(min_cuda_gpus=1)), pytest.param(""mps"", marks=RunIf(mps=True))]\n+)\n+def test_ema_accelerator(tmp_path, accelerator):\n+    _train(tmp_path, EMATestCallback(), accelerator=accelerator, devices=1)\n+\n+\n+@RunIf(min_cuda_gpus=2, standalone=True)\n+def test_ema_ddp(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp"", accelerator=""gpu"", devices=2)\n+\n+\n+@RunIf(min_cuda_gpus=2)\n+def test_ema_ddp_spawn(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp_spawn"", accelerator=""gpu"", devices=2)\n+\n+\n+@RunIf(skip_windows=True)\n+def test_ema_ddp_spawn_cpu(tmp_path):\n+    _train(tmp_path, EMATestCallback(devices=2), strategy=""ddp_spawn"", accelerator=""cpu"", devices=2)\n+\n+\n+@pytest.mark.parametrize(""crash_on_epoch"", [1, 3])\n+def test_ema_resume(tmp_path, crash_on_epoch):\n+    _train_and_resume(tmp_path, crash_on_epoch=crash_on_epoch)\n+\n+\n+@RunIf(skip_windows=True)\n+def test_ema_resume_ddp(tmp_path):\n+    _train_and_resume(tmp_path, crash_on_epoch=3, use_ddp=True)\n+\n+\n+def test_swa(tmp_path):\n+    _train(tmp_path, SWATestCallback())\n+\n+\n+def _train(\n+    tmp_path: str,\n+    callback: WeightAveraging,\n+    batch_norm: bool = True,\n+    strategy: str = ""auto"",\n+    accelerator: str = ""cpu"",\n+    devices: int = 1,\n+    iterable_dataset: bool = False,\n+    checkpoint_path: Optional[str] = None,\n+    crash_on_epoch: Optional[int] = None,\n+) -> None:\n+    trainer = Trainer(\n+        default_root_dir=tmp_path,\n+        enable_progress_bar=False,\n+        enable_model_summary=False,\n+        logger=False,\n+        max_epochs=8,\n+        num_sanity_val_steps=0,\n+        callbacks=callback,\n+        accumulate_grad_batches=2,\n+        strategy=strategy,\n+        accelerator=accelerator,\n+        devices=devices,\n+    )\n+    model = WeightAveragingTestModel(\n+        batch_norm=batch_norm, iterable_dataset=iterable_dataset, crash_on_epoch=crash_on_epoch\n+    )\n+\n+    if crash_on_epoch is None:\n+        trainer.fit(model, ckpt_path=checkpoint_path)\n+    else:\n+        with pytest.raises(Exception, match=""CRASH TEST""):\n+            trainer.fit(model, ckpt_path=checkpoint_path)\n+\n+    assert trainer.lightning_module == model\n+\n+\n+def _train_and_resume(tmp_path: str, crash_on_epoch: int, use_ddp: bool = False) -> None:",0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430080,Lightning-AI/pytorch-lightning,2265938780,1913880031,lantiga,191033,2025-01-13T22:24:19+00:00,2025-01-13T22:31:06+00:00,```suggestion\r\n    import lightning as L\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430081,Lightning-AI/pytorch-lightning,2265938780,1913880182,lantiga,191033,2025-01-13T22:24:30+00:00,2025-01-13T22:31:06+00:00,```suggestion\r\n    trainer = L.Trainer(logger=mlf_logger)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430082,Lightning-AI/pytorch-lightning,2265938780,1913880312,lantiga,191033,2025-01-13T22:24:39+00:00,2025-01-13T22:31:06+00:00,```suggestion\r\n    class LitModel(L.LightningModule):\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430083,Lightning-AI/pytorch-lightning,2265938780,1913881482,lantiga,191033,2025-01-13T22:26:03+00:00,2025-01-13T22:31:06+00:00,"I would simplify the name of the argument to be `checkpoint_path_prefix`. The extra `artifact` doesn't add much imo and simpler is better, wdyt?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430084,Lightning-AI/pytorch-lightning,2265938780,1914384591,benglewis,3817460,2025-01-14T07:52:16+00:00,2025-01-14T09:16:08+00:00,"Personally, I thought that `artifact` was indicating that it was the path on MLflow... but given that the context is the `MLFlowLogger`, I guess that it doesn't actually add anything. I will change it",false,NaN,NaN,NaN,1,1,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec78430085,Lightning-AI/pytorch-lightning,2265938780,1946645811,joncarter1,42900403,2025-02-07T14:46:30+00:00,2025-02-07T14:46:30+00:00,"@benglewis doc and example are stale but otherwise looks great, thanks for fixing this!\r\n\r\nI'd just complained on the original PR (https://github.com/Lightning-AI/pytorch-lightning/pull/20325#issuecomment-2637363515) that caused this as it silently broke my code innocuously too!",false,NaN,NaN,NaN,0,0,0,0,0,0,0,Lightning-AI_pytorch-lightning
6824cd900ce5dcec784303a6,microsoft/Data-Science-For-Beginners,916473601,972270406,paladique,1414307,2022-09-15T17:44:16+00:00,2022-09-15T17:44:16+00:00,"Hi! this line should be translated too, thanks!",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303a7,microsoft/Data-Science-For-Beginners,916473601,972270406,paladique,1414307,2022-09-15T17:44:16+00:00,2022-09-15T17:44:16+00:00,"Hi! this line should be translated too, thanks!",true,1-Introduction/01-defining-data-science/translations/README.it.md,25,"@@ -0,0 +1,167 @@\n+# Definendo la  Data Science\n+\n+| ![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/01-Definitions.png) |\n+| :----------------------------------------------------------------------------------------------------: |\n+|              Definedo la  Data Science - _Sketchnote di [@nitya](https://twitter.com/nitya)_               |\n+\n+---\n+\n+[![Definendo la  Data Science Video](images/video-def-ds.png)](https://youtu.be/beZ7Mb_oz9I)\n+\n+## [Quiz pre-lezione](https://red-water-0103e7a0f.azurestaticapps.net/quiz/0)\n+\n+## Cosa sono i Dati?\n+Nella vita di tutti i giorni, siamo costantemente circondati dai dati.Il testo che stai leggendo ora è parte dei dati. La lista dei numeri di telefono dei tuoi amici nel tuo smartphone fa parte dei dati, come anche l'ora mostrata dal tuo orologio in questo momento.Come abitudine umana, noi operiamo con i dati contando i soldi che abbamo, o scrivendo lettere ai nostri amici.\n+\n+In ogni caso, i dati sono diventati più importanti con l'invenzione dei computers. Il ruolo principale dei computers è di fare calcoli, ma essi necessitano di dati per operare. Di conseguenza abbiamo bisogno di capire come i computers processano i dati.\n+\n+Con l'emergere di Internet, il ruolo dei computers come manipolatori di dati è aumentato. Se pensi che, noi ora usiamo i computer sempre più per processare dati e per comunicare piuttosto che per calcoli effettivi. Quando scriviamo una e-mail ad un amico o cerchiamo delle informazioni online - noi stiamo essenzialmente creando, immagazinando, trasmettendo e manipolando dati. \n+> Riesci a ricordate l'ultima volta che hai usato il computer per fare effettivamente dei calcoli?\n+\n+## Cos'è la Data Science?\n+\n+Su [Wikipedia](https://en.wikipedia.org/wiki/Data_science), **Data Science** è definita come  *un campo scientifico che fa uso di metodi scientifici al fine di estrarre informazioni e intuizioni da dati struttirati e non , e applica conoscenze e approfondimenti attuabili dai dati in un'ampia gamma di domini applicativi*\n+\n+This definition highlights the following important aspects of data science:",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303a8,microsoft/Data-Science-For-Beginners,875971373,823763063,jlooper,1450004,2022-03-10T14:16:18+00:00,2022-03-10T14:18:13+00:00,Please roll this back to avoid unnecessary edits,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303a9,microsoft/Data-Science-For-Beginners,875971373,823763733,jlooper,1450004,2022-03-10T14:16:56+00:00,2022-03-10T14:18:14+00:00,these links 404,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303aa,microsoft/Data-Science-For-Beginners,875971373,823764536,jlooper,1450004,2022-03-10T14:17:41+00:00,2022-03-10T14:18:14+00:00,This link is a 404,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ab,microsoft/Data-Science-For-Beginners,875971373,823765511,jlooper,1450004,2022-03-10T14:18:39+00:00,2022-03-10T17:31:56+00:00,needs to be translated,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ac,microsoft/Data-Science-For-Beginners,875971373,823802232,SachinDabhade,78628027,2022-03-10T14:52:15+00:00,2022-03-10T14:52:15+00:00,"Can you please guide me how, because I am slightly unclear about it that what commands I can use to roll back\r\n",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ad,microsoft/Data-Science-For-Beginners,875971373,823819506,SachinDabhade,78628027,2022-03-10T15:06:50+00:00,2022-03-10T15:06:50+00:00,This links aren't working in the original repository (i.e in microsoft github account),false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ae,microsoft/Data-Science-For-Beginners,875971373,823976311,jlooper,1450004,2022-03-10T17:30:44+00:00,2022-03-10T17:31:56+00:00,"That's important to know! Rather than propagating errors it would be better to fix the original. I have edited it, so you will need to do a pull to your fork and re-edit. ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303af,microsoft/Data-Science-For-Beginners,875971373,823977173,jlooper,1450004,2022-03-10T17:31:40+00:00,2022-03-10T17:31:56+00:00,You can discard all changes made to this notebook file,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b0,microsoft/Data-Science-For-Beginners,875971373,824358277,SachinDabhade,78628027,2022-03-11T03:29:02+00:00,2022-03-11T03:29:02+00:00,"Ok, I have copied original file and change the changed one with it",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b1,microsoft/Data-Science-For-Beginners,875971373,824367164,SachinDabhade,78628027,2022-03-11T03:56:40+00:00,2022-03-11T03:56:41+00:00,"Done, I have done it",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b2,microsoft/Data-Science-For-Beginners,875971373,824367921,SachinDabhade,78628027,2022-03-11T03:59:04+00:00,2022-03-11T03:59:04+00:00,"My apologies jlooper, actualy I am new to open-source and github so thanks for your support and guidance",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b3,microsoft/Data-Science-For-Beginners,875971373,824369705,SachinDabhade,78628027,2022-03-11T04:04:20+00:00,2022-03-11T04:04:20+00:00,Changes are done,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b4,microsoft/Data-Science-For-Beginners,875971373,824803792,jlooper,1450004,2022-03-11T15:06:16+00:00,2022-03-11T15:07:39+00:00,This is going to 404 because it should now point to README.hi.md#3-case-studies,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b5,microsoft/Data-Science-For-Beginners,875971373,824804882,jlooper,1450004,2022-03-11T15:07:23+00:00,2022-03-11T15:07:39+00:00,This is still a 404! Please go through every single link and every single image in this PR and link them properly,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b6,microsoft/Data-Science-For-Beginners,875971373,824830860,SachinDabhade,78628027,2022-03-11T15:36:10+00:00,2022-03-11T15:36:11+00:00,Fixed this error,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b7,microsoft/Data-Science-For-Beginners,875971373,824860451,SachinDabhade,78628027,2022-03-11T16:09:11+00:00,2022-03-11T16:09:12+00:00,"Fixed all finded error, if any remain then please review it.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b8,microsoft/Data-Science-For-Beginners,875971373,825479681,jlooper,1450004,2022-03-13T17:25:29+00:00,2022-03-13T17:25:29+00:00,I think you can remove this,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303b9,microsoft/Data-Science-For-Beginners,875971373,825479750,jlooper,1450004,2022-03-13T17:26:09+00:00,2022-03-13T17:26:09+00:00,Let's use a standard vocabulary to link the Pre-lecture and Post-lecture quizzes,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ba,microsoft/Data-Science-For-Beginners,875971373,825479966,jlooper,1450004,2022-03-13T17:28:04+00:00,2022-03-13T17:28:04+00:00,malformed link here,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303bb,microsoft/Data-Science-For-Beginners,875971373,825479992,jlooper,1450004,2022-03-13T17:28:25+00:00,2022-03-13T17:28:25+00:00,missed translation,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303bc,microsoft/Data-Science-For-Beginners,875971373,825480163,jlooper,1450004,2022-03-13T17:30:11+00:00,2022-03-13T17:30:11+00:00,standard translation here please,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303bd,microsoft/Data-Science-For-Beginners,875971373,825480183,jlooper,1450004,2022-03-13T17:30:31+00:00,2022-03-13T17:30:31+00:00,I think this is mislinked - you can link to translated assignments if they exist,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303be,microsoft/Data-Science-For-Beginners,875971373,825480474,jlooper,1450004,2022-03-13T17:32:48+00:00,2022-03-13T17:32:48+00:00,should be able to link to the translated assignment here,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303bf,microsoft/Data-Science-For-Beginners,875971373,825575742,SachinDabhade,78628027,2022-03-14T03:53:56+00:00,2022-03-14T03:53:56+00:00,Fixed duplicate line,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c0,microsoft/Data-Science-For-Beginners,875971373,825575743,SachinDabhade,78628027,2022-03-14T03:53:56+00:00,2022-03-14T03:53:56+00:00,Fixed Duplicate Links,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c1,microsoft/Data-Science-For-Beginners,875971373,825576761,SachinDabhade,78628027,2022-03-14T03:57:35+00:00,2022-03-14T03:57:36+00:00,Fixed malformed link,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c2,microsoft/Data-Science-For-Beginners,875971373,825577321,SachinDabhade,78628027,2022-03-14T03:59:59+00:00,2022-03-14T03:59:59+00:00,Translated some missed translate word,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c3,microsoft/Data-Science-For-Beginners,875971373,823763063,jlooper,1450004,2022-03-10T14:16:18+00:00,2022-03-10T14:18:13+00:00,Please roll this back to avoid unnecessary edits,true,1-Introduction/01-defining-data-science/README.md,NaN,"@@ -162,4 +162,4 @@ Visit [`notebook.ipynb`](/1-Introduction/01-defining-data-science/notebook.ipynb\n \n ## Credits\n \n-This lesson has been authored with ♥️ by [Dmitry Soshnikov](http://soshnikov.com)\n+This lesson has been authored with ♥️ by [Dmitry Soshnikov](http://soshnikov.com)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c4,microsoft/Data-Science-For-Beginners,875971373,823763733,jlooper,1450004,2022-03-10T14:16:56+00:00,2022-03-10T14:18:14+00:00,these links 404,true,1-Introduction/02-ethics/translations/assignment.hi.md,NaN,"@@ -0,0 +1,19 @@\n+## डेटा एथिक्स केस स्टडी लिखें\n+\n+## निर्देश\n+\n+आपने विभिन्न [डेटा नैतिकता चुनौतियों](README?id=_2-ethics-challenges) के बारे में सीखा है और [केस स्टडीज](README?id=_3-case-studies) के कुछ उदाहरण देखे हैं जो वास्तविक दुनिया में डेटा नैतिकता चुनौतियों को दर्शाते हैं। संदर्भ",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c5,microsoft/Data-Science-For-Beginners,875971373,823764536,jlooper,1450004,2022-03-10T14:17:41+00:00,2022-03-10T14:18:14+00:00,This link is a 404,true,1-Introduction/01-defining-data-science/translations/README.hi.md,4,"@@ -0,0 +1,164 @@\n+# डेटा विज्ञान को परिभाषित करना\n+\n+| ![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/01-Definitions.png) |\n+| :----------------------------------------------------------------------------------------------------: |",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c6,microsoft/Data-Science-For-Beginners,875971373,823765511,jlooper,1450004,2022-03-10T14:18:39+00:00,2022-03-10T17:31:56+00:00,needs to be translated,true,2-Working-With-Data/05-relational-databases/translations/assignment.hi.md,NaN,"@@ -0,0 +1,59 @@\n+# हवाईअड्डा डेटा प्रदर्शित करना\n+\n+आपको एक [डेटाबेस](https://raw.githubusercontent.com/Microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/airports.db)प्रदान किया जायेगा। बनाया गया है [SQLite](https://sqlite.org/index.html) पर जिसमें हवाई अड्डों के बारे में जानकारी होती है। स्कीमा नीचे प्रदर्शित किया गया है। आप [विजुअल स्टूडियो कोड](https://code.) में [SQLite एक्सटेंशन](https://marketplace.visualstudio.com/items?itemName=alexcvzz.vscode-sqlite&WT.mc_id=academic-40229-cxa) का इस्तेमाल करेंगे। Visualstudio.com?WT.mc_id=academic-40229-cxa) विभिन्न शहरों के हवाई अड्डों के बारे में जानकारी प्रदर्शित करने के लिए।\n+\n+## निर्देश\n+\n+असाइनमेंट के साथ आरंभ करने के लिए, आपको कुछ चरणों का पालन करना होगा। आपको कुछ टूलींग स्थापित करने और नमूना डेटाबेस डाउनलोड करने की आवश्यकता होगी।\n+\n+### अपना सिस्टम सेटअप करें\n+\n+आप डेटाबेस के साथ इंटरैक्ट करने के लिए विजुअल स्टूडियो कोड और SQLite एक्सटेंशन का उपयोग कर सकते हैं।\n+\n+1. [code.visualstudio.com](https://code.visualstudio.com?WT.mc_id=academic-40229-cxa) पर नेविगेट करें और विजुअल स्टूडियो कोड इंस्टॉल करने के लिए निर्देशों का पालन करें\n+1. मार्केटप्लेस पेज पर दिए निर्देशों के अनुसार [SQLite एक्सटेंशन](https://marketplace.visualstudio.com/items?itemName=alexcvzz.vscode-sqlite&WT.mc_id=academic-40229-cxa) एक्सटेंशन इंस्टॉल करें\n+\n+### डेटाबेस डाउनलोड करें और खोलें\n+\n+इसके बाद आप एक ओपन डेटाबेस डाउनलोड करेंगे।\n+\n+1. [GitHub से डेटाबेस फ़ाइल](https://raw.githubusercontent.com/Microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/airports.db) डाउनलोड करें और इसे एक निर्देशिका में सहेजें\n+1. विजुअल स्टूडियो कोड खोलें\n+1. SQLite एक्सटेंशन में डेटाबेस को **Ctl-Shift-P** (या मैक पर **Cmd-Shift-P**) चुनकर और `SQLite: Open database` टाइप करके खोलें।\n+1. **फ़ाइल से डेटाबेस चुनें** चुनें और **airports.db** फ़ाइल खोलें जिसे आपने पहले डाउनलोड किया था\n+1. डेटाबेस खोलने के बाद (आप स्क्रीन पर अपडेट नहीं देखेंगे), **Ctl-Shift-P** (या मैक पर **Cmd-Shift-P**) का चयन करके एक नई क्वेरी विंडो बनाएं। और `SQLite: new query` टाइप करना\n+\n+एक बार खुलने के बाद, नई क्वेरी विंडो का उपयोग डेटाबेस के विरुद्ध SQL कथन चलाने के लिए किया जा सकता है। डेटाबेस के विरुद्ध क्वेरी चलाने के लिए आप **Ctl-Shift-Q** (या मैक पर **Cmd-Shift-Q**) कमांड का उपयोग कर सकते हैं।\n+\n+> [!नोट] SQLite एक्सटेंशन के बारे में अधिक जानकारी के लिए, आप [दस्तावेज़ीकरण](https://marketplace.visualstudio.com/items?itemName=alexcvzz.vscode-sqlite&WT.mc_id=academic-40229-cxa) से परामर्श कर सकते हैं।\n+\n+## डेटाबेस स्कीमा\n+\n+एक डेटाबेस की स्कीमा इसकी टेबल डिजाइन और संरचना है। **airports** डेटाबेस दो तालिकाओं के रूप में, `cities`, जिसमें यूनाइटेड किंगडम और आयरलैंड के शहरों की सूची है, और `airports`, जिसमें सभी हवाई अड्डों की सूची है। क्योंकि कुछ शहरों में कई हवाई अड्डे हो सकते हैं, जानकारी संग्रहीत करने के लिए दो टेबल बनाए गए थे। इस अभ्यास में आप विभिन्न शहरों की जानकारी प्रदर्शित करने के लिए जॉइन का उपयोग करेंगे।\n+\n+| Cities           |\n+| ---------------- |\n+| id (PK, integer) |\n+| city (text)      |\n+| country (text)   |\n+\n+| Airports                         |\n+| -------------------------------- |\n+| id (PK, integer)                 |\n+| name (text)                      |\n+| code (text)                      |\n+| city_id (FK to id in **Cities**) |\n+\n+## कार्यभार\n+\n+निम्नलिखित जानकारी वापस करने के लिए प्रश्न बनाएं:\n+\n+1. `Cities` तालिका में सभी शहर के नाम\n+1. आयरलैंड के सभी शहर `Cities` तालिका . में\n+1. सभी हवाई अड्डों के नाम उनके शहर और देश के साथ\n+1. लंदन, यूनाइटेड किंगडम में सभी हवाई अड्डे\n+\n+## रूब्रिक\n+\n+| Exemplary | Adequate | Needs Improvement |",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c7,microsoft/Data-Science-For-Beginners,875971373,823802232,SachinDabhade,78628027,2022-03-10T14:52:15+00:00,2022-03-10T14:52:15+00:00,"Can you please guide me how, because I am slightly unclear about it that what commands I can use to roll back\r\n",true,1-Introduction/01-defining-data-science/README.md,NaN,"@@ -162,4 +162,4 @@ Visit [`notebook.ipynb`](/1-Introduction/01-defining-data-science/notebook.ipynb\n \n ## Credits\n \n-This lesson has been authored with ♥️ by [Dmitry Soshnikov](http://soshnikov.com)\n+This lesson has been authored with ♥️ by [Dmitry Soshnikov](http://soshnikov.com)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c8,microsoft/Data-Science-For-Beginners,875971373,823819506,SachinDabhade,78628027,2022-03-10T15:06:50+00:00,2022-03-10T15:06:50+00:00,This links aren't working in the original repository (i.e in microsoft github account),true,1-Introduction/02-ethics/translations/assignment.hi.md,NaN,"@@ -0,0 +1,19 @@\n+## डेटा एथिक्स केस स्टडी लिखें\n+\n+## निर्देश\n+\n+आपने विभिन्न [डेटा नैतिकता चुनौतियों](README?id=_2-ethics-challenges) के बारे में सीखा है और [केस स्टडीज](README?id=_3-case-studies) के कुछ उदाहरण देखे हैं जो वास्तविक दुनिया में डेटा नैतिकता चुनौतियों को दर्शाते हैं। संदर्भ",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303c9,microsoft/Data-Science-For-Beginners,875971373,823976311,jlooper,1450004,2022-03-10T17:30:44+00:00,2022-03-10T17:31:56+00:00,"That's important to know! Rather than propagating errors it would be better to fix the original. I have edited it, so you will need to do a pull to your fork and re-edit. ",true,1-Introduction/02-ethics/translations/assignment.hi.md,NaN,"@@ -0,0 +1,19 @@\n+## डेटा एथिक्स केस स्टडी लिखें\n+\n+## निर्देश\n+\n+आपने विभिन्न [डेटा नैतिकता चुनौतियों](README?id=_2-ethics-challenges) के बारे में सीखा है और [केस स्टडीज](README?id=_3-case-studies) के कुछ उदाहरण देखे हैं जो वास्तविक दुनिया में डेटा नैतिकता चुनौतियों को दर्शाते हैं। संदर्भ",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ca,microsoft/Data-Science-For-Beginners,875971373,823977173,jlooper,1450004,2022-03-10T17:31:40+00:00,2022-03-10T17:31:56+00:00,You can discard all changes made to this notebook file,true,1-Introduction/01-defining-data-science/README.md,NaN,"@@ -162,4 +162,8 @@ Visit [`notebook.ipynb`](/1-Introduction/01-defining-data-science/notebook.ipynb\n \n ## Credits\n \n+<<<<<<< HEAD",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303cb,microsoft/Data-Science-For-Beginners,875971373,824358277,SachinDabhade,78628027,2022-03-11T03:29:02+00:00,2022-03-11T03:29:02+00:00,"Ok, I have copied original file and change the changed one with it",true,1-Introduction/01-defining-data-science/README.md,NaN,"@@ -162,4 +162,4 @@ Visit [`notebook.ipynb`](/1-Introduction/01-defining-data-science/notebook.ipynb\n \n ## Credits\n \n-This lesson has been authored with ♥️ by [Dmitry Soshnikov](http://soshnikov.com)\n+This lesson has been authored with ♥️ by [Dmitry Soshnikov](http://soshnikov.com)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303cc,microsoft/Data-Science-For-Beginners,875971373,824367164,SachinDabhade,78628027,2022-03-11T03:56:40+00:00,2022-03-11T03:56:41+00:00,"Done, I have done it",true,1-Introduction/01-defining-data-science/translations/README.hi.md,4,"@@ -0,0 +1,164 @@\n+# डेटा विज्ञान को परिभाषित करना\n+\n+| ![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/01-Definitions.png) |\n+| :----------------------------------------------------------------------------------------------------: |",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303cd,microsoft/Data-Science-For-Beginners,875971373,824367921,SachinDabhade,78628027,2022-03-11T03:59:04+00:00,2022-03-11T03:59:04+00:00,"My apologies jlooper, actualy I am new to open-source and github so thanks for your support and guidance",true,1-Introduction/02-ethics/translations/assignment.hi.md,NaN,"@@ -0,0 +1,19 @@\n+## डेटा एथिक्स केस स्टडी लिखें\n+\n+## निर्देश\n+\n+आपने विभिन्न [डेटा नैतिकता चुनौतियों](README?id=_2-ethics-challenges) के बारे में सीखा है और [केस स्टडीज](README?id=_3-case-studies) के कुछ उदाहरण देखे हैं जो वास्तविक दुनिया में डेटा नैतिकता चुनौतियों को दर्शाते हैं। संदर्भ",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ce,microsoft/Data-Science-For-Beginners,875971373,824369705,SachinDabhade,78628027,2022-03-11T04:04:20+00:00,2022-03-11T04:04:20+00:00,Changes are done,true,2-Working-With-Data/05-relational-databases/translations/assignment.hi.md,NaN,"@@ -0,0 +1,59 @@\n+# हवाईअड्डा डेटा प्रदर्शित करना\n+\n+आपको एक [डेटाबेस](https://raw.githubusercontent.com/Microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/airports.db)प्रदान किया जायेगा। बनाया गया है [SQLite](https://sqlite.org/index.html) पर जिसमें हवाई अड्डों के बारे में जानकारी होती है। स्कीमा नीचे प्रदर्शित किया गया है। आप [विजुअल स्टूडियो कोड](https://code.) में [SQLite एक्सटेंशन](https://marketplace.visualstudio.com/items?itemName=alexcvzz.vscode-sqlite&WT.mc_id=academic-40229-cxa) का इस्तेमाल करेंगे। Visualstudio.com?WT.mc_id=academic-40229-cxa) विभिन्न शहरों के हवाई अड्डों के बारे में जानकारी प्रदर्शित करने के लिए।\n+\n+## निर्देश\n+\n+असाइनमेंट के साथ आरंभ करने के लिए, आपको कुछ चरणों का पालन करना होगा। आपको कुछ टूलींग स्थापित करने और नमूना डेटाबेस डाउनलोड करने की आवश्यकता होगी।\n+\n+### अपना सिस्टम सेटअप करें\n+\n+आप डेटाबेस के साथ इंटरैक्ट करने के लिए विजुअल स्टूडियो कोड और SQLite एक्सटेंशन का उपयोग कर सकते हैं।\n+\n+1. [code.visualstudio.com](https://code.visualstudio.com?WT.mc_id=academic-40229-cxa) पर नेविगेट करें और विजुअल स्टूडियो कोड इंस्टॉल करने के लिए निर्देशों का पालन करें\n+1. मार्केटप्लेस पेज पर दिए निर्देशों के अनुसार [SQLite एक्सटेंशन](https://marketplace.visualstudio.com/items?itemName=alexcvzz.vscode-sqlite&WT.mc_id=academic-40229-cxa) एक्सटेंशन इंस्टॉल करें\n+\n+### डेटाबेस डाउनलोड करें और खोलें\n+\n+इसके बाद आप एक ओपन डेटाबेस डाउनलोड करेंगे।\n+\n+1. [GitHub से डेटाबेस फ़ाइल](https://raw.githubusercontent.com/Microsoft/Data-Science-For-Beginners/main/2-Working-With-Data/05-relational-databases/airports.db) डाउनलोड करें और इसे एक निर्देशिका में सहेजें\n+1. विजुअल स्टूडियो कोड खोलें\n+1. SQLite एक्सटेंशन में डेटाबेस को **Ctl-Shift-P** (या मैक पर **Cmd-Shift-P**) चुनकर और `SQLite: Open database` टाइप करके खोलें।\n+1. **फ़ाइल से डेटाबेस चुनें** चुनें और **airports.db** फ़ाइल खोलें जिसे आपने पहले डाउनलोड किया था\n+1. डेटाबेस खोलने के बाद (आप स्क्रीन पर अपडेट नहीं देखेंगे), **Ctl-Shift-P** (या मैक पर **Cmd-Shift-P**) का चयन करके एक नई क्वेरी विंडो बनाएं। और `SQLite: new query` टाइप करना\n+\n+एक बार खुलने के बाद, नई क्वेरी विंडो का उपयोग डेटाबेस के विरुद्ध SQL कथन चलाने के लिए किया जा सकता है। डेटाबेस के विरुद्ध क्वेरी चलाने के लिए आप **Ctl-Shift-Q** (या मैक पर **Cmd-Shift-Q**) कमांड का उपयोग कर सकते हैं।\n+\n+> [!नोट] SQLite एक्सटेंशन के बारे में अधिक जानकारी के लिए, आप [दस्तावेज़ीकरण](https://marketplace.visualstudio.com/items?itemName=alexcvzz.vscode-sqlite&WT.mc_id=academic-40229-cxa) से परामर्श कर सकते हैं।\n+\n+## डेटाबेस स्कीमा\n+\n+एक डेटाबेस की स्कीमा इसकी टेबल डिजाइन और संरचना है। **airports** डेटाबेस दो तालिकाओं के रूप में, `cities`, जिसमें यूनाइटेड किंगडम और आयरलैंड के शहरों की सूची है, और `airports`, जिसमें सभी हवाई अड्डों की सूची है। क्योंकि कुछ शहरों में कई हवाई अड्डे हो सकते हैं, जानकारी संग्रहीत करने के लिए दो टेबल बनाए गए थे। इस अभ्यास में आप विभिन्न शहरों की जानकारी प्रदर्शित करने के लिए जॉइन का उपयोग करेंगे।\n+\n+| Cities           |\n+| ---------------- |\n+| id (PK, integer) |\n+| city (text)      |\n+| country (text)   |\n+\n+| Airports                         |\n+| -------------------------------- |\n+| id (PK, integer)                 |\n+| name (text)                      |\n+| code (text)                      |\n+| city_id (FK to id in **Cities**) |\n+\n+## कार्यभार\n+\n+निम्नलिखित जानकारी वापस करने के लिए प्रश्न बनाएं:\n+\n+1. `Cities` तालिका में सभी शहर के नाम\n+1. आयरलैंड के सभी शहर `Cities` तालिका . में\n+1. सभी हवाई अड्डों के नाम उनके शहर और देश के साथ\n+1. लंदन, यूनाइटेड किंगडम में सभी हवाई अड्डे\n+\n+## रूब्रिक\n+\n+| Exemplary | Adequate | Needs Improvement |",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303cf,microsoft/Data-Science-For-Beginners,875971373,824803792,jlooper,1450004,2022-03-11T15:06:16+00:00,2022-03-11T15:07:39+00:00,This is going to 404 because it should now point to README.hi.md#3-case-studies,true,1-Introduction/02-ethics/translations/assignment.hi.md,NaN,"@@ -0,0 +1,19 @@\n+## डेटा एथिक्स केस स्टडी लिखें\n+\n+## निर्देश\n+\n+आपने विभिन्न [डेटा नैतिकता चुनौतियों](README.md#2-ethics-challenges) के बारे में सीखा है और [केस स्टडीज](README.md#3-case-studies) के कुछ उदाहरण देखे हैं जो वास्तविक दुनिया में डेटा नैतिकता चुनौतियों को दर्शाते हैं। संदर्भ",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d0,microsoft/Data-Science-For-Beginners,875971373,824804882,jlooper,1450004,2022-03-11T15:07:23+00:00,2022-03-11T15:07:39+00:00,This is still a 404! Please go through every single link and every single image in this PR and link them properly,true,1-Introduction/04-stats-and-probability/translations/README.hi.md,NaN,"@@ -0,0 +1,268 @@\n+# सांख्यिकी और संभाव्यता का संक्षिप्त परिचय\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/04-Statistics-Probability.png)|\n+|:---:|\n+| सांख्यिकी और संभावना - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+सांख्यिकी और संभाव्यता सिद्धांत गणित के दो अत्यधिक संबंधित क्षेत्र हैं जो डेटा विज्ञान के लिए अत्यधिक प्रासंगिक हैं। गणित के गहन ज्ञान के बिना डेटा के साथ काम करना संभव है, लेकिन कम से कम कुछ बुनियादी अवधारणाओं को जानना अभी भी बेहतर है। यहां हम एक संक्षिप्त परिचय प्रस्तुत करेंगे जो आपको आरंभ करने में मदद करेगा।\n+\n+[![Intro Video](images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d1,microsoft/Data-Science-For-Beginners,875971373,824830860,SachinDabhade,78628027,2022-03-11T15:36:10+00:00,2022-03-11T15:36:11+00:00,Fixed this error,true,1-Introduction/02-ethics/translations/assignment.hi.md,NaN,"@@ -0,0 +1,19 @@\n+## डेटा एथिक्स केस स्टडी लिखें\n+\n+## निर्देश\n+\n+आपने विभिन्न [डेटा नैतिकता चुनौतियों](README.md#2-ethics-challenges) के बारे में सीखा है और [केस स्टडीज](README.md#3-case-studies) के कुछ उदाहरण देखे हैं जो वास्तविक दुनिया में डेटा नैतिकता चुनौतियों को दर्शाते हैं। संदर्भ",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d2,microsoft/Data-Science-For-Beginners,875971373,824860451,SachinDabhade,78628027,2022-03-11T16:09:11+00:00,2022-03-11T16:09:12+00:00,"Fixed all finded error, if any remain then please review it.",true,1-Introduction/04-stats-and-probability/translations/README.hi.md,NaN,"@@ -0,0 +1,268 @@\n+# सांख्यिकी और संभाव्यता का संक्षिप्त परिचय\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/04-Statistics-Probability.png)|\n+|:---:|\n+| सांख्यिकी और संभावना - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+सांख्यिकी और संभाव्यता सिद्धांत गणित के दो अत्यधिक संबंधित क्षेत्र हैं जो डेटा विज्ञान के लिए अत्यधिक प्रासंगिक हैं। गणित के गहन ज्ञान के बिना डेटा के साथ काम करना संभव है, लेकिन कम से कम कुछ बुनियादी अवधारणाओं को जानना अभी भी बेहतर है। यहां हम एक संक्षिप्त परिचय प्रस्तुत करेंगे जो आपको आरंभ करने में मदद करेगा।\n+\n+[![Intro Video](images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d3,microsoft/Data-Science-For-Beginners,875971373,825479681,jlooper,1450004,2022-03-13T17:25:29+00:00,2022-03-13T17:25:29+00:00,I think you can remove this,true,2-Working-With-Data/05-relational-databases/translations/README.hi.md,NaN,"@@ -0,0 +1,181 @@\n+# डेटा के साथ कार्य करना: संबंधपरक डेटाबेस\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](/sketchnotes/05-RelationalData.png)|\n+|:---:|\n+| डेटा के साथ कार्य करना: संबंधपरक डेटाबेस - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+संभावना है कि आपने जानकारी संग्रहीत करने के लिए अतीत में एक स्प्रेडशीट का उपयोग किया है। आपके पास पंक्तियों और स्तंभों का एक सेट था, जहाँ पंक्तियों में जानकारी (या डेटा) होती थी, और स्तंभों में जानकारी (कभी-कभी मेटाडेटा कहा जाता है) का वर्णन होता था। तालिकाओं में स्तंभों और पंक्तियों के इस मूल सिद्धांत पर एक संबंधपरक डेटाबेस बनाया गया है, जिससे आप कई तालिकाओं में जानकारी फैला सकते हैं। इससे आप अधिक जटिल डेटा के साथ काम कर सकते हैं, दोहराव से बच सकते हैं, और डेटा को एक्सप्लोर करने के तरीके में लचीलापन रख सकते हैं। आइए एक रिलेशनल डेटाबेस की अवधारणाओं का पता लगाएं।\n+\n+## [प्री-लेक्चर क्विज](https://red-water-0103e7a0f.azurestaticapps.net/quiz/8)\n+\n+## यह सब टेबल से शुरू होता है\n+\n+एक रिलेशनल डेटाबेस में इसकी कोर टेबल होती है। स्प्रैडशीट की तरह ही, तालिका स्तंभों और पंक्तियों का एक संग्रह है। पंक्ति में वह डेटा या जानकारी होती है जिसके साथ हम काम करना चाहते हैं, जैसे किसी शहर का नाम या वर्षा की मात्रा। कॉलम उनके द्वारा संग्रहीत डेटा का वर्णन करते हैं।\n+\n+आइए शहरों के बारे में जानकारी संग्रहीत करने के लिए एक तालिका शुरू करके अपनी खोज शुरू करें। हम उनके नाम और देश से शुरुआत कर सकते हैं। आप इसे एक टेबल में इस प्रकार स्टोर कर सकते हैं:\n+\n+| City     | Country       |\n+| -------- | ------------- |\n+| Tokyo    | Japan         |\n+| Atlanta  | United States |\n+| Auckland | New Zealand   |\n+\n+ध्यान दें कि **शहर**, **देश** और **जनसंख्या** के कॉलम नाम संग्रहीत किए जा रहे डेटा का वर्णन करते हैं, और प्रत्येक पंक्ति में एक शहर के बारे में जानकारी होती है।\n+\n+## सिंगल टेबल अप्रोच की कमियां\n+\n+संभावना है, ऊपर दी गई तालिका आपको अपेक्षाकृत परिचित लगती है। आइए अपने बढ़ते डेटाबेस में कुछ अतिरिक्त डेटा जोड़ना शुरू करें - वार्षिक वर्षा (मिलीमीटर में)। हम वर्ष 2018, 2019 और 2020 पर ध्यान केंद्रित करेंगे। अगर हम इसे टोक्यो के लिए जोड़ते हैं, तो यह कुछ इस तरह दिख सकता है:\n+\n+| City  | Country | Year | Amount |\n+| ----- | ------- | ---- | ------ |\n+| Tokyo | Japan   | 2020 | 1690   |\n+| Tokyo | Japan   | 2019 | 1874   |\n+| Tokyo | Japan   | 2018 | 1445   |\n+\n+आप हमारी तालिका के बारे में क्या देखते हैं? आप देख सकते हैं कि हम शहर के नाम और देश को बार-बार दोहरा रहे हैं। इसमें काफी स्टोरेज लग सकता है, और इसकी कई प्रतियां रखने के लिए काफी हद तक अनावश्यक है। आखिरकार, टोक्यो का केवल एक ही नाम है जिसमें हम रुचि रखते हैं।\n+\n+ठीक है, चलो कुछ और कोशिश करते हैं। आइए प्रत्येक वर्ष के लिए नए कॉलम जोड़ें:\n+\n+| City     | Country       | 2018 | 2019 | 2020 |\n+| -------- | ------------- | ---- | ---- | ---- |\n+| Tokyo    | Japan         | 1445 | 1874 | 1690 |\n+| Atlanta  | United States | 1779 | 1111 | 1683 |\n+| Auckland | New Zealand   | 1386 | 942  | 1176 |\n+\n+हालांकि यह पंक्ति दोहराव से बचा जाता है, लेकिन यह कुछ अन्य चुनौतियों को भी जोड़ता है। हर बार नया साल आने पर हमें अपनी तालिका की संरचना को संशोधित करने की आवश्यकता होगी। इसके अतिरिक्त, जैसे-जैसे हमारा डेटा बढ़ता है, वैसे-वैसे हमारे वर्षों में कॉलम के रूप में मूल्यों को पुनः प्राप्त करना और गणना करना मुश्किल हो जाएगा।\n+\n+यही कारण है कि हमें कई तालिकाओं और संबंधों की आवश्यकता है। अपने डेटा को अलग करके हम दोहराव से बच सकते हैं और हम अपने डेटा के साथ काम करने के तरीके में अधिक लचीलापन रखते हैं।\n+\n+## रिश्तों की अवधारणा\n+\n+आइए अपने डेटा पर वापस लौटें और निर्धारित करें कि हम चीजों को कैसे विभाजित करना चाहते हैं। हम जानते हैं कि हम अपने शहरों के लिए नाम और देश को संग्रहित करना चाहते हैं, इसलिए यह शायद एक टेबल में सबसे अच्छा काम करेगा।\n+\n+| City     | Country       |\n+| -------- | ------------- |\n+| Tokyo    | Japan         |\n+| Atlanta  | United States |\n+| Auckland | New Zealand   |\n+\n+लेकिन इससे पहले कि हम अगली तालिका बनाएं, हमें यह पता लगाना होगा कि प्रत्येक शहर को कैसे संदर्भित किया जाए। हमें किसी पहचानकर्ता, आईडी या (तकनीकी डेटाबेस के संदर्भ में) प्राथमिक कुंजी की आवश्यकता है। प्राथमिक कुंजी एक मान है जिसका उपयोग किसी तालिका में एक विशिष्ट पंक्ति की पहचान करने के लिए किया जाता है। हालांकि यह स्वयं एक मूल्य पर आधारित हो सकता है (उदाहरण के लिए, हम शहर के नाम का उपयोग कर सकते हैं), यह लगभग हमेशा एक संख्या या अन्य पहचानकर्ता होना चाहिए। हम नहीं चाहते कि आईडी कभी बदले क्योंकि इससे रिश्ता टूट जाएगा। आप ज्यादातर मामलों में पाएंगे कि प्राथमिक कुंजी या आईडी एक स्वतः उत्पन्न संख्या होगी।\n+\n+> प्राथमिक कुंजी को अक्सर पीके के रूप में संक्षिप्त किया जाता है\n+\n+### शहरों\n+\n+| city_id | City     | Country       |\n+| ------- | -------- | ------------- |\n+| 1       | Tokyo    | Japan         |\n+| 2       | Atlanta  | United States |\n+| 3       | Auckland | New Zealand   |\n+\n+> ✅ आप देखेंगे कि हम इस पाठ के दौरान ""आईडी"" और ""प्राथमिक कुंजी"" शब्दों का परस्पर उपयोग करते हैं। यहाँ की अवधारणाएँ DataFrames पर लागू होती हैं, जिन्हें आप बाद में एक्सप्लोर करेंगे। डेटाफ़्रेम ""प्राथमिक कुंजी"" की शब्दावली का उपयोग नहीं करते हैं, हालांकि आप देखेंगे कि वे उसी तरह से बहुत अधिक व्यवहार करते हैं।\n+\n+हमारे शहरों की तालिका बनाने के साथ, आइए वर्षा को संग्रहित करें। हम शहर के बारे में पूरी जानकारी की नकल करने के बजाय आईडी का उपयोग कर सकते हैं। हमें यह भी सुनिश्चित करना चाहिए कि नई बनाई गई तालिका में एक *id* कॉलम भी हो, क्योंकि सभी तालिकाओं में एक आईडी या प्राथमिक कुंजी होनी चाहिए।\n+\n+### बारिश\n+\n+| rainfall_id | city_id | Year | Amount |\n+| ----------- | ------- | ---- | ------ |\n+| 1           | 1       | 2018 | 1445   |\n+| 2           | 1       | 2019 | 1874   |\n+| 3           | 1       | 2020 | 1690   |\n+| 4           | 2       | 2018 | 1779   |\n+| 5           | 2       | 2019 | 1111   |\n+| 6           | 2       | 2020 | 1683   |\n+| 7           | 3       | 2018 | 1386   |\n+| 8           | 3       | 2019 | 942    |\n+| 9           | 3       | 2020 | 1176   |\n+\n+नव निर्मित **वर्षा** तालिका के अंदर **city_id** कॉलम पर ध्यान दें। इस कॉलम में वे मान हैं जो **शहरों** तालिका में आईडी का संदर्भ देते हैं। तकनीकी संबंधपरक डेटा के संदर्भ में, इसे **विदेशी कुंजी** कहा जाता है; यह किसी अन्य तालिका से प्राथमिक कुंजी है। आप इसे केवल एक संदर्भ या सूचक के रूप में सोच सकते हैं। **सिटी_आईडी** 1 संदर्भ टोक्यो।\n+\n+> [!नोट] विदेशी कुंजी को अक्सर FK . के रूप में संक्षिप्त किया जाता है\n+\n+## डेटा पुनर्प्राप्त करना\n+\n+हमारे डेटा को दो तालिकाओं में विभाजित करके, आप सोच रहे होंगे कि हम इसे कैसे पुनः प्राप्त करते हैं। यदि हम एक रिलेशनल डेटाबेस जैसे MySQL, SQL सर्वर या Oracle का उपयोग कर रहे हैं, तो हम स्ट्रक्चर्ड क्वेरी लैंग्वेज या SQL नामक भाषा का उपयोग कर सकते हैं। SQL (कभी-कभी उच्चारित अगली कड़ी) एक मानक भाषा है जिसका उपयोग रिलेशनल डेटाबेस में डेटा को पुनः प्राप्त करने और संशोधित करने के लिए किया जाता है।\n+\n+डेटा पुनर्प्राप्त करने के लिए आप `SELECT` कमांड का उपयोग करते हैं। इसके मूल में, आप उन स्तंभों को **चुनते हैं जिन्हें आप देखना चाहते हैं **से** उस तालिका में जिसमें वे शामिल हैं। यदि आप केवल शहरों के नाम प्रदर्शित करना चाहते हैं, तो आप निम्न का उपयोग कर सकते हैं:\n+\n+```sql\n+SELECT city\n+FROM cities;\n+\n+-- Output:\n+-- Tokyo\n+-- Atlanta\n+-- Auckland\n+```\n+\n+`चयन` वह जगह है जहां आप कॉलम सूचीबद्ध करते हैं, और `से` वह जगह है जहां आप टेबल सूचीबद्ध करते हैं।\n+\n+> [नोट] SQL सिंटैक्स केस-असंवेदनशील है, जिसका अर्थ है `चुनें` और `चयन` का मतलब एक ही है। हालाँकि, आप जिस प्रकार के डेटाबेस का उपयोग कर रहे हैं उसके आधार पर कॉलम और टेबल केस संवेदनशील हो सकते हैं। नतीजतन, प्रोग्रामिंग में हर चीज को हमेशा केस संवेदी की तरह व्यवहार करना सबसे अच्छा अभ्यास है। SQL क्वेरी लिखते समय सामान्य परंपरा यह है कि कीवर्ड को सभी अपर-केस अक्षरों में रखा जाए।\n+\n+उपरोक्त क्वेरी सभी शहरों को प्रदर्शित करेगी। आइए कल्पना करें कि हम केवल न्यूजीलैंड में शहरों को प्रदर्शित करना चाहते थे। हमें किसी प्रकार के फ़िल्टर की आवश्यकता है। इसके लिए SQL कीवर्ड `WHERE` या ""जहां कुछ सच है"" है।\n+\n+```sql\n+SELECT city\n+FROM cities\n+WHERE country = 'New Zealand';\n+\n+-- Output:\n+-- Auckland\n+```\n+\n+## डेटा में शामिल होना\n+\n+अब तक हमने एक ही टेबल से डेटा रिकवर किया है। अब हम **शहरों** और **वर्षा** दोनों के डेटा को एक साथ लाना चाहते हैं। यह उन्हें एक साथ *शामिल करके* किया जाता है। आप प्रभावी रूप से दो तालिकाओं के बीच एक सीम बनाएंगे, और प्रत्येक तालिका से एक कॉलम के मानों का मिलान करेंगे।\n+\n+हमारे उदाहरण में, हम **वर्षा** में **शहर_आईडी** कॉलम का मिलान **शहरों** में **शहर_आईडी** कॉलम से करेंगे। यह अपने संबंधित शहर के साथ वर्षा मूल्य से मेल खाएगा। हम जिस प्रकार के जॉइन करेंगे, उसे *इनर* जॉइन कहा जाता है, जिसका अर्थ है कि यदि कोई पंक्तियाँ दूसरी टेबल की किसी भी चीज़ से मेल नहीं खाती हैं तो वे प्रदर्शित नहीं होंगी। हमारे मामले में हर शहर में बारिश होती है, इसलिए सब कुछ प्रदर्शित किया जाएगा।\n+\n+आइए हमारे सभी शहरों के लिए 2019 की वर्षा को पुनः प्राप्त करें।\n+\n+हम इसे चरणों में करने जा रहे हैं। पहला कदम सीम के लिए कॉलम को इंगित करके डेटा को एक साथ जोड़ना है - **city_id** जैसा कि पहले हाइलाइट किया गया था।\n+\n+```sql\n+SELECT cities.city\n+    rainfall.amount\n+FROM cities\n+    INNER JOIN rainfall ON cities.city_id = rainfall.city_id\n+```\n+\n+हमने उन दो कॉलमों को हाइलाइट किया है जो हम चाहते हैं, और तथ्य यह है कि हम टेबल्स को **city_id** द्वारा एक साथ जोड़ना चाहते हैं। अब हम केवल वर्ष 2019 को फ़िल्टर करने के लिए `WHERE` स्टेटमेंट जोड़ सकते हैं।\n+\n+```sql\n+SELECT cities.city\n+    rainfall.amount\n+FROM cities\n+    INNER JOIN rainfall ON cities.city_id = rainfall.city_id\n+WHERE rainfall.year = 2019\n+\n+-- Output\n+\n+-- city     | amount\n+-- -------- | ------\n+-- Tokyo    | 1874\n+-- Atlanta  | 1111\n+-- Auckland |  942\n+```\n+\n+## सारांश\n+\n+रिलेशनल डेटाबेस कई तालिकाओं के बीच सूचनाओं को विभाजित करने के आसपास केंद्रित होते हैं जिन्हें बाद में प्रदर्शन और विश्लेषण के लिए एक साथ लाया जाता है। यह गणना करने और अन्यथा डेटा में हेरफेर करने के लिए उच्च स्तर की लचीलापन प्रदान करता है। आपने रिलेशनल डेटाबेस की मूल अवधारणाओं को देखा है, और दो तालिकाओं के बीच जुड़ने का तरीका देखा है।\n+\n+## चुनौती\n+\n+इंटरनेट पर कई रिलेशनल डेटाबेस उपलब्ध हैं। आप ऊपर सीखे गए कौशल का उपयोग करके डेटा का पता लगा सकते हैं।\n+\n+## व्याख्यान के बाद प्रश्नोत्तरी",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d4,microsoft/Data-Science-For-Beginners,875971373,825479750,jlooper,1450004,2022-03-13T17:26:09+00:00,2022-03-13T17:26:09+00:00,Let's use a standard vocabulary to link the Pre-lecture and Post-lecture quizzes,true,1-Introduction/04-stats-and-probability/translations/README.hi.md,NaN,"@@ -0,0 +1,268 @@\n+# सांख्यिकी और संभाव्यता का संक्षिप्त परिचय\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev)](/sketchnotes/04-Statistics-Probability.png)|\n+|:---:|\n+| सांख्यिकी और संभावना - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+सांख्यिकी और संभाव्यता सिद्धांत गणित के दो अत्यधिक संबंधित क्षेत्र हैं जो डेटा विज्ञान के लिए अत्यधिक प्रासंगिक हैं। गणित के गहन ज्ञान के बिना डेटा के साथ काम करना संभव है, लेकिन कम से कम कुछ बुनियादी अवधारणाओं को जानना अभी भी बेहतर है। यहां हम एक संक्षिप्त परिचय प्रस्तुत करेंगे जो आपको आरंभ करने में मदद करेगा।\n+\n+[![Intro Video](/1-Introduction/04-stats-and-probability/images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)\n+\n+\n+## [Pre-lecture quiz](https://red-water-0103e7a0f.azurestaticapps.net/quiz/6)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d5,microsoft/Data-Science-For-Beginners,875971373,825479966,jlooper,1450004,2022-03-13T17:28:04+00:00,2022-03-13T17:28:04+00:00,malformed link here,true,1-Introduction/04-stats-and-probability/translations/README.hi.md,NaN,"@@ -0,0 +1,268 @@\n+# सांख्यिकी और संभाव्यता का संक्षिप्त परिचय\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev)](/sketchnotes/04-Statistics-Probability.png)|\n+|:---:|\n+| सांख्यिकी और संभावना - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+सांख्यिकी और संभाव्यता सिद्धांत गणित के दो अत्यधिक संबंधित क्षेत्र हैं जो डेटा विज्ञान के लिए अत्यधिक प्रासंगिक हैं। गणित के गहन ज्ञान के बिना डेटा के साथ काम करना संभव है, लेकिन कम से कम कुछ बुनियादी अवधारणाओं को जानना अभी भी बेहतर है। यहां हम एक संक्षिप्त परिचय प्रस्तुत करेंगे जो आपको आरंभ करने में मदद करेगा।\n+\n+[![Intro Video](/1-Introduction/04-stats-and-probability/images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)\n+\n+\n+## [Pre-lecture quiz](https://red-water-0103e7a0f.azurestaticapps.net/quiz/6)\n+\n+## प्रायिकता और यादृच्छिक चर\n+\n+**प्रायिकता** 0 और 1 के बीच की एक संख्या है जो यह व्यक्त करती है कि **ईवेंट** कितनी संभावित है। इसे कई सकारात्मक परिणामों के रूप में परिभाषित किया गया है (जो घटना की ओर ले जाते हैं), परिणामों की कुल संख्या से विभाजित, यह देखते हुए कि सभी परिणाम समान रूप से संभावित हैं। उदाहरण के लिए, जब हम एक पासे को उछालते हैं, तो हमें एक सम संख्या प्राप्त होने की प्रायिकता 3/6 = 0.5 होती है।\n+\n+जब हम घटनाओं के बारे में बात करते हैं, तो हम **यादृच्छिक चर** का उपयोग करते हैं। उदाहरण के लिए, यादृच्छिक चर जो एक पासे को घुमाते समय प्राप्त संख्या का प्रतिनिधित्व करता है, 1 से 6 तक मान लेगा। 1 से 6 तक की संख्याओं के सेट को **नमूना स्थान** कहा जाता है। हम एक निश्चित मान लेने वाले यादृच्छिक चर की संभावना के बारे में बात कर सकते हैं, उदाहरण के लिए पी (एक्स = 3) = 1/6।\n+\n+पिछले उदाहरण में यादृच्छिक चर को **असतत** कहा जाता है, क्योंकि इसमें एक गणनीय नमूना स्थान होता है, अर्थात अलग-अलग मान होते हैं जिन्हें गिना जा सकता है। ऐसे मामले हैं जब नमूना स्थान वास्तविक संख्याओं की एक श्रृंखला है, या वास्तविक संख्याओं का पूरा सेट है। ऐसे चरों को **सतत** कहा जाता है। एक अच्छा उदाहरण वह समय है जब बस आती है।\n+\n+## प्रायिकता वितरण\n+\n+असतत यादृच्छिक चर के मामले में, फ़ंक्शन P(X) द्वारा प्रत्येक घटना की प्रायिकता का वर्णन करना आसान है। नमूना स्थान *S* से प्रत्येक मान *s* के लिए यह 0 से 1 तक की संख्या देगा, जैसे कि सभी घटनाओं के लिए P(X=s) के सभी मानों का योग 1 होगा।\n+\n+सबसे प्रसिद्ध असतत वितरण **समान वितरण** है, जिसमें N तत्वों का एक नमूना स्थान होता है, जिनमें से प्रत्येक के लिए 1/N की समान संभावना होती है।\n+\n+एक सतत चर के संभाव्यता वितरण का वर्णन करना अधिक कठिन है, कुछ अंतराल [ए, बी], या वास्तविक संख्याओं के पूरे सेट से लिए गए मानों के साथ &Ropf;। बस आगमन समय के मामले पर विचार करें। वास्तव में, प्रत्येक सटीक आगमन समय *t* के लिए, ठीक उसी समय पर बस के आने की प्रायिकता 0 है!\n+\n+> अब आप जानते हैं कि 0 प्रायिकता वाली घटनाएँ होती हैं, और बहुत बार! कम से कम हर बार जब बस आती है!\n+\n+हम केवल दिए गए मानों के अंतराल में एक चर के गिरने की प्रायिकता के बारे में बात कर सकते हैं, उदाहरण के लिए। P(t<sub>1</sub>&le;X&lt;t<sub>2</sub>)। इस मामले में, प्रायिकता बंटन को **प्रायिकता घनत्व फलन** p(x) द्वारा वर्णित किया जाता है, जैसे कि\n+\n+![P(t_1\le X<t_2)=\int_{t_1}^{t_2}p(x)dx](/1-Introduction/04-stats-and-probability/images/probability-density.png)\n+  \n+एकसमान वितरण के एक सतत एनालॉग को **निरंतर वर्दी** कहा जाता है, जिसे एक सीमित अंतराल पर परिभाषित किया जाता है। एक संभावना है कि मान X लंबाई l के अंतराल में आता है l के समानुपाती है, और 1 तक बढ़ जाता है।\n+\n+एक अन्य महत्वपूर्ण वितरण **सामान्य वितरण** है, जिसके बारे में हम नीचे विस्तार से बात करेंगे।\n+\n+## माध्य, प्रसरण और मानक विचलन\n+मान लीजिए कि हम एक यादृच्छिक चर X के n नमूनों का एक क्रम बनाते हैं: x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>। हम पारंपरिक तरीके से अनुक्रम के **माध्य** (या **अंकगणित औसत**) मान को परिभाषित कर सकते हैं (x<sub>1</sub>+x<sub>2</sub>+x<sub >एन</उप>)/एन। जैसे-जैसे हम नमूने का आकार बढ़ाते हैं (अर्थात n&rr;&infin; के साथ सीमा लेते हैं), हम वितरण का माध्य (जिसे **अपेक्षा** भी कहते हैं) प्राप्त करेंगे। हम उम्मीद को **E**(x) से निरूपित करेंगे।\n+> यह प्रदर्शित किया जा सकता है कि मूल्यों के साथ किसी भी असतत वितरण के लिए {x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>N</sub>} and corresponding probabilities p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>N</sub>, the expectation would equal to E(X)=x<sub>1</sub>p<sub>1</sub>+x<sub>2</sub>p<sub>2</sub>+...+x<sub>N</sub>p<sub>N</sub>.\n+\n+यह पहचानने के लिए कि मान कितनी दूर तक फैले हुए हैं, हम प्रसरण की गणना कर सकते हैं &sigma;<sup>2</sup> = &sum;(x<sub>i</sub> - &mu;)<sup>2</sup>/ एन, जहां & एमयू; अनुक्रम का माध्य है। मूल्य &सिग्मा; इसे **मानक विचलन** कहा जाता है, और &sigma;<sup>2</sup> को **विचरण** कहा जाता है।\n+\n+## बहुलक, माध्यिका और चतुर्थक\n+\n+कभी-कभी, माध्य डेटा के लिए ""विशिष्ट"" मान का पर्याप्त रूप से प्रतिनिधित्व नहीं करता है। उदाहरण के लिए, जब कुछ चरम मान पूरी तरह से सीमा से बाहर होते हैं, तो वे माध्य को प्रभावित कर सकते हैं। एक और अच्छा संकेत एक **माध्य** है, एक मान ऐसा है कि आधा डेटा बिंदु इससे कम है, और दूसरा आधा - अधिक है।\n+\n+डेटा के वितरण को समझने में हमारी मदद करने के लिए, **चतुर्थक** के बारे में बात करना मददगार होगा:\n+\n+* प्रथम चतुर्थक, या Q1, एक मान है, जैसे कि 25% डेटा इससे नीचे आता है\n+* तीसरा चतुर्थक, या Q3, एक मान है कि 75% डेटा इससे नीचे आता है\n+\n+ग्राफिक रूप से हम **बॉक्स प्लॉट** नामक आरेख में माध्यिका और चतुर्थक के बीच संबंध का प्रतिनिधित्व कर सकते हैं:\n+\n+<img src=""/1-Introduction/04-stats-and-probability/images/boxplot_explanation.png"" width=""50%""/>\n+\n+यहां हम **अंतर-चतुर्थक श्रेणी** IQR=Q3-Q1, और तथाकथित **आउटलेयर** - मानों की भी गणना करते हैं, जो सीमाओं के बाहर होते हैं [Q1-1.5*IQR,Q3+1.5*IQR]।\n+\n+परिमित वितरण के लिए जिसमें कम संख्या में संभावित मान होते हैं, एक अच्छा ""विशिष्ट"" मान वह होता है जो सबसे अधिक बार प्रकट होता है, जिसे **मोड** कहा जाता है। इसे अक्सर रंग जैसे श्रेणीबद्ध डेटा पर लागू किया जाता है। एक ऐसी स्थिति पर विचार करें जब हमारे पास लोगों के दो समूह हों - कुछ जो लाल रंग को अधिक पसंद करते हैं, और अन्य जो नीले रंग को पसंद करते हैं। यदि हम रंगों को संख्याओं के आधार पर कोडित करते हैं, तो पसंदीदा रंग का माध्य मान नारंगी-हरे रंग के स्पेक्ट्रम में कहीं होगा, जो किसी भी समूह पर वास्तविक वरीयता को इंगित नहीं करता है। हालांकि, मोड या तो रंगों में से एक होगा, या दोनों रंग, यदि उनके लिए मतदान करने वाले लोगों की संख्या बराबर है (इस मामले में हम नमूने को **मल्टीमॉडल** कहते हैं)।\n+\n+## वास्तविक दुनिया का डेटा\n+\n+जब हम वास्तविक जीवन से डेटा का विश्लेषण करते हैं, तो वे अक्सर यादृच्छिक चर नहीं होते हैं, इस अर्थ में कि हम अज्ञात परिणाम के साथ प्रयोग नहीं करते हैं। उदाहरण के लिए, बेसबॉल खिलाड़ियों की एक टीम और उनके शरीर के डेटा, जैसे ऊंचाई, वजन और उम्र पर विचार करें। वे संख्याएँ बिल्कुल यादृच्छिक नहीं हैं, लेकिन हम अभी भी उन्हीं गणितीय अवधारणाओं को लागू कर सकते हैं। उदाहरण के लिए, लोगों के वजन के अनुक्रम को कुछ यादृच्छिक चर से निकाले गए मानों का अनुक्रम माना जा सकता है। [इस डेटासेट](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) से लिए गए [मेजर लीग बेसबॉल](http://mlb.mlb.com/index.jsp) से वास्तविक बेसबॉल खिलाड़ियों के वज़न का क्रम नीचे दिया गया ह। (आपकी सुविधा के लिए, केवल पहले 20 मान दिखाए गए हैं):\n+\n+\n+```\n+[180.0, 215.0, 210.0, 210.0, 188.0, 176.0, 209.0, 200.0, 231.0, 180.0, 188.0, 180.0, 185.0, 160.0, 180.0, 185.0, 197.0, 189.0, 185.0, 219.0]\n+```\n+\n+> **नोट**: इस डेटासेट के साथ काम करने का उदाहरण देखने के लिए, [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) पर एक नज़र डालें। इस पूरे पाठ में कई चुनौतियाँ भी हैं, और आप उस नोटबुक में कुछ कोड जोड़कर उन्हें पूरा कर सकते हैं। यदि आप सुनिश्चित नहीं हैं कि डेटा पर कैसे काम करना है, तो चिंता न करें - हम बाद में पायथन का उपयोग करके डेटा के साथ काम करने के लिए वापस आएंगे। यदि आप जुपिटर नोटबुक में कोड चलाना नहीं जानते हैं, तो [इस लेख](https://soshnikov.com/education/how-to-execute-notebooks-from-github/) पर एक नज़र डालें।\n+\n+हमारे डेटा के लिए माध्य, माध्यिका और चतुर्थक दिखाने वाला बॉक्स प्लॉट यहां दिया गया है:\n+\n+![वेट बॉक्स प्लॉट](/1-Introduction/04-stats-and-probability/images/weight-boxplot.png)\n+\n+चूंकि हमारे डेटा में अलग-अलग खिलाड़ी **भूमिकाएं** के बारे में जानकारी है, इसलिए हम भूमिका के आधार पर बॉक्स प्लॉट भी कर सकते हैं - यह हमें यह विचार प्राप्त करने की अनुमति देगा कि कैसे पैरामीटर मान भूमिकाओं में भिन्न होते हैं। इस बार हम ऊंचाई पर विचार करेंगे:\n+\n+![भूमिका के अनुसार बॉक्स प्लॉट](/1-Introduction/04-stats-and-probability/images/boxplot_byrole.png)\n+\n+यह आरेख बताता है कि, औसतन, पहले बेसमेन की ऊंचाई दूसरे बेसमेन की ऊंचाई से अधिक होती है। बाद में इस पाठ में हम सीखेंगे कि हम इस परिकल्पना का अधिक औपचारिक रूप से परीक्षण कैसे कर सकते हैं, और यह कैसे प्रदर्शित करें कि हमारा डेटा सांख्यिकीय रूप से महत्वपूर्ण है।\n+\n+> वास्तविक दुनिया के डेटा के साथ काम करते समय, हम मानते हैं कि सभी डेटा बिंदु कुछ संभाव्यता वितरण से लिए गए नमूने हैं। यह धारणा हमें मशीन लर्निंग तकनीकों को लागू करने और कार्यशील भविष्य कहनेवाला मॉडल बनाने की अनुमति देती है।\n+\n+यह देखने के लिए कि हमारे डेटा का वितरण क्या है, हम एक ग्राफ बना सकते हैं जिसे **हिस्टोग्राम** कहा जाता है। एक्स-अक्ष में कई अलग-अलग वज़न अंतराल (तथाकथित **बिन्स**) होंगे, और ऊर्ध्वाधर अक्ष दिखाएगा कि हमारा यादृच्छिक चर नमूना किसी दिए गए अंतराल के अंदर कितनी बार था।\n+\n+![वास्तविक विश्व डेटा का हिस्टोग्राम](/1-Introduction/04-stats-and-probability/images/weight-histogram.png)\n+\n+इस हिस्टोग्राम से आप देख सकते हैं कि सभी मान निश्चित औसत वजन के आसपास केंद्रित होते हैं, और हम उस वजन से जितना आगे जाते हैं - उस मान के कम वजन का सामना करना पड़ता है। यानी, यह बहुत ही असंभव है कि बेसबॉल खिलाड़ी का वजन औसत वजन से बहुत अलग होगा। भार में भिन्नता यह दर्शाती है कि भार किस हद तक माध्य से भिन्न होने की संभावना है।\n+\n+> अगर हम बेसबॉल लीग से नहीं, बल्कि अन्य लोगों का वजन लेते हैं, तो वितरण अलग होने की संभावना है। हालाँकि, वितरण का आकार समान होगा, लेकिन माध्य और विचरण बदल जाएगा। इसलिए, यदि हम अपने मॉडल को बेसबॉल खिलाड़ियों पर प्रशिक्षित करते हैं, तो विश्वविद्यालय के छात्रों पर लागू होने पर यह गलत परिणाम देने की संभावना है, क्योंकि अंतर्निहित वितरण अलग है।\n+\n+## सामान्य वितरण\n+\n+वजन का वितरण जो हमने ऊपर देखा है वह बहुत विशिष्ट है, और वास्तविक दुनिया से कई माप एक ही प्रकार के वितरण का पालन करते हैं, लेकिन अलग-अलग माध्य और भिन्नता के साथ। इस वितरण को **सामान्य वितरण** कहा जाता है, और यह आंकड़ों में बहुत महत्वपूर्ण भूमिका निभाता है।\n+\n+सामान्य वितरण का उपयोग करना संभावित बेसबॉल खिलाड़ियों के यादृच्छिक भार उत्पन्न करने का एक सही तरीका है। एक बार जब हम माध्य वजन `माध्य` और मानक विचलन `एसटीडी` जान लेते हैं, तो हम निम्नलिखित तरीके से 1000 वजन के नमूने तैयार कर सकते हैं:\n+```python\n+samples = np.random.normal(mean,std,1000)\n+```\n+\n+यदि हम उत्पन्न नमूनों के हिस्टोग्राम की साजिश करते हैं तो हम ऊपर दिखाए गए चित्र के समान ही चित्र देखेंगे। और अगर हम नमूनों की संख्या और डिब्बे की संख्या में वृद्धि करते हैं, तो हम एक सामान्य वितरण की एक तस्वीर उत्पन्न कर सकते हैं जो आदर्श के अधिक करीब है:\n+\n+![माध्य = 0 और एसटीडी.देव = 1 के साथ सामान्य वितरण](/1-Introduction/04-stats-and-probability/images/normal-histogram.png)\n+\n+\n+*माध्य = 0 और std.dev=1* के साथ सामान्य वितरण\n+\n+## विश्वास अंतराल\n+\n+जब हम बेसबॉल खिलाड़ियों के वजन के बारे में बात करते हैं, तो हम मानते हैं कि कुछ निश्चित **यादृच्छिक चर W** है जो सभी बेसबॉल खिलाड़ियों (तथाकथित **जनसंख्या**) के वजन के आदर्श संभाव्यता वितरण से मेल खाती है। वजन का हमारा क्रम सभी बेसबॉल खिलाड़ियों के एक उपसमुच्चय से मेल खाता है जिसे हम **नमूना** कहते हैं। एक दिलचस्प सवाल यह है कि क्या हम डब्ल्यू के वितरण के मापदंडों को जान सकते हैं, यानी जनसंख्या का माध्य और विचरण?\n+\n+सबसे आसान उत्तर हमारे नमूने के माध्य और विचरण की गणना करना होगा। हालाँकि, ऐसा हो सकता है कि हमारा यादृच्छिक नमूना पूर्ण जनसंख्या का सटीक रूप से प्रतिनिधित्व नहीं करता है। इस प्रकार **कॉन्फिडेंस इंटरवल** के बारे में बात करना समझ में आता है।\n+\n+> **विश्वास अंतराल** हमारे नमूने को देखते हुए जनसंख्या के वास्तविक माध्य का अनुमान है, जो एक निश्चित प्रायिकता (या **विश्वास का स्तर**) सटीक है।\n+\n+मान लीजिए हमारे पास हमारे वितरण से एक नमूना X<sub>1</sub>, ..., X<sub>n</sub> है। हर बार जब हम अपने वितरण से एक नमूना लेते हैं, तो हम अलग-अलग माध्य मान के साथ समाप्त होते हैं। इस प्रकार &mu; एक यादृच्छिक चर माना जा सकता है। एक **विश्वास अंतराल** विश्वास के साथ p मानों की एक जोड़ी है (L<sub>p</sub>,R<sub>p</sub>), जैसे कि **P**(L<sub>p </sub>&leq;&mu;&leq;R<sub>p</sub>) = p, यानी अंतराल के भीतर मापे गए माध्य मान के गिरने की प्रायिकता p के बराबर होती है।\n+\n+यह विस्तार से चर्चा करने के लिए हमारे संक्षिप्त परिचय से परे है कि उन आत्मविश्वास अंतराल की गणना कैसे की जाती है। कुछ और विवरण [विकिपीडिया पर](https://en.wikipedia.org/wiki/Confidence_interval) देखे जा सकते हैं। संक्षेप में, हम जनसंख्या के वास्तविक माध्य के सापेक्ष परिकलित नमूना माध्य के वितरण को परिभाषित करते हैं, जिसे **छात्र वितरण** कहा जाता है।\n+\n+> **दिलचस्प तथ्य**: छात्र वितरण का नाम गणितज्ञ विलियम सीली गॉसेट के नाम पर रखा गया है, जिन्होंने छद्म नाम ""स्टूडेंट"" के तहत अपना पेपर प्रकाशित किया था। उन्होंने गिनीज शराब की भठ्ठी में काम किया, और, एक संस्करण के अनुसार, उनके नियोक्ता नहीं चाहते थे कि आम जनता को पता चले कि वे कच्चे माल की गुणवत्ता निर्धारित करने के लिए सांख्यिकीय परीक्षणों का उपयोग कर रहे थे।\n+\n+यदि हम माध्य का अनुमान लगाना चाहते हैं &mu; हमारी जनसंख्या का विश्वास p के साथ, हमें छात्र वितरण A का *(1-p)/2-th प्रतिशत* लेने की आवश्यकता है, जिसे या तो तालिकाओं से लिया जा सकता है, या कंप्यूटर सांख्यिकीय सॉफ़्टवेयर के कुछ अंतर्निहित कार्यों का उपयोग करके (उदाहरण के लिए पायथन, आर, आदि)। फिर अंतराल के लिए &mu; X&pm;A*D/&radic;n द्वारा दिया जाएगा, जहां X नमूने का प्राप्त माध्य है, D मानक विचलन है।\n+\n+> **नोट**: हम [स्वतंत्रता की डिग्री](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) की एक महत्वपूर्ण अवधारणा की चर्चा को भी छोड़ देते हैं, जो छात्र वितरण के संबंध में महत्वपूर्ण है। इस अवधारणा को गहराई से समझने के लिए आप सांख्यिकी पर अधिक संपूर्ण पुस्तकों का उल्लेख कर सकते हैं।\n+\n+वजन और ऊंचाई के लिए आत्मविश्वास अंतराल की गणना का एक उदाहरण [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) में दिया गया है।\n+\n+| p | Weight mean |\n+|-----|-----------|\n+| 0.85 | 201.73±0.94 |\n+| 0.90 | 201.73±1.08 |\n+| 0.95 | 201.73±1.28 |\n+\n+ध्यान दें कि आत्मविश्वास की संभावना जितनी अधिक होगी, विश्वास अंतराल उतना ही व्यापक होगा।\n+\n+## परिकल्पना परीक्षण \n+\n+हमारे बेसबॉल खिलाड़ियों के डेटासेट में, अलग-अलग खिलाड़ी भूमिकाएँ होती हैं, जिन्हें नीचे संक्षेप में प्रस्तुत किया जा सकता है (इस तालिका की गणना कैसे की जा सकती है, यह देखने के लिए [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) देखें):\n+\n+| Role | Height | Weight | Count |\n+|------|--------|--------|-------|\n+| Catcher | 72.723684 | 204.328947 | 76 |\n+| Designated_Hitter | 74.222222 | 220.888889 | 18 |\n+| First_Baseman | 74.000000 | 213.109091 | 55 |\n+| Outfielder | 73.010309 | 199.113402 | 194 |\n+| Relief_Pitcher | 74.374603 | 203.517460 | 315 |\n+| Second_Baseman | 71.362069 | 184.344828 | 58 |\n+| Shortstop | 71.903846 | 182.923077 | 52 |\n+| Starting_Pitcher | 74.719457 | 205.163636 | 221 |\n+| Third_Baseman | 73.044444 | 200.955556 | 45 |\n+\n+हम देख सकते हैं कि पहले बेसमेन की औसत ऊंचाई दूसरे बेसमेन की तुलना में अधिक है। इस प्रकार, हम यह निष्कर्ष निकालने के लिए ललचा सकते हैं कि **पहले बेसमेन दूसरे बेसमेन से अधिक हैं**।\n+\n+> इस कथन को **एक परिकल्पना** कहा जाता है, क्योंकि हम नहीं जानते कि तथ्य वास्तव में सत्य है या नहीं।\n+\n+हालांकि, यह हमेशा स्पष्ट नहीं होता है कि क्या हम यह निष्कर्ष निकाल सकते हैं। ऊपर की चर्चा से हम जानते हैं कि प्रत्येक माध्य का एक संबद्ध विश्वास अंतराल होता है, और इस प्रकार यह अंतर केवल एक सांख्यिकीय त्रुटि हो सकता है। हमें अपनी परिकल्पना का परीक्षण करने के लिए कुछ और औपचारिक तरीके की आवश्यकता है।\n+\n+आइए पहले और दूसरे बेसमेन की ऊंचाई के लिए अलग-अलग आत्मविश्वास अंतराल की गणना करें:\n+\n+| Confidence | First Basemen | Second Basemen |\n+|------------|---------------|----------------|\n+| 0.85 | 73.62..74.38 | 71.04..71.69 |\n+| 0.90 | 73.56..74.44 | 70.99..71.73 |\n+| 0.95 | 73.47..74.53 | 70.92..71.81 |\n+\n+हम देख सकते हैं कि बिना किसी विश्वास के अंतराल ओवरलैप हो जाते हैं। इससे हमारी परिकल्पना सिद्ध होती है कि पहले बेसमेन दूसरे बेसमेन से ऊंचे होते हैं।\n+\n+अधिक औपचारिक रूप से, हम जिस समस्या को हल कर रहे हैं वह यह देखना है कि क्या **दो संभाव्यता वितरण समान हैं**, या कम से कम समान पैरामीटर हैं। वितरण के आधार पर, हमें उसके लिए विभिन्न परीक्षणों का उपयोग करने की आवश्यकता है। यदि हम जानते हैं कि हमारे वितरण सामान्य हैं, तो हम **[स्टूडेंट टी-टेस्ट](https://en.wikipedia.org/wiki/Student%27s_t-test)** लागू कर सकते हैं।\n+\n+स्टूडेंट टी-टेस्ट में, हम तथाकथित **टी-वैल्यू** की गणना करते हैं, जो भिन्नता को ध्यान में रखते हुए, साधनों के बीच अंतर को इंगित करता है। यह प्रदर्शित किया जाता है कि टी-मान **छात्र वितरण** का अनुसरण करता है, जो हमें दिए गए आत्मविश्वास स्तर **p** के लिए थ्रेशोल्ड मान प्राप्त करने की अनुमति देता है (इसकी गणना की जा सकती है, या संख्यात्मक तालिकाओं में देखा जा सकता है)। फिर हम परिकल्पना को स्वीकृत या अस्वीकार करने के लिए टी-मान की तुलना इस सीमा से करते हैं।\n+\n+पायथन में, हम **SciPy** पैकेज का उपयोग कर सकते हैं, जिसमें `ttest_ind` फ़ंक्शन शामिल है (कई अन्य उपयोगी सांख्यिकीय कार्यों के अलावा!)। यह हमारे लिए टी-वैल्यू की गणना करता है, और कॉन्फिडेंस पी-वैल्यू की रिवर्स लुकअप भी करता है, ताकि हम निष्कर्ष निकालने के लिए कॉन्फिडेंस को देख सकें।\n+\n+उदाहरण के लिए, पहले और दूसरे बेसमेन की ऊंचाई के बीच हमारी तुलना हमें निम्नलिखित परिणाम देती है:\n+\n+```python\n+from scipy.stats import ttest_ind\n+\n+tval, pval = ttest_ind(df.loc[df['Role']=='First_Baseman',['Height']], df.loc[df['Role']=='Designated_Hitter',['Height']],equal_var=False)\n+print(f""T-value = {tval[0]:.2f}\nP-value: {pval[0]}"")\n+```\n+```\n+T-value = 7.65\n+P-value: 9.137321189738925e-12\n+```\n+\n+हमारे मामले में, पी-वैल्यू बहुत कम है, जिसका अर्थ है कि इस बात का समर्थन करने वाले मजबूत सबूत हैं कि पहले बेसमेन लम्बे होते हैं।\n+\n+अन्य विभिन्न प्रकार की परिकल्पनाएँ भी हैं जिनका हम परीक्षण करना चाहते हैं, उदाहरण के लिए:\n+* यह साबित करने के लिए कि दिया गया नमूना कुछ वितरण का अनुसरण करता है। हमारे मामले में हमने मान लिया है कि ऊंचाई सामान्य रूप से वितरित की जाती है, लेकिन इसके लिए औपचारिक सांख्यिकीय सत्यापन की आवश्यकता होती है।\n+* यह सिद्ध करने के लिए कि नमूने का माध्य मान कुछ पूर्वनिर्धारित मान से मेल खाता है\n+* कई नमूनों के साधनों की तुलना करना (उदाहरण के लिए विभिन्न आयु समूहों में खुशी के स्तर में क्या अंतर है)\n+\n+## बड़ी संख्या का नियम और केंद्रीय सीमा प्रमेय\n+\n+सामान्य वितरण के इतना महत्वपूर्ण होने का एक कारण तथाकथित **केंद्रीय सीमा प्रमेय** है। मान लीजिए कि हमारे पास स्वतंत्र N मानों X<sub>1</sub>, ..., X<sub>N</sub> का एक बड़ा नमूना है, जिसे माध्य &mu; और विचरण &सिग्मा;<sup>2</sup>. फिर, पर्याप्त रूप से बड़े N के लिए (दूसरे शब्दों में, जब N&rarr;&infin;), माध्य &Sigma;<sub>i</sub>X<sub>i</sub> को सामान्य रूप से माध्य &mu के साथ वितरित किया जाएगा; और विचरण &sigma;<sup>2</sup>/N.\n+\n+> केंद्रीय सीमा प्रमेय की व्याख्या करने का एक अन्य तरीका यह कहना है कि वितरण की परवाह किए बिना, जब आप किसी भी यादृच्छिक चर मानों के योग के माध्य की गणना करते हैं तो आप सामान्य वितरण के साथ समाप्त होते हैं।\n+\n+केंद्रीय सीमा प्रमेय से यह भी पता चलता है कि, जब N&rar;&infin;, नमूने के माध्य की प्रायिकता &mu; बन जाता है 1. इसे **बड़ी संख्या का नियम** कहते हैं।\n+\n+## सहप्रसरण और सहसंबंध\n+\n+डेटा साइंस द्वारा की जाने वाली चीजों में से एक डेटा के बीच संबंध ढूंढ रहा है। हम कहते हैं कि दो अनुक्रम **सहसम्बन्ध** तब होते हैं जब वे एक ही समय में समान व्यवहार प्रदर्शित करते हैं, अर्थात वे या तो एक साथ उठते/गिरते हैं, या एक क्रम ऊपर उठता है जब दूसरा गिरता है और इसके विपरीत। दूसरे शब्दों में, दो अनुक्रमों के बीच कुछ संबंध प्रतीत होता है।\n+\n+> सहसंबंध आवश्यक रूप से दो अनुक्रमों के बीच कारण संबंध को इंगित नहीं करता है; कभी-कभी दोनों चर किसी बाहरी कारण पर निर्भर हो सकते हैं, या यह विशुद्ध रूप से संयोग से दो अनुक्रम सहसंबद्ध हो सकते हैं। हालांकि, मजबूत गणितीय सहसंबंध एक अच्छा संकेत है कि दो चर किसी न किसी तरह से जुड़े हुए हैं।\n+\n+गणितीय रूप से, मुख्य अवधारणा जो दो यादृच्छिक चर के बीच संबंध दिखाती है, वह है **सहप्रसरण**, जिसकी गणना इस प्रकार की जाती है: Cov(X,Y) = **E**\[(X-**E**(X) ))(वाई-**ई**(वाई))\]। हम दोनों चरों के विचलन की गणना उनके माध्य मानों से करते हैं, और फिर उन विचलनों के गुणनफल की गणना करते हैं। यदि दोनों चर एक साथ विचलित होते हैं, तो उत्पाद हमेशा एक सकारात्मक मूल्य होगा, जो कि सकारात्मक सहप्रसरण को जोड़ देगा। यदि दोनों चर आउट-ऑफ-सिंक विचलित हो जाते हैं (अर्थात एक औसत से नीचे गिर जाता है जब दूसरा औसत से ऊपर उठता है), तो हमें हमेशा ऋणात्मक संख्याएँ मिलेंगी, जो कि ऋणात्मक सहप्रसरण को जोड़ देंगी। यदि विचलन निर्भर नहीं हैं, तो वे लगभग शून्य तक जोड़ देंगे।\n+\n+सहप्रसरण का निरपेक्ष मान हमें यह नहीं बताता कि सहसंबंध कितना बड़ा है, क्योंकि यह वास्तविक मूल्यों के परिमाण पर निर्भर करता है। इसे सामान्य करने के लिए, हम **सहसंबंध** प्राप्त करने के लिए, दोनों चरों के मानक विचलन द्वारा सहप्रसरण को विभाजित कर सकते हैं। अच्छी बात यह है कि सहसंबंध हमेशा [-1,1] की सीमा में होता है, जहां 1 मूल्यों के बीच मजबूत सकारात्मक सहसंबंध को इंगित करता है, -1 - मजबूत नकारात्मक सहसंबंध, और 0 - बिल्कुल भी कोई संबंध नहीं (चर स्वतंत्र हैं)।\n+\n+**उदाहरण**: हम ऊपर बताए गए डेटासेट से बेसबॉल खिलाड़ियों के वज़न और ऊंचाई के बीच सहसंबंध की गणना कर सकते हैं:\n+\n+```python\n+print(np.corrcoef(weights,heights))\n+```\n+As a result, we get **correlation matrix** like this one:\n+```\n+array([[1.        , 0.52959196],\n+       [0.52959196, 1.        ]])\n+```\n+\n+> सहसंबंध मैट्रिक्स C की गणना किसी भी इनपुट अनुक्रम S<sub>1</sub>, ..., S<sub>n</sub> के लिए की जा सकती है। C<sub>ij</sub> का मान S<sub>i</sub> और S<sub>j</sub> के बीच सहसंबंध है, और विकर्ण तत्व हमेशा 1 होते हैं (जो कि स्व-सहसंबंध भी है एस<उप>मैं</sub>)।\n+\n+हमारे मामले में, मान 0.53 इंगित करता है कि किसी व्यक्ति के वजन और ऊंचाई के बीच कुछ संबंध है। हम रिश्ते को देखने के लिए दूसरे के खिलाफ एक मूल्य का स्कैटर प्लॉट भी बना सकते हैं:\n+\n+![वजन और ऊंचाई के बीच संबंध](/1-Introduction/04-stats-and-probability/images/weight-height-relationship.png)\n+\n+> सहसंबंध और सहप्रसरण के अधिक उदाहरण [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) में पाए जा सकते हैं।\n+\n+## निष्कर्ष\n+\n+इस भाग में हमने सीखा है:\n+\n+* डेटा के बुनियादी सांख्यिकीय गुण, जैसे माध्य, विचरण, मोड और चतुर्थक\n+* सामान्य वितरण सहित यादृच्छिक चर के विभिन्न वितरण\n+* विभिन्न गुणों के बीच सहसंबंध कैसे खोजें\n+* कुछ परिकल्पनाओं को सिद्ध करने के लिए गणित और सांख्यिकी के ध्वनि उपकरण का उपयोग कैसे करें,\n+* यादृच्छिक चर दिए गए डेटा नमूने के लिए विश्वास अंतराल की गणना कैसे करें\n+\n+हालांकि यह निश्चित रूप से उन विषयों की संपूर्ण सूची नहीं है जो संभाव्यता और आंकड़ों के भीतर मौजूद हैं, यह आपको इस पाठ्यक्रम में एक अच्छी शुरुआत देने के लिए पर्याप्त होना चाहिए।\n+\n+## चुनौती\n+\n+अन्य परिकल्पना का परीक्षण करने के लिए नोटबुक में नमूना कोड का उपयोग करें जो:\n+1. पहले बेसमेन दूसरे बेसमेन से बड़े होते हैं\n+2. पहले बेसमेन तीसरे बेसमेन से लम्बे होते हैं\n+3. शॉर्टस्टॉप दूसरे बेसमेन से लम्बे होते हैं\n+\n+## [व्याख्यान के बाद प्रश्नोत्तरी](https://red-water-0103e7a0f.azurestaticapps.net/quiz/7)\n+\n+## समीक्षा और आत्म अध्ययन\n+\n+संभाव्यता और सांख्यिकी इतना व्यापक विषय है कि यह अपने पाठ्यक्रम के योग्य है। यदि आप सिद्धांत में गहराई तक जाने में रुचि रखते हैं, तो आप निम्नलिखित में से कुछ पुस्तकों को पढ़ना जारी रख सकते हैं:\n+\n+1. न्यूयॉर्क विश्वविद्यालय के [कार्लोस फर्नांडीज-ग्रांडा](https://cims.nyu.edu/~cfgranda/) के पास डेटा साइंस के लिए [संभाव्यता और सांख्यिकी](https://cims.nyu.edu/~cfgranda/pages/stuff/probability_stats_for_DS.pdf) (ऑनलाइन उपलब्ध) के लिए महान व्याख्यान नोट्स हैं।\n+1. [पीटर और एंड्रयू ब्रूस। डेटा वैज्ञानिकों के लिए व्यावहारिक सांख्यिकी](https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/) । [[आर . में नमूना कोड](https://github.com/PacktPublishing/Statistics-for-Data-Science)]\n+1. [जेम्स डी. मिलर। डेटा विज्ञान के लिए सांख्यिकी](https://www.packtpub.com/product/statistics-for-data-science/9781788290678) [[आर . में नमूना कोड](https://github.com/PacktPublishing/Statistics-for-Data-Science)] \n+\n+## कार्यभार\n+\n+[लघु मधुमेह अध्ययन] (असाइनमेंट.एमडी)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d6,microsoft/Data-Science-For-Beginners,875971373,825479992,jlooper,1450004,2022-03-13T17:28:25+00:00,2022-03-13T17:28:25+00:00,missed translation,true,1-Introduction/04-stats-and-probability/translations/README.hi.md,NaN,"@@ -0,0 +1,268 @@\n+# सांख्यिकी और संभाव्यता का संक्षिप्त परिचय\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev)](/sketchnotes/04-Statistics-Probability.png)|\n+|:---:|\n+| सांख्यिकी और संभावना - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+सांख्यिकी और संभाव्यता सिद्धांत गणित के दो अत्यधिक संबंधित क्षेत्र हैं जो डेटा विज्ञान के लिए अत्यधिक प्रासंगिक हैं। गणित के गहन ज्ञान के बिना डेटा के साथ काम करना संभव है, लेकिन कम से कम कुछ बुनियादी अवधारणाओं को जानना अभी भी बेहतर है। यहां हम एक संक्षिप्त परिचय प्रस्तुत करेंगे जो आपको आरंभ करने में मदद करेगा।\n+\n+[![Intro Video](/1-Introduction/04-stats-and-probability/images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)\n+\n+\n+## [Pre-lecture quiz](https://red-water-0103e7a0f.azurestaticapps.net/quiz/6)\n+\n+## प्रायिकता और यादृच्छिक चर\n+\n+**प्रायिकता** 0 और 1 के बीच की एक संख्या है जो यह व्यक्त करती है कि **ईवेंट** कितनी संभावित है। इसे कई सकारात्मक परिणामों के रूप में परिभाषित किया गया है (जो घटना की ओर ले जाते हैं), परिणामों की कुल संख्या से विभाजित, यह देखते हुए कि सभी परिणाम समान रूप से संभावित हैं। उदाहरण के लिए, जब हम एक पासे को उछालते हैं, तो हमें एक सम संख्या प्राप्त होने की प्रायिकता 3/6 = 0.5 होती है।\n+\n+जब हम घटनाओं के बारे में बात करते हैं, तो हम **यादृच्छिक चर** का उपयोग करते हैं। उदाहरण के लिए, यादृच्छिक चर जो एक पासे को घुमाते समय प्राप्त संख्या का प्रतिनिधित्व करता है, 1 से 6 तक मान लेगा। 1 से 6 तक की संख्याओं के सेट को **नमूना स्थान** कहा जाता है। हम एक निश्चित मान लेने वाले यादृच्छिक चर की संभावना के बारे में बात कर सकते हैं, उदाहरण के लिए पी (एक्स = 3) = 1/6।\n+\n+पिछले उदाहरण में यादृच्छिक चर को **असतत** कहा जाता है, क्योंकि इसमें एक गणनीय नमूना स्थान होता है, अर्थात अलग-अलग मान होते हैं जिन्हें गिना जा सकता है। ऐसे मामले हैं जब नमूना स्थान वास्तविक संख्याओं की एक श्रृंखला है, या वास्तविक संख्याओं का पूरा सेट है। ऐसे चरों को **सतत** कहा जाता है। एक अच्छा उदाहरण वह समय है जब बस आती है।\n+\n+## प्रायिकता वितरण\n+\n+असतत यादृच्छिक चर के मामले में, फ़ंक्शन P(X) द्वारा प्रत्येक घटना की प्रायिकता का वर्णन करना आसान है। नमूना स्थान *S* से प्रत्येक मान *s* के लिए यह 0 से 1 तक की संख्या देगा, जैसे कि सभी घटनाओं के लिए P(X=s) के सभी मानों का योग 1 होगा।\n+\n+सबसे प्रसिद्ध असतत वितरण **समान वितरण** है, जिसमें N तत्वों का एक नमूना स्थान होता है, जिनमें से प्रत्येक के लिए 1/N की समान संभावना होती है।\n+\n+एक सतत चर के संभाव्यता वितरण का वर्णन करना अधिक कठिन है, कुछ अंतराल [ए, बी], या वास्तविक संख्याओं के पूरे सेट से लिए गए मानों के साथ &Ropf;। बस आगमन समय के मामले पर विचार करें। वास्तव में, प्रत्येक सटीक आगमन समय *t* के लिए, ठीक उसी समय पर बस के आने की प्रायिकता 0 है!\n+\n+> अब आप जानते हैं कि 0 प्रायिकता वाली घटनाएँ होती हैं, और बहुत बार! कम से कम हर बार जब बस आती है!\n+\n+हम केवल दिए गए मानों के अंतराल में एक चर के गिरने की प्रायिकता के बारे में बात कर सकते हैं, उदाहरण के लिए। P(t<sub>1</sub>&le;X&lt;t<sub>2</sub>)। इस मामले में, प्रायिकता बंटन को **प्रायिकता घनत्व फलन** p(x) द्वारा वर्णित किया जाता है, जैसे कि\n+\n+![P(t_1\le X<t_2)=\int_{t_1}^{t_2}p(x)dx](/1-Introduction/04-stats-and-probability/images/probability-density.png)\n+  \n+एकसमान वितरण के एक सतत एनालॉग को **निरंतर वर्दी** कहा जाता है, जिसे एक सीमित अंतराल पर परिभाषित किया जाता है। एक संभावना है कि मान X लंबाई l के अंतराल में आता है l के समानुपाती है, और 1 तक बढ़ जाता है।\n+\n+एक अन्य महत्वपूर्ण वितरण **सामान्य वितरण** है, जिसके बारे में हम नीचे विस्तार से बात करेंगे।\n+\n+## माध्य, प्रसरण और मानक विचलन\n+मान लीजिए कि हम एक यादृच्छिक चर X के n नमूनों का एक क्रम बनाते हैं: x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>। हम पारंपरिक तरीके से अनुक्रम के **माध्य** (या **अंकगणित औसत**) मान को परिभाषित कर सकते हैं (x<sub>1</sub>+x<sub>2</sub>+x<sub >एन</उप>)/एन। जैसे-जैसे हम नमूने का आकार बढ़ाते हैं (अर्थात n&rr;&infin; के साथ सीमा लेते हैं), हम वितरण का माध्य (जिसे **अपेक्षा** भी कहते हैं) प्राप्त करेंगे। हम उम्मीद को **E**(x) से निरूपित करेंगे।\n+> यह प्रदर्शित किया जा सकता है कि मूल्यों के साथ किसी भी असतत वितरण के लिए {x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>N</sub>} and corresponding probabilities p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>N</sub>, the expectation would equal to E(X)=x<sub>1</sub>p<sub>1</sub>+x<sub>2</sub>p<sub>2</sub>+...+x<sub>N</sub>p<sub>N</sub>.\n+\n+यह पहचानने के लिए कि मान कितनी दूर तक फैले हुए हैं, हम प्रसरण की गणना कर सकते हैं &sigma;<sup>2</sup> = &sum;(x<sub>i</sub> - &mu;)<sup>2</sup>/ एन, जहां & एमयू; अनुक्रम का माध्य है। मूल्य &सिग्मा; इसे **मानक विचलन** कहा जाता है, और &sigma;<sup>2</sup> को **विचरण** कहा जाता है।\n+\n+## बहुलक, माध्यिका और चतुर्थक\n+\n+कभी-कभी, माध्य डेटा के लिए ""विशिष्ट"" मान का पर्याप्त रूप से प्रतिनिधित्व नहीं करता है। उदाहरण के लिए, जब कुछ चरम मान पूरी तरह से सीमा से बाहर होते हैं, तो वे माध्य को प्रभावित कर सकते हैं। एक और अच्छा संकेत एक **माध्य** है, एक मान ऐसा है कि आधा डेटा बिंदु इससे कम है, और दूसरा आधा - अधिक है।\n+\n+डेटा के वितरण को समझने में हमारी मदद करने के लिए, **चतुर्थक** के बारे में बात करना मददगार होगा:\n+\n+* प्रथम चतुर्थक, या Q1, एक मान है, जैसे कि 25% डेटा इससे नीचे आता है\n+* तीसरा चतुर्थक, या Q3, एक मान है कि 75% डेटा इससे नीचे आता है\n+\n+ग्राफिक रूप से हम **बॉक्स प्लॉट** नामक आरेख में माध्यिका और चतुर्थक के बीच संबंध का प्रतिनिधित्व कर सकते हैं:\n+\n+<img src=""/1-Introduction/04-stats-and-probability/images/boxplot_explanation.png"" width=""50%""/>\n+\n+यहां हम **अंतर-चतुर्थक श्रेणी** IQR=Q3-Q1, और तथाकथित **आउटलेयर** - मानों की भी गणना करते हैं, जो सीमाओं के बाहर होते हैं [Q1-1.5*IQR,Q3+1.5*IQR]।\n+\n+परिमित वितरण के लिए जिसमें कम संख्या में संभावित मान होते हैं, एक अच्छा ""विशिष्ट"" मान वह होता है जो सबसे अधिक बार प्रकट होता है, जिसे **मोड** कहा जाता है। इसे अक्सर रंग जैसे श्रेणीबद्ध डेटा पर लागू किया जाता है। एक ऐसी स्थिति पर विचार करें जब हमारे पास लोगों के दो समूह हों - कुछ जो लाल रंग को अधिक पसंद करते हैं, और अन्य जो नीले रंग को पसंद करते हैं। यदि हम रंगों को संख्याओं के आधार पर कोडित करते हैं, तो पसंदीदा रंग का माध्य मान नारंगी-हरे रंग के स्पेक्ट्रम में कहीं होगा, जो किसी भी समूह पर वास्तविक वरीयता को इंगित नहीं करता है। हालांकि, मोड या तो रंगों में से एक होगा, या दोनों रंग, यदि उनके लिए मतदान करने वाले लोगों की संख्या बराबर है (इस मामले में हम नमूने को **मल्टीमॉडल** कहते हैं)।\n+\n+## वास्तविक दुनिया का डेटा\n+\n+जब हम वास्तविक जीवन से डेटा का विश्लेषण करते हैं, तो वे अक्सर यादृच्छिक चर नहीं होते हैं, इस अर्थ में कि हम अज्ञात परिणाम के साथ प्रयोग नहीं करते हैं। उदाहरण के लिए, बेसबॉल खिलाड़ियों की एक टीम और उनके शरीर के डेटा, जैसे ऊंचाई, वजन और उम्र पर विचार करें। वे संख्याएँ बिल्कुल यादृच्छिक नहीं हैं, लेकिन हम अभी भी उन्हीं गणितीय अवधारणाओं को लागू कर सकते हैं। उदाहरण के लिए, लोगों के वजन के अनुक्रम को कुछ यादृच्छिक चर से निकाले गए मानों का अनुक्रम माना जा सकता है। [इस डेटासेट](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) से लिए गए [मेजर लीग बेसबॉल](http://mlb.mlb.com/index.jsp) से वास्तविक बेसबॉल खिलाड़ियों के वज़न का क्रम नीचे दिया गया ह। (आपकी सुविधा के लिए, केवल पहले 20 मान दिखाए गए हैं):\n+\n+\n+```\n+[180.0, 215.0, 210.0, 210.0, 188.0, 176.0, 209.0, 200.0, 231.0, 180.0, 188.0, 180.0, 185.0, 160.0, 180.0, 185.0, 197.0, 189.0, 185.0, 219.0]\n+```\n+\n+> **नोट**: इस डेटासेट के साथ काम करने का उदाहरण देखने के लिए, [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) पर एक नज़र डालें। इस पूरे पाठ में कई चुनौतियाँ भी हैं, और आप उस नोटबुक में कुछ कोड जोड़कर उन्हें पूरा कर सकते हैं। यदि आप सुनिश्चित नहीं हैं कि डेटा पर कैसे काम करना है, तो चिंता न करें - हम बाद में पायथन का उपयोग करके डेटा के साथ काम करने के लिए वापस आएंगे। यदि आप जुपिटर नोटबुक में कोड चलाना नहीं जानते हैं, तो [इस लेख](https://soshnikov.com/education/how-to-execute-notebooks-from-github/) पर एक नज़र डालें।\n+\n+हमारे डेटा के लिए माध्य, माध्यिका और चतुर्थक दिखाने वाला बॉक्स प्लॉट यहां दिया गया है:\n+\n+![वेट बॉक्स प्लॉट](/1-Introduction/04-stats-and-probability/images/weight-boxplot.png)\n+\n+चूंकि हमारे डेटा में अलग-अलग खिलाड़ी **भूमिकाएं** के बारे में जानकारी है, इसलिए हम भूमिका के आधार पर बॉक्स प्लॉट भी कर सकते हैं - यह हमें यह विचार प्राप्त करने की अनुमति देगा कि कैसे पैरामीटर मान भूमिकाओं में भिन्न होते हैं। इस बार हम ऊंचाई पर विचार करेंगे:\n+\n+![भूमिका के अनुसार बॉक्स प्लॉट](/1-Introduction/04-stats-and-probability/images/boxplot_byrole.png)\n+\n+यह आरेख बताता है कि, औसतन, पहले बेसमेन की ऊंचाई दूसरे बेसमेन की ऊंचाई से अधिक होती है। बाद में इस पाठ में हम सीखेंगे कि हम इस परिकल्पना का अधिक औपचारिक रूप से परीक्षण कैसे कर सकते हैं, और यह कैसे प्रदर्शित करें कि हमारा डेटा सांख्यिकीय रूप से महत्वपूर्ण है।\n+\n+> वास्तविक दुनिया के डेटा के साथ काम करते समय, हम मानते हैं कि सभी डेटा बिंदु कुछ संभाव्यता वितरण से लिए गए नमूने हैं। यह धारणा हमें मशीन लर्निंग तकनीकों को लागू करने और कार्यशील भविष्य कहनेवाला मॉडल बनाने की अनुमति देती है।\n+\n+यह देखने के लिए कि हमारे डेटा का वितरण क्या है, हम एक ग्राफ बना सकते हैं जिसे **हिस्टोग्राम** कहा जाता है। एक्स-अक्ष में कई अलग-अलग वज़न अंतराल (तथाकथित **बिन्स**) होंगे, और ऊर्ध्वाधर अक्ष दिखाएगा कि हमारा यादृच्छिक चर नमूना किसी दिए गए अंतराल के अंदर कितनी बार था।\n+\n+![वास्तविक विश्व डेटा का हिस्टोग्राम](/1-Introduction/04-stats-and-probability/images/weight-histogram.png)\n+\n+इस हिस्टोग्राम से आप देख सकते हैं कि सभी मान निश्चित औसत वजन के आसपास केंद्रित होते हैं, और हम उस वजन से जितना आगे जाते हैं - उस मान के कम वजन का सामना करना पड़ता है। यानी, यह बहुत ही असंभव है कि बेसबॉल खिलाड़ी का वजन औसत वजन से बहुत अलग होगा। भार में भिन्नता यह दर्शाती है कि भार किस हद तक माध्य से भिन्न होने की संभावना है।\n+\n+> अगर हम बेसबॉल लीग से नहीं, बल्कि अन्य लोगों का वजन लेते हैं, तो वितरण अलग होने की संभावना है। हालाँकि, वितरण का आकार समान होगा, लेकिन माध्य और विचरण बदल जाएगा। इसलिए, यदि हम अपने मॉडल को बेसबॉल खिलाड़ियों पर प्रशिक्षित करते हैं, तो विश्वविद्यालय के छात्रों पर लागू होने पर यह गलत परिणाम देने की संभावना है, क्योंकि अंतर्निहित वितरण अलग है।\n+\n+## सामान्य वितरण\n+\n+वजन का वितरण जो हमने ऊपर देखा है वह बहुत विशिष्ट है, और वास्तविक दुनिया से कई माप एक ही प्रकार के वितरण का पालन करते हैं, लेकिन अलग-अलग माध्य और भिन्नता के साथ। इस वितरण को **सामान्य वितरण** कहा जाता है, और यह आंकड़ों में बहुत महत्वपूर्ण भूमिका निभाता है।\n+\n+सामान्य वितरण का उपयोग करना संभावित बेसबॉल खिलाड़ियों के यादृच्छिक भार उत्पन्न करने का एक सही तरीका है। एक बार जब हम माध्य वजन `माध्य` और मानक विचलन `एसटीडी` जान लेते हैं, तो हम निम्नलिखित तरीके से 1000 वजन के नमूने तैयार कर सकते हैं:\n+```python\n+samples = np.random.normal(mean,std,1000)\n+```\n+\n+यदि हम उत्पन्न नमूनों के हिस्टोग्राम की साजिश करते हैं तो हम ऊपर दिखाए गए चित्र के समान ही चित्र देखेंगे। और अगर हम नमूनों की संख्या और डिब्बे की संख्या में वृद्धि करते हैं, तो हम एक सामान्य वितरण की एक तस्वीर उत्पन्न कर सकते हैं जो आदर्श के अधिक करीब है:\n+\n+![माध्य = 0 और एसटीडी.देव = 1 के साथ सामान्य वितरण](/1-Introduction/04-stats-and-probability/images/normal-histogram.png)\n+\n+\n+*माध्य = 0 और std.dev=1* के साथ सामान्य वितरण\n+\n+## विश्वास अंतराल\n+\n+जब हम बेसबॉल खिलाड़ियों के वजन के बारे में बात करते हैं, तो हम मानते हैं कि कुछ निश्चित **यादृच्छिक चर W** है जो सभी बेसबॉल खिलाड़ियों (तथाकथित **जनसंख्या**) के वजन के आदर्श संभाव्यता वितरण से मेल खाती है। वजन का हमारा क्रम सभी बेसबॉल खिलाड़ियों के एक उपसमुच्चय से मेल खाता है जिसे हम **नमूना** कहते हैं। एक दिलचस्प सवाल यह है कि क्या हम डब्ल्यू के वितरण के मापदंडों को जान सकते हैं, यानी जनसंख्या का माध्य और विचरण?\n+\n+सबसे आसान उत्तर हमारे नमूने के माध्य और विचरण की गणना करना होगा। हालाँकि, ऐसा हो सकता है कि हमारा यादृच्छिक नमूना पूर्ण जनसंख्या का सटीक रूप से प्रतिनिधित्व नहीं करता है। इस प्रकार **कॉन्फिडेंस इंटरवल** के बारे में बात करना समझ में आता है।\n+\n+> **विश्वास अंतराल** हमारे नमूने को देखते हुए जनसंख्या के वास्तविक माध्य का अनुमान है, जो एक निश्चित प्रायिकता (या **विश्वास का स्तर**) सटीक है।\n+\n+मान लीजिए हमारे पास हमारे वितरण से एक नमूना X<sub>1</sub>, ..., X<sub>n</sub> है। हर बार जब हम अपने वितरण से एक नमूना लेते हैं, तो हम अलग-अलग माध्य मान के साथ समाप्त होते हैं। इस प्रकार &mu; एक यादृच्छिक चर माना जा सकता है। एक **विश्वास अंतराल** विश्वास के साथ p मानों की एक जोड़ी है (L<sub>p</sub>,R<sub>p</sub>), जैसे कि **P**(L<sub>p </sub>&leq;&mu;&leq;R<sub>p</sub>) = p, यानी अंतराल के भीतर मापे गए माध्य मान के गिरने की प्रायिकता p के बराबर होती है।\n+\n+यह विस्तार से चर्चा करने के लिए हमारे संक्षिप्त परिचय से परे है कि उन आत्मविश्वास अंतराल की गणना कैसे की जाती है। कुछ और विवरण [विकिपीडिया पर](https://en.wikipedia.org/wiki/Confidence_interval) देखे जा सकते हैं। संक्षेप में, हम जनसंख्या के वास्तविक माध्य के सापेक्ष परिकलित नमूना माध्य के वितरण को परिभाषित करते हैं, जिसे **छात्र वितरण** कहा जाता है।\n+\n+> **दिलचस्प तथ्य**: छात्र वितरण का नाम गणितज्ञ विलियम सीली गॉसेट के नाम पर रखा गया है, जिन्होंने छद्म नाम ""स्टूडेंट"" के तहत अपना पेपर प्रकाशित किया था। उन्होंने गिनीज शराब की भठ्ठी में काम किया, और, एक संस्करण के अनुसार, उनके नियोक्ता नहीं चाहते थे कि आम जनता को पता चले कि वे कच्चे माल की गुणवत्ता निर्धारित करने के लिए सांख्यिकीय परीक्षणों का उपयोग कर रहे थे।\n+\n+यदि हम माध्य का अनुमान लगाना चाहते हैं &mu; हमारी जनसंख्या का विश्वास p के साथ, हमें छात्र वितरण A का *(1-p)/2-th प्रतिशत* लेने की आवश्यकता है, जिसे या तो तालिकाओं से लिया जा सकता है, या कंप्यूटर सांख्यिकीय सॉफ़्टवेयर के कुछ अंतर्निहित कार्यों का उपयोग करके (उदाहरण के लिए पायथन, आर, आदि)। फिर अंतराल के लिए &mu; X&pm;A*D/&radic;n द्वारा दिया जाएगा, जहां X नमूने का प्राप्त माध्य है, D मानक विचलन है।\n+\n+> **नोट**: हम [स्वतंत्रता की डिग्री](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) की एक महत्वपूर्ण अवधारणा की चर्चा को भी छोड़ देते हैं, जो छात्र वितरण के संबंध में महत्वपूर्ण है। इस अवधारणा को गहराई से समझने के लिए आप सांख्यिकी पर अधिक संपूर्ण पुस्तकों का उल्लेख कर सकते हैं।\n+\n+वजन और ऊंचाई के लिए आत्मविश्वास अंतराल की गणना का एक उदाहरण [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) में दिया गया है।\n+\n+| p | Weight mean |\n+|-----|-----------|\n+| 0.85 | 201.73±0.94 |\n+| 0.90 | 201.73±1.08 |\n+| 0.95 | 201.73±1.28 |\n+\n+ध्यान दें कि आत्मविश्वास की संभावना जितनी अधिक होगी, विश्वास अंतराल उतना ही व्यापक होगा।\n+\n+## परिकल्पना परीक्षण \n+\n+हमारे बेसबॉल खिलाड़ियों के डेटासेट में, अलग-अलग खिलाड़ी भूमिकाएँ होती हैं, जिन्हें नीचे संक्षेप में प्रस्तुत किया जा सकता है (इस तालिका की गणना कैसे की जा सकती है, यह देखने के लिए [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) देखें):\n+\n+| Role | Height | Weight | Count |\n+|------|--------|--------|-------|\n+| Catcher | 72.723684 | 204.328947 | 76 |\n+| Designated_Hitter | 74.222222 | 220.888889 | 18 |\n+| First_Baseman | 74.000000 | 213.109091 | 55 |\n+| Outfielder | 73.010309 | 199.113402 | 194 |\n+| Relief_Pitcher | 74.374603 | 203.517460 | 315 |\n+| Second_Baseman | 71.362069 | 184.344828 | 58 |\n+| Shortstop | 71.903846 | 182.923077 | 52 |\n+| Starting_Pitcher | 74.719457 | 205.163636 | 221 |\n+| Third_Baseman | 73.044444 | 200.955556 | 45 |\n+\n+हम देख सकते हैं कि पहले बेसमेन की औसत ऊंचाई दूसरे बेसमेन की तुलना में अधिक है। इस प्रकार, हम यह निष्कर्ष निकालने के लिए ललचा सकते हैं कि **पहले बेसमेन दूसरे बेसमेन से अधिक हैं**।\n+\n+> इस कथन को **एक परिकल्पना** कहा जाता है, क्योंकि हम नहीं जानते कि तथ्य वास्तव में सत्य है या नहीं।\n+\n+हालांकि, यह हमेशा स्पष्ट नहीं होता है कि क्या हम यह निष्कर्ष निकाल सकते हैं। ऊपर की चर्चा से हम जानते हैं कि प्रत्येक माध्य का एक संबद्ध विश्वास अंतराल होता है, और इस प्रकार यह अंतर केवल एक सांख्यिकीय त्रुटि हो सकता है। हमें अपनी परिकल्पना का परीक्षण करने के लिए कुछ और औपचारिक तरीके की आवश्यकता है।\n+\n+आइए पहले और दूसरे बेसमेन की ऊंचाई के लिए अलग-अलग आत्मविश्वास अंतराल की गणना करें:\n+\n+| Confidence | First Basemen | Second Basemen |\n+|------------|---------------|----------------|\n+| 0.85 | 73.62..74.38 | 71.04..71.69 |\n+| 0.90 | 73.56..74.44 | 70.99..71.73 |\n+| 0.95 | 73.47..74.53 | 70.92..71.81 |\n+\n+हम देख सकते हैं कि बिना किसी विश्वास के अंतराल ओवरलैप हो जाते हैं। इससे हमारी परिकल्पना सिद्ध होती है कि पहले बेसमेन दूसरे बेसमेन से ऊंचे होते हैं।\n+\n+अधिक औपचारिक रूप से, हम जिस समस्या को हल कर रहे हैं वह यह देखना है कि क्या **दो संभाव्यता वितरण समान हैं**, या कम से कम समान पैरामीटर हैं। वितरण के आधार पर, हमें उसके लिए विभिन्न परीक्षणों का उपयोग करने की आवश्यकता है। यदि हम जानते हैं कि हमारे वितरण सामान्य हैं, तो हम **[स्टूडेंट टी-टेस्ट](https://en.wikipedia.org/wiki/Student%27s_t-test)** लागू कर सकते हैं।\n+\n+स्टूडेंट टी-टेस्ट में, हम तथाकथित **टी-वैल्यू** की गणना करते हैं, जो भिन्नता को ध्यान में रखते हुए, साधनों के बीच अंतर को इंगित करता है। यह प्रदर्शित किया जाता है कि टी-मान **छात्र वितरण** का अनुसरण करता है, जो हमें दिए गए आत्मविश्वास स्तर **p** के लिए थ्रेशोल्ड मान प्राप्त करने की अनुमति देता है (इसकी गणना की जा सकती है, या संख्यात्मक तालिकाओं में देखा जा सकता है)। फिर हम परिकल्पना को स्वीकृत या अस्वीकार करने के लिए टी-मान की तुलना इस सीमा से करते हैं।\n+\n+पायथन में, हम **SciPy** पैकेज का उपयोग कर सकते हैं, जिसमें `ttest_ind` फ़ंक्शन शामिल है (कई अन्य उपयोगी सांख्यिकीय कार्यों के अलावा!)। यह हमारे लिए टी-वैल्यू की गणना करता है, और कॉन्फिडेंस पी-वैल्यू की रिवर्स लुकअप भी करता है, ताकि हम निष्कर्ष निकालने के लिए कॉन्फिडेंस को देख सकें।\n+\n+उदाहरण के लिए, पहले और दूसरे बेसमेन की ऊंचाई के बीच हमारी तुलना हमें निम्नलिखित परिणाम देती है:\n+\n+```python\n+from scipy.stats import ttest_ind\n+\n+tval, pval = ttest_ind(df.loc[df['Role']=='First_Baseman',['Height']], df.loc[df['Role']=='Designated_Hitter',['Height']],equal_var=False)\n+print(f""T-value = {tval[0]:.2f}\nP-value: {pval[0]}"")\n+```\n+```\n+T-value = 7.65\n+P-value: 9.137321189738925e-12\n+```\n+\n+हमारे मामले में, पी-वैल्यू बहुत कम है, जिसका अर्थ है कि इस बात का समर्थन करने वाले मजबूत सबूत हैं कि पहले बेसमेन लम्बे होते हैं।\n+\n+अन्य विभिन्न प्रकार की परिकल्पनाएँ भी हैं जिनका हम परीक्षण करना चाहते हैं, उदाहरण के लिए:\n+* यह साबित करने के लिए कि दिया गया नमूना कुछ वितरण का अनुसरण करता है। हमारे मामले में हमने मान लिया है कि ऊंचाई सामान्य रूप से वितरित की जाती है, लेकिन इसके लिए औपचारिक सांख्यिकीय सत्यापन की आवश्यकता होती है।\n+* यह सिद्ध करने के लिए कि नमूने का माध्य मान कुछ पूर्वनिर्धारित मान से मेल खाता है\n+* कई नमूनों के साधनों की तुलना करना (उदाहरण के लिए विभिन्न आयु समूहों में खुशी के स्तर में क्या अंतर है)\n+\n+## बड़ी संख्या का नियम और केंद्रीय सीमा प्रमेय\n+\n+सामान्य वितरण के इतना महत्वपूर्ण होने का एक कारण तथाकथित **केंद्रीय सीमा प्रमेय** है। मान लीजिए कि हमारे पास स्वतंत्र N मानों X<sub>1</sub>, ..., X<sub>N</sub> का एक बड़ा नमूना है, जिसे माध्य &mu; और विचरण &सिग्मा;<sup>2</sup>. फिर, पर्याप्त रूप से बड़े N के लिए (दूसरे शब्दों में, जब N&rarr;&infin;), माध्य &Sigma;<sub>i</sub>X<sub>i</sub> को सामान्य रूप से माध्य &mu के साथ वितरित किया जाएगा; और विचरण &sigma;<sup>2</sup>/N.\n+\n+> केंद्रीय सीमा प्रमेय की व्याख्या करने का एक अन्य तरीका यह कहना है कि वितरण की परवाह किए बिना, जब आप किसी भी यादृच्छिक चर मानों के योग के माध्य की गणना करते हैं तो आप सामान्य वितरण के साथ समाप्त होते हैं।\n+\n+केंद्रीय सीमा प्रमेय से यह भी पता चलता है कि, जब N&rar;&infin;, नमूने के माध्य की प्रायिकता &mu; बन जाता है 1. इसे **बड़ी संख्या का नियम** कहते हैं।\n+\n+## सहप्रसरण और सहसंबंध\n+\n+डेटा साइंस द्वारा की जाने वाली चीजों में से एक डेटा के बीच संबंध ढूंढ रहा है। हम कहते हैं कि दो अनुक्रम **सहसम्बन्ध** तब होते हैं जब वे एक ही समय में समान व्यवहार प्रदर्शित करते हैं, अर्थात वे या तो एक साथ उठते/गिरते हैं, या एक क्रम ऊपर उठता है जब दूसरा गिरता है और इसके विपरीत। दूसरे शब्दों में, दो अनुक्रमों के बीच कुछ संबंध प्रतीत होता है।\n+\n+> सहसंबंध आवश्यक रूप से दो अनुक्रमों के बीच कारण संबंध को इंगित नहीं करता है; कभी-कभी दोनों चर किसी बाहरी कारण पर निर्भर हो सकते हैं, या यह विशुद्ध रूप से संयोग से दो अनुक्रम सहसंबद्ध हो सकते हैं। हालांकि, मजबूत गणितीय सहसंबंध एक अच्छा संकेत है कि दो चर किसी न किसी तरह से जुड़े हुए हैं।\n+\n+गणितीय रूप से, मुख्य अवधारणा जो दो यादृच्छिक चर के बीच संबंध दिखाती है, वह है **सहप्रसरण**, जिसकी गणना इस प्रकार की जाती है: Cov(X,Y) = **E**\[(X-**E**(X) ))(वाई-**ई**(वाई))\]। हम दोनों चरों के विचलन की गणना उनके माध्य मानों से करते हैं, और फिर उन विचलनों के गुणनफल की गणना करते हैं। यदि दोनों चर एक साथ विचलित होते हैं, तो उत्पाद हमेशा एक सकारात्मक मूल्य होगा, जो कि सकारात्मक सहप्रसरण को जोड़ देगा। यदि दोनों चर आउट-ऑफ-सिंक विचलित हो जाते हैं (अर्थात एक औसत से नीचे गिर जाता है जब दूसरा औसत से ऊपर उठता है), तो हमें हमेशा ऋणात्मक संख्याएँ मिलेंगी, जो कि ऋणात्मक सहप्रसरण को जोड़ देंगी। यदि विचलन निर्भर नहीं हैं, तो वे लगभग शून्य तक जोड़ देंगे।\n+\n+सहप्रसरण का निरपेक्ष मान हमें यह नहीं बताता कि सहसंबंध कितना बड़ा है, क्योंकि यह वास्तविक मूल्यों के परिमाण पर निर्भर करता है। इसे सामान्य करने के लिए, हम **सहसंबंध** प्राप्त करने के लिए, दोनों चरों के मानक विचलन द्वारा सहप्रसरण को विभाजित कर सकते हैं। अच्छी बात यह है कि सहसंबंध हमेशा [-1,1] की सीमा में होता है, जहां 1 मूल्यों के बीच मजबूत सकारात्मक सहसंबंध को इंगित करता है, -1 - मजबूत नकारात्मक सहसंबंध, और 0 - बिल्कुल भी कोई संबंध नहीं (चर स्वतंत्र हैं)।\n+\n+**उदाहरण**: हम ऊपर बताए गए डेटासेट से बेसबॉल खिलाड़ियों के वज़न और ऊंचाई के बीच सहसंबंध की गणना कर सकते हैं:\n+\n+```python\n+print(np.corrcoef(weights,heights))\n+```\n+As a result, we get **correlation matrix** like this one:",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d7,microsoft/Data-Science-For-Beginners,875971373,825480163,jlooper,1450004,2022-03-13T17:30:11+00:00,2022-03-13T17:30:11+00:00,standard translation here please,true,1-Introduction/01-defining-data-science/translations/README.hi.md,NaN,"@@ -0,0 +1,164 @@\n+# डेटा विज्ञान को परिभाषित करना\n+\n+| ![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev)](/sketchnotes/01-Definitions.png)  |\n+| :----------------------------------------------------------------------------------------------------: |\n+|              डेटा विज्ञान को परिभाषित करना - _Sketchnote by [@nitya](https://twitter.com/nitya)_               |\n+\n+---\n+\n+[![Defining Data Science Video](/1-Introduction/01-defining-data-science/images/video-def-ds.png)](https://youtu.be/beZ7Mb_oz9I)\n+\n+## [प्री-लेक्चर क्विज](https://red-water-0103e7a0f.azurestaticapps.net/quiz/0)\n+\n+## डेटा क्या है?\n+अपने दैनिक जीवन में हम लगातार डेटा से घिरे रहते हैं। अभी आप जो पाठ पढ़ रहे हैं वह डेटा है। आपके स्मार्टफ़ोन में आपके मित्रों के फ़ोन नंबरों की सूची डेटा है, साथ ही आपकी घड़ी पर प्रदर्शित वर्तमान समय भी है। मनुष्य के रूप में, हम स्वाभाविक रूप से हमारे पास मौजूद धन की गणना करके या अपने मित्रों को पत्र लिखकर डेटा के साथ काम करते हैं।\n+\n+हालाँकि, कंप्यूटर के निर्माण के साथ डेटा बहुत अधिक महत्वपूर्ण हो गया। कंप्यूटर की प्राथमिक भूमिका संगणना करना है, लेकिन उन्हें संचालित करने के लिए डेटा की आवश्यकता होती है। इस प्रकार, हमें यह समझने की आवश्यकता है कि कंप्यूटर डेटा को कैसे संग्रहीत और संसाधित करता है।\n+\n+इंटरनेट के उद्भव के साथ, डेटा हैंडलिंग उपकरणों के रूप में कंप्यूटर की भूमिका में वृद्धि हुई है। यदि आप इसके बारे में सोचते हैं, तो हम अब वास्तविक गणनाओं के बजाय डेटा प्रोसेसिंग और संचार के लिए कंप्यूटर का अधिक से अधिक उपयोग करते हैं। जब हम किसी मित्र को ई-मेल लिखते हैं या इंटरनेट पर कुछ जानकारी खोजते हैं - तो हम अनिवार्य रूप से डेटा बना रहे हैं, स्टोर कर रहे हैं, ट्रांसमिट कर रहे हैं और उसमें हेरफेर कर रहे हैं।\n+> क्या आपको याद है कि पिछली बार आपने किसी चीज़ की गणना करने के लिए कंप्यूटर का उपयोग कब किया था? \n+\n+## डेटा साइंस क्या है?\n+\n+In [Wikipedia](https://en.wikipedia.org/wiki/Data_science), **डेटा साइंस** *एक वैज्ञानिक क्षेत्र के रूप में परिभाषित किया गया है जो संरचित और असंरचित डेटा से ज्ञान और अंतर्दृष्टि निकालने के लिए वैज्ञानिक तरीकों का उपयोग करता है, और आवेदन डोमेन की एक विस्तृत श्रृंखला में डेटा से ज्ञान और कार्रवाई योग्य अंतर्दृष्टि लागू करता है।*. \n+\n+यह परिभाषा डेटा विज्ञान के निम्नलिखित महत्वपूर्ण पहलुओं पर प्रकाश डालती है::\n+\n+* डेटा साइंस का मुख्य लक्ष्य डेटा से **ज्ञान निकालना** है, दूसरे शब्दों में - डेटा को **समझना**, कुछ छिपे हुए संबंध ढूंढना और **मॉडल** बनाना।\n+* डेटा विज्ञान **वैज्ञानिक विधियों** का उपयोग करता है, जैसे कि प्रायिकता और सांख्यिकी। वास्तव में, जब *डेटा साइंस* शब्द पहली बार पेश किया गया था, तो कुछ लोगों ने तर्क दिया कि डेटा साइंस आंकड़ों के लिए सिर्फ एक नया फैंसी नाम था। आजकल यह स्पष्ट हो गया है कि क्षेत्र बहुत व्यापक है।\n+* प्राप्त ज्ञान को कुछ **कार्रवाई योग्य अंतर्दृष्टि**, यानी व्यावहारिक अंतर्दृष्टि उत्पन्न करने के लिए लागू किया जाना चाहिए जिसे आप वास्तविक व्यावसायिक स्थितियों पर लागू कर सकते हैं।\n+* हमें **संरचित** और **असंरचित** दोनों डेटा पर काम करने में सक्षम होना चाहिए। हम पाठ्यक्रम में बाद में विभिन्न प्रकार के डेटा पर चर्चा करने के लिए वापस आएंगे।\n+* **एप्लिकेशन डोमेन** एक महत्वपूर्ण अवधारणा है, और डेटा वैज्ञानिकों को अक्सर समस्या क्षेत्र में कम से कम कुछ हद तक विशेषज्ञता की आवश्यकता होती है, उदाहरण के लिए: वित्त, चिकित्सा, विपणन, आदि।\n+\n+> डेटा साइंस का एक अन्य महत्वपूर्ण पहलू यह है कि यह अध्ययन करता है कि कंप्यूटर का उपयोग करके डेटा कैसे एकत्र, संग्रहीत और संचालित किया जा सकता है। जबकि सांख्यिकी हमें गणितीय आधार प्रदान करती है, डेटा विज्ञान वास्तव में डेटा से अंतर्दृष्टि प्राप्त करने के लिए गणितीय अवधारणाओं को लागू करता है।\n+\n+(attributed to [Jim Gray](https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist))) डेटा विज्ञान को देखने का एक तरीका यह है कि इसे विज्ञान का एक अलग प्रतिमान माना जाए:\n+* **अनुभवजन्य**, जिसमें हम ज्यादातर अवलोकनों और प्रयोगों के परिणामों पर भरोसा करते हैं\n+* **सैद्धांतिक**, जहां मौजूदा वैज्ञानिक ज्ञान से नई अवधारणाएं उभरती हैं\n+* **कम्प्यूटेशनल**, जहां हम कुछ कम्प्यूटेशनल प्रयोगों के आधार पर नए सिद्धांतों की खोज करते हैं\n+* **डेटा-चालित**, डेटा में संबंधों और पैटर्न की खोज पर आधारित\n+\n+## अन्य संबंधित क्षेत्र\n+\n+चूंकि डेटा व्यापक है, डेटा विज्ञान भी एक व्यापक क्षेत्र है, जो कई अन्य विषयों को छूता है।\n+\n+<dl>\n+<dt>डेटाबेस</dt>\n+<dd>\n+डेटा को **कैसे स्टोर करें** एक महत्वपूर्ण विचार है, यानी इसे इस तरह से कैसे संरचित किया जाए जिससे तेजी से प्रोसेसिंग हो सके। विभिन्न प्रकार के डेटाबेस हैं जो संरचित और असंरचित डेटा संग्रहीत करते हैं, <a href=""../../2-Working-With-Data/README.md"">जिन पर हम अपने पाठ्यक्रम में विचार करेंगे</a>.\n+</dd>\n+<dt>बड़ा डेटा</dt>\n+<dd>\n+अक्सर हमें अपेक्षाकृत सरल संरचना के साथ बहुत बड़ी मात्रा में डेटा को स्टोर और संसाधित करने की आवश्यकता होती है। उस डेटा को एक कंप्यूटर क्लस्टर पर वितरित तरीके से संग्रहीत करने और इसे कुशलतापूर्वक संसाधित करने के लिए विशेष दृष्टिकोण और उपकरण हैं।\n+</dd>\n+<dt>यंत्र अधिगम</dt>\n+<dd>\n+डेटा को समझने का एक तरीका **एक मॉडल बनाना** है जो वांछित परिणाम की भविष्यवाणी करने में सक्षम होगा। डेटा से मॉडल विकसित करना **मशीन लर्निंग** कहलाता है। इसके बारे में अधिक जानने के लिए आप हमारे <a href=""https://aka.ms/ml-beginners"">मशीन लर्निंग फॉर बिगिनर्स</a> पाठ्यक्रम को देखना चाहेंगे।.\n+</dd>\n+<dt>कृत्रिम होशियारी</dt>\n+<dd>\n+आर्टिफिशियल इंटेलिजेंस (एआई) के रूप में जाना जाने वाला मशीन लर्निंग का एक क्षेत्र भी डेटा पर निर्भर करता है, और इसमें उच्च जटिलता वाले मॉडल बनाना शामिल है जो मानव विचार प्रक्रियाओं की नकल करते हैं। एआई विधियां अक्सर हमें असंरचित डेटा (जैसे प्राकृतिक भाषा) को संरचित अंतर्दृष्टि में बदलने की अनुमति देती हैं।\n+</dd>\n+<dt>प्रत्योक्षकरण</dt>\n+<dd>\n+एक इंसान के लिए बड़ी मात्रा में डेटा समझ से बाहर है, लेकिन एक बार जब हम उस डेटा का उपयोग करके उपयोगी विज़ुअलाइज़ेशन बनाते हैं, तो हम डेटा की अधिक समझ बना सकते हैं, और कुछ निष्कर्ष निकाल सकते हैं। इस प्रकार, जानकारी की कल्पना करने के कई तरीके जानना महत्वपूर्ण है - कुछ ऐसा जिसे हम अपने पाठ्यक्रम के <a href=""../../3-Data-Visualization/README.md"">धारा 3</a> में शामिल करेंगे। . संबंधित क्षेत्रों में सामान्य रूप से **इन्फोग्राफिक्स**, और **मानव-कंप्यूटर इंटरैक्शन** भी शामिल हैं।\n+</dd>\n+</dl>\n+\n+## डेटा के प्रकार\n+\n+जैसा कि हमने पहले ही उल्लेख किया है, डेटा हर जगह है। बस जरूरत है इसे सही तरीके से पकड़ने की! **संरचित** और **असंरचित** डेटा के बीच अंतर करना उपयोगी है। पूर्व को आम तौर पर कुछ अच्छी तरह से संरचित रूप में दर्शाया जाता है, अक्सर तालिका या तालिकाओं की संख्या के रूप में, जबकि बाद वाला केवल फाइलों का संग्रह होता है। कभी-कभी हम **अर्ध-संरचित** डेटा के बारे में भी बात कर सकते हैं, जिसमें किसी प्रकार की संरचना होती है जो बहुत भिन्न हो सकती है।\n+\n+| संरचित                                                                   | अर्ध-संरचित                                                                                | असंरचित                            |\n+| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- | --------------------------------------- |\n+| उन लोगों की सूची जिनके फ़ोन नंबर हैं | विकिपीडिया पृष्ठ लिंक के साथ | एनसाइक्लोपीडिया ब्रिटानिका का पाठ |\n+| पिछले 20 वर्षों से हर मिनट एक इमारत के सभी कमरों में तापमान | लेखकों के साथ JSON प्रारूप में वैज्ञानिक पत्रों का संग्रह, प्रकाशन का डेटा, और सार | कॉर्पोरेट दस्तावेज़ों के साथ फ़ाइल साझा करें |\n+| भवन में प्रवेश करने वाले सभी लोगों की आयु और लिंग का डेटा | इंटरनेट पेज | निगरानी कैमरे से कच्चा वीडियो फ़ीड |\n+## डेटा कहाँ से प्राप्त करें\n+\n+डेटा के कई संभावित स्रोत हैं, और उन सभी को सूचीबद्ध करना असंभव होगा! हालाँकि, आइए कुछ विशिष्ट स्थानों का उल्लेख करें जहाँ आप डेटा प्राप्त कर सकते हैं:\n+\n+* **संरचित**\n+  - **इंटरनेट ऑफ थिंग्स** (IoT), जिसमें तापमान या दबाव सेंसर जैसे विभिन्न सेंसर के डेटा शामिल हैं, बहुत उपयोगी डेटा प्रदान करता है। उदाहरण के लिए, यदि कोई कार्यालय भवन IoT सेंसर से लैस है, तो हम लागत को कम करने के लिए स्वचालित रूप से हीटिंग और प्रकाश व्यवस्था को नियंत्रित कर सकते हैं।\n+  - **सर्वेक्षण** जो हम उपयोगकर्ताओं को खरीदारी के बाद, या किसी वेब साइट पर जाने के बाद पूरा करने के लिए कहते हैं।\n+  - **व्यवहार का विश्लेषण**, उदाहरण के लिए, यह समझने में हमारी सहायता कर सकता है कि उपयोगकर्ता किसी साइट में कितनी गहराई तक जाता है, और साइट छोड़ने का सामान्य कारण क्या है।\n+* **असंरचित**\n+  - **पाठ** अंतर्दृष्टि का एक समृद्ध स्रोत हो सकता है, जैसे समग्र **भावना स्कोर**, या कीवर्ड और अर्थ अर्थ निकालना।\n+  - **छवियां** या **वीडियो**। सड़क पर यातायात का अनुमान लगाने और संभावित ट्रैफिक जाम के बारे में लोगों को सूचित करने के लिए एक निगरानी कैमरे से एक वीडियो का उपयोग किया जा सकता है।\n+  - वेब सर्वर **लॉग** का उपयोग यह समझने के लिए किया जा सकता है कि हमारी साइट के कौन से पृष्ठ सबसे अधिक बार देखे जाते हैं, और कितने समय के लिए।\n+* **अर्ध-संरचित**\n+  - **सोशल नेटवर्क** ग्राफ़ उपयोगकर्ता के व्यक्तित्व के बारे में डेटा के महान स्रोत हो सकते हैं और जानकारी के प्रसार में संभावित प्रभावशीलता हो सकती है।\n+  - जब हमारे पास किसी पार्टी से तस्वीरों का एक गुच्छा होता है, तो हम एक दूसरे के साथ तस्वीरें लेने वाले लोगों का ग्राफ बनाकर **ग्रुप डायनेमिक्स** डेटा निकालने का प्रयास कर सकते हैं।\n+\n+डेटा के विभिन्न संभावित स्रोतों को जानकर, आप विभिन्न परिदृश्यों के बारे में सोचने की कोशिश कर सकते हैं जहां स्थिति को बेहतर तरीके से जानने और व्यावसायिक प्रक्रियाओं को बेहतर बनाने के लिए डेटा विज्ञान तकनीकों को लागू किया जा सकता है।\n+\n+## आप डेटा के साथ क्या कर सकते हैं\n+\n+डेटा साइंस में, हम डेटा यात्रा के निम्नलिखित चरणों पर ध्यान केंद्रित करते हैं:\n+\n+<dl>\n+<dt>1) डेटा अधिग्रहण</dt>\n+<dd>\n+पहला कदम डेटा एकत्र करना है। जबकि कई मामलों में यह एक सीधी प्रक्रिया हो सकती है, जैसे वेब एप्लिकेशन से डेटाबेस में डेटा आना, कभी-कभी हमें विशेष तकनीकों का उपयोग करने की आवश्यकता होती है। उदाहरण के लिए, IoT सेंसर से डेटा भारी हो सकता है, और आगे की प्रक्रिया से पहले सभी डेटा एकत्र करने के लिए IoT हब जैसे बफरिंग एंडपॉइंट का उपयोग करना एक अच्छा अभ्यास है।\n+</dd>\n+<dt>2) डाटा स्टोरेज</dt>\n+<dd>\n+डेटा स्टोर करना चुनौतीपूर्ण हो सकता है, खासकर अगर हम बड़े डेटा के बारे में बात कर रहे हैं। डेटा को स्टोर करने का तरीका तय करते समय, भविष्य में डेटा को क्वेरी करने के तरीके का अनुमान लगाना समझ में आता है। डेटा को स्टोर करने के कई तरीके हैं:\n+<ul>\n+<li>एक रिलेशनल डेटाबेस तालिकाओं के संग्रह को संग्रहीत करता है, और उन्हें क्वेरी करने के लिए SQL नामक एक विशेष भाषा का उपयोग करता है। आमतौर पर, तालिकाओं को विभिन्न समूहों में व्यवस्थित किया जाता है जिन्हें स्कीमा कहा जाता है। कई मामलों में हमें स्कीमा को फिट करने के लिए डेटा को मूल रूप से परिवर्तित करने की आवश्यकता होती है।</li>\n+<li><a href=""https://en.wikipedia.org/wiki/NoSQL"">एक NoSQL</a> डेटाबेस, जैसे कि <a href=""https://azure.microsoft.com/services/cosmos-db/?WT.mc_id=academic-31812-dmitryso"">CosmosDB</a>, करता है डेटा पर स्कीमा लागू नहीं करता है, और अधिक जटिल डेटा संग्रहीत करने की अनुमति देता है, उदाहरण के लिए, पदानुक्रमित JSON दस्तावेज़ या ग्राफ़। हालाँकि, NoSQL डेटाबेस में SQL की समृद्ध क्वेरी क्षमता नहीं होती है, और यह संदर्भात्मक अखंडता को लागू नहीं कर सकता है, अर्थात डेटा को तालिकाओं में कैसे संरचित किया जाता है और तालिकाओं के बीच संबंधों को नियंत्रित करने के नियम।</li>\n+<li><a href=""https://en.wikipedia.org/wiki/Data_lake"">डेटा लेक</a> संग्रहण का उपयोग कच्चे, असंरचित रूप में डेटा के बड़े संग्रह के लिए किया जाता है। डेटा झीलों का उपयोग अक्सर बड़े डेटा के साथ किया जाता है, जहां सभी डेटा एक मशीन पर फिट नहीं हो सकते हैं, और सर्वरों के एक समूह द्वारा संग्रहीत और संसाधित किया जाना है। <a href=""https://en.wikipedia.org/wiki/Apache_Parquet"">Parquet</a> डेटा प्रारूप है जिसे अक्सर बड़े डेटा के संयोजन में उपयोग किया जाता है।</li>\n+</ul>\n+</dd>\n+<dt>3) डाटा प्रोसेसिंग</dt>\n+<dd>\n+यह डेटा यात्रा का सबसे रोमांचक हिस्सा है, जिसमें डेटा को उसके मूल रूप से एक ऐसे रूप में परिवर्तित करना शामिल है जिसका उपयोग विज़ुअलाइज़ेशन/मॉडल प्रशिक्षण के लिए किया जा सकता है। टेक्स्ट या इमेज जैसे असंरचित डेटा के साथ काम करते समय, हमें डेटा से **फीचर्स** निकालने के लिए कुछ एआई तकनीकों का उपयोग करने की आवश्यकता हो सकती है, इस प्रकार इसे संरचित रूप में परिवर्तित किया जा सकता है।\n+</dd>\n+<dt>4) विज़ुअलाइज़ेशन / मानव अंतर्दृष्टि</dt>\n+<dd>\n+अक्सर, डेटा को समझने के लिए, हमें इसकी कल्पना करने की आवश्यकता होती है। हमारे टूलबॉक्स में कई अलग-अलग विज़ुअलाइज़ेशन तकनीकें होने से, हम एक अंतर्दृष्टि बनाने के लिए सही दृश्य ढूंढ सकते हैं। अक्सर, एक डेटा वैज्ञानिक को ""डेटा के साथ खेलने"" की आवश्यकता होती है, इसे कई बार विज़ुअलाइज़ करना और कुछ रिश्तों की तलाश करना। इसके अलावा, हम एक परिकल्पना का परीक्षण करने या डेटा के विभिन्न टुकड़ों के बीच संबंध साबित करने के लिए सांख्यिकीय तकनीकों का उपयोग कर सकते हैं।   \n+</dd>\n+<dt>5) एक भविष्य कहनेवाला मॉडल प्रशिक्षण</dt>\n+<dd>\n+चूंकि डेटा विज्ञान का अंतिम लक्ष्य डेटा के आधार पर निर्णय लेने में सक्षम होना है, इसलिए हम <a href=""http://github.com/microsoft/ml-for-beginners"">मशीन लर्निंग की तकनीकों का उपयोग करना चाह सकते हैं। </a> एक भविष्य कहनेवाला मॉडल बनाने के लिए। इसके बाद हम समान संरचनाओं वाले नए डेटा सेट का उपयोग करके भविष्यवाणियां करने के लिए इसका उपयोग कर सकते हैं।\n+</dd>\n+</dl>\n+\n+बेशक, वास्तविक डेटा के आधार पर, कुछ चरण गायब हो सकते हैं (उदाहरण के लिए, जब हमारे पास पहले से ही डेटाबेस में डेटा है, या जब हमें मॉडल प्रशिक्षण की आवश्यकता नहीं है), या कुछ चरणों को कई बार दोहराया जा सकता है (जैसे डेटा प्रोसेसिंग )\n+\n+## डिजिटलीकरण और डिजिटल परिवर्तन\n+\n+पिछले दशक में, कई व्यवसायों ने व्यावसायिक निर्णय लेते समय डेटा के महत्व को समझना शुरू कर दिया। व्यवसाय चलाने के लिए डेटा विज्ञान सिद्धांतों को लागू करने के लिए, पहले कुछ डेटा एकत्र करने की आवश्यकता होती है, अर्थात व्यावसायिक प्रक्रियाओं का डिजिटल रूप में अनुवाद करना। इसे **डिजिटलाइजेशन** के नाम से जाना जाता है। निर्णयों को निर्देशित करने के लिए इस डेटा में डेटा विज्ञान तकनीकों को लागू करने से उत्पादकता (या यहां तक ​​कि व्यावसायिक धुरी) में उल्लेखनीय वृद्धि हो सकती है, जिसे **डिजिटल परिवर्तन** कहा जाता है।\n+\n+आइए एक उदाहरण पर विचार करें। मान लीजिए कि हमारे पास एक डेटा विज्ञान पाठ्यक्रम है (जैसे यह एक) जिसे हम छात्रों को ऑनलाइन वितरित करते हैं, और हम इसे सुधारने के लिए डेटा विज्ञान का उपयोग करना चाहते हैं। हम यह कैसे कर सकते हैं?\n+\n+हम ""क्या डिजीटल किया जा सकता है?"" पूछकर शुरू कर सकते हैं। सबसे आसान तरीका यह होगा कि प्रत्येक छात्र को प्रत्येक मॉड्यूल को पूरा करने में लगने वाले समय को मापें, और प्रत्येक मॉड्यूल के अंत में एक बहुविकल्पीय परीक्षा देकर प्राप्त ज्ञान को मापें। सभी छात्रों में औसत समय-से-पूरा करके, हम यह पता लगा सकते हैं कि कौन से मॉड्यूल छात्रों के लिए सबसे अधिक कठिनाइयों का कारण बनते हैं, और उन्हें सरल बनाने पर काम करते हैं।\n+\n+> आप तर्क दे सकते हैं कि यह दृष्टिकोण आदर्श नहीं है, क्योंकि मॉड्यूल अलग-अलग लंबाई के हो सकते हैं। मॉड्यूल की लंबाई (वर्णों की संख्या में) से समय को विभाजित करना शायद अधिक उचित है, और इसके बजाय उन मानों की तुलना करें।\n+\n+जब हम बहुविकल्पीय परीक्षणों के परिणामों का विश्लेषण करना शुरू करते हैं, तो हम यह निर्धारित करने का प्रयास कर सकते हैं कि छात्रों को किन अवधारणाओं को समझने में कठिनाई होती है, और उस जानकारी का उपयोग सामग्री को बेहतर बनाने के लिए कर सकते हैं। ऐसा करने के लिए, हमें परीक्षणों को इस तरह से डिजाइन करने की आवश्यकता है कि प्रत्येक प्रश्न एक निश्चित अवधारणा या ज्ञान के हिस्से से मेल खाता हो।\n+\n+यदि हम और अधिक जटिल होना चाहते हैं, तो हम छात्रों की आयु वर्ग के विरुद्ध प्रत्येक मॉड्यूल के लिए लिए गए समय को प्लॉट कर सकते हैं। हमें पता चल सकता है कि कुछ आयु वर्गों के लिए मॉड्यूल को पूरा करने में अनुचित रूप से लंबा समय लगता है, या छात्र इसे पूरा करने से पहले छोड़ देते हैं। यह हमें मॉड्यूल के लिए उम्र की सिफारिशें प्रदान करने में मदद कर सकता है, और गलत उम्मीदों से लोगों के असंतोष को कम कर सकता है।\n+\n+## 🚀 चुनौती\n+\n+इस चुनौती में, हम ग्रंथों को देखकर डेटा विज्ञान के क्षेत्र से प्रासंगिक अवधारणाओं को खोजने का प्रयास करेंगे। हम डेटा विज्ञान पर एक विकिपीडिया लेख लेंगे, पाठ को डाउनलोड और संसाधित करेंगे, और फिर इस तरह एक शब्द क्लाउड का निर्माण करेंगे:\n+\n+![डेटा साइंस के लिए वर्ड क्लाउड](/1-Introduction/01-defining-data-science/images/ds_wordcloud.png)\n+\n+Visit [`notebook.ipynb`](/1-Introduction/01-defining-data-science/notebook.ipynb ':ignore') कोड के माध्यम से पढ़ने के लिए। आप कोड भी चला सकते हैं, और देख सकते हैं कि यह वास्तविक समय में सभी डेटा परिवर्तन कैसे करता है।\n+\n+> यदि आप नहीं जानते कि जुपिटर नोटबुक में कोड कैसे चलाना है, तो एक नज़र डालें [this article](https://soshnikov.com/education/how-to-execute-notebooks-from-github/).\n+\n+\n+\n+## [Post-lecture quiz](https://red-water-0103e7a0f.azurestaticapps.net/quiz/1)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d8,microsoft/Data-Science-For-Beginners,875971373,825480183,jlooper,1450004,2022-03-13T17:30:31+00:00,2022-03-13T17:30:31+00:00,I think this is mislinked - you can link to translated assignments if they exist,true,1-Introduction/01-defining-data-science/translations/README.hi.md,NaN,"@@ -0,0 +1,164 @@\n+# डेटा विज्ञान को परिभाषित करना\n+\n+| ![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev)](/sketchnotes/01-Definitions.png)  |\n+| :----------------------------------------------------------------------------------------------------: |\n+|              डेटा विज्ञान को परिभाषित करना - _Sketchnote by [@nitya](https://twitter.com/nitya)_               |\n+\n+---\n+\n+[![Defining Data Science Video](/1-Introduction/01-defining-data-science/images/video-def-ds.png)](https://youtu.be/beZ7Mb_oz9I)\n+\n+## [प्री-लेक्चर क्विज](https://red-water-0103e7a0f.azurestaticapps.net/quiz/0)\n+\n+## डेटा क्या है?\n+अपने दैनिक जीवन में हम लगातार डेटा से घिरे रहते हैं। अभी आप जो पाठ पढ़ रहे हैं वह डेटा है। आपके स्मार्टफ़ोन में आपके मित्रों के फ़ोन नंबरों की सूची डेटा है, साथ ही आपकी घड़ी पर प्रदर्शित वर्तमान समय भी है। मनुष्य के रूप में, हम स्वाभाविक रूप से हमारे पास मौजूद धन की गणना करके या अपने मित्रों को पत्र लिखकर डेटा के साथ काम करते हैं।\n+\n+हालाँकि, कंप्यूटर के निर्माण के साथ डेटा बहुत अधिक महत्वपूर्ण हो गया। कंप्यूटर की प्राथमिक भूमिका संगणना करना है, लेकिन उन्हें संचालित करने के लिए डेटा की आवश्यकता होती है। इस प्रकार, हमें यह समझने की आवश्यकता है कि कंप्यूटर डेटा को कैसे संग्रहीत और संसाधित करता है।\n+\n+इंटरनेट के उद्भव के साथ, डेटा हैंडलिंग उपकरणों के रूप में कंप्यूटर की भूमिका में वृद्धि हुई है। यदि आप इसके बारे में सोचते हैं, तो हम अब वास्तविक गणनाओं के बजाय डेटा प्रोसेसिंग और संचार के लिए कंप्यूटर का अधिक से अधिक उपयोग करते हैं। जब हम किसी मित्र को ई-मेल लिखते हैं या इंटरनेट पर कुछ जानकारी खोजते हैं - तो हम अनिवार्य रूप से डेटा बना रहे हैं, स्टोर कर रहे हैं, ट्रांसमिट कर रहे हैं और उसमें हेरफेर कर रहे हैं।\n+> क्या आपको याद है कि पिछली बार आपने किसी चीज़ की गणना करने के लिए कंप्यूटर का उपयोग कब किया था? \n+\n+## डेटा साइंस क्या है?\n+\n+In [Wikipedia](https://en.wikipedia.org/wiki/Data_science), **डेटा साइंस** *एक वैज्ञानिक क्षेत्र के रूप में परिभाषित किया गया है जो संरचित और असंरचित डेटा से ज्ञान और अंतर्दृष्टि निकालने के लिए वैज्ञानिक तरीकों का उपयोग करता है, और आवेदन डोमेन की एक विस्तृत श्रृंखला में डेटा से ज्ञान और कार्रवाई योग्य अंतर्दृष्टि लागू करता है।*. \n+\n+यह परिभाषा डेटा विज्ञान के निम्नलिखित महत्वपूर्ण पहलुओं पर प्रकाश डालती है::\n+\n+* डेटा साइंस का मुख्य लक्ष्य डेटा से **ज्ञान निकालना** है, दूसरे शब्दों में - डेटा को **समझना**, कुछ छिपे हुए संबंध ढूंढना और **मॉडल** बनाना।\n+* डेटा विज्ञान **वैज्ञानिक विधियों** का उपयोग करता है, जैसे कि प्रायिकता और सांख्यिकी। वास्तव में, जब *डेटा साइंस* शब्द पहली बार पेश किया गया था, तो कुछ लोगों ने तर्क दिया कि डेटा साइंस आंकड़ों के लिए सिर्फ एक नया फैंसी नाम था। आजकल यह स्पष्ट हो गया है कि क्षेत्र बहुत व्यापक है।\n+* प्राप्त ज्ञान को कुछ **कार्रवाई योग्य अंतर्दृष्टि**, यानी व्यावहारिक अंतर्दृष्टि उत्पन्न करने के लिए लागू किया जाना चाहिए जिसे आप वास्तविक व्यावसायिक स्थितियों पर लागू कर सकते हैं।\n+* हमें **संरचित** और **असंरचित** दोनों डेटा पर काम करने में सक्षम होना चाहिए। हम पाठ्यक्रम में बाद में विभिन्न प्रकार के डेटा पर चर्चा करने के लिए वापस आएंगे।\n+* **एप्लिकेशन डोमेन** एक महत्वपूर्ण अवधारणा है, और डेटा वैज्ञानिकों को अक्सर समस्या क्षेत्र में कम से कम कुछ हद तक विशेषज्ञता की आवश्यकता होती है, उदाहरण के लिए: वित्त, चिकित्सा, विपणन, आदि।\n+\n+> डेटा साइंस का एक अन्य महत्वपूर्ण पहलू यह है कि यह अध्ययन करता है कि कंप्यूटर का उपयोग करके डेटा कैसे एकत्र, संग्रहीत और संचालित किया जा सकता है। जबकि सांख्यिकी हमें गणितीय आधार प्रदान करती है, डेटा विज्ञान वास्तव में डेटा से अंतर्दृष्टि प्राप्त करने के लिए गणितीय अवधारणाओं को लागू करता है।\n+\n+(attributed to [Jim Gray](https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist))) डेटा विज्ञान को देखने का एक तरीका यह है कि इसे विज्ञान का एक अलग प्रतिमान माना जाए:\n+* **अनुभवजन्य**, जिसमें हम ज्यादातर अवलोकनों और प्रयोगों के परिणामों पर भरोसा करते हैं\n+* **सैद्धांतिक**, जहां मौजूदा वैज्ञानिक ज्ञान से नई अवधारणाएं उभरती हैं\n+* **कम्प्यूटेशनल**, जहां हम कुछ कम्प्यूटेशनल प्रयोगों के आधार पर नए सिद्धांतों की खोज करते हैं\n+* **डेटा-चालित**, डेटा में संबंधों और पैटर्न की खोज पर आधारित\n+\n+## अन्य संबंधित क्षेत्र\n+\n+चूंकि डेटा व्यापक है, डेटा विज्ञान भी एक व्यापक क्षेत्र है, जो कई अन्य विषयों को छूता है।\n+\n+<dl>\n+<dt>डेटाबेस</dt>\n+<dd>\n+डेटा को **कैसे स्टोर करें** एक महत्वपूर्ण विचार है, यानी इसे इस तरह से कैसे संरचित किया जाए जिससे तेजी से प्रोसेसिंग हो सके। विभिन्न प्रकार के डेटाबेस हैं जो संरचित और असंरचित डेटा संग्रहीत करते हैं, <a href=""../../2-Working-With-Data/README.md"">जिन पर हम अपने पाठ्यक्रम में विचार करेंगे</a>.\n+</dd>\n+<dt>बड़ा डेटा</dt>\n+<dd>\n+अक्सर हमें अपेक्षाकृत सरल संरचना के साथ बहुत बड़ी मात्रा में डेटा को स्टोर और संसाधित करने की आवश्यकता होती है। उस डेटा को एक कंप्यूटर क्लस्टर पर वितरित तरीके से संग्रहीत करने और इसे कुशलतापूर्वक संसाधित करने के लिए विशेष दृष्टिकोण और उपकरण हैं।\n+</dd>\n+<dt>यंत्र अधिगम</dt>\n+<dd>\n+डेटा को समझने का एक तरीका **एक मॉडल बनाना** है जो वांछित परिणाम की भविष्यवाणी करने में सक्षम होगा। डेटा से मॉडल विकसित करना **मशीन लर्निंग** कहलाता है। इसके बारे में अधिक जानने के लिए आप हमारे <a href=""https://aka.ms/ml-beginners"">मशीन लर्निंग फॉर बिगिनर्स</a> पाठ्यक्रम को देखना चाहेंगे।.\n+</dd>\n+<dt>कृत्रिम होशियारी</dt>\n+<dd>\n+आर्टिफिशियल इंटेलिजेंस (एआई) के रूप में जाना जाने वाला मशीन लर्निंग का एक क्षेत्र भी डेटा पर निर्भर करता है, और इसमें उच्च जटिलता वाले मॉडल बनाना शामिल है जो मानव विचार प्रक्रियाओं की नकल करते हैं। एआई विधियां अक्सर हमें असंरचित डेटा (जैसे प्राकृतिक भाषा) को संरचित अंतर्दृष्टि में बदलने की अनुमति देती हैं।\n+</dd>\n+<dt>प्रत्योक्षकरण</dt>\n+<dd>\n+एक इंसान के लिए बड़ी मात्रा में डेटा समझ से बाहर है, लेकिन एक बार जब हम उस डेटा का उपयोग करके उपयोगी विज़ुअलाइज़ेशन बनाते हैं, तो हम डेटा की अधिक समझ बना सकते हैं, और कुछ निष्कर्ष निकाल सकते हैं। इस प्रकार, जानकारी की कल्पना करने के कई तरीके जानना महत्वपूर्ण है - कुछ ऐसा जिसे हम अपने पाठ्यक्रम के <a href=""../../3-Data-Visualization/README.md"">धारा 3</a> में शामिल करेंगे। . संबंधित क्षेत्रों में सामान्य रूप से **इन्फोग्राफिक्स**, और **मानव-कंप्यूटर इंटरैक्शन** भी शामिल हैं।\n+</dd>\n+</dl>\n+\n+## डेटा के प्रकार\n+\n+जैसा कि हमने पहले ही उल्लेख किया है, डेटा हर जगह है। बस जरूरत है इसे सही तरीके से पकड़ने की! **संरचित** और **असंरचित** डेटा के बीच अंतर करना उपयोगी है। पूर्व को आम तौर पर कुछ अच्छी तरह से संरचित रूप में दर्शाया जाता है, अक्सर तालिका या तालिकाओं की संख्या के रूप में, जबकि बाद वाला केवल फाइलों का संग्रह होता है। कभी-कभी हम **अर्ध-संरचित** डेटा के बारे में भी बात कर सकते हैं, जिसमें किसी प्रकार की संरचना होती है जो बहुत भिन्न हो सकती है।\n+\n+| संरचित                                                                   | अर्ध-संरचित                                                                                | असंरचित                            |\n+| ---------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------- | --------------------------------------- |\n+| उन लोगों की सूची जिनके फ़ोन नंबर हैं | विकिपीडिया पृष्ठ लिंक के साथ | एनसाइक्लोपीडिया ब्रिटानिका का पाठ |\n+| पिछले 20 वर्षों से हर मिनट एक इमारत के सभी कमरों में तापमान | लेखकों के साथ JSON प्रारूप में वैज्ञानिक पत्रों का संग्रह, प्रकाशन का डेटा, और सार | कॉर्पोरेट दस्तावेज़ों के साथ फ़ाइल साझा करें |\n+| भवन में प्रवेश करने वाले सभी लोगों की आयु और लिंग का डेटा | इंटरनेट पेज | निगरानी कैमरे से कच्चा वीडियो फ़ीड |\n+## डेटा कहाँ से प्राप्त करें\n+\n+डेटा के कई संभावित स्रोत हैं, और उन सभी को सूचीबद्ध करना असंभव होगा! हालाँकि, आइए कुछ विशिष्ट स्थानों का उल्लेख करें जहाँ आप डेटा प्राप्त कर सकते हैं:\n+\n+* **संरचित**\n+  - **इंटरनेट ऑफ थिंग्स** (IoT), जिसमें तापमान या दबाव सेंसर जैसे विभिन्न सेंसर के डेटा शामिल हैं, बहुत उपयोगी डेटा प्रदान करता है। उदाहरण के लिए, यदि कोई कार्यालय भवन IoT सेंसर से लैस है, तो हम लागत को कम करने के लिए स्वचालित रूप से हीटिंग और प्रकाश व्यवस्था को नियंत्रित कर सकते हैं।\n+  - **सर्वेक्षण** जो हम उपयोगकर्ताओं को खरीदारी के बाद, या किसी वेब साइट पर जाने के बाद पूरा करने के लिए कहते हैं।\n+  - **व्यवहार का विश्लेषण**, उदाहरण के लिए, यह समझने में हमारी सहायता कर सकता है कि उपयोगकर्ता किसी साइट में कितनी गहराई तक जाता है, और साइट छोड़ने का सामान्य कारण क्या है।\n+* **असंरचित**\n+  - **पाठ** अंतर्दृष्टि का एक समृद्ध स्रोत हो सकता है, जैसे समग्र **भावना स्कोर**, या कीवर्ड और अर्थ अर्थ निकालना।\n+  - **छवियां** या **वीडियो**। सड़क पर यातायात का अनुमान लगाने और संभावित ट्रैफिक जाम के बारे में लोगों को सूचित करने के लिए एक निगरानी कैमरे से एक वीडियो का उपयोग किया जा सकता है।\n+  - वेब सर्वर **लॉग** का उपयोग यह समझने के लिए किया जा सकता है कि हमारी साइट के कौन से पृष्ठ सबसे अधिक बार देखे जाते हैं, और कितने समय के लिए।\n+* **अर्ध-संरचित**\n+  - **सोशल नेटवर्क** ग्राफ़ उपयोगकर्ता के व्यक्तित्व के बारे में डेटा के महान स्रोत हो सकते हैं और जानकारी के प्रसार में संभावित प्रभावशीलता हो सकती है।\n+  - जब हमारे पास किसी पार्टी से तस्वीरों का एक गुच्छा होता है, तो हम एक दूसरे के साथ तस्वीरें लेने वाले लोगों का ग्राफ बनाकर **ग्रुप डायनेमिक्स** डेटा निकालने का प्रयास कर सकते हैं।\n+\n+डेटा के विभिन्न संभावित स्रोतों को जानकर, आप विभिन्न परिदृश्यों के बारे में सोचने की कोशिश कर सकते हैं जहां स्थिति को बेहतर तरीके से जानने और व्यावसायिक प्रक्रियाओं को बेहतर बनाने के लिए डेटा विज्ञान तकनीकों को लागू किया जा सकता है।\n+\n+## आप डेटा के साथ क्या कर सकते हैं\n+\n+डेटा साइंस में, हम डेटा यात्रा के निम्नलिखित चरणों पर ध्यान केंद्रित करते हैं:\n+\n+<dl>\n+<dt>1) डेटा अधिग्रहण</dt>\n+<dd>\n+पहला कदम डेटा एकत्र करना है। जबकि कई मामलों में यह एक सीधी प्रक्रिया हो सकती है, जैसे वेब एप्लिकेशन से डेटाबेस में डेटा आना, कभी-कभी हमें विशेष तकनीकों का उपयोग करने की आवश्यकता होती है। उदाहरण के लिए, IoT सेंसर से डेटा भारी हो सकता है, और आगे की प्रक्रिया से पहले सभी डेटा एकत्र करने के लिए IoT हब जैसे बफरिंग एंडपॉइंट का उपयोग करना एक अच्छा अभ्यास है।\n+</dd>\n+<dt>2) डाटा स्टोरेज</dt>\n+<dd>\n+डेटा स्टोर करना चुनौतीपूर्ण हो सकता है, खासकर अगर हम बड़े डेटा के बारे में बात कर रहे हैं। डेटा को स्टोर करने का तरीका तय करते समय, भविष्य में डेटा को क्वेरी करने के तरीके का अनुमान लगाना समझ में आता है। डेटा को स्टोर करने के कई तरीके हैं:\n+<ul>\n+<li>एक रिलेशनल डेटाबेस तालिकाओं के संग्रह को संग्रहीत करता है, और उन्हें क्वेरी करने के लिए SQL नामक एक विशेष भाषा का उपयोग करता है। आमतौर पर, तालिकाओं को विभिन्न समूहों में व्यवस्थित किया जाता है जिन्हें स्कीमा कहा जाता है। कई मामलों में हमें स्कीमा को फिट करने के लिए डेटा को मूल रूप से परिवर्तित करने की आवश्यकता होती है।</li>\n+<li><a href=""https://en.wikipedia.org/wiki/NoSQL"">एक NoSQL</a> डेटाबेस, जैसे कि <a href=""https://azure.microsoft.com/services/cosmos-db/?WT.mc_id=academic-31812-dmitryso"">CosmosDB</a>, करता है डेटा पर स्कीमा लागू नहीं करता है, और अधिक जटिल डेटा संग्रहीत करने की अनुमति देता है, उदाहरण के लिए, पदानुक्रमित JSON दस्तावेज़ या ग्राफ़। हालाँकि, NoSQL डेटाबेस में SQL की समृद्ध क्वेरी क्षमता नहीं होती है, और यह संदर्भात्मक अखंडता को लागू नहीं कर सकता है, अर्थात डेटा को तालिकाओं में कैसे संरचित किया जाता है और तालिकाओं के बीच संबंधों को नियंत्रित करने के नियम।</li>\n+<li><a href=""https://en.wikipedia.org/wiki/Data_lake"">डेटा लेक</a> संग्रहण का उपयोग कच्चे, असंरचित रूप में डेटा के बड़े संग्रह के लिए किया जाता है। डेटा झीलों का उपयोग अक्सर बड़े डेटा के साथ किया जाता है, जहां सभी डेटा एक मशीन पर फिट नहीं हो सकते हैं, और सर्वरों के एक समूह द्वारा संग्रहीत और संसाधित किया जाना है। <a href=""https://en.wikipedia.org/wiki/Apache_Parquet"">Parquet</a> डेटा प्रारूप है जिसे अक्सर बड़े डेटा के संयोजन में उपयोग किया जाता है।</li>\n+</ul>\n+</dd>\n+<dt>3) डाटा प्रोसेसिंग</dt>\n+<dd>\n+यह डेटा यात्रा का सबसे रोमांचक हिस्सा है, जिसमें डेटा को उसके मूल रूप से एक ऐसे रूप में परिवर्तित करना शामिल है जिसका उपयोग विज़ुअलाइज़ेशन/मॉडल प्रशिक्षण के लिए किया जा सकता है। टेक्स्ट या इमेज जैसे असंरचित डेटा के साथ काम करते समय, हमें डेटा से **फीचर्स** निकालने के लिए कुछ एआई तकनीकों का उपयोग करने की आवश्यकता हो सकती है, इस प्रकार इसे संरचित रूप में परिवर्तित किया जा सकता है।\n+</dd>\n+<dt>4) विज़ुअलाइज़ेशन / मानव अंतर्दृष्टि</dt>\n+<dd>\n+अक्सर, डेटा को समझने के लिए, हमें इसकी कल्पना करने की आवश्यकता होती है। हमारे टूलबॉक्स में कई अलग-अलग विज़ुअलाइज़ेशन तकनीकें होने से, हम एक अंतर्दृष्टि बनाने के लिए सही दृश्य ढूंढ सकते हैं। अक्सर, एक डेटा वैज्ञानिक को ""डेटा के साथ खेलने"" की आवश्यकता होती है, इसे कई बार विज़ुअलाइज़ करना और कुछ रिश्तों की तलाश करना। इसके अलावा, हम एक परिकल्पना का परीक्षण करने या डेटा के विभिन्न टुकड़ों के बीच संबंध साबित करने के लिए सांख्यिकीय तकनीकों का उपयोग कर सकते हैं।   \n+</dd>\n+<dt>5) एक भविष्य कहनेवाला मॉडल प्रशिक्षण</dt>\n+<dd>\n+चूंकि डेटा विज्ञान का अंतिम लक्ष्य डेटा के आधार पर निर्णय लेने में सक्षम होना है, इसलिए हम <a href=""http://github.com/microsoft/ml-for-beginners"">मशीन लर्निंग की तकनीकों का उपयोग करना चाह सकते हैं। </a> एक भविष्य कहनेवाला मॉडल बनाने के लिए। इसके बाद हम समान संरचनाओं वाले नए डेटा सेट का उपयोग करके भविष्यवाणियां करने के लिए इसका उपयोग कर सकते हैं।\n+</dd>\n+</dl>\n+\n+बेशक, वास्तविक डेटा के आधार पर, कुछ चरण गायब हो सकते हैं (उदाहरण के लिए, जब हमारे पास पहले से ही डेटाबेस में डेटा है, या जब हमें मॉडल प्रशिक्षण की आवश्यकता नहीं है), या कुछ चरणों को कई बार दोहराया जा सकता है (जैसे डेटा प्रोसेसिंग )\n+\n+## डिजिटलीकरण और डिजिटल परिवर्तन\n+\n+पिछले दशक में, कई व्यवसायों ने व्यावसायिक निर्णय लेते समय डेटा के महत्व को समझना शुरू कर दिया। व्यवसाय चलाने के लिए डेटा विज्ञान सिद्धांतों को लागू करने के लिए, पहले कुछ डेटा एकत्र करने की आवश्यकता होती है, अर्थात व्यावसायिक प्रक्रियाओं का डिजिटल रूप में अनुवाद करना। इसे **डिजिटलाइजेशन** के नाम से जाना जाता है। निर्णयों को निर्देशित करने के लिए इस डेटा में डेटा विज्ञान तकनीकों को लागू करने से उत्पादकता (या यहां तक ​​कि व्यावसायिक धुरी) में उल्लेखनीय वृद्धि हो सकती है, जिसे **डिजिटल परिवर्तन** कहा जाता है।\n+\n+आइए एक उदाहरण पर विचार करें। मान लीजिए कि हमारे पास एक डेटा विज्ञान पाठ्यक्रम है (जैसे यह एक) जिसे हम छात्रों को ऑनलाइन वितरित करते हैं, और हम इसे सुधारने के लिए डेटा विज्ञान का उपयोग करना चाहते हैं। हम यह कैसे कर सकते हैं?\n+\n+हम ""क्या डिजीटल किया जा सकता है?"" पूछकर शुरू कर सकते हैं। सबसे आसान तरीका यह होगा कि प्रत्येक छात्र को प्रत्येक मॉड्यूल को पूरा करने में लगने वाले समय को मापें, और प्रत्येक मॉड्यूल के अंत में एक बहुविकल्पीय परीक्षा देकर प्राप्त ज्ञान को मापें। सभी छात्रों में औसत समय-से-पूरा करके, हम यह पता लगा सकते हैं कि कौन से मॉड्यूल छात्रों के लिए सबसे अधिक कठिनाइयों का कारण बनते हैं, और उन्हें सरल बनाने पर काम करते हैं।\n+\n+> आप तर्क दे सकते हैं कि यह दृष्टिकोण आदर्श नहीं है, क्योंकि मॉड्यूल अलग-अलग लंबाई के हो सकते हैं। मॉड्यूल की लंबाई (वर्णों की संख्या में) से समय को विभाजित करना शायद अधिक उचित है, और इसके बजाय उन मानों की तुलना करें।\n+\n+जब हम बहुविकल्पीय परीक्षणों के परिणामों का विश्लेषण करना शुरू करते हैं, तो हम यह निर्धारित करने का प्रयास कर सकते हैं कि छात्रों को किन अवधारणाओं को समझने में कठिनाई होती है, और उस जानकारी का उपयोग सामग्री को बेहतर बनाने के लिए कर सकते हैं। ऐसा करने के लिए, हमें परीक्षणों को इस तरह से डिजाइन करने की आवश्यकता है कि प्रत्येक प्रश्न एक निश्चित अवधारणा या ज्ञान के हिस्से से मेल खाता हो।\n+\n+यदि हम और अधिक जटिल होना चाहते हैं, तो हम छात्रों की आयु वर्ग के विरुद्ध प्रत्येक मॉड्यूल के लिए लिए गए समय को प्लॉट कर सकते हैं। हमें पता चल सकता है कि कुछ आयु वर्गों के लिए मॉड्यूल को पूरा करने में अनुचित रूप से लंबा समय लगता है, या छात्र इसे पूरा करने से पहले छोड़ देते हैं। यह हमें मॉड्यूल के लिए उम्र की सिफारिशें प्रदान करने में मदद कर सकता है, और गलत उम्मीदों से लोगों के असंतोष को कम कर सकता है।\n+\n+## 🚀 चुनौती\n+\n+इस चुनौती में, हम ग्रंथों को देखकर डेटा विज्ञान के क्षेत्र से प्रासंगिक अवधारणाओं को खोजने का प्रयास करेंगे। हम डेटा विज्ञान पर एक विकिपीडिया लेख लेंगे, पाठ को डाउनलोड और संसाधित करेंगे, और फिर इस तरह एक शब्द क्लाउड का निर्माण करेंगे:\n+\n+![डेटा साइंस के लिए वर्ड क्लाउड](/1-Introduction/01-defining-data-science/images/ds_wordcloud.png)\n+\n+Visit [`notebook.ipynb`](/1-Introduction/01-defining-data-science/notebook.ipynb ':ignore') कोड के माध्यम से पढ़ने के लिए। आप कोड भी चला सकते हैं, और देख सकते हैं कि यह वास्तविक समय में सभी डेटा परिवर्तन कैसे करता है।\n+\n+> यदि आप नहीं जानते कि जुपिटर नोटबुक में कोड कैसे चलाना है, तो एक नज़र डालें [this article](https://soshnikov.com/education/how-to-execute-notebooks-from-github/).\n+\n+\n+\n+## [Post-lecture quiz](https://red-water-0103e7a0f.azurestaticapps.net/quiz/1)\n+\n+## कार्य (Assignments)\n+\n+* **कार्य 1**: **बिग डेटा** और **मशीन लर्निंग** के क्षेत्रों के लिए संबंधित अवधारणाओं का पता लगाने के लिए ऊपर दिए गए कोड को संशोधित करें\n+* **कार्य 2**: [डेटा विज्ञान परिदृश्यों के बारे में सोचें](assignment.md)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303d9,microsoft/Data-Science-For-Beginners,875971373,825480474,jlooper,1450004,2022-03-13T17:32:48+00:00,2022-03-13T17:32:48+00:00,should be able to link to the translated assignment here,true,2-Working-With-Data/05-relational-databases/translations/README.hi.md,NaN,"@@ -0,0 +1,181 @@\n+# डेटा के साथ कार्य करना: संबंधपरक डेटाबेस\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](/sketchnotes/05-RelationalData.png)|\n+|:---:|\n+| डेटा के साथ कार्य करना: संबंधपरक डेटाबेस - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+संभावना है कि आपने जानकारी संग्रहीत करने के लिए अतीत में एक स्प्रेडशीट का उपयोग किया है। आपके पास पंक्तियों और स्तंभों का एक सेट था, जहाँ पंक्तियों में जानकारी (या डेटा) होती थी, और स्तंभों में जानकारी (कभी-कभी मेटाडेटा कहा जाता है) का वर्णन होता था। तालिकाओं में स्तंभों और पंक्तियों के इस मूल सिद्धांत पर एक संबंधपरक डेटाबेस बनाया गया है, जिससे आप कई तालिकाओं में जानकारी फैला सकते हैं। इससे आप अधिक जटिल डेटा के साथ काम कर सकते हैं, दोहराव से बच सकते हैं, और डेटा को एक्सप्लोर करने के तरीके में लचीलापन रख सकते हैं। आइए एक रिलेशनल डेटाबेस की अवधारणाओं का पता लगाएं।\n+\n+## [प्री-लेक्चर क्विज](https://red-water-0103e7a0f.azurestaticapps.net/quiz/8)\n+\n+## यह सब टेबल से शुरू होता है\n+\n+एक रिलेशनल डेटाबेस में इसकी कोर टेबल होती है। स्प्रैडशीट की तरह ही, तालिका स्तंभों और पंक्तियों का एक संग्रह है। पंक्ति में वह डेटा या जानकारी होती है जिसके साथ हम काम करना चाहते हैं, जैसे किसी शहर का नाम या वर्षा की मात्रा। कॉलम उनके द्वारा संग्रहीत डेटा का वर्णन करते हैं।\n+\n+आइए शहरों के बारे में जानकारी संग्रहीत करने के लिए एक तालिका शुरू करके अपनी खोज शुरू करें। हम उनके नाम और देश से शुरुआत कर सकते हैं। आप इसे एक टेबल में इस प्रकार स्टोर कर सकते हैं:\n+\n+| City     | Country       |\n+| -------- | ------------- |\n+| Tokyo    | Japan         |\n+| Atlanta  | United States |\n+| Auckland | New Zealand   |\n+\n+ध्यान दें कि **शहर**, **देश** और **जनसंख्या** के कॉलम नाम संग्रहीत किए जा रहे डेटा का वर्णन करते हैं, और प्रत्येक पंक्ति में एक शहर के बारे में जानकारी होती है।\n+\n+## सिंगल टेबल अप्रोच की कमियां\n+\n+संभावना है, ऊपर दी गई तालिका आपको अपेक्षाकृत परिचित लगती है। आइए अपने बढ़ते डेटाबेस में कुछ अतिरिक्त डेटा जोड़ना शुरू करें - वार्षिक वर्षा (मिलीमीटर में)। हम वर्ष 2018, 2019 और 2020 पर ध्यान केंद्रित करेंगे। अगर हम इसे टोक्यो के लिए जोड़ते हैं, तो यह कुछ इस तरह दिख सकता है:\n+\n+| City  | Country | Year | Amount |\n+| ----- | ------- | ---- | ------ |\n+| Tokyo | Japan   | 2020 | 1690   |\n+| Tokyo | Japan   | 2019 | 1874   |\n+| Tokyo | Japan   | 2018 | 1445   |\n+\n+आप हमारी तालिका के बारे में क्या देखते हैं? आप देख सकते हैं कि हम शहर के नाम और देश को बार-बार दोहरा रहे हैं। इसमें काफी स्टोरेज लग सकता है, और इसकी कई प्रतियां रखने के लिए काफी हद तक अनावश्यक है। आखिरकार, टोक्यो का केवल एक ही नाम है जिसमें हम रुचि रखते हैं।\n+\n+ठीक है, चलो कुछ और कोशिश करते हैं। आइए प्रत्येक वर्ष के लिए नए कॉलम जोड़ें:\n+\n+| City     | Country       | 2018 | 2019 | 2020 |\n+| -------- | ------------- | ---- | ---- | ---- |\n+| Tokyo    | Japan         | 1445 | 1874 | 1690 |\n+| Atlanta  | United States | 1779 | 1111 | 1683 |\n+| Auckland | New Zealand   | 1386 | 942  | 1176 |\n+\n+हालांकि यह पंक्ति दोहराव से बचा जाता है, लेकिन यह कुछ अन्य चुनौतियों को भी जोड़ता है। हर बार नया साल आने पर हमें अपनी तालिका की संरचना को संशोधित करने की आवश्यकता होगी। इसके अतिरिक्त, जैसे-जैसे हमारा डेटा बढ़ता है, वैसे-वैसे हमारे वर्षों में कॉलम के रूप में मूल्यों को पुनः प्राप्त करना और गणना करना मुश्किल हो जाएगा।\n+\n+यही कारण है कि हमें कई तालिकाओं और संबंधों की आवश्यकता है। अपने डेटा को अलग करके हम दोहराव से बच सकते हैं और हम अपने डेटा के साथ काम करने के तरीके में अधिक लचीलापन रखते हैं।\n+\n+## रिश्तों की अवधारणा\n+\n+आइए अपने डेटा पर वापस लौटें और निर्धारित करें कि हम चीजों को कैसे विभाजित करना चाहते हैं। हम जानते हैं कि हम अपने शहरों के लिए नाम और देश को संग्रहित करना चाहते हैं, इसलिए यह शायद एक टेबल में सबसे अच्छा काम करेगा।\n+\n+| City     | Country       |\n+| -------- | ------------- |\n+| Tokyo    | Japan         |\n+| Atlanta  | United States |\n+| Auckland | New Zealand   |\n+\n+लेकिन इससे पहले कि हम अगली तालिका बनाएं, हमें यह पता लगाना होगा कि प्रत्येक शहर को कैसे संदर्भित किया जाए। हमें किसी पहचानकर्ता, आईडी या (तकनीकी डेटाबेस के संदर्भ में) प्राथमिक कुंजी की आवश्यकता है। प्राथमिक कुंजी एक मान है जिसका उपयोग किसी तालिका में एक विशिष्ट पंक्ति की पहचान करने के लिए किया जाता है। हालांकि यह स्वयं एक मूल्य पर आधारित हो सकता है (उदाहरण के लिए, हम शहर के नाम का उपयोग कर सकते हैं), यह लगभग हमेशा एक संख्या या अन्य पहचानकर्ता होना चाहिए। हम नहीं चाहते कि आईडी कभी बदले क्योंकि इससे रिश्ता टूट जाएगा। आप ज्यादातर मामलों में पाएंगे कि प्राथमिक कुंजी या आईडी एक स्वतः उत्पन्न संख्या होगी।\n+\n+> प्राथमिक कुंजी को अक्सर पीके के रूप में संक्षिप्त किया जाता है\n+\n+### शहरों\n+\n+| city_id | City     | Country       |\n+| ------- | -------- | ------------- |\n+| 1       | Tokyo    | Japan         |\n+| 2       | Atlanta  | United States |\n+| 3       | Auckland | New Zealand   |\n+\n+> ✅ आप देखेंगे कि हम इस पाठ के दौरान ""आईडी"" और ""प्राथमिक कुंजी"" शब्दों का परस्पर उपयोग करते हैं। यहाँ की अवधारणाएँ DataFrames पर लागू होती हैं, जिन्हें आप बाद में एक्सप्लोर करेंगे। डेटाफ़्रेम ""प्राथमिक कुंजी"" की शब्दावली का उपयोग नहीं करते हैं, हालांकि आप देखेंगे कि वे उसी तरह से बहुत अधिक व्यवहार करते हैं।\n+\n+हमारे शहरों की तालिका बनाने के साथ, आइए वर्षा को संग्रहित करें। हम शहर के बारे में पूरी जानकारी की नकल करने के बजाय आईडी का उपयोग कर सकते हैं। हमें यह भी सुनिश्चित करना चाहिए कि नई बनाई गई तालिका में एक *id* कॉलम भी हो, क्योंकि सभी तालिकाओं में एक आईडी या प्राथमिक कुंजी होनी चाहिए।\n+\n+### बारिश\n+\n+| rainfall_id | city_id | Year | Amount |\n+| ----------- | ------- | ---- | ------ |\n+| 1           | 1       | 2018 | 1445   |\n+| 2           | 1       | 2019 | 1874   |\n+| 3           | 1       | 2020 | 1690   |\n+| 4           | 2       | 2018 | 1779   |\n+| 5           | 2       | 2019 | 1111   |\n+| 6           | 2       | 2020 | 1683   |\n+| 7           | 3       | 2018 | 1386   |\n+| 8           | 3       | 2019 | 942    |\n+| 9           | 3       | 2020 | 1176   |\n+\n+नव निर्मित **वर्षा** तालिका के अंदर **city_id** कॉलम पर ध्यान दें। इस कॉलम में वे मान हैं जो **शहरों** तालिका में आईडी का संदर्भ देते हैं। तकनीकी संबंधपरक डेटा के संदर्भ में, इसे **विदेशी कुंजी** कहा जाता है; यह किसी अन्य तालिका से प्राथमिक कुंजी है। आप इसे केवल एक संदर्भ या सूचक के रूप में सोच सकते हैं। **सिटी_आईडी** 1 संदर्भ टोक्यो।\n+\n+> [!नोट] विदेशी कुंजी को अक्सर FK . के रूप में संक्षिप्त किया जाता है\n+\n+## डेटा पुनर्प्राप्त करना\n+\n+हमारे डेटा को दो तालिकाओं में विभाजित करके, आप सोच रहे होंगे कि हम इसे कैसे पुनः प्राप्त करते हैं। यदि हम एक रिलेशनल डेटाबेस जैसे MySQL, SQL सर्वर या Oracle का उपयोग कर रहे हैं, तो हम स्ट्रक्चर्ड क्वेरी लैंग्वेज या SQL नामक भाषा का उपयोग कर सकते हैं। SQL (कभी-कभी उच्चारित अगली कड़ी) एक मानक भाषा है जिसका उपयोग रिलेशनल डेटाबेस में डेटा को पुनः प्राप्त करने और संशोधित करने के लिए किया जाता है।\n+\n+डेटा पुनर्प्राप्त करने के लिए आप `SELECT` कमांड का उपयोग करते हैं। इसके मूल में, आप उन स्तंभों को **चुनते हैं जिन्हें आप देखना चाहते हैं **से** उस तालिका में जिसमें वे शामिल हैं। यदि आप केवल शहरों के नाम प्रदर्शित करना चाहते हैं, तो आप निम्न का उपयोग कर सकते हैं:\n+\n+```sql\n+SELECT city\n+FROM cities;\n+\n+-- Output:\n+-- Tokyo\n+-- Atlanta\n+-- Auckland\n+```\n+\n+`चयन` वह जगह है जहां आप कॉलम सूचीबद्ध करते हैं, और `से` वह जगह है जहां आप टेबल सूचीबद्ध करते हैं।\n+\n+> [नोट] SQL सिंटैक्स केस-असंवेदनशील है, जिसका अर्थ है `चुनें` और `चयन` का मतलब एक ही है। हालाँकि, आप जिस प्रकार के डेटाबेस का उपयोग कर रहे हैं उसके आधार पर कॉलम और टेबल केस संवेदनशील हो सकते हैं। नतीजतन, प्रोग्रामिंग में हर चीज को हमेशा केस संवेदी की तरह व्यवहार करना सबसे अच्छा अभ्यास है। SQL क्वेरी लिखते समय सामान्य परंपरा यह है कि कीवर्ड को सभी अपर-केस अक्षरों में रखा जाए।\n+\n+उपरोक्त क्वेरी सभी शहरों को प्रदर्शित करेगी। आइए कल्पना करें कि हम केवल न्यूजीलैंड में शहरों को प्रदर्शित करना चाहते थे। हमें किसी प्रकार के फ़िल्टर की आवश्यकता है। इसके लिए SQL कीवर्ड `WHERE` या ""जहां कुछ सच है"" है।\n+\n+```sql\n+SELECT city\n+FROM cities\n+WHERE country = 'New Zealand';\n+\n+-- Output:\n+-- Auckland\n+```\n+\n+## डेटा में शामिल होना\n+\n+अब तक हमने एक ही टेबल से डेटा रिकवर किया है। अब हम **शहरों** और **वर्षा** दोनों के डेटा को एक साथ लाना चाहते हैं। यह उन्हें एक साथ *शामिल करके* किया जाता है। आप प्रभावी रूप से दो तालिकाओं के बीच एक सीम बनाएंगे, और प्रत्येक तालिका से एक कॉलम के मानों का मिलान करेंगे।\n+\n+हमारे उदाहरण में, हम **वर्षा** में **शहर_आईडी** कॉलम का मिलान **शहरों** में **शहर_आईडी** कॉलम से करेंगे। यह अपने संबंधित शहर के साथ वर्षा मूल्य से मेल खाएगा। हम जिस प्रकार के जॉइन करेंगे, उसे *इनर* जॉइन कहा जाता है, जिसका अर्थ है कि यदि कोई पंक्तियाँ दूसरी टेबल की किसी भी चीज़ से मेल नहीं खाती हैं तो वे प्रदर्शित नहीं होंगी। हमारे मामले में हर शहर में बारिश होती है, इसलिए सब कुछ प्रदर्शित किया जाएगा।\n+\n+आइए हमारे सभी शहरों के लिए 2019 की वर्षा को पुनः प्राप्त करें।\n+\n+हम इसे चरणों में करने जा रहे हैं। पहला कदम सीम के लिए कॉलम को इंगित करके डेटा को एक साथ जोड़ना है - **city_id** जैसा कि पहले हाइलाइट किया गया था।\n+\n+```sql\n+SELECT cities.city\n+    rainfall.amount\n+FROM cities\n+    INNER JOIN rainfall ON cities.city_id = rainfall.city_id\n+```\n+\n+हमने उन दो कॉलमों को हाइलाइट किया है जो हम चाहते हैं, और तथ्य यह है कि हम टेबल्स को **city_id** द्वारा एक साथ जोड़ना चाहते हैं। अब हम केवल वर्ष 2019 को फ़िल्टर करने के लिए `WHERE` स्टेटमेंट जोड़ सकते हैं।\n+\n+```sql\n+SELECT cities.city\n+    rainfall.amount\n+FROM cities\n+    INNER JOIN rainfall ON cities.city_id = rainfall.city_id\n+WHERE rainfall.year = 2019\n+\n+-- Output\n+\n+-- city     | amount\n+-- -------- | ------\n+-- Tokyo    | 1874\n+-- Atlanta  | 1111\n+-- Auckland |  942\n+```\n+\n+## सारांश\n+\n+रिलेशनल डेटाबेस कई तालिकाओं के बीच सूचनाओं को विभाजित करने के आसपास केंद्रित होते हैं जिन्हें बाद में प्रदर्शन और विश्लेषण के लिए एक साथ लाया जाता है। यह गणना करने और अन्यथा डेटा में हेरफेर करने के लिए उच्च स्तर की लचीलापन प्रदान करता है। आपने रिलेशनल डेटाबेस की मूल अवधारणाओं को देखा है, और दो तालिकाओं के बीच जुड़ने का तरीका देखा है।\n+\n+## चुनौती\n+\n+इंटरनेट पर कई रिलेशनल डेटाबेस उपलब्ध हैं। आप ऊपर सीखे गए कौशल का उपयोग करके डेटा का पता लगा सकते हैं।\n+\n+## व्याख्यान के बाद प्रश्नोत्तरी\n+\n+## [व्याख्यान के बाद प्रश्नोत्तरी](https://red-water-0103e7a0f.azurestaticapps.net/quiz/9)\n+\n+## समीक्षा और आत्म अध्ययन\n+\n+आपके लिए SQL और रिलेशनल डेटाबेस अवधारणाओं की खोज जारी रखने के लिए [Microsoft Learn](https://docs.microsoft.com/learn?WT.mc_id=academic-40229-cxa) पर कई संसाधन उपलब्ध हैं\n+\n+- [संबंधपरक डेटा की अवधारणाओं का वर्णन करें](https://docs.microsoft.com//learn/modules/describe-concepts-of-relational-data?WT.mc_id=academic-40229-cxa)\n+- [Transact-SQL के साथ क्वेरी करना प्रारंभ करें](https://docs.microsoft.com//learn/paths/get-started-querying-with-transact-sql?WT.mc_id=academic-40229-cxa) (ट्रांजैक्ट-एसक्यूएल एसक्यूएल का एक संस्करण है)\n+- [Microsoft पर SQL सामग्री जानें](https://docs.microsoft.com/learn/browse/?products=azure-sql-database%2Csql-server&expanded=azure&WT.mc_id=academic-40229-cxa)\n+\n+## कार्यभार\n+\n+[असाइनमेंट शीर्षक](/2-Working-With-Data/05-relational-databases/assignment.md)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303da,microsoft/Data-Science-For-Beginners,875971373,825575742,SachinDabhade,78628027,2022-03-14T03:53:56+00:00,2022-03-14T03:53:56+00:00,Fixed duplicate line,true,1-Introduction/04-stats-and-probability/translations/README.hi.md,NaN,"@@ -0,0 +1,268 @@\n+# सांख्यिकी और संभाव्यता का संक्षिप्त परिचय\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev)](/sketchnotes/04-Statistics-Probability.png)|\n+|:---:|\n+| सांख्यिकी और संभावना - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+सांख्यिकी और संभाव्यता सिद्धांत गणित के दो अत्यधिक संबंधित क्षेत्र हैं जो डेटा विज्ञान के लिए अत्यधिक प्रासंगिक हैं। गणित के गहन ज्ञान के बिना डेटा के साथ काम करना संभव है, लेकिन कम से कम कुछ बुनियादी अवधारणाओं को जानना अभी भी बेहतर है। यहां हम एक संक्षिप्त परिचय प्रस्तुत करेंगे जो आपको आरंभ करने में मदद करेगा।\n+\n+[![Intro Video](/1-Introduction/04-stats-and-probability/images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)\n+\n+\n+## [Pre-lecture quiz](https://red-water-0103e7a0f.azurestaticapps.net/quiz/6)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303db,microsoft/Data-Science-For-Beginners,875971373,825575743,SachinDabhade,78628027,2022-03-14T03:53:56+00:00,2022-03-14T03:53:56+00:00,Fixed Duplicate Links,true,2-Working-With-Data/05-relational-databases/translations/README.hi.md,NaN,"@@ -0,0 +1,181 @@\n+# डेटा के साथ कार्य करना: संबंधपरक डेटाबेस\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](/sketchnotes/05-RelationalData.png)|\n+|:---:|\n+| डेटा के साथ कार्य करना: संबंधपरक डेटाबेस - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+संभावना है कि आपने जानकारी संग्रहीत करने के लिए अतीत में एक स्प्रेडशीट का उपयोग किया है। आपके पास पंक्तियों और स्तंभों का एक सेट था, जहाँ पंक्तियों में जानकारी (या डेटा) होती थी, और स्तंभों में जानकारी (कभी-कभी मेटाडेटा कहा जाता है) का वर्णन होता था। तालिकाओं में स्तंभों और पंक्तियों के इस मूल सिद्धांत पर एक संबंधपरक डेटाबेस बनाया गया है, जिससे आप कई तालिकाओं में जानकारी फैला सकते हैं। इससे आप अधिक जटिल डेटा के साथ काम कर सकते हैं, दोहराव से बच सकते हैं, और डेटा को एक्सप्लोर करने के तरीके में लचीलापन रख सकते हैं। आइए एक रिलेशनल डेटाबेस की अवधारणाओं का पता लगाएं।\n+\n+## [प्री-लेक्चर क्विज](https://red-water-0103e7a0f.azurestaticapps.net/quiz/8)\n+\n+## यह सब टेबल से शुरू होता है\n+\n+एक रिलेशनल डेटाबेस में इसकी कोर टेबल होती है। स्प्रैडशीट की तरह ही, तालिका स्तंभों और पंक्तियों का एक संग्रह है। पंक्ति में वह डेटा या जानकारी होती है जिसके साथ हम काम करना चाहते हैं, जैसे किसी शहर का नाम या वर्षा की मात्रा। कॉलम उनके द्वारा संग्रहीत डेटा का वर्णन करते हैं।\n+\n+आइए शहरों के बारे में जानकारी संग्रहीत करने के लिए एक तालिका शुरू करके अपनी खोज शुरू करें। हम उनके नाम और देश से शुरुआत कर सकते हैं। आप इसे एक टेबल में इस प्रकार स्टोर कर सकते हैं:\n+\n+| City     | Country       |\n+| -------- | ------------- |\n+| Tokyo    | Japan         |\n+| Atlanta  | United States |\n+| Auckland | New Zealand   |\n+\n+ध्यान दें कि **शहर**, **देश** और **जनसंख्या** के कॉलम नाम संग्रहीत किए जा रहे डेटा का वर्णन करते हैं, और प्रत्येक पंक्ति में एक शहर के बारे में जानकारी होती है।\n+\n+## सिंगल टेबल अप्रोच की कमियां\n+\n+संभावना है, ऊपर दी गई तालिका आपको अपेक्षाकृत परिचित लगती है। आइए अपने बढ़ते डेटाबेस में कुछ अतिरिक्त डेटा जोड़ना शुरू करें - वार्षिक वर्षा (मिलीमीटर में)। हम वर्ष 2018, 2019 और 2020 पर ध्यान केंद्रित करेंगे। अगर हम इसे टोक्यो के लिए जोड़ते हैं, तो यह कुछ इस तरह दिख सकता है:\n+\n+| City  | Country | Year | Amount |\n+| ----- | ------- | ---- | ------ |\n+| Tokyo | Japan   | 2020 | 1690   |\n+| Tokyo | Japan   | 2019 | 1874   |\n+| Tokyo | Japan   | 2018 | 1445   |\n+\n+आप हमारी तालिका के बारे में क्या देखते हैं? आप देख सकते हैं कि हम शहर के नाम और देश को बार-बार दोहरा रहे हैं। इसमें काफी स्टोरेज लग सकता है, और इसकी कई प्रतियां रखने के लिए काफी हद तक अनावश्यक है। आखिरकार, टोक्यो का केवल एक ही नाम है जिसमें हम रुचि रखते हैं।\n+\n+ठीक है, चलो कुछ और कोशिश करते हैं। आइए प्रत्येक वर्ष के लिए नए कॉलम जोड़ें:\n+\n+| City     | Country       | 2018 | 2019 | 2020 |\n+| -------- | ------------- | ---- | ---- | ---- |\n+| Tokyo    | Japan         | 1445 | 1874 | 1690 |\n+| Atlanta  | United States | 1779 | 1111 | 1683 |\n+| Auckland | New Zealand   | 1386 | 942  | 1176 |\n+\n+हालांकि यह पंक्ति दोहराव से बचा जाता है, लेकिन यह कुछ अन्य चुनौतियों को भी जोड़ता है। हर बार नया साल आने पर हमें अपनी तालिका की संरचना को संशोधित करने की आवश्यकता होगी। इसके अतिरिक्त, जैसे-जैसे हमारा डेटा बढ़ता है, वैसे-वैसे हमारे वर्षों में कॉलम के रूप में मूल्यों को पुनः प्राप्त करना और गणना करना मुश्किल हो जाएगा।\n+\n+यही कारण है कि हमें कई तालिकाओं और संबंधों की आवश्यकता है। अपने डेटा को अलग करके हम दोहराव से बच सकते हैं और हम अपने डेटा के साथ काम करने के तरीके में अधिक लचीलापन रखते हैं।\n+\n+## रिश्तों की अवधारणा\n+\n+आइए अपने डेटा पर वापस लौटें और निर्धारित करें कि हम चीजों को कैसे विभाजित करना चाहते हैं। हम जानते हैं कि हम अपने शहरों के लिए नाम और देश को संग्रहित करना चाहते हैं, इसलिए यह शायद एक टेबल में सबसे अच्छा काम करेगा।\n+\n+| City     | Country       |\n+| -------- | ------------- |\n+| Tokyo    | Japan         |\n+| Atlanta  | United States |\n+| Auckland | New Zealand   |\n+\n+लेकिन इससे पहले कि हम अगली तालिका बनाएं, हमें यह पता लगाना होगा कि प्रत्येक शहर को कैसे संदर्भित किया जाए। हमें किसी पहचानकर्ता, आईडी या (तकनीकी डेटाबेस के संदर्भ में) प्राथमिक कुंजी की आवश्यकता है। प्राथमिक कुंजी एक मान है जिसका उपयोग किसी तालिका में एक विशिष्ट पंक्ति की पहचान करने के लिए किया जाता है। हालांकि यह स्वयं एक मूल्य पर आधारित हो सकता है (उदाहरण के लिए, हम शहर के नाम का उपयोग कर सकते हैं), यह लगभग हमेशा एक संख्या या अन्य पहचानकर्ता होना चाहिए। हम नहीं चाहते कि आईडी कभी बदले क्योंकि इससे रिश्ता टूट जाएगा। आप ज्यादातर मामलों में पाएंगे कि प्राथमिक कुंजी या आईडी एक स्वतः उत्पन्न संख्या होगी।\n+\n+> प्राथमिक कुंजी को अक्सर पीके के रूप में संक्षिप्त किया जाता है\n+\n+### शहरों\n+\n+| city_id | City     | Country       |\n+| ------- | -------- | ------------- |\n+| 1       | Tokyo    | Japan         |\n+| 2       | Atlanta  | United States |\n+| 3       | Auckland | New Zealand   |\n+\n+> ✅ आप देखेंगे कि हम इस पाठ के दौरान ""आईडी"" और ""प्राथमिक कुंजी"" शब्दों का परस्पर उपयोग करते हैं। यहाँ की अवधारणाएँ DataFrames पर लागू होती हैं, जिन्हें आप बाद में एक्सप्लोर करेंगे। डेटाफ़्रेम ""प्राथमिक कुंजी"" की शब्दावली का उपयोग नहीं करते हैं, हालांकि आप देखेंगे कि वे उसी तरह से बहुत अधिक व्यवहार करते हैं।\n+\n+हमारे शहरों की तालिका बनाने के साथ, आइए वर्षा को संग्रहित करें। हम शहर के बारे में पूरी जानकारी की नकल करने के बजाय आईडी का उपयोग कर सकते हैं। हमें यह भी सुनिश्चित करना चाहिए कि नई बनाई गई तालिका में एक *id* कॉलम भी हो, क्योंकि सभी तालिकाओं में एक आईडी या प्राथमिक कुंजी होनी चाहिए।\n+\n+### बारिश\n+\n+| rainfall_id | city_id | Year | Amount |\n+| ----------- | ------- | ---- | ------ |\n+| 1           | 1       | 2018 | 1445   |\n+| 2           | 1       | 2019 | 1874   |\n+| 3           | 1       | 2020 | 1690   |\n+| 4           | 2       | 2018 | 1779   |\n+| 5           | 2       | 2019 | 1111   |\n+| 6           | 2       | 2020 | 1683   |\n+| 7           | 3       | 2018 | 1386   |\n+| 8           | 3       | 2019 | 942    |\n+| 9           | 3       | 2020 | 1176   |\n+\n+नव निर्मित **वर्षा** तालिका के अंदर **city_id** कॉलम पर ध्यान दें। इस कॉलम में वे मान हैं जो **शहरों** तालिका में आईडी का संदर्भ देते हैं। तकनीकी संबंधपरक डेटा के संदर्भ में, इसे **विदेशी कुंजी** कहा जाता है; यह किसी अन्य तालिका से प्राथमिक कुंजी है। आप इसे केवल एक संदर्भ या सूचक के रूप में सोच सकते हैं। **सिटी_आईडी** 1 संदर्भ टोक्यो।\n+\n+> [!नोट] विदेशी कुंजी को अक्सर FK . के रूप में संक्षिप्त किया जाता है\n+\n+## डेटा पुनर्प्राप्त करना\n+\n+हमारे डेटा को दो तालिकाओं में विभाजित करके, आप सोच रहे होंगे कि हम इसे कैसे पुनः प्राप्त करते हैं। यदि हम एक रिलेशनल डेटाबेस जैसे MySQL, SQL सर्वर या Oracle का उपयोग कर रहे हैं, तो हम स्ट्रक्चर्ड क्वेरी लैंग्वेज या SQL नामक भाषा का उपयोग कर सकते हैं। SQL (कभी-कभी उच्चारित अगली कड़ी) एक मानक भाषा है जिसका उपयोग रिलेशनल डेटाबेस में डेटा को पुनः प्राप्त करने और संशोधित करने के लिए किया जाता है।\n+\n+डेटा पुनर्प्राप्त करने के लिए आप `SELECT` कमांड का उपयोग करते हैं। इसके मूल में, आप उन स्तंभों को **चुनते हैं जिन्हें आप देखना चाहते हैं **से** उस तालिका में जिसमें वे शामिल हैं। यदि आप केवल शहरों के नाम प्रदर्शित करना चाहते हैं, तो आप निम्न का उपयोग कर सकते हैं:\n+\n+```sql\n+SELECT city\n+FROM cities;\n+\n+-- Output:\n+-- Tokyo\n+-- Atlanta\n+-- Auckland\n+```\n+\n+`चयन` वह जगह है जहां आप कॉलम सूचीबद्ध करते हैं, और `से` वह जगह है जहां आप टेबल सूचीबद्ध करते हैं।\n+\n+> [नोट] SQL सिंटैक्स केस-असंवेदनशील है, जिसका अर्थ है `चुनें` और `चयन` का मतलब एक ही है। हालाँकि, आप जिस प्रकार के डेटाबेस का उपयोग कर रहे हैं उसके आधार पर कॉलम और टेबल केस संवेदनशील हो सकते हैं। नतीजतन, प्रोग्रामिंग में हर चीज को हमेशा केस संवेदी की तरह व्यवहार करना सबसे अच्छा अभ्यास है। SQL क्वेरी लिखते समय सामान्य परंपरा यह है कि कीवर्ड को सभी अपर-केस अक्षरों में रखा जाए।\n+\n+उपरोक्त क्वेरी सभी शहरों को प्रदर्शित करेगी। आइए कल्पना करें कि हम केवल न्यूजीलैंड में शहरों को प्रदर्शित करना चाहते थे। हमें किसी प्रकार के फ़िल्टर की आवश्यकता है। इसके लिए SQL कीवर्ड `WHERE` या ""जहां कुछ सच है"" है।\n+\n+```sql\n+SELECT city\n+FROM cities\n+WHERE country = 'New Zealand';\n+\n+-- Output:\n+-- Auckland\n+```\n+\n+## डेटा में शामिल होना\n+\n+अब तक हमने एक ही टेबल से डेटा रिकवर किया है। अब हम **शहरों** और **वर्षा** दोनों के डेटा को एक साथ लाना चाहते हैं। यह उन्हें एक साथ *शामिल करके* किया जाता है। आप प्रभावी रूप से दो तालिकाओं के बीच एक सीम बनाएंगे, और प्रत्येक तालिका से एक कॉलम के मानों का मिलान करेंगे।\n+\n+हमारे उदाहरण में, हम **वर्षा** में **शहर_आईडी** कॉलम का मिलान **शहरों** में **शहर_आईडी** कॉलम से करेंगे। यह अपने संबंधित शहर के साथ वर्षा मूल्य से मेल खाएगा। हम जिस प्रकार के जॉइन करेंगे, उसे *इनर* जॉइन कहा जाता है, जिसका अर्थ है कि यदि कोई पंक्तियाँ दूसरी टेबल की किसी भी चीज़ से मेल नहीं खाती हैं तो वे प्रदर्शित नहीं होंगी। हमारे मामले में हर शहर में बारिश होती है, इसलिए सब कुछ प्रदर्शित किया जाएगा।\n+\n+आइए हमारे सभी शहरों के लिए 2019 की वर्षा को पुनः प्राप्त करें।\n+\n+हम इसे चरणों में करने जा रहे हैं। पहला कदम सीम के लिए कॉलम को इंगित करके डेटा को एक साथ जोड़ना है - **city_id** जैसा कि पहले हाइलाइट किया गया था।\n+\n+```sql\n+SELECT cities.city\n+    rainfall.amount\n+FROM cities\n+    INNER JOIN rainfall ON cities.city_id = rainfall.city_id\n+```\n+\n+हमने उन दो कॉलमों को हाइलाइट किया है जो हम चाहते हैं, और तथ्य यह है कि हम टेबल्स को **city_id** द्वारा एक साथ जोड़ना चाहते हैं। अब हम केवल वर्ष 2019 को फ़िल्टर करने के लिए `WHERE` स्टेटमेंट जोड़ सकते हैं।\n+\n+```sql\n+SELECT cities.city\n+    rainfall.amount\n+FROM cities\n+    INNER JOIN rainfall ON cities.city_id = rainfall.city_id\n+WHERE rainfall.year = 2019\n+\n+-- Output\n+\n+-- city     | amount\n+-- -------- | ------\n+-- Tokyo    | 1874\n+-- Atlanta  | 1111\n+-- Auckland |  942\n+```\n+\n+## सारांश\n+\n+रिलेशनल डेटाबेस कई तालिकाओं के बीच सूचनाओं को विभाजित करने के आसपास केंद्रित होते हैं जिन्हें बाद में प्रदर्शन और विश्लेषण के लिए एक साथ लाया जाता है। यह गणना करने और अन्यथा डेटा में हेरफेर करने के लिए उच्च स्तर की लचीलापन प्रदान करता है। आपने रिलेशनल डेटाबेस की मूल अवधारणाओं को देखा है, और दो तालिकाओं के बीच जुड़ने का तरीका देखा है।\n+\n+## चुनौती\n+\n+इंटरनेट पर कई रिलेशनल डेटाबेस उपलब्ध हैं। आप ऊपर सीखे गए कौशल का उपयोग करके डेटा का पता लगा सकते हैं।\n+\n+## व्याख्यान के बाद प्रश्नोत्तरी",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303dc,microsoft/Data-Science-For-Beginners,875971373,825576761,SachinDabhade,78628027,2022-03-14T03:57:35+00:00,2022-03-14T03:57:36+00:00,Fixed malformed link,true,1-Introduction/04-stats-and-probability/translations/README.hi.md,NaN,"@@ -0,0 +1,268 @@\n+# सांख्यिकी और संभाव्यता का संक्षिप्त परिचय\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev)](/sketchnotes/04-Statistics-Probability.png)|\n+|:---:|\n+| सांख्यिकी और संभावना - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+सांख्यिकी और संभाव्यता सिद्धांत गणित के दो अत्यधिक संबंधित क्षेत्र हैं जो डेटा विज्ञान के लिए अत्यधिक प्रासंगिक हैं। गणित के गहन ज्ञान के बिना डेटा के साथ काम करना संभव है, लेकिन कम से कम कुछ बुनियादी अवधारणाओं को जानना अभी भी बेहतर है। यहां हम एक संक्षिप्त परिचय प्रस्तुत करेंगे जो आपको आरंभ करने में मदद करेगा।\n+\n+[![Intro Video](/1-Introduction/04-stats-and-probability/images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)\n+\n+\n+## [Pre-lecture quiz](https://red-water-0103e7a0f.azurestaticapps.net/quiz/6)\n+\n+## प्रायिकता और यादृच्छिक चर\n+\n+**प्रायिकता** 0 और 1 के बीच की एक संख्या है जो यह व्यक्त करती है कि **ईवेंट** कितनी संभावित है। इसे कई सकारात्मक परिणामों के रूप में परिभाषित किया गया है (जो घटना की ओर ले जाते हैं), परिणामों की कुल संख्या से विभाजित, यह देखते हुए कि सभी परिणाम समान रूप से संभावित हैं। उदाहरण के लिए, जब हम एक पासे को उछालते हैं, तो हमें एक सम संख्या प्राप्त होने की प्रायिकता 3/6 = 0.5 होती है।\n+\n+जब हम घटनाओं के बारे में बात करते हैं, तो हम **यादृच्छिक चर** का उपयोग करते हैं। उदाहरण के लिए, यादृच्छिक चर जो एक पासे को घुमाते समय प्राप्त संख्या का प्रतिनिधित्व करता है, 1 से 6 तक मान लेगा। 1 से 6 तक की संख्याओं के सेट को **नमूना स्थान** कहा जाता है। हम एक निश्चित मान लेने वाले यादृच्छिक चर की संभावना के बारे में बात कर सकते हैं, उदाहरण के लिए पी (एक्स = 3) = 1/6।\n+\n+पिछले उदाहरण में यादृच्छिक चर को **असतत** कहा जाता है, क्योंकि इसमें एक गणनीय नमूना स्थान होता है, अर्थात अलग-अलग मान होते हैं जिन्हें गिना जा सकता है। ऐसे मामले हैं जब नमूना स्थान वास्तविक संख्याओं की एक श्रृंखला है, या वास्तविक संख्याओं का पूरा सेट है। ऐसे चरों को **सतत** कहा जाता है। एक अच्छा उदाहरण वह समय है जब बस आती है।\n+\n+## प्रायिकता वितरण\n+\n+असतत यादृच्छिक चर के मामले में, फ़ंक्शन P(X) द्वारा प्रत्येक घटना की प्रायिकता का वर्णन करना आसान है। नमूना स्थान *S* से प्रत्येक मान *s* के लिए यह 0 से 1 तक की संख्या देगा, जैसे कि सभी घटनाओं के लिए P(X=s) के सभी मानों का योग 1 होगा।\n+\n+सबसे प्रसिद्ध असतत वितरण **समान वितरण** है, जिसमें N तत्वों का एक नमूना स्थान होता है, जिनमें से प्रत्येक के लिए 1/N की समान संभावना होती है।\n+\n+एक सतत चर के संभाव्यता वितरण का वर्णन करना अधिक कठिन है, कुछ अंतराल [ए, बी], या वास्तविक संख्याओं के पूरे सेट से लिए गए मानों के साथ &Ropf;। बस आगमन समय के मामले पर विचार करें। वास्तव में, प्रत्येक सटीक आगमन समय *t* के लिए, ठीक उसी समय पर बस के आने की प्रायिकता 0 है!\n+\n+> अब आप जानते हैं कि 0 प्रायिकता वाली घटनाएँ होती हैं, और बहुत बार! कम से कम हर बार जब बस आती है!\n+\n+हम केवल दिए गए मानों के अंतराल में एक चर के गिरने की प्रायिकता के बारे में बात कर सकते हैं, उदाहरण के लिए। P(t<sub>1</sub>&le;X&lt;t<sub>2</sub>)। इस मामले में, प्रायिकता बंटन को **प्रायिकता घनत्व फलन** p(x) द्वारा वर्णित किया जाता है, जैसे कि\n+\n+![P(t_1\le X<t_2)=\int_{t_1}^{t_2}p(x)dx](/1-Introduction/04-stats-and-probability/images/probability-density.png)\n+  \n+एकसमान वितरण के एक सतत एनालॉग को **निरंतर वर्दी** कहा जाता है, जिसे एक सीमित अंतराल पर परिभाषित किया जाता है। एक संभावना है कि मान X लंबाई l के अंतराल में आता है l के समानुपाती है, और 1 तक बढ़ जाता है।\n+\n+एक अन्य महत्वपूर्ण वितरण **सामान्य वितरण** है, जिसके बारे में हम नीचे विस्तार से बात करेंगे।\n+\n+## माध्य, प्रसरण और मानक विचलन\n+मान लीजिए कि हम एक यादृच्छिक चर X के n नमूनों का एक क्रम बनाते हैं: x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>। हम पारंपरिक तरीके से अनुक्रम के **माध्य** (या **अंकगणित औसत**) मान को परिभाषित कर सकते हैं (x<sub>1</sub>+x<sub>2</sub>+x<sub >एन</उप>)/एन। जैसे-जैसे हम नमूने का आकार बढ़ाते हैं (अर्थात n&rr;&infin; के साथ सीमा लेते हैं), हम वितरण का माध्य (जिसे **अपेक्षा** भी कहते हैं) प्राप्त करेंगे। हम उम्मीद को **E**(x) से निरूपित करेंगे।\n+> यह प्रदर्शित किया जा सकता है कि मूल्यों के साथ किसी भी असतत वितरण के लिए {x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>N</sub>} and corresponding probabilities p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>N</sub>, the expectation would equal to E(X)=x<sub>1</sub>p<sub>1</sub>+x<sub>2</sub>p<sub>2</sub>+...+x<sub>N</sub>p<sub>N</sub>.\n+\n+यह पहचानने के लिए कि मान कितनी दूर तक फैले हुए हैं, हम प्रसरण की गणना कर सकते हैं &sigma;<sup>2</sup> = &sum;(x<sub>i</sub> - &mu;)<sup>2</sup>/ एन, जहां & एमयू; अनुक्रम का माध्य है। मूल्य &सिग्मा; इसे **मानक विचलन** कहा जाता है, और &sigma;<sup>2</sup> को **विचरण** कहा जाता है।\n+\n+## बहुलक, माध्यिका और चतुर्थक\n+\n+कभी-कभी, माध्य डेटा के लिए ""विशिष्ट"" मान का पर्याप्त रूप से प्रतिनिधित्व नहीं करता है। उदाहरण के लिए, जब कुछ चरम मान पूरी तरह से सीमा से बाहर होते हैं, तो वे माध्य को प्रभावित कर सकते हैं। एक और अच्छा संकेत एक **माध्य** है, एक मान ऐसा है कि आधा डेटा बिंदु इससे कम है, और दूसरा आधा - अधिक है।\n+\n+डेटा के वितरण को समझने में हमारी मदद करने के लिए, **चतुर्थक** के बारे में बात करना मददगार होगा:\n+\n+* प्रथम चतुर्थक, या Q1, एक मान है, जैसे कि 25% डेटा इससे नीचे आता है\n+* तीसरा चतुर्थक, या Q3, एक मान है कि 75% डेटा इससे नीचे आता है\n+\n+ग्राफिक रूप से हम **बॉक्स प्लॉट** नामक आरेख में माध्यिका और चतुर्थक के बीच संबंध का प्रतिनिधित्व कर सकते हैं:\n+\n+<img src=""/1-Introduction/04-stats-and-probability/images/boxplot_explanation.png"" width=""50%""/>\n+\n+यहां हम **अंतर-चतुर्थक श्रेणी** IQR=Q3-Q1, और तथाकथित **आउटलेयर** - मानों की भी गणना करते हैं, जो सीमाओं के बाहर होते हैं [Q1-1.5*IQR,Q3+1.5*IQR]।\n+\n+परिमित वितरण के लिए जिसमें कम संख्या में संभावित मान होते हैं, एक अच्छा ""विशिष्ट"" मान वह होता है जो सबसे अधिक बार प्रकट होता है, जिसे **मोड** कहा जाता है। इसे अक्सर रंग जैसे श्रेणीबद्ध डेटा पर लागू किया जाता है। एक ऐसी स्थिति पर विचार करें जब हमारे पास लोगों के दो समूह हों - कुछ जो लाल रंग को अधिक पसंद करते हैं, और अन्य जो नीले रंग को पसंद करते हैं। यदि हम रंगों को संख्याओं के आधार पर कोडित करते हैं, तो पसंदीदा रंग का माध्य मान नारंगी-हरे रंग के स्पेक्ट्रम में कहीं होगा, जो किसी भी समूह पर वास्तविक वरीयता को इंगित नहीं करता है। हालांकि, मोड या तो रंगों में से एक होगा, या दोनों रंग, यदि उनके लिए मतदान करने वाले लोगों की संख्या बराबर है (इस मामले में हम नमूने को **मल्टीमॉडल** कहते हैं)।\n+\n+## वास्तविक दुनिया का डेटा\n+\n+जब हम वास्तविक जीवन से डेटा का विश्लेषण करते हैं, तो वे अक्सर यादृच्छिक चर नहीं होते हैं, इस अर्थ में कि हम अज्ञात परिणाम के साथ प्रयोग नहीं करते हैं। उदाहरण के लिए, बेसबॉल खिलाड़ियों की एक टीम और उनके शरीर के डेटा, जैसे ऊंचाई, वजन और उम्र पर विचार करें। वे संख्याएँ बिल्कुल यादृच्छिक नहीं हैं, लेकिन हम अभी भी उन्हीं गणितीय अवधारणाओं को लागू कर सकते हैं। उदाहरण के लिए, लोगों के वजन के अनुक्रम को कुछ यादृच्छिक चर से निकाले गए मानों का अनुक्रम माना जा सकता है। [इस डेटासेट](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) से लिए गए [मेजर लीग बेसबॉल](http://mlb.mlb.com/index.jsp) से वास्तविक बेसबॉल खिलाड़ियों के वज़न का क्रम नीचे दिया गया ह। (आपकी सुविधा के लिए, केवल पहले 20 मान दिखाए गए हैं):\n+\n+\n+```\n+[180.0, 215.0, 210.0, 210.0, 188.0, 176.0, 209.0, 200.0, 231.0, 180.0, 188.0, 180.0, 185.0, 160.0, 180.0, 185.0, 197.0, 189.0, 185.0, 219.0]\n+```\n+\n+> **नोट**: इस डेटासेट के साथ काम करने का उदाहरण देखने के लिए, [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) पर एक नज़र डालें। इस पूरे पाठ में कई चुनौतियाँ भी हैं, और आप उस नोटबुक में कुछ कोड जोड़कर उन्हें पूरा कर सकते हैं। यदि आप सुनिश्चित नहीं हैं कि डेटा पर कैसे काम करना है, तो चिंता न करें - हम बाद में पायथन का उपयोग करके डेटा के साथ काम करने के लिए वापस आएंगे। यदि आप जुपिटर नोटबुक में कोड चलाना नहीं जानते हैं, तो [इस लेख](https://soshnikov.com/education/how-to-execute-notebooks-from-github/) पर एक नज़र डालें।\n+\n+हमारे डेटा के लिए माध्य, माध्यिका और चतुर्थक दिखाने वाला बॉक्स प्लॉट यहां दिया गया है:\n+\n+![वेट बॉक्स प्लॉट](/1-Introduction/04-stats-and-probability/images/weight-boxplot.png)\n+\n+चूंकि हमारे डेटा में अलग-अलग खिलाड़ी **भूमिकाएं** के बारे में जानकारी है, इसलिए हम भूमिका के आधार पर बॉक्स प्लॉट भी कर सकते हैं - यह हमें यह विचार प्राप्त करने की अनुमति देगा कि कैसे पैरामीटर मान भूमिकाओं में भिन्न होते हैं। इस बार हम ऊंचाई पर विचार करेंगे:\n+\n+![भूमिका के अनुसार बॉक्स प्लॉट](/1-Introduction/04-stats-and-probability/images/boxplot_byrole.png)\n+\n+यह आरेख बताता है कि, औसतन, पहले बेसमेन की ऊंचाई दूसरे बेसमेन की ऊंचाई से अधिक होती है। बाद में इस पाठ में हम सीखेंगे कि हम इस परिकल्पना का अधिक औपचारिक रूप से परीक्षण कैसे कर सकते हैं, और यह कैसे प्रदर्शित करें कि हमारा डेटा सांख्यिकीय रूप से महत्वपूर्ण है।\n+\n+> वास्तविक दुनिया के डेटा के साथ काम करते समय, हम मानते हैं कि सभी डेटा बिंदु कुछ संभाव्यता वितरण से लिए गए नमूने हैं। यह धारणा हमें मशीन लर्निंग तकनीकों को लागू करने और कार्यशील भविष्य कहनेवाला मॉडल बनाने की अनुमति देती है।\n+\n+यह देखने के लिए कि हमारे डेटा का वितरण क्या है, हम एक ग्राफ बना सकते हैं जिसे **हिस्टोग्राम** कहा जाता है। एक्स-अक्ष में कई अलग-अलग वज़न अंतराल (तथाकथित **बिन्स**) होंगे, और ऊर्ध्वाधर अक्ष दिखाएगा कि हमारा यादृच्छिक चर नमूना किसी दिए गए अंतराल के अंदर कितनी बार था।\n+\n+![वास्तविक विश्व डेटा का हिस्टोग्राम](/1-Introduction/04-stats-and-probability/images/weight-histogram.png)\n+\n+इस हिस्टोग्राम से आप देख सकते हैं कि सभी मान निश्चित औसत वजन के आसपास केंद्रित होते हैं, और हम उस वजन से जितना आगे जाते हैं - उस मान के कम वजन का सामना करना पड़ता है। यानी, यह बहुत ही असंभव है कि बेसबॉल खिलाड़ी का वजन औसत वजन से बहुत अलग होगा। भार में भिन्नता यह दर्शाती है कि भार किस हद तक माध्य से भिन्न होने की संभावना है।\n+\n+> अगर हम बेसबॉल लीग से नहीं, बल्कि अन्य लोगों का वजन लेते हैं, तो वितरण अलग होने की संभावना है। हालाँकि, वितरण का आकार समान होगा, लेकिन माध्य और विचरण बदल जाएगा। इसलिए, यदि हम अपने मॉडल को बेसबॉल खिलाड़ियों पर प्रशिक्षित करते हैं, तो विश्वविद्यालय के छात्रों पर लागू होने पर यह गलत परिणाम देने की संभावना है, क्योंकि अंतर्निहित वितरण अलग है।\n+\n+## सामान्य वितरण\n+\n+वजन का वितरण जो हमने ऊपर देखा है वह बहुत विशिष्ट है, और वास्तविक दुनिया से कई माप एक ही प्रकार के वितरण का पालन करते हैं, लेकिन अलग-अलग माध्य और भिन्नता के साथ। इस वितरण को **सामान्य वितरण** कहा जाता है, और यह आंकड़ों में बहुत महत्वपूर्ण भूमिका निभाता है।\n+\n+सामान्य वितरण का उपयोग करना संभावित बेसबॉल खिलाड़ियों के यादृच्छिक भार उत्पन्न करने का एक सही तरीका है। एक बार जब हम माध्य वजन `माध्य` और मानक विचलन `एसटीडी` जान लेते हैं, तो हम निम्नलिखित तरीके से 1000 वजन के नमूने तैयार कर सकते हैं:\n+```python\n+samples = np.random.normal(mean,std,1000)\n+```\n+\n+यदि हम उत्पन्न नमूनों के हिस्टोग्राम की साजिश करते हैं तो हम ऊपर दिखाए गए चित्र के समान ही चित्र देखेंगे। और अगर हम नमूनों की संख्या और डिब्बे की संख्या में वृद्धि करते हैं, तो हम एक सामान्य वितरण की एक तस्वीर उत्पन्न कर सकते हैं जो आदर्श के अधिक करीब है:\n+\n+![माध्य = 0 और एसटीडी.देव = 1 के साथ सामान्य वितरण](/1-Introduction/04-stats-and-probability/images/normal-histogram.png)\n+\n+\n+*माध्य = 0 और std.dev=1* के साथ सामान्य वितरण\n+\n+## विश्वास अंतराल\n+\n+जब हम बेसबॉल खिलाड़ियों के वजन के बारे में बात करते हैं, तो हम मानते हैं कि कुछ निश्चित **यादृच्छिक चर W** है जो सभी बेसबॉल खिलाड़ियों (तथाकथित **जनसंख्या**) के वजन के आदर्श संभाव्यता वितरण से मेल खाती है। वजन का हमारा क्रम सभी बेसबॉल खिलाड़ियों के एक उपसमुच्चय से मेल खाता है जिसे हम **नमूना** कहते हैं। एक दिलचस्प सवाल यह है कि क्या हम डब्ल्यू के वितरण के मापदंडों को जान सकते हैं, यानी जनसंख्या का माध्य और विचरण?\n+\n+सबसे आसान उत्तर हमारे नमूने के माध्य और विचरण की गणना करना होगा। हालाँकि, ऐसा हो सकता है कि हमारा यादृच्छिक नमूना पूर्ण जनसंख्या का सटीक रूप से प्रतिनिधित्व नहीं करता है। इस प्रकार **कॉन्फिडेंस इंटरवल** के बारे में बात करना समझ में आता है।\n+\n+> **विश्वास अंतराल** हमारे नमूने को देखते हुए जनसंख्या के वास्तविक माध्य का अनुमान है, जो एक निश्चित प्रायिकता (या **विश्वास का स्तर**) सटीक है।\n+\n+मान लीजिए हमारे पास हमारे वितरण से एक नमूना X<sub>1</sub>, ..., X<sub>n</sub> है। हर बार जब हम अपने वितरण से एक नमूना लेते हैं, तो हम अलग-अलग माध्य मान के साथ समाप्त होते हैं। इस प्रकार &mu; एक यादृच्छिक चर माना जा सकता है। एक **विश्वास अंतराल** विश्वास के साथ p मानों की एक जोड़ी है (L<sub>p</sub>,R<sub>p</sub>), जैसे कि **P**(L<sub>p </sub>&leq;&mu;&leq;R<sub>p</sub>) = p, यानी अंतराल के भीतर मापे गए माध्य मान के गिरने की प्रायिकता p के बराबर होती है।\n+\n+यह विस्तार से चर्चा करने के लिए हमारे संक्षिप्त परिचय से परे है कि उन आत्मविश्वास अंतराल की गणना कैसे की जाती है। कुछ और विवरण [विकिपीडिया पर](https://en.wikipedia.org/wiki/Confidence_interval) देखे जा सकते हैं। संक्षेप में, हम जनसंख्या के वास्तविक माध्य के सापेक्ष परिकलित नमूना माध्य के वितरण को परिभाषित करते हैं, जिसे **छात्र वितरण** कहा जाता है।\n+\n+> **दिलचस्प तथ्य**: छात्र वितरण का नाम गणितज्ञ विलियम सीली गॉसेट के नाम पर रखा गया है, जिन्होंने छद्म नाम ""स्टूडेंट"" के तहत अपना पेपर प्रकाशित किया था। उन्होंने गिनीज शराब की भठ्ठी में काम किया, और, एक संस्करण के अनुसार, उनके नियोक्ता नहीं चाहते थे कि आम जनता को पता चले कि वे कच्चे माल की गुणवत्ता निर्धारित करने के लिए सांख्यिकीय परीक्षणों का उपयोग कर रहे थे।\n+\n+यदि हम माध्य का अनुमान लगाना चाहते हैं &mu; हमारी जनसंख्या का विश्वास p के साथ, हमें छात्र वितरण A का *(1-p)/2-th प्रतिशत* लेने की आवश्यकता है, जिसे या तो तालिकाओं से लिया जा सकता है, या कंप्यूटर सांख्यिकीय सॉफ़्टवेयर के कुछ अंतर्निहित कार्यों का उपयोग करके (उदाहरण के लिए पायथन, आर, आदि)। फिर अंतराल के लिए &mu; X&pm;A*D/&radic;n द्वारा दिया जाएगा, जहां X नमूने का प्राप्त माध्य है, D मानक विचलन है।\n+\n+> **नोट**: हम [स्वतंत्रता की डिग्री](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) की एक महत्वपूर्ण अवधारणा की चर्चा को भी छोड़ देते हैं, जो छात्र वितरण के संबंध में महत्वपूर्ण है। इस अवधारणा को गहराई से समझने के लिए आप सांख्यिकी पर अधिक संपूर्ण पुस्तकों का उल्लेख कर सकते हैं।\n+\n+वजन और ऊंचाई के लिए आत्मविश्वास अंतराल की गणना का एक उदाहरण [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) में दिया गया है।\n+\n+| p | Weight mean |\n+|-----|-----------|\n+| 0.85 | 201.73±0.94 |\n+| 0.90 | 201.73±1.08 |\n+| 0.95 | 201.73±1.28 |\n+\n+ध्यान दें कि आत्मविश्वास की संभावना जितनी अधिक होगी, विश्वास अंतराल उतना ही व्यापक होगा।\n+\n+## परिकल्पना परीक्षण \n+\n+हमारे बेसबॉल खिलाड़ियों के डेटासेट में, अलग-अलग खिलाड़ी भूमिकाएँ होती हैं, जिन्हें नीचे संक्षेप में प्रस्तुत किया जा सकता है (इस तालिका की गणना कैसे की जा सकती है, यह देखने के लिए [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) देखें):\n+\n+| Role | Height | Weight | Count |\n+|------|--------|--------|-------|\n+| Catcher | 72.723684 | 204.328947 | 76 |\n+| Designated_Hitter | 74.222222 | 220.888889 | 18 |\n+| First_Baseman | 74.000000 | 213.109091 | 55 |\n+| Outfielder | 73.010309 | 199.113402 | 194 |\n+| Relief_Pitcher | 74.374603 | 203.517460 | 315 |\n+| Second_Baseman | 71.362069 | 184.344828 | 58 |\n+| Shortstop | 71.903846 | 182.923077 | 52 |\n+| Starting_Pitcher | 74.719457 | 205.163636 | 221 |\n+| Third_Baseman | 73.044444 | 200.955556 | 45 |\n+\n+हम देख सकते हैं कि पहले बेसमेन की औसत ऊंचाई दूसरे बेसमेन की तुलना में अधिक है। इस प्रकार, हम यह निष्कर्ष निकालने के लिए ललचा सकते हैं कि **पहले बेसमेन दूसरे बेसमेन से अधिक हैं**।\n+\n+> इस कथन को **एक परिकल्पना** कहा जाता है, क्योंकि हम नहीं जानते कि तथ्य वास्तव में सत्य है या नहीं।\n+\n+हालांकि, यह हमेशा स्पष्ट नहीं होता है कि क्या हम यह निष्कर्ष निकाल सकते हैं। ऊपर की चर्चा से हम जानते हैं कि प्रत्येक माध्य का एक संबद्ध विश्वास अंतराल होता है, और इस प्रकार यह अंतर केवल एक सांख्यिकीय त्रुटि हो सकता है। हमें अपनी परिकल्पना का परीक्षण करने के लिए कुछ और औपचारिक तरीके की आवश्यकता है।\n+\n+आइए पहले और दूसरे बेसमेन की ऊंचाई के लिए अलग-अलग आत्मविश्वास अंतराल की गणना करें:\n+\n+| Confidence | First Basemen | Second Basemen |\n+|------------|---------------|----------------|\n+| 0.85 | 73.62..74.38 | 71.04..71.69 |\n+| 0.90 | 73.56..74.44 | 70.99..71.73 |\n+| 0.95 | 73.47..74.53 | 70.92..71.81 |\n+\n+हम देख सकते हैं कि बिना किसी विश्वास के अंतराल ओवरलैप हो जाते हैं। इससे हमारी परिकल्पना सिद्ध होती है कि पहले बेसमेन दूसरे बेसमेन से ऊंचे होते हैं।\n+\n+अधिक औपचारिक रूप से, हम जिस समस्या को हल कर रहे हैं वह यह देखना है कि क्या **दो संभाव्यता वितरण समान हैं**, या कम से कम समान पैरामीटर हैं। वितरण के आधार पर, हमें उसके लिए विभिन्न परीक्षणों का उपयोग करने की आवश्यकता है। यदि हम जानते हैं कि हमारे वितरण सामान्य हैं, तो हम **[स्टूडेंट टी-टेस्ट](https://en.wikipedia.org/wiki/Student%27s_t-test)** लागू कर सकते हैं।\n+\n+स्टूडेंट टी-टेस्ट में, हम तथाकथित **टी-वैल्यू** की गणना करते हैं, जो भिन्नता को ध्यान में रखते हुए, साधनों के बीच अंतर को इंगित करता है। यह प्रदर्शित किया जाता है कि टी-मान **छात्र वितरण** का अनुसरण करता है, जो हमें दिए गए आत्मविश्वास स्तर **p** के लिए थ्रेशोल्ड मान प्राप्त करने की अनुमति देता है (इसकी गणना की जा सकती है, या संख्यात्मक तालिकाओं में देखा जा सकता है)। फिर हम परिकल्पना को स्वीकृत या अस्वीकार करने के लिए टी-मान की तुलना इस सीमा से करते हैं।\n+\n+पायथन में, हम **SciPy** पैकेज का उपयोग कर सकते हैं, जिसमें `ttest_ind` फ़ंक्शन शामिल है (कई अन्य उपयोगी सांख्यिकीय कार्यों के अलावा!)। यह हमारे लिए टी-वैल्यू की गणना करता है, और कॉन्फिडेंस पी-वैल्यू की रिवर्स लुकअप भी करता है, ताकि हम निष्कर्ष निकालने के लिए कॉन्फिडेंस को देख सकें।\n+\n+उदाहरण के लिए, पहले और दूसरे बेसमेन की ऊंचाई के बीच हमारी तुलना हमें निम्नलिखित परिणाम देती है:\n+\n+```python\n+from scipy.stats import ttest_ind\n+\n+tval, pval = ttest_ind(df.loc[df['Role']=='First_Baseman',['Height']], df.loc[df['Role']=='Designated_Hitter',['Height']],equal_var=False)\n+print(f""T-value = {tval[0]:.2f}\nP-value: {pval[0]}"")\n+```\n+```\n+T-value = 7.65\n+P-value: 9.137321189738925e-12\n+```\n+\n+हमारे मामले में, पी-वैल्यू बहुत कम है, जिसका अर्थ है कि इस बात का समर्थन करने वाले मजबूत सबूत हैं कि पहले बेसमेन लम्बे होते हैं।\n+\n+अन्य विभिन्न प्रकार की परिकल्पनाएँ भी हैं जिनका हम परीक्षण करना चाहते हैं, उदाहरण के लिए:\n+* यह साबित करने के लिए कि दिया गया नमूना कुछ वितरण का अनुसरण करता है। हमारे मामले में हमने मान लिया है कि ऊंचाई सामान्य रूप से वितरित की जाती है, लेकिन इसके लिए औपचारिक सांख्यिकीय सत्यापन की आवश्यकता होती है।\n+* यह सिद्ध करने के लिए कि नमूने का माध्य मान कुछ पूर्वनिर्धारित मान से मेल खाता है\n+* कई नमूनों के साधनों की तुलना करना (उदाहरण के लिए विभिन्न आयु समूहों में खुशी के स्तर में क्या अंतर है)\n+\n+## बड़ी संख्या का नियम और केंद्रीय सीमा प्रमेय\n+\n+सामान्य वितरण के इतना महत्वपूर्ण होने का एक कारण तथाकथित **केंद्रीय सीमा प्रमेय** है। मान लीजिए कि हमारे पास स्वतंत्र N मानों X<sub>1</sub>, ..., X<sub>N</sub> का एक बड़ा नमूना है, जिसे माध्य &mu; और विचरण &सिग्मा;<sup>2</sup>. फिर, पर्याप्त रूप से बड़े N के लिए (दूसरे शब्दों में, जब N&rarr;&infin;), माध्य &Sigma;<sub>i</sub>X<sub>i</sub> को सामान्य रूप से माध्य &mu के साथ वितरित किया जाएगा; और विचरण &sigma;<sup>2</sup>/N.\n+\n+> केंद्रीय सीमा प्रमेय की व्याख्या करने का एक अन्य तरीका यह कहना है कि वितरण की परवाह किए बिना, जब आप किसी भी यादृच्छिक चर मानों के योग के माध्य की गणना करते हैं तो आप सामान्य वितरण के साथ समाप्त होते हैं।\n+\n+केंद्रीय सीमा प्रमेय से यह भी पता चलता है कि, जब N&rar;&infin;, नमूने के माध्य की प्रायिकता &mu; बन जाता है 1. इसे **बड़ी संख्या का नियम** कहते हैं।\n+\n+## सहप्रसरण और सहसंबंध\n+\n+डेटा साइंस द्वारा की जाने वाली चीजों में से एक डेटा के बीच संबंध ढूंढ रहा है। हम कहते हैं कि दो अनुक्रम **सहसम्बन्ध** तब होते हैं जब वे एक ही समय में समान व्यवहार प्रदर्शित करते हैं, अर्थात वे या तो एक साथ उठते/गिरते हैं, या एक क्रम ऊपर उठता है जब दूसरा गिरता है और इसके विपरीत। दूसरे शब्दों में, दो अनुक्रमों के बीच कुछ संबंध प्रतीत होता है।\n+\n+> सहसंबंध आवश्यक रूप से दो अनुक्रमों के बीच कारण संबंध को इंगित नहीं करता है; कभी-कभी दोनों चर किसी बाहरी कारण पर निर्भर हो सकते हैं, या यह विशुद्ध रूप से संयोग से दो अनुक्रम सहसंबद्ध हो सकते हैं। हालांकि, मजबूत गणितीय सहसंबंध एक अच्छा संकेत है कि दो चर किसी न किसी तरह से जुड़े हुए हैं।\n+\n+गणितीय रूप से, मुख्य अवधारणा जो दो यादृच्छिक चर के बीच संबंध दिखाती है, वह है **सहप्रसरण**, जिसकी गणना इस प्रकार की जाती है: Cov(X,Y) = **E**\[(X-**E**(X) ))(वाई-**ई**(वाई))\]। हम दोनों चरों के विचलन की गणना उनके माध्य मानों से करते हैं, और फिर उन विचलनों के गुणनफल की गणना करते हैं। यदि दोनों चर एक साथ विचलित होते हैं, तो उत्पाद हमेशा एक सकारात्मक मूल्य होगा, जो कि सकारात्मक सहप्रसरण को जोड़ देगा। यदि दोनों चर आउट-ऑफ-सिंक विचलित हो जाते हैं (अर्थात एक औसत से नीचे गिर जाता है जब दूसरा औसत से ऊपर उठता है), तो हमें हमेशा ऋणात्मक संख्याएँ मिलेंगी, जो कि ऋणात्मक सहप्रसरण को जोड़ देंगी। यदि विचलन निर्भर नहीं हैं, तो वे लगभग शून्य तक जोड़ देंगे।\n+\n+सहप्रसरण का निरपेक्ष मान हमें यह नहीं बताता कि सहसंबंध कितना बड़ा है, क्योंकि यह वास्तविक मूल्यों के परिमाण पर निर्भर करता है। इसे सामान्य करने के लिए, हम **सहसंबंध** प्राप्त करने के लिए, दोनों चरों के मानक विचलन द्वारा सहप्रसरण को विभाजित कर सकते हैं। अच्छी बात यह है कि सहसंबंध हमेशा [-1,1] की सीमा में होता है, जहां 1 मूल्यों के बीच मजबूत सकारात्मक सहसंबंध को इंगित करता है, -1 - मजबूत नकारात्मक सहसंबंध, और 0 - बिल्कुल भी कोई संबंध नहीं (चर स्वतंत्र हैं)।\n+\n+**उदाहरण**: हम ऊपर बताए गए डेटासेट से बेसबॉल खिलाड़ियों के वज़न और ऊंचाई के बीच सहसंबंध की गणना कर सकते हैं:\n+\n+```python\n+print(np.corrcoef(weights,heights))\n+```\n+As a result, we get **correlation matrix** like this one:\n+```\n+array([[1.        , 0.52959196],\n+       [0.52959196, 1.        ]])\n+```\n+\n+> सहसंबंध मैट्रिक्स C की गणना किसी भी इनपुट अनुक्रम S<sub>1</sub>, ..., S<sub>n</sub> के लिए की जा सकती है। C<sub>ij</sub> का मान S<sub>i</sub> और S<sub>j</sub> के बीच सहसंबंध है, और विकर्ण तत्व हमेशा 1 होते हैं (जो कि स्व-सहसंबंध भी है एस<उप>मैं</sub>)।\n+\n+हमारे मामले में, मान 0.53 इंगित करता है कि किसी व्यक्ति के वजन और ऊंचाई के बीच कुछ संबंध है। हम रिश्ते को देखने के लिए दूसरे के खिलाफ एक मूल्य का स्कैटर प्लॉट भी बना सकते हैं:\n+\n+![वजन और ऊंचाई के बीच संबंध](/1-Introduction/04-stats-and-probability/images/weight-height-relationship.png)\n+\n+> सहसंबंध और सहप्रसरण के अधिक उदाहरण [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) में पाए जा सकते हैं।\n+\n+## निष्कर्ष\n+\n+इस भाग में हमने सीखा है:\n+\n+* डेटा के बुनियादी सांख्यिकीय गुण, जैसे माध्य, विचरण, मोड और चतुर्थक\n+* सामान्य वितरण सहित यादृच्छिक चर के विभिन्न वितरण\n+* विभिन्न गुणों के बीच सहसंबंध कैसे खोजें\n+* कुछ परिकल्पनाओं को सिद्ध करने के लिए गणित और सांख्यिकी के ध्वनि उपकरण का उपयोग कैसे करें,\n+* यादृच्छिक चर दिए गए डेटा नमूने के लिए विश्वास अंतराल की गणना कैसे करें\n+\n+हालांकि यह निश्चित रूप से उन विषयों की संपूर्ण सूची नहीं है जो संभाव्यता और आंकड़ों के भीतर मौजूद हैं, यह आपको इस पाठ्यक्रम में एक अच्छी शुरुआत देने के लिए पर्याप्त होना चाहिए।\n+\n+## चुनौती\n+\n+अन्य परिकल्पना का परीक्षण करने के लिए नोटबुक में नमूना कोड का उपयोग करें जो:\n+1. पहले बेसमेन दूसरे बेसमेन से बड़े होते हैं\n+2. पहले बेसमेन तीसरे बेसमेन से लम्बे होते हैं\n+3. शॉर्टस्टॉप दूसरे बेसमेन से लम्बे होते हैं\n+\n+## [व्याख्यान के बाद प्रश्नोत्तरी](https://red-water-0103e7a0f.azurestaticapps.net/quiz/7)\n+\n+## समीक्षा और आत्म अध्ययन\n+\n+संभाव्यता और सांख्यिकी इतना व्यापक विषय है कि यह अपने पाठ्यक्रम के योग्य है। यदि आप सिद्धांत में गहराई तक जाने में रुचि रखते हैं, तो आप निम्नलिखित में से कुछ पुस्तकों को पढ़ना जारी रख सकते हैं:\n+\n+1. न्यूयॉर्क विश्वविद्यालय के [कार्लोस फर्नांडीज-ग्रांडा](https://cims.nyu.edu/~cfgranda/) के पास डेटा साइंस के लिए [संभाव्यता और सांख्यिकी](https://cims.nyu.edu/~cfgranda/pages/stuff/probability_stats_for_DS.pdf) (ऑनलाइन उपलब्ध) के लिए महान व्याख्यान नोट्स हैं।\n+1. [पीटर और एंड्रयू ब्रूस। डेटा वैज्ञानिकों के लिए व्यावहारिक सांख्यिकी](https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/) । [[आर . में नमूना कोड](https://github.com/PacktPublishing/Statistics-for-Data-Science)]\n+1. [जेम्स डी. मिलर। डेटा विज्ञान के लिए सांख्यिकी](https://www.packtpub.com/product/statistics-for-data-science/9781788290678) [[आर . में नमूना कोड](https://github.com/PacktPublishing/Statistics-for-Data-Science)] \n+\n+## कार्यभार\n+\n+[लघु मधुमेह अध्ययन] (असाइनमेंट.एमडी)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303dd,microsoft/Data-Science-For-Beginners,875971373,825577321,SachinDabhade,78628027,2022-03-14T03:59:59+00:00,2022-03-14T03:59:59+00:00,Translated some missed translate word,true,1-Introduction/04-stats-and-probability/translations/README.hi.md,NaN,"@@ -0,0 +1,268 @@\n+# सांख्यिकी और संभाव्यता का संक्षिप्त परिचय\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev)](/sketchnotes/04-Statistics-Probability.png)|\n+|:---:|\n+| सांख्यिकी और संभावना - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+सांख्यिकी और संभाव्यता सिद्धांत गणित के दो अत्यधिक संबंधित क्षेत्र हैं जो डेटा विज्ञान के लिए अत्यधिक प्रासंगिक हैं। गणित के गहन ज्ञान के बिना डेटा के साथ काम करना संभव है, लेकिन कम से कम कुछ बुनियादी अवधारणाओं को जानना अभी भी बेहतर है। यहां हम एक संक्षिप्त परिचय प्रस्तुत करेंगे जो आपको आरंभ करने में मदद करेगा।\n+\n+[![Intro Video](/1-Introduction/04-stats-and-probability/images/video-prob-and-stats.png)](https://youtu.be/Z5Zy85g4Yjw)\n+\n+\n+## [Pre-lecture quiz](https://red-water-0103e7a0f.azurestaticapps.net/quiz/6)\n+\n+## प्रायिकता और यादृच्छिक चर\n+\n+**प्रायिकता** 0 और 1 के बीच की एक संख्या है जो यह व्यक्त करती है कि **ईवेंट** कितनी संभावित है। इसे कई सकारात्मक परिणामों के रूप में परिभाषित किया गया है (जो घटना की ओर ले जाते हैं), परिणामों की कुल संख्या से विभाजित, यह देखते हुए कि सभी परिणाम समान रूप से संभावित हैं। उदाहरण के लिए, जब हम एक पासे को उछालते हैं, तो हमें एक सम संख्या प्राप्त होने की प्रायिकता 3/6 = 0.5 होती है।\n+\n+जब हम घटनाओं के बारे में बात करते हैं, तो हम **यादृच्छिक चर** का उपयोग करते हैं। उदाहरण के लिए, यादृच्छिक चर जो एक पासे को घुमाते समय प्राप्त संख्या का प्रतिनिधित्व करता है, 1 से 6 तक मान लेगा। 1 से 6 तक की संख्याओं के सेट को **नमूना स्थान** कहा जाता है। हम एक निश्चित मान लेने वाले यादृच्छिक चर की संभावना के बारे में बात कर सकते हैं, उदाहरण के लिए पी (एक्स = 3) = 1/6।\n+\n+पिछले उदाहरण में यादृच्छिक चर को **असतत** कहा जाता है, क्योंकि इसमें एक गणनीय नमूना स्थान होता है, अर्थात अलग-अलग मान होते हैं जिन्हें गिना जा सकता है। ऐसे मामले हैं जब नमूना स्थान वास्तविक संख्याओं की एक श्रृंखला है, या वास्तविक संख्याओं का पूरा सेट है। ऐसे चरों को **सतत** कहा जाता है। एक अच्छा उदाहरण वह समय है जब बस आती है।\n+\n+## प्रायिकता वितरण\n+\n+असतत यादृच्छिक चर के मामले में, फ़ंक्शन P(X) द्वारा प्रत्येक घटना की प्रायिकता का वर्णन करना आसान है। नमूना स्थान *S* से प्रत्येक मान *s* के लिए यह 0 से 1 तक की संख्या देगा, जैसे कि सभी घटनाओं के लिए P(X=s) के सभी मानों का योग 1 होगा।\n+\n+सबसे प्रसिद्ध असतत वितरण **समान वितरण** है, जिसमें N तत्वों का एक नमूना स्थान होता है, जिनमें से प्रत्येक के लिए 1/N की समान संभावना होती है।\n+\n+एक सतत चर के संभाव्यता वितरण का वर्णन करना अधिक कठिन है, कुछ अंतराल [ए, बी], या वास्तविक संख्याओं के पूरे सेट से लिए गए मानों के साथ &Ropf;। बस आगमन समय के मामले पर विचार करें। वास्तव में, प्रत्येक सटीक आगमन समय *t* के लिए, ठीक उसी समय पर बस के आने की प्रायिकता 0 है!\n+\n+> अब आप जानते हैं कि 0 प्रायिकता वाली घटनाएँ होती हैं, और बहुत बार! कम से कम हर बार जब बस आती है!\n+\n+हम केवल दिए गए मानों के अंतराल में एक चर के गिरने की प्रायिकता के बारे में बात कर सकते हैं, उदाहरण के लिए। P(t<sub>1</sub>&le;X&lt;t<sub>2</sub>)। इस मामले में, प्रायिकता बंटन को **प्रायिकता घनत्व फलन** p(x) द्वारा वर्णित किया जाता है, जैसे कि\n+\n+![P(t_1\le X<t_2)=\int_{t_1}^{t_2}p(x)dx](/1-Introduction/04-stats-and-probability/images/probability-density.png)\n+  \n+एकसमान वितरण के एक सतत एनालॉग को **निरंतर वर्दी** कहा जाता है, जिसे एक सीमित अंतराल पर परिभाषित किया जाता है। एक संभावना है कि मान X लंबाई l के अंतराल में आता है l के समानुपाती है, और 1 तक बढ़ जाता है।\n+\n+एक अन्य महत्वपूर्ण वितरण **सामान्य वितरण** है, जिसके बारे में हम नीचे विस्तार से बात करेंगे।\n+\n+## माध्य, प्रसरण और मानक विचलन\n+मान लीजिए कि हम एक यादृच्छिक चर X के n नमूनों का एक क्रम बनाते हैं: x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>। हम पारंपरिक तरीके से अनुक्रम के **माध्य** (या **अंकगणित औसत**) मान को परिभाषित कर सकते हैं (x<sub>1</sub>+x<sub>2</sub>+x<sub >एन</उप>)/एन। जैसे-जैसे हम नमूने का आकार बढ़ाते हैं (अर्थात n&rr;&infin; के साथ सीमा लेते हैं), हम वितरण का माध्य (जिसे **अपेक्षा** भी कहते हैं) प्राप्त करेंगे। हम उम्मीद को **E**(x) से निरूपित करेंगे।\n+> यह प्रदर्शित किया जा सकता है कि मूल्यों के साथ किसी भी असतत वितरण के लिए {x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>N</sub>} and corresponding probabilities p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>N</sub>, the expectation would equal to E(X)=x<sub>1</sub>p<sub>1</sub>+x<sub>2</sub>p<sub>2</sub>+...+x<sub>N</sub>p<sub>N</sub>.\n+\n+यह पहचानने के लिए कि मान कितनी दूर तक फैले हुए हैं, हम प्रसरण की गणना कर सकते हैं &sigma;<sup>2</sup> = &sum;(x<sub>i</sub> - &mu;)<sup>2</sup>/ एन, जहां & एमयू; अनुक्रम का माध्य है। मूल्य &सिग्मा; इसे **मानक विचलन** कहा जाता है, और &sigma;<sup>2</sup> को **विचरण** कहा जाता है।\n+\n+## बहुलक, माध्यिका और चतुर्थक\n+\n+कभी-कभी, माध्य डेटा के लिए ""विशिष्ट"" मान का पर्याप्त रूप से प्रतिनिधित्व नहीं करता है। उदाहरण के लिए, जब कुछ चरम मान पूरी तरह से सीमा से बाहर होते हैं, तो वे माध्य को प्रभावित कर सकते हैं। एक और अच्छा संकेत एक **माध्य** है, एक मान ऐसा है कि आधा डेटा बिंदु इससे कम है, और दूसरा आधा - अधिक है।\n+\n+डेटा के वितरण को समझने में हमारी मदद करने के लिए, **चतुर्थक** के बारे में बात करना मददगार होगा:\n+\n+* प्रथम चतुर्थक, या Q1, एक मान है, जैसे कि 25% डेटा इससे नीचे आता है\n+* तीसरा चतुर्थक, या Q3, एक मान है कि 75% डेटा इससे नीचे आता है\n+\n+ग्राफिक रूप से हम **बॉक्स प्लॉट** नामक आरेख में माध्यिका और चतुर्थक के बीच संबंध का प्रतिनिधित्व कर सकते हैं:\n+\n+<img src=""/1-Introduction/04-stats-and-probability/images/boxplot_explanation.png"" width=""50%""/>\n+\n+यहां हम **अंतर-चतुर्थक श्रेणी** IQR=Q3-Q1, और तथाकथित **आउटलेयर** - मानों की भी गणना करते हैं, जो सीमाओं के बाहर होते हैं [Q1-1.5*IQR,Q3+1.5*IQR]।\n+\n+परिमित वितरण के लिए जिसमें कम संख्या में संभावित मान होते हैं, एक अच्छा ""विशिष्ट"" मान वह होता है जो सबसे अधिक बार प्रकट होता है, जिसे **मोड** कहा जाता है। इसे अक्सर रंग जैसे श्रेणीबद्ध डेटा पर लागू किया जाता है। एक ऐसी स्थिति पर विचार करें जब हमारे पास लोगों के दो समूह हों - कुछ जो लाल रंग को अधिक पसंद करते हैं, और अन्य जो नीले रंग को पसंद करते हैं। यदि हम रंगों को संख्याओं के आधार पर कोडित करते हैं, तो पसंदीदा रंग का माध्य मान नारंगी-हरे रंग के स्पेक्ट्रम में कहीं होगा, जो किसी भी समूह पर वास्तविक वरीयता को इंगित नहीं करता है। हालांकि, मोड या तो रंगों में से एक होगा, या दोनों रंग, यदि उनके लिए मतदान करने वाले लोगों की संख्या बराबर है (इस मामले में हम नमूने को **मल्टीमॉडल** कहते हैं)।\n+\n+## वास्तविक दुनिया का डेटा\n+\n+जब हम वास्तविक जीवन से डेटा का विश्लेषण करते हैं, तो वे अक्सर यादृच्छिक चर नहीं होते हैं, इस अर्थ में कि हम अज्ञात परिणाम के साथ प्रयोग नहीं करते हैं। उदाहरण के लिए, बेसबॉल खिलाड़ियों की एक टीम और उनके शरीर के डेटा, जैसे ऊंचाई, वजन और उम्र पर विचार करें। वे संख्याएँ बिल्कुल यादृच्छिक नहीं हैं, लेकिन हम अभी भी उन्हीं गणितीय अवधारणाओं को लागू कर सकते हैं। उदाहरण के लिए, लोगों के वजन के अनुक्रम को कुछ यादृच्छिक चर से निकाले गए मानों का अनुक्रम माना जा सकता है। [इस डेटासेट](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights) से लिए गए [मेजर लीग बेसबॉल](http://mlb.mlb.com/index.jsp) से वास्तविक बेसबॉल खिलाड़ियों के वज़न का क्रम नीचे दिया गया ह। (आपकी सुविधा के लिए, केवल पहले 20 मान दिखाए गए हैं):\n+\n+\n+```\n+[180.0, 215.0, 210.0, 210.0, 188.0, 176.0, 209.0, 200.0, 231.0, 180.0, 188.0, 180.0, 185.0, 160.0, 180.0, 185.0, 197.0, 189.0, 185.0, 219.0]\n+```\n+\n+> **नोट**: इस डेटासेट के साथ काम करने का उदाहरण देखने के लिए, [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) पर एक नज़र डालें। इस पूरे पाठ में कई चुनौतियाँ भी हैं, और आप उस नोटबुक में कुछ कोड जोड़कर उन्हें पूरा कर सकते हैं। यदि आप सुनिश्चित नहीं हैं कि डेटा पर कैसे काम करना है, तो चिंता न करें - हम बाद में पायथन का उपयोग करके डेटा के साथ काम करने के लिए वापस आएंगे। यदि आप जुपिटर नोटबुक में कोड चलाना नहीं जानते हैं, तो [इस लेख](https://soshnikov.com/education/how-to-execute-notebooks-from-github/) पर एक नज़र डालें।\n+\n+हमारे डेटा के लिए माध्य, माध्यिका और चतुर्थक दिखाने वाला बॉक्स प्लॉट यहां दिया गया है:\n+\n+![वेट बॉक्स प्लॉट](/1-Introduction/04-stats-and-probability/images/weight-boxplot.png)\n+\n+चूंकि हमारे डेटा में अलग-अलग खिलाड़ी **भूमिकाएं** के बारे में जानकारी है, इसलिए हम भूमिका के आधार पर बॉक्स प्लॉट भी कर सकते हैं - यह हमें यह विचार प्राप्त करने की अनुमति देगा कि कैसे पैरामीटर मान भूमिकाओं में भिन्न होते हैं। इस बार हम ऊंचाई पर विचार करेंगे:\n+\n+![भूमिका के अनुसार बॉक्स प्लॉट](/1-Introduction/04-stats-and-probability/images/boxplot_byrole.png)\n+\n+यह आरेख बताता है कि, औसतन, पहले बेसमेन की ऊंचाई दूसरे बेसमेन की ऊंचाई से अधिक होती है। बाद में इस पाठ में हम सीखेंगे कि हम इस परिकल्पना का अधिक औपचारिक रूप से परीक्षण कैसे कर सकते हैं, और यह कैसे प्रदर्शित करें कि हमारा डेटा सांख्यिकीय रूप से महत्वपूर्ण है।\n+\n+> वास्तविक दुनिया के डेटा के साथ काम करते समय, हम मानते हैं कि सभी डेटा बिंदु कुछ संभाव्यता वितरण से लिए गए नमूने हैं। यह धारणा हमें मशीन लर्निंग तकनीकों को लागू करने और कार्यशील भविष्य कहनेवाला मॉडल बनाने की अनुमति देती है।\n+\n+यह देखने के लिए कि हमारे डेटा का वितरण क्या है, हम एक ग्राफ बना सकते हैं जिसे **हिस्टोग्राम** कहा जाता है। एक्स-अक्ष में कई अलग-अलग वज़न अंतराल (तथाकथित **बिन्स**) होंगे, और ऊर्ध्वाधर अक्ष दिखाएगा कि हमारा यादृच्छिक चर नमूना किसी दिए गए अंतराल के अंदर कितनी बार था।\n+\n+![वास्तविक विश्व डेटा का हिस्टोग्राम](/1-Introduction/04-stats-and-probability/images/weight-histogram.png)\n+\n+इस हिस्टोग्राम से आप देख सकते हैं कि सभी मान निश्चित औसत वजन के आसपास केंद्रित होते हैं, और हम उस वजन से जितना आगे जाते हैं - उस मान के कम वजन का सामना करना पड़ता है। यानी, यह बहुत ही असंभव है कि बेसबॉल खिलाड़ी का वजन औसत वजन से बहुत अलग होगा। भार में भिन्नता यह दर्शाती है कि भार किस हद तक माध्य से भिन्न होने की संभावना है।\n+\n+> अगर हम बेसबॉल लीग से नहीं, बल्कि अन्य लोगों का वजन लेते हैं, तो वितरण अलग होने की संभावना है। हालाँकि, वितरण का आकार समान होगा, लेकिन माध्य और विचरण बदल जाएगा। इसलिए, यदि हम अपने मॉडल को बेसबॉल खिलाड़ियों पर प्रशिक्षित करते हैं, तो विश्वविद्यालय के छात्रों पर लागू होने पर यह गलत परिणाम देने की संभावना है, क्योंकि अंतर्निहित वितरण अलग है।\n+\n+## सामान्य वितरण\n+\n+वजन का वितरण जो हमने ऊपर देखा है वह बहुत विशिष्ट है, और वास्तविक दुनिया से कई माप एक ही प्रकार के वितरण का पालन करते हैं, लेकिन अलग-अलग माध्य और भिन्नता के साथ। इस वितरण को **सामान्य वितरण** कहा जाता है, और यह आंकड़ों में बहुत महत्वपूर्ण भूमिका निभाता है।\n+\n+सामान्य वितरण का उपयोग करना संभावित बेसबॉल खिलाड़ियों के यादृच्छिक भार उत्पन्न करने का एक सही तरीका है। एक बार जब हम माध्य वजन `माध्य` और मानक विचलन `एसटीडी` जान लेते हैं, तो हम निम्नलिखित तरीके से 1000 वजन के नमूने तैयार कर सकते हैं:\n+```python\n+samples = np.random.normal(mean,std,1000)\n+```\n+\n+यदि हम उत्पन्न नमूनों के हिस्टोग्राम की साजिश करते हैं तो हम ऊपर दिखाए गए चित्र के समान ही चित्र देखेंगे। और अगर हम नमूनों की संख्या और डिब्बे की संख्या में वृद्धि करते हैं, तो हम एक सामान्य वितरण की एक तस्वीर उत्पन्न कर सकते हैं जो आदर्श के अधिक करीब है:\n+\n+![माध्य = 0 और एसटीडी.देव = 1 के साथ सामान्य वितरण](/1-Introduction/04-stats-and-probability/images/normal-histogram.png)\n+\n+\n+*माध्य = 0 और std.dev=1* के साथ सामान्य वितरण\n+\n+## विश्वास अंतराल\n+\n+जब हम बेसबॉल खिलाड़ियों के वजन के बारे में बात करते हैं, तो हम मानते हैं कि कुछ निश्चित **यादृच्छिक चर W** है जो सभी बेसबॉल खिलाड़ियों (तथाकथित **जनसंख्या**) के वजन के आदर्श संभाव्यता वितरण से मेल खाती है। वजन का हमारा क्रम सभी बेसबॉल खिलाड़ियों के एक उपसमुच्चय से मेल खाता है जिसे हम **नमूना** कहते हैं। एक दिलचस्प सवाल यह है कि क्या हम डब्ल्यू के वितरण के मापदंडों को जान सकते हैं, यानी जनसंख्या का माध्य और विचरण?\n+\n+सबसे आसान उत्तर हमारे नमूने के माध्य और विचरण की गणना करना होगा। हालाँकि, ऐसा हो सकता है कि हमारा यादृच्छिक नमूना पूर्ण जनसंख्या का सटीक रूप से प्रतिनिधित्व नहीं करता है। इस प्रकार **कॉन्फिडेंस इंटरवल** के बारे में बात करना समझ में आता है।\n+\n+> **विश्वास अंतराल** हमारे नमूने को देखते हुए जनसंख्या के वास्तविक माध्य का अनुमान है, जो एक निश्चित प्रायिकता (या **विश्वास का स्तर**) सटीक है।\n+\n+मान लीजिए हमारे पास हमारे वितरण से एक नमूना X<sub>1</sub>, ..., X<sub>n</sub> है। हर बार जब हम अपने वितरण से एक नमूना लेते हैं, तो हम अलग-अलग माध्य मान के साथ समाप्त होते हैं। इस प्रकार &mu; एक यादृच्छिक चर माना जा सकता है। एक **विश्वास अंतराल** विश्वास के साथ p मानों की एक जोड़ी है (L<sub>p</sub>,R<sub>p</sub>), जैसे कि **P**(L<sub>p </sub>&leq;&mu;&leq;R<sub>p</sub>) = p, यानी अंतराल के भीतर मापे गए माध्य मान के गिरने की प्रायिकता p के बराबर होती है।\n+\n+यह विस्तार से चर्चा करने के लिए हमारे संक्षिप्त परिचय से परे है कि उन आत्मविश्वास अंतराल की गणना कैसे की जाती है। कुछ और विवरण [विकिपीडिया पर](https://en.wikipedia.org/wiki/Confidence_interval) देखे जा सकते हैं। संक्षेप में, हम जनसंख्या के वास्तविक माध्य के सापेक्ष परिकलित नमूना माध्य के वितरण को परिभाषित करते हैं, जिसे **छात्र वितरण** कहा जाता है।\n+\n+> **दिलचस्प तथ्य**: छात्र वितरण का नाम गणितज्ञ विलियम सीली गॉसेट के नाम पर रखा गया है, जिन्होंने छद्म नाम ""स्टूडेंट"" के तहत अपना पेपर प्रकाशित किया था। उन्होंने गिनीज शराब की भठ्ठी में काम किया, और, एक संस्करण के अनुसार, उनके नियोक्ता नहीं चाहते थे कि आम जनता को पता चले कि वे कच्चे माल की गुणवत्ता निर्धारित करने के लिए सांख्यिकीय परीक्षणों का उपयोग कर रहे थे।\n+\n+यदि हम माध्य का अनुमान लगाना चाहते हैं &mu; हमारी जनसंख्या का विश्वास p के साथ, हमें छात्र वितरण A का *(1-p)/2-th प्रतिशत* लेने की आवश्यकता है, जिसे या तो तालिकाओं से लिया जा सकता है, या कंप्यूटर सांख्यिकीय सॉफ़्टवेयर के कुछ अंतर्निहित कार्यों का उपयोग करके (उदाहरण के लिए पायथन, आर, आदि)। फिर अंतराल के लिए &mu; X&pm;A*D/&radic;n द्वारा दिया जाएगा, जहां X नमूने का प्राप्त माध्य है, D मानक विचलन है।\n+\n+> **नोट**: हम [स्वतंत्रता की डिग्री](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)) की एक महत्वपूर्ण अवधारणा की चर्चा को भी छोड़ देते हैं, जो छात्र वितरण के संबंध में महत्वपूर्ण है। इस अवधारणा को गहराई से समझने के लिए आप सांख्यिकी पर अधिक संपूर्ण पुस्तकों का उल्लेख कर सकते हैं।\n+\n+वजन और ऊंचाई के लिए आत्मविश्वास अंतराल की गणना का एक उदाहरण [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) में दिया गया है।\n+\n+| p | Weight mean |\n+|-----|-----------|\n+| 0.85 | 201.73±0.94 |\n+| 0.90 | 201.73±1.08 |\n+| 0.95 | 201.73±1.28 |\n+\n+ध्यान दें कि आत्मविश्वास की संभावना जितनी अधिक होगी, विश्वास अंतराल उतना ही व्यापक होगा।\n+\n+## परिकल्पना परीक्षण \n+\n+हमारे बेसबॉल खिलाड़ियों के डेटासेट में, अलग-अलग खिलाड़ी भूमिकाएँ होती हैं, जिन्हें नीचे संक्षेप में प्रस्तुत किया जा सकता है (इस तालिका की गणना कैसे की जा सकती है, यह देखने के लिए [साथ वाली नोटबुक](/1-Introduction/04-stats-and-probability/notebook.ipynb) देखें):\n+\n+| Role | Height | Weight | Count |\n+|------|--------|--------|-------|\n+| Catcher | 72.723684 | 204.328947 | 76 |\n+| Designated_Hitter | 74.222222 | 220.888889 | 18 |\n+| First_Baseman | 74.000000 | 213.109091 | 55 |\n+| Outfielder | 73.010309 | 199.113402 | 194 |\n+| Relief_Pitcher | 74.374603 | 203.517460 | 315 |\n+| Second_Baseman | 71.362069 | 184.344828 | 58 |\n+| Shortstop | 71.903846 | 182.923077 | 52 |\n+| Starting_Pitcher | 74.719457 | 205.163636 | 221 |\n+| Third_Baseman | 73.044444 | 200.955556 | 45 |\n+\n+हम देख सकते हैं कि पहले बेसमेन की औसत ऊंचाई दूसरे बेसमेन की तुलना में अधिक है। इस प्रकार, हम यह निष्कर्ष निकालने के लिए ललचा सकते हैं कि **पहले बेसमेन दूसरे बेसमेन से अधिक हैं**।\n+\n+> इस कथन को **एक परिकल्पना** कहा जाता है, क्योंकि हम नहीं जानते कि तथ्य वास्तव में सत्य है या नहीं।\n+\n+हालांकि, यह हमेशा स्पष्ट नहीं होता है कि क्या हम यह निष्कर्ष निकाल सकते हैं। ऊपर की चर्चा से हम जानते हैं कि प्रत्येक माध्य का एक संबद्ध विश्वास अंतराल होता है, और इस प्रकार यह अंतर केवल एक सांख्यिकीय त्रुटि हो सकता है। हमें अपनी परिकल्पना का परीक्षण करने के लिए कुछ और औपचारिक तरीके की आवश्यकता है।\n+\n+आइए पहले और दूसरे बेसमेन की ऊंचाई के लिए अलग-अलग आत्मविश्वास अंतराल की गणना करें:\n+\n+| Confidence | First Basemen | Second Basemen |\n+|------------|---------------|----------------|\n+| 0.85 | 73.62..74.38 | 71.04..71.69 |\n+| 0.90 | 73.56..74.44 | 70.99..71.73 |\n+| 0.95 | 73.47..74.53 | 70.92..71.81 |\n+\n+हम देख सकते हैं कि बिना किसी विश्वास के अंतराल ओवरलैप हो जाते हैं। इससे हमारी परिकल्पना सिद्ध होती है कि पहले बेसमेन दूसरे बेसमेन से ऊंचे होते हैं।\n+\n+अधिक औपचारिक रूप से, हम जिस समस्या को हल कर रहे हैं वह यह देखना है कि क्या **दो संभाव्यता वितरण समान हैं**, या कम से कम समान पैरामीटर हैं। वितरण के आधार पर, हमें उसके लिए विभिन्न परीक्षणों का उपयोग करने की आवश्यकता है। यदि हम जानते हैं कि हमारे वितरण सामान्य हैं, तो हम **[स्टूडेंट टी-टेस्ट](https://en.wikipedia.org/wiki/Student%27s_t-test)** लागू कर सकते हैं।\n+\n+स्टूडेंट टी-टेस्ट में, हम तथाकथित **टी-वैल्यू** की गणना करते हैं, जो भिन्नता को ध्यान में रखते हुए, साधनों के बीच अंतर को इंगित करता है। यह प्रदर्शित किया जाता है कि टी-मान **छात्र वितरण** का अनुसरण करता है, जो हमें दिए गए आत्मविश्वास स्तर **p** के लिए थ्रेशोल्ड मान प्राप्त करने की अनुमति देता है (इसकी गणना की जा सकती है, या संख्यात्मक तालिकाओं में देखा जा सकता है)। फिर हम परिकल्पना को स्वीकृत या अस्वीकार करने के लिए टी-मान की तुलना इस सीमा से करते हैं।\n+\n+पायथन में, हम **SciPy** पैकेज का उपयोग कर सकते हैं, जिसमें `ttest_ind` फ़ंक्शन शामिल है (कई अन्य उपयोगी सांख्यिकीय कार्यों के अलावा!)। यह हमारे लिए टी-वैल्यू की गणना करता है, और कॉन्फिडेंस पी-वैल्यू की रिवर्स लुकअप भी करता है, ताकि हम निष्कर्ष निकालने के लिए कॉन्फिडेंस को देख सकें।\n+\n+उदाहरण के लिए, पहले और दूसरे बेसमेन की ऊंचाई के बीच हमारी तुलना हमें निम्नलिखित परिणाम देती है:\n+\n+```python\n+from scipy.stats import ttest_ind\n+\n+tval, pval = ttest_ind(df.loc[df['Role']=='First_Baseman',['Height']], df.loc[df['Role']=='Designated_Hitter',['Height']],equal_var=False)\n+print(f""T-value = {tval[0]:.2f}\nP-value: {pval[0]}"")\n+```\n+```\n+T-value = 7.65\n+P-value: 9.137321189738925e-12\n+```\n+\n+हमारे मामले में, पी-वैल्यू बहुत कम है, जिसका अर्थ है कि इस बात का समर्थन करने वाले मजबूत सबूत हैं कि पहले बेसमेन लम्बे होते हैं।\n+\n+अन्य विभिन्न प्रकार की परिकल्पनाएँ भी हैं जिनका हम परीक्षण करना चाहते हैं, उदाहरण के लिए:\n+* यह साबित करने के लिए कि दिया गया नमूना कुछ वितरण का अनुसरण करता है। हमारे मामले में हमने मान लिया है कि ऊंचाई सामान्य रूप से वितरित की जाती है, लेकिन इसके लिए औपचारिक सांख्यिकीय सत्यापन की आवश्यकता होती है।\n+* यह सिद्ध करने के लिए कि नमूने का माध्य मान कुछ पूर्वनिर्धारित मान से मेल खाता है\n+* कई नमूनों के साधनों की तुलना करना (उदाहरण के लिए विभिन्न आयु समूहों में खुशी के स्तर में क्या अंतर है)\n+\n+## बड़ी संख्या का नियम और केंद्रीय सीमा प्रमेय\n+\n+सामान्य वितरण के इतना महत्वपूर्ण होने का एक कारण तथाकथित **केंद्रीय सीमा प्रमेय** है। मान लीजिए कि हमारे पास स्वतंत्र N मानों X<sub>1</sub>, ..., X<sub>N</sub> का एक बड़ा नमूना है, जिसे माध्य &mu; और विचरण &सिग्मा;<sup>2</sup>. फिर, पर्याप्त रूप से बड़े N के लिए (दूसरे शब्दों में, जब N&rarr;&infin;), माध्य &Sigma;<sub>i</sub>X<sub>i</sub> को सामान्य रूप से माध्य &mu के साथ वितरित किया जाएगा; और विचरण &sigma;<sup>2</sup>/N.\n+\n+> केंद्रीय सीमा प्रमेय की व्याख्या करने का एक अन्य तरीका यह कहना है कि वितरण की परवाह किए बिना, जब आप किसी भी यादृच्छिक चर मानों के योग के माध्य की गणना करते हैं तो आप सामान्य वितरण के साथ समाप्त होते हैं।\n+\n+केंद्रीय सीमा प्रमेय से यह भी पता चलता है कि, जब N&rar;&infin;, नमूने के माध्य की प्रायिकता &mu; बन जाता है 1. इसे **बड़ी संख्या का नियम** कहते हैं।\n+\n+## सहप्रसरण और सहसंबंध\n+\n+डेटा साइंस द्वारा की जाने वाली चीजों में से एक डेटा के बीच संबंध ढूंढ रहा है। हम कहते हैं कि दो अनुक्रम **सहसम्बन्ध** तब होते हैं जब वे एक ही समय में समान व्यवहार प्रदर्शित करते हैं, अर्थात वे या तो एक साथ उठते/गिरते हैं, या एक क्रम ऊपर उठता है जब दूसरा गिरता है और इसके विपरीत। दूसरे शब्दों में, दो अनुक्रमों के बीच कुछ संबंध प्रतीत होता है।\n+\n+> सहसंबंध आवश्यक रूप से दो अनुक्रमों के बीच कारण संबंध को इंगित नहीं करता है; कभी-कभी दोनों चर किसी बाहरी कारण पर निर्भर हो सकते हैं, या यह विशुद्ध रूप से संयोग से दो अनुक्रम सहसंबद्ध हो सकते हैं। हालांकि, मजबूत गणितीय सहसंबंध एक अच्छा संकेत है कि दो चर किसी न किसी तरह से जुड़े हुए हैं।\n+\n+गणितीय रूप से, मुख्य अवधारणा जो दो यादृच्छिक चर के बीच संबंध दिखाती है, वह है **सहप्रसरण**, जिसकी गणना इस प्रकार की जाती है: Cov(X,Y) = **E**\[(X-**E**(X) ))(वाई-**ई**(वाई))\]। हम दोनों चरों के विचलन की गणना उनके माध्य मानों से करते हैं, और फिर उन विचलनों के गुणनफल की गणना करते हैं। यदि दोनों चर एक साथ विचलित होते हैं, तो उत्पाद हमेशा एक सकारात्मक मूल्य होगा, जो कि सकारात्मक सहप्रसरण को जोड़ देगा। यदि दोनों चर आउट-ऑफ-सिंक विचलित हो जाते हैं (अर्थात एक औसत से नीचे गिर जाता है जब दूसरा औसत से ऊपर उठता है), तो हमें हमेशा ऋणात्मक संख्याएँ मिलेंगी, जो कि ऋणात्मक सहप्रसरण को जोड़ देंगी। यदि विचलन निर्भर नहीं हैं, तो वे लगभग शून्य तक जोड़ देंगे।\n+\n+सहप्रसरण का निरपेक्ष मान हमें यह नहीं बताता कि सहसंबंध कितना बड़ा है, क्योंकि यह वास्तविक मूल्यों के परिमाण पर निर्भर करता है। इसे सामान्य करने के लिए, हम **सहसंबंध** प्राप्त करने के लिए, दोनों चरों के मानक विचलन द्वारा सहप्रसरण को विभाजित कर सकते हैं। अच्छी बात यह है कि सहसंबंध हमेशा [-1,1] की सीमा में होता है, जहां 1 मूल्यों के बीच मजबूत सकारात्मक सहसंबंध को इंगित करता है, -1 - मजबूत नकारात्मक सहसंबंध, और 0 - बिल्कुल भी कोई संबंध नहीं (चर स्वतंत्र हैं)।\n+\n+**उदाहरण**: हम ऊपर बताए गए डेटासेट से बेसबॉल खिलाड़ियों के वज़न और ऊंचाई के बीच सहसंबंध की गणना कर सकते हैं:\n+\n+```python\n+print(np.corrcoef(weights,heights))\n+```\n+As a result, we get **correlation matrix** like this one:",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303de,microsoft/Data-Science-For-Beginners,874849926,823005421,jlooper,1450004,2022-03-09T19:26:05+00:00,2022-03-09T19:29:53+00:00,what's the change made here?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303df,microsoft/Data-Science-For-Beginners,874849926,823005981,jlooper,1450004,2022-03-09T19:26:52+00:00,2022-03-09T19:29:53+00:00,Please make sure all images are correctly linked with the right relative paths,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e0,microsoft/Data-Science-For-Beginners,874849926,823007903,jlooper,1450004,2022-03-09T19:29:31+00:00,2022-03-09T19:29:53+00:00,Same for all links,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e1,microsoft/Data-Science-For-Beginners,874849926,823005421,jlooper,1450004,2022-03-09T19:26:05+00:00,2022-03-09T19:29:53+00:00,what's the change made here?,true,1-Introduction/01-defining-data-science/README.md,5,"@@ -162,4 +162,4 @@ Visit [`notebook.ipynb`](/1-Introduction/01-defining-data-science/notebook.ipynb\n \n ## Credits\n \n-This lesson has been authored with ♥️ by [Dmitry Soshnikov](http://soshnikov.com)\n+This lesson has been authored with ♥️ by [Dmitry Soshnikov](http://soshnikov.com)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e2,microsoft/Data-Science-For-Beginners,874849926,823005981,jlooper,1450004,2022-03-09T19:26:52+00:00,2022-03-09T19:29:53+00:00,Please make sure all images are correctly linked with the right relative paths,true,translations/README.hi.md,NaN,"@@ -0,0 +1,164 @@\n+# डेटा विज्ञान को परिभाषित करना\n+\n+| ![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/01-Definitions.png) |",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e3,microsoft/Data-Science-For-Beginners,874849926,823007903,jlooper,1450004,2022-03-09T19:29:31+00:00,2022-03-09T19:29:53+00:00,Same for all links,true,translations/README.hi.md,NaN,"@@ -0,0 +1,164 @@\n+# डेटा विज्ञान को परिभाषित करना\n+\n+| ![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/01-Definitions.png) |",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e4,microsoft/Data-Science-For-Beginners,847188629,804669819,jlooper,1450004,2022-02-11T13:53:28+00:00,2022-02-11T13:56:57+00:00,"hi, you can link the standard quiz here",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e5,microsoft/Data-Science-For-Beginners,847188629,804670976,jlooper,1450004,2022-02-11T13:54:49+00:00,2022-02-11T13:56:57+00:00,here too,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e6,microsoft/Data-Science-For-Beginners,847188629,804671219,jlooper,1450004,2022-02-11T13:55:03+00:00,2022-02-11T13:56:57+00:00,you can link an R assignment here,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e7,microsoft/Data-Science-For-Beginners,847188629,804908599,Vidushi-Gupta,55969597,2022-02-11T18:32:15+00:00,2022-02-11T18:32:15+00:00,Just wanted to confirm if the standard quiz = the quiz on the Python Readme for the same topic?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e8,microsoft/Data-Science-For-Beginners,847188629,804908665,Vidushi-Gupta,55969597,2022-02-11T18:32:21+00:00,2022-02-11T18:32:21+00:00,Sure,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303e9,microsoft/Data-Science-For-Beginners,847188629,813103836,jlooper,1450004,2022-02-23T16:52:09+00:00,2022-02-23T16:52:09+00:00,"hi, @Vidushi-Gupta sorry I didn't see this. Yes, I think we have to keep this the same quiz",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ea,microsoft/Data-Science-For-Beginners,847188629,804669819,jlooper,1450004,2022-02-11T13:53:28+00:00,2022-02-11T13:56:57+00:00,"hi, you can link the standard quiz here",true,3-Data-Visualization/R/09-visualization-quantities/README.md,NaN,"@@ -0,0 +1,216 @@\n+# Visualizing Quantities\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/sketchnotes/09-Visualizing-Quantities.png)|\n+|:---:|\n+| Visualizing Quantities - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+In this lesson you will explore how to use some of the many available R packages libraries to learn how to create interesting visualizations all around the concept of quantity. Using a cleaned dataset about the birds of Minnesota, you can learn many interesting facts about local wildlife. \n+## [Pre-lecture quiz]() TO BE LINKED",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303eb,microsoft/Data-Science-For-Beginners,847188629,804670976,jlooper,1450004,2022-02-11T13:54:49+00:00,2022-02-11T13:56:57+00:00,here too,true,3-Data-Visualization/R/09-visualization-quantities/README.md,NaN,"@@ -0,0 +1,216 @@\n+# Visualizing Quantities\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/sketchnotes/09-Visualizing-Quantities.png)|\n+|:---:|\n+| Visualizing Quantities - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+In this lesson you will explore how to use some of the many available R packages libraries to learn how to create interesting visualizations all around the concept of quantity. Using a cleaned dataset about the birds of Minnesota, you can learn many interesting facts about local wildlife. \n+## [Pre-lecture quiz]() TO BE LINKED\n+\n+## Observe wingspan with ggplot2\n+An excellent library to create both simple and sophisticated plots and charts of various kinds is [ggplot2](https://cran.r-project.org/web/packages/ggplot2/index.html). In general terms, the process of plotting data using these libraries includes identifying the parts of your dataframe that you want to target, performing any transforms on that data necessary, assigning its x and y axis values, deciding what kind of plot to show, and then showing the plot.\n+\n+`ggplot2` is a system for declaratively creating graphics, based on The Grammar of Graphics. The [Grammar of Graphics](https://en.wikipedia.org/wiki/Ggplot2) is a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers. In other words, the ease of creating plots and graphs for univariate or multivariate data with little code makes `ggplot2` the most popular package used for visualizations in R. The user tells `ggplot2` how to map the variables to aesthetics, the graphical primitives to use, and `ggplot2` takes care of the remaining.\n+\n+> ✅ Plot = Data + Aesthetics + Geometry\n+> - Data refers to the dataset\n+> - Aesthetics indicate the variables to be studied (x and y variables)\n+> - Geometry refers to the type of plot (line plot, bar plot, etc.)\n+\n+Choose the best geometry (type of plot) according to your data and the story you want to tell through the plot. \n+\n+> - To analyze trends: line, column\n+> - To compare values: bar, column, pie, scatterplot\n+> - To show how parts relate to a whole: pie\n+> - To show distribution of data: scatterplot, bar\n+> - To show relationships between values: line, scatterplot, bubble\n+\n+✅ You can also checkout this descriptive [cheatsheet](https://nyu-cdsc.github.io/learningr/assets/data-visualization-2.1.pdf) for ggplot2.\n+\n+## Build a line plot about bird wingspan values\n+\n+Open the R console and import the dataset. \n+> Note: The dataset is stored in the root of this repo in the `/data` folder.\n+\n+Let's import the dataset and observe the head (top 5 rows) of the data.\n+\n+```r\n+birds <- read.csv(""../../data/birds.csv"",fileEncoding=""UTF-8-BOM"")\n+head(birds)\n+```\n+The head of the data has a mix of text and numbers:\n+\n+|      | Name                         | ScientificName         | Category              | Order        | Family   | Genus       | ConservationStatus | MinLength | MaxLength | MinBodyMass | MaxBodyMass | MinWingspan | MaxWingspan |\n+| ---: | :--------------------------- | :--------------------- | :-------------------- | :----------- | :------- | :---------- | :----------------- | --------: | --------: | ----------: | ----------: | ----------: | ----------: |\n+|    0 | Black-bellied whistling-duck | Dendrocygna autumnalis | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Dendrocygna | LC                 |        47 |        56 |         652 |        1020 |          76 |          94 |\n+|    1 | Fulvous whistling-duck       | Dendrocygna bicolor    | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Dendrocygna | LC                 |        45 |        53 |         712 |        1050 |          85 |          93 |\n+|    2 | Snow goose                   | Anser caerulescens     | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |        64 |        79 |        2050 |        4050 |         135 |         165 |\n+|    3 | Ross's goose                 | Anser rossii           | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |      57.3 |        64 |        1066 |        1567 |         113 |         116 |\n+|    4 | Greater white-fronted goose  | Anser albifrons        | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |        64 |        81 |        1930 |        3310 |         130 |         165 |\n+\n+Let's start by plotting some of the numeric data using a basic line plot. Suppose you wanted a view of the maximum wingspan for these interesting birds.\n+\n+```r\n+install.packages(""ggplot2"")\n+library(""ggplot2"")\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_line() \n+```\n+Here, you install the `ggplot2` package and then import it into the workspace using the `library(""ggplot2"")` command. To plot any plot in ggplot, the `ggplot()` function is used and you specify the dataset, x and y variables as attributes. In this case, we use the `geom_line()` function since we aim to plot a line plot.\n+\n+![MaxWingspan-lineplot](images/MaxWingspan-lineplot.png)\n+\n+What do you notice immediately? There seems to be at least one outlier - that's quite a wingspan! A 2000+ centimeter wingspan equals more than 20 meters - are there Pterodactyls roaming Minnesota? Let's investigate.\n+\n+While you could do a quick sort in Excel to find those outliers, which are probably typos, continue the visualization process by working from within the plot.\n+\n+Add labels to the x-axis to show what kind of birds are in question:\n+\n+```r\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_line() +\n+  theme(axis.text.x = element_text(angle = 45, hjust=1))+\n+  xlab(""Birds"") +\n+  ylab(""Wingspan (CM)"") +\n+  ggtitle(""Max Wingspan in Centimeters"")\n+```\n+We specify the angle in the `theme` and specify the x and y axis labels in `xlab()` and `ylab()` respectively. The `ggtitle()` gives a name to the graph/plot.\n+\n+![MaxWingspan-lineplot-improved](images/MaxWingspan-lineplot-improved.png)\n+\n+Even with the rotation of the labels set to 45 degrees, there are too many to read. Let's try a different strategy: label only those outliers and set the labels within the chart. You can use a scatter chart to make more room for the labeling:\n+\n+```r\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_point() +\n+  geom_text(aes(label=ifelse(MaxWingspan>500,as.character(Name),'')),hjust=0,vjust=0) + \n+  theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank())\n+  ylab(""Wingspan (CM)"") +\n+  ggtitle(""Max Wingspan in Centimeters"") + \n+```\n+What's going on here? You used the `geom_point()` function to plot scatter points. With this, you added labels for birds who had their `MaxWingspan > 500` and also hid the labels on the x axis to declutter the plot. \n+\n+What do you discover?\n+\n+![MaxWingspan-scatterplot](images/MaxWingspan-scatterplot.png)\n+\n+## Filter your data\n+\n+Both the Bald Eagle and the Prairie Falcon, while probably very large birds, appear to be mislabeled, with an extra 0 added to their maximum wingspan. It's unlikely that you'll meet a Bald Eagle with a 25 meter wingspan, but if so, please let us know! Let's create a new dataframe without those two outliers:\n+\n+```r\n+birds_filtered <- subset(birds, MaxWingspan < 500)\n+\n+ggplot(data=birds_filtered, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_point() +\n+  ylab(""Wingspan (CM)"") +\n+  xlab(""Birds"") +\n+  ggtitle(""Max Wingspan in Centimeters"") + \n+  geom_text(aes(label=ifelse(MaxWingspan>500,as.character(Name),'')),hjust=0,vjust=0) +\n+  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n+```\n+We made a new dataframe `birds_filtered` and then plotted a scatter plot. By filtering out outliers, your data is now more cohesive and understandable.\n+\n+![MaxWingspan-scatterplot-improved](images/MaxWingspan-scatterplot-improved.png)\n+\n+Now that we have a cleaner dataset at least in terms of wingspan, let's discover more about these birds.\n+\n+While line and scatter plots can display information about data values and their distributions, we want to think about the values inherent in this dataset. You could create visualizations to answer the following questions about quantity:\n+\n+> How many categories of birds are there, and what are their numbers?\n+> How many birds are extinct, endangered, rare, or common?\n+> How many are there of the various genus and orders in Linnaeus's terminology?\n+## Explore bar charts\n+\n+Bar charts are practical when you need to show groupings of data. Let's explore the categories of birds that exist in this dataset to see which is the most common by number.\n+Let's create a bar chart on filtered data.\n+\n+```r\n+install.packages(""dplyr"")\n+install.packages(""tidyverse"")\n+\n+library(lubridate)\n+library(scales)\n+library(dplyr)\n+library(ggplot2)\n+library(tidyverse)\n+\n+birds_filtered %>% group_by(Category) %>%\n+  summarise(n=n(),\n+  MinLength = mean(MinLength),\n+  MaxLength = mean(MaxLength),\n+  MinBodyMass = mean(MinBodyMass),\n+  MaxBodyMass = mean(MaxBodyMass),\n+  MinWingspan=mean(MinWingspan),\n+  MaxWingspan=mean(MaxWingspan)) %>% \n+  gather(""key"", ""value"", - c(Category, n)) %>%\n+  ggplot(aes(x = Category, y = value, group = key, fill = key)) +\n+  geom_bar(stat = ""identity"") +\n+  scale_fill_manual(values = c(""#D62728"", ""#FF7F0E"", ""#8C564B"",""#2CA02C"", ""#1F77B4"", ""#9467BD"")) +                   \n+  xlab(""Category"")+ggtitle(""Birds of Minnesota"")\n+\n+```\n+In the following snippet, we install the [dplyr](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8) and [lubridate](https://www.rdocumentation.org/packages/lubridate/versions/1.8.0) packages to help manipulate and group data in order to plot a stacked bar chart. First, you group the data by the `Category` of bird and then summarise the `MinLength`, `MaxLength`, `MinBodyMass`,`MaxdyMass`,`MinWingspan`,`MaxWingspan` columns. Then, plot the bar chart using `ggplot2` package and specify the colours for the different category and the labels. \n+\n+![Stacked bar chart](images/stacked-bar-chart.png)\n+\n+This bar chart, however, is unreadable because there is too much non-grouped data. You need to select only the data that you want to plot, so let's look at the length of birds based on their category.\n+\n+Filter your data to include only the bird's category.\n+\n+Since there are many categories, you can display this chart vertically and tweak its height to account for all the data:\n+\n+```r\n+birds_count<-dplyr::count(birds_filtered, Category, sort = TRUE)\n+birds_count$Category <- factor(birds_count$Category, levels = birds_count$Category)\n+ggplot(birds_count,aes(Category,n))+geom_bar(stat=""identity"")+coord_flip()\n+```\n+You first count unique values in the `Category` column and then sort them into a new dataframe `birds_count`.This sorted data is then factored in the same level so that it is plotted in the sorted way. Using `ggplot2` you then plot the data in a bar chart. The `coord_flip()` plots horizontal bars. \n+\n+![category-length](images/category-length.png)\n+\n+This bar chart shows a good view of the number of birds in each category. In a blink of an eye, you see that the largest number of birds in this region are in the Ducks/Geese/Waterfowl category. Minnesota is the 'land of 10,000 lakes' so this isn't surprising!\n+\n+✅ Try some other counts on this dataset. Does anything surprise you?\n+\n+## Comparing data\n+\n+You can try different comparisons of grouped data by creating new axes. Try a comparison of the MaxLength of a bird, based on its category:\n+\n+```r\n+birds_grouped <- birds_filtered %>%\n+  group_by(Category) %>%\n+  summarise(\n+  MaxLength = max(MaxLength, na.rm = T),\n+  MinLength = max(MinLength, na.rm = T)\n+           ) %>%\n+  arrange(Category)\n+  \n+ggplot(birds_grouped,aes(Category,MaxLength))+geom_bar(stat=""identity"")+coord_flip()\n+```\n+We group the `birds_filtered` data by `Category` and then plot a bar graph. \n+\n+![comparing data](images/comparingdata.png)\n+\n+Nothing is surprising here: hummingbirds have the least MaxLength compared to Pelicans or Geese. It's good when data makes logical sense!\n+\n+You can create more interesting visualizations of bar charts by superimposing data. Let's superimpose Minimum and Maximum Length on a given bird category:\n+\n+```r\n+ggplot(data=birds_grouped, aes(x=Category)) +\n+  geom_bar(aes(y=MaxLength), stat=""identity"", position =""identity"",  fill='blue') +\n+  geom_bar(aes(y=MinLength), stat=""identity"", position=""identity"", fill='orange')+\n+  coord_flip()\n+```\n+![super-imposed values](images/superimposed-values.png)\n+\n+## 🚀 Challenge\n+\n+This bird dataset offers a wealth of information about different types of birds within a particular ecosystem. Search around the internet and see if you can find other bird-oriented datasets. Practice building charts and graphs around these birds to discover facts you didn't realize.\n+## [Post-lecture quiz]() To be linked",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ec,microsoft/Data-Science-For-Beginners,847188629,804671219,jlooper,1450004,2022-02-11T13:55:03+00:00,2022-02-11T13:56:57+00:00,you can link an R assignment here,true,3-Data-Visualization/R/09-visualization-quantities/README.md,215,"@@ -0,0 +1,216 @@\n+# Visualizing Quantities\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/sketchnotes/09-Visualizing-Quantities.png)|\n+|:---:|\n+| Visualizing Quantities - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+In this lesson you will explore how to use some of the many available R packages libraries to learn how to create interesting visualizations all around the concept of quantity. Using a cleaned dataset about the birds of Minnesota, you can learn many interesting facts about local wildlife. \n+## [Pre-lecture quiz]() TO BE LINKED\n+\n+## Observe wingspan with ggplot2\n+An excellent library to create both simple and sophisticated plots and charts of various kinds is [ggplot2](https://cran.r-project.org/web/packages/ggplot2/index.html). In general terms, the process of plotting data using these libraries includes identifying the parts of your dataframe that you want to target, performing any transforms on that data necessary, assigning its x and y axis values, deciding what kind of plot to show, and then showing the plot.\n+\n+`ggplot2` is a system for declaratively creating graphics, based on The Grammar of Graphics. The [Grammar of Graphics](https://en.wikipedia.org/wiki/Ggplot2) is a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers. In other words, the ease of creating plots and graphs for univariate or multivariate data with little code makes `ggplot2` the most popular package used for visualizations in R. The user tells `ggplot2` how to map the variables to aesthetics, the graphical primitives to use, and `ggplot2` takes care of the remaining.\n+\n+> ✅ Plot = Data + Aesthetics + Geometry\n+> - Data refers to the dataset\n+> - Aesthetics indicate the variables to be studied (x and y variables)\n+> - Geometry refers to the type of plot (line plot, bar plot, etc.)\n+\n+Choose the best geometry (type of plot) according to your data and the story you want to tell through the plot. \n+\n+> - To analyze trends: line, column\n+> - To compare values: bar, column, pie, scatterplot\n+> - To show how parts relate to a whole: pie\n+> - To show distribution of data: scatterplot, bar\n+> - To show relationships between values: line, scatterplot, bubble\n+\n+✅ You can also checkout this descriptive [cheatsheet](https://nyu-cdsc.github.io/learningr/assets/data-visualization-2.1.pdf) for ggplot2.\n+\n+## Build a line plot about bird wingspan values\n+\n+Open the R console and import the dataset. \n+> Note: The dataset is stored in the root of this repo in the `/data` folder.\n+\n+Let's import the dataset and observe the head (top 5 rows) of the data.\n+\n+```r\n+birds <- read.csv(""../../data/birds.csv"",fileEncoding=""UTF-8-BOM"")\n+head(birds)\n+```\n+The head of the data has a mix of text and numbers:\n+\n+|      | Name                         | ScientificName         | Category              | Order        | Family   | Genus       | ConservationStatus | MinLength | MaxLength | MinBodyMass | MaxBodyMass | MinWingspan | MaxWingspan |\n+| ---: | :--------------------------- | :--------------------- | :-------------------- | :----------- | :------- | :---------- | :----------------- | --------: | --------: | ----------: | ----------: | ----------: | ----------: |\n+|    0 | Black-bellied whistling-duck | Dendrocygna autumnalis | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Dendrocygna | LC                 |        47 |        56 |         652 |        1020 |          76 |          94 |\n+|    1 | Fulvous whistling-duck       | Dendrocygna bicolor    | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Dendrocygna | LC                 |        45 |        53 |         712 |        1050 |          85 |          93 |\n+|    2 | Snow goose                   | Anser caerulescens     | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |        64 |        79 |        2050 |        4050 |         135 |         165 |\n+|    3 | Ross's goose                 | Anser rossii           | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |      57.3 |        64 |        1066 |        1567 |         113 |         116 |\n+|    4 | Greater white-fronted goose  | Anser albifrons        | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |        64 |        81 |        1930 |        3310 |         130 |         165 |\n+\n+Let's start by plotting some of the numeric data using a basic line plot. Suppose you wanted a view of the maximum wingspan for these interesting birds.\n+\n+```r\n+install.packages(""ggplot2"")\n+library(""ggplot2"")\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_line() \n+```\n+Here, you install the `ggplot2` package and then import it into the workspace using the `library(""ggplot2"")` command. To plot any plot in ggplot, the `ggplot()` function is used and you specify the dataset, x and y variables as attributes. In this case, we use the `geom_line()` function since we aim to plot a line plot.\n+\n+![MaxWingspan-lineplot](images/MaxWingspan-lineplot.png)\n+\n+What do you notice immediately? There seems to be at least one outlier - that's quite a wingspan! A 2000+ centimeter wingspan equals more than 20 meters - are there Pterodactyls roaming Minnesota? Let's investigate.\n+\n+While you could do a quick sort in Excel to find those outliers, which are probably typos, continue the visualization process by working from within the plot.\n+\n+Add labels to the x-axis to show what kind of birds are in question:\n+\n+```r\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_line() +\n+  theme(axis.text.x = element_text(angle = 45, hjust=1))+\n+  xlab(""Birds"") +\n+  ylab(""Wingspan (CM)"") +\n+  ggtitle(""Max Wingspan in Centimeters"")\n+```\n+We specify the angle in the `theme` and specify the x and y axis labels in `xlab()` and `ylab()` respectively. The `ggtitle()` gives a name to the graph/plot.\n+\n+![MaxWingspan-lineplot-improved](images/MaxWingspan-lineplot-improved.png)\n+\n+Even with the rotation of the labels set to 45 degrees, there are too many to read. Let's try a different strategy: label only those outliers and set the labels within the chart. You can use a scatter chart to make more room for the labeling:\n+\n+```r\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_point() +\n+  geom_text(aes(label=ifelse(MaxWingspan>500,as.character(Name),'')),hjust=0,vjust=0) + \n+  theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank())\n+  ylab(""Wingspan (CM)"") +\n+  ggtitle(""Max Wingspan in Centimeters"") + \n+```\n+What's going on here? You used the `geom_point()` function to plot scatter points. With this, you added labels for birds who had their `MaxWingspan > 500` and also hid the labels on the x axis to declutter the plot. \n+\n+What do you discover?\n+\n+![MaxWingspan-scatterplot](images/MaxWingspan-scatterplot.png)\n+\n+## Filter your data\n+\n+Both the Bald Eagle and the Prairie Falcon, while probably very large birds, appear to be mislabeled, with an extra 0 added to their maximum wingspan. It's unlikely that you'll meet a Bald Eagle with a 25 meter wingspan, but if so, please let us know! Let's create a new dataframe without those two outliers:\n+\n+```r\n+birds_filtered <- subset(birds, MaxWingspan < 500)\n+\n+ggplot(data=birds_filtered, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_point() +\n+  ylab(""Wingspan (CM)"") +\n+  xlab(""Birds"") +\n+  ggtitle(""Max Wingspan in Centimeters"") + \n+  geom_text(aes(label=ifelse(MaxWingspan>500,as.character(Name),'')),hjust=0,vjust=0) +\n+  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n+```\n+We made a new dataframe `birds_filtered` and then plotted a scatter plot. By filtering out outliers, your data is now more cohesive and understandable.\n+\n+![MaxWingspan-scatterplot-improved](images/MaxWingspan-scatterplot-improved.png)\n+\n+Now that we have a cleaner dataset at least in terms of wingspan, let's discover more about these birds.\n+\n+While line and scatter plots can display information about data values and their distributions, we want to think about the values inherent in this dataset. You could create visualizations to answer the following questions about quantity:\n+\n+> How many categories of birds are there, and what are their numbers?\n+> How many birds are extinct, endangered, rare, or common?\n+> How many are there of the various genus and orders in Linnaeus's terminology?\n+## Explore bar charts\n+\n+Bar charts are practical when you need to show groupings of data. Let's explore the categories of birds that exist in this dataset to see which is the most common by number.\n+Let's create a bar chart on filtered data.\n+\n+```r\n+install.packages(""dplyr"")\n+install.packages(""tidyverse"")\n+\n+library(lubridate)\n+library(scales)\n+library(dplyr)\n+library(ggplot2)\n+library(tidyverse)\n+\n+birds_filtered %>% group_by(Category) %>%\n+  summarise(n=n(),\n+  MinLength = mean(MinLength),\n+  MaxLength = mean(MaxLength),\n+  MinBodyMass = mean(MinBodyMass),\n+  MaxBodyMass = mean(MaxBodyMass),\n+  MinWingspan=mean(MinWingspan),\n+  MaxWingspan=mean(MaxWingspan)) %>% \n+  gather(""key"", ""value"", - c(Category, n)) %>%\n+  ggplot(aes(x = Category, y = value, group = key, fill = key)) +\n+  geom_bar(stat = ""identity"") +\n+  scale_fill_manual(values = c(""#D62728"", ""#FF7F0E"", ""#8C564B"",""#2CA02C"", ""#1F77B4"", ""#9467BD"")) +                   \n+  xlab(""Category"")+ggtitle(""Birds of Minnesota"")\n+\n+```\n+In the following snippet, we install the [dplyr](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8) and [lubridate](https://www.rdocumentation.org/packages/lubridate/versions/1.8.0) packages to help manipulate and group data in order to plot a stacked bar chart. First, you group the data by the `Category` of bird and then summarise the `MinLength`, `MaxLength`, `MinBodyMass`,`MaxdyMass`,`MinWingspan`,`MaxWingspan` columns. Then, plot the bar chart using `ggplot2` package and specify the colours for the different category and the labels. \n+\n+![Stacked bar chart](images/stacked-bar-chart.png)\n+\n+This bar chart, however, is unreadable because there is too much non-grouped data. You need to select only the data that you want to plot, so let's look at the length of birds based on their category.\n+\n+Filter your data to include only the bird's category.\n+\n+Since there are many categories, you can display this chart vertically and tweak its height to account for all the data:\n+\n+```r\n+birds_count<-dplyr::count(birds_filtered, Category, sort = TRUE)\n+birds_count$Category <- factor(birds_count$Category, levels = birds_count$Category)\n+ggplot(birds_count,aes(Category,n))+geom_bar(stat=""identity"")+coord_flip()\n+```\n+You first count unique values in the `Category` column and then sort them into a new dataframe `birds_count`.This sorted data is then factored in the same level so that it is plotted in the sorted way. Using `ggplot2` you then plot the data in a bar chart. The `coord_flip()` plots horizontal bars. \n+\n+![category-length](images/category-length.png)\n+\n+This bar chart shows a good view of the number of birds in each category. In a blink of an eye, you see that the largest number of birds in this region are in the Ducks/Geese/Waterfowl category. Minnesota is the 'land of 10,000 lakes' so this isn't surprising!\n+\n+✅ Try some other counts on this dataset. Does anything surprise you?\n+\n+## Comparing data\n+\n+You can try different comparisons of grouped data by creating new axes. Try a comparison of the MaxLength of a bird, based on its category:\n+\n+```r\n+birds_grouped <- birds_filtered %>%\n+  group_by(Category) %>%\n+  summarise(\n+  MaxLength = max(MaxLength, na.rm = T),\n+  MinLength = max(MinLength, na.rm = T)\n+           ) %>%\n+  arrange(Category)\n+  \n+ggplot(birds_grouped,aes(Category,MaxLength))+geom_bar(stat=""identity"")+coord_flip()\n+```\n+We group the `birds_filtered` data by `Category` and then plot a bar graph. \n+\n+![comparing data](images/comparingdata.png)\n+\n+Nothing is surprising here: hummingbirds have the least MaxLength compared to Pelicans or Geese. It's good when data makes logical sense!\n+\n+You can create more interesting visualizations of bar charts by superimposing data. Let's superimpose Minimum and Maximum Length on a given bird category:\n+\n+```r\n+ggplot(data=birds_grouped, aes(x=Category)) +\n+  geom_bar(aes(y=MaxLength), stat=""identity"", position =""identity"",  fill='blue') +\n+  geom_bar(aes(y=MinLength), stat=""identity"", position=""identity"", fill='orange')+\n+  coord_flip()\n+```\n+![super-imposed values](images/superimposed-values.png)\n+\n+## 🚀 Challenge\n+\n+This bird dataset offers a wealth of information about different types of birds within a particular ecosystem. Search around the internet and see if you can find other bird-oriented datasets. Practice building charts and graphs around these birds to discover facts you didn't realize.\n+## [Post-lecture quiz]() To be linked\n+\n+## Review & Self Study\n+\n+This first lesson has given you some information about how to use `ggplot2`to visualize quantities. Do some research around other ways to work with datasets for visualization. Research and lookout for datasets that you could visualize using other packages like [Lattice](https://stat.ethz.ch/R-manual/R-devel/library/lattice/html/Lattice.html) and [Plotly](https://github.com/plotly/plotly.R#readme)\n+\n+## Assignment",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ed,microsoft/Data-Science-For-Beginners,847188629,804908599,Vidushi-Gupta,55969597,2022-02-11T18:32:15+00:00,2022-02-11T18:32:15+00:00,Just wanted to confirm if the standard quiz = the quiz on the Python Readme for the same topic?,true,3-Data-Visualization/R/09-visualization-quantities/README.md,NaN,"@@ -0,0 +1,216 @@\n+# Visualizing Quantities\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/sketchnotes/09-Visualizing-Quantities.png)|\n+|:---:|\n+| Visualizing Quantities - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+In this lesson you will explore how to use some of the many available R packages libraries to learn how to create interesting visualizations all around the concept of quantity. Using a cleaned dataset about the birds of Minnesota, you can learn many interesting facts about local wildlife. \n+## [Pre-lecture quiz]() TO BE LINKED",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ee,microsoft/Data-Science-For-Beginners,847188629,804908665,Vidushi-Gupta,55969597,2022-02-11T18:32:21+00:00,2022-02-11T18:32:21+00:00,Sure,true,3-Data-Visualization/R/09-visualization-quantities/README.md,215,"@@ -0,0 +1,216 @@\n+# Visualizing Quantities\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/sketchnotes/09-Visualizing-Quantities.png)|\n+|:---:|\n+| Visualizing Quantities - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+In this lesson you will explore how to use some of the many available R packages libraries to learn how to create interesting visualizations all around the concept of quantity. Using a cleaned dataset about the birds of Minnesota, you can learn many interesting facts about local wildlife. \n+## [Pre-lecture quiz]() TO BE LINKED\n+\n+## Observe wingspan with ggplot2\n+An excellent library to create both simple and sophisticated plots and charts of various kinds is [ggplot2](https://cran.r-project.org/web/packages/ggplot2/index.html). In general terms, the process of plotting data using these libraries includes identifying the parts of your dataframe that you want to target, performing any transforms on that data necessary, assigning its x and y axis values, deciding what kind of plot to show, and then showing the plot.\n+\n+`ggplot2` is a system for declaratively creating graphics, based on The Grammar of Graphics. The [Grammar of Graphics](https://en.wikipedia.org/wiki/Ggplot2) is a general scheme for data visualization which breaks up graphs into semantic components such as scales and layers. In other words, the ease of creating plots and graphs for univariate or multivariate data with little code makes `ggplot2` the most popular package used for visualizations in R. The user tells `ggplot2` how to map the variables to aesthetics, the graphical primitives to use, and `ggplot2` takes care of the remaining.\n+\n+> ✅ Plot = Data + Aesthetics + Geometry\n+> - Data refers to the dataset\n+> - Aesthetics indicate the variables to be studied (x and y variables)\n+> - Geometry refers to the type of plot (line plot, bar plot, etc.)\n+\n+Choose the best geometry (type of plot) according to your data and the story you want to tell through the plot. \n+\n+> - To analyze trends: line, column\n+> - To compare values: bar, column, pie, scatterplot\n+> - To show how parts relate to a whole: pie\n+> - To show distribution of data: scatterplot, bar\n+> - To show relationships between values: line, scatterplot, bubble\n+\n+✅ You can also checkout this descriptive [cheatsheet](https://nyu-cdsc.github.io/learningr/assets/data-visualization-2.1.pdf) for ggplot2.\n+\n+## Build a line plot about bird wingspan values\n+\n+Open the R console and import the dataset. \n+> Note: The dataset is stored in the root of this repo in the `/data` folder.\n+\n+Let's import the dataset and observe the head (top 5 rows) of the data.\n+\n+```r\n+birds <- read.csv(""../../data/birds.csv"",fileEncoding=""UTF-8-BOM"")\n+head(birds)\n+```\n+The head of the data has a mix of text and numbers:\n+\n+|      | Name                         | ScientificName         | Category              | Order        | Family   | Genus       | ConservationStatus | MinLength | MaxLength | MinBodyMass | MaxBodyMass | MinWingspan | MaxWingspan |\n+| ---: | :--------------------------- | :--------------------- | :-------------------- | :----------- | :------- | :---------- | :----------------- | --------: | --------: | ----------: | ----------: | ----------: | ----------: |\n+|    0 | Black-bellied whistling-duck | Dendrocygna autumnalis | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Dendrocygna | LC                 |        47 |        56 |         652 |        1020 |          76 |          94 |\n+|    1 | Fulvous whistling-duck       | Dendrocygna bicolor    | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Dendrocygna | LC                 |        45 |        53 |         712 |        1050 |          85 |          93 |\n+|    2 | Snow goose                   | Anser caerulescens     | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |        64 |        79 |        2050 |        4050 |         135 |         165 |\n+|    3 | Ross's goose                 | Anser rossii           | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |      57.3 |        64 |        1066 |        1567 |         113 |         116 |\n+|    4 | Greater white-fronted goose  | Anser albifrons        | Ducks/Geese/Waterfowl | Anseriformes | Anatidae | Anser       | LC                 |        64 |        81 |        1930 |        3310 |         130 |         165 |\n+\n+Let's start by plotting some of the numeric data using a basic line plot. Suppose you wanted a view of the maximum wingspan for these interesting birds.\n+\n+```r\n+install.packages(""ggplot2"")\n+library(""ggplot2"")\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_line() \n+```\n+Here, you install the `ggplot2` package and then import it into the workspace using the `library(""ggplot2"")` command. To plot any plot in ggplot, the `ggplot()` function is used and you specify the dataset, x and y variables as attributes. In this case, we use the `geom_line()` function since we aim to plot a line plot.\n+\n+![MaxWingspan-lineplot](images/MaxWingspan-lineplot.png)\n+\n+What do you notice immediately? There seems to be at least one outlier - that's quite a wingspan! A 2000+ centimeter wingspan equals more than 20 meters - are there Pterodactyls roaming Minnesota? Let's investigate.\n+\n+While you could do a quick sort in Excel to find those outliers, which are probably typos, continue the visualization process by working from within the plot.\n+\n+Add labels to the x-axis to show what kind of birds are in question:\n+\n+```r\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_line() +\n+  theme(axis.text.x = element_text(angle = 45, hjust=1))+\n+  xlab(""Birds"") +\n+  ylab(""Wingspan (CM)"") +\n+  ggtitle(""Max Wingspan in Centimeters"")\n+```\n+We specify the angle in the `theme` and specify the x and y axis labels in `xlab()` and `ylab()` respectively. The `ggtitle()` gives a name to the graph/plot.\n+\n+![MaxWingspan-lineplot-improved](images/MaxWingspan-lineplot-improved.png)\n+\n+Even with the rotation of the labels set to 45 degrees, there are too many to read. Let's try a different strategy: label only those outliers and set the labels within the chart. You can use a scatter chart to make more room for the labeling:\n+\n+```r\n+ggplot(data=birds, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_point() +\n+  geom_text(aes(label=ifelse(MaxWingspan>500,as.character(Name),'')),hjust=0,vjust=0) + \n+  theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank())\n+  ylab(""Wingspan (CM)"") +\n+  ggtitle(""Max Wingspan in Centimeters"") + \n+```\n+What's going on here? You used the `geom_point()` function to plot scatter points. With this, you added labels for birds who had their `MaxWingspan > 500` and also hid the labels on the x axis to declutter the plot. \n+\n+What do you discover?\n+\n+![MaxWingspan-scatterplot](images/MaxWingspan-scatterplot.png)\n+\n+## Filter your data\n+\n+Both the Bald Eagle and the Prairie Falcon, while probably very large birds, appear to be mislabeled, with an extra 0 added to their maximum wingspan. It's unlikely that you'll meet a Bald Eagle with a 25 meter wingspan, but if so, please let us know! Let's create a new dataframe without those two outliers:\n+\n+```r\n+birds_filtered <- subset(birds, MaxWingspan < 500)\n+\n+ggplot(data=birds_filtered, aes(x=Name, y=MaxWingspan,group=1)) +\n+  geom_point() +\n+  ylab(""Wingspan (CM)"") +\n+  xlab(""Birds"") +\n+  ggtitle(""Max Wingspan in Centimeters"") + \n+  geom_text(aes(label=ifelse(MaxWingspan>500,as.character(Name),'')),hjust=0,vjust=0) +\n+  theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())\n+```\n+We made a new dataframe `birds_filtered` and then plotted a scatter plot. By filtering out outliers, your data is now more cohesive and understandable.\n+\n+![MaxWingspan-scatterplot-improved](images/MaxWingspan-scatterplot-improved.png)\n+\n+Now that we have a cleaner dataset at least in terms of wingspan, let's discover more about these birds.\n+\n+While line and scatter plots can display information about data values and their distributions, we want to think about the values inherent in this dataset. You could create visualizations to answer the following questions about quantity:\n+\n+> How many categories of birds are there, and what are their numbers?\n+> How many birds are extinct, endangered, rare, or common?\n+> How many are there of the various genus and orders in Linnaeus's terminology?\n+## Explore bar charts\n+\n+Bar charts are practical when you need to show groupings of data. Let's explore the categories of birds that exist in this dataset to see which is the most common by number.\n+Let's create a bar chart on filtered data.\n+\n+```r\n+install.packages(""dplyr"")\n+install.packages(""tidyverse"")\n+\n+library(lubridate)\n+library(scales)\n+library(dplyr)\n+library(ggplot2)\n+library(tidyverse)\n+\n+birds_filtered %>% group_by(Category) %>%\n+  summarise(n=n(),\n+  MinLength = mean(MinLength),\n+  MaxLength = mean(MaxLength),\n+  MinBodyMass = mean(MinBodyMass),\n+  MaxBodyMass = mean(MaxBodyMass),\n+  MinWingspan=mean(MinWingspan),\n+  MaxWingspan=mean(MaxWingspan)) %>% \n+  gather(""key"", ""value"", - c(Category, n)) %>%\n+  ggplot(aes(x = Category, y = value, group = key, fill = key)) +\n+  geom_bar(stat = ""identity"") +\n+  scale_fill_manual(values = c(""#D62728"", ""#FF7F0E"", ""#8C564B"",""#2CA02C"", ""#1F77B4"", ""#9467BD"")) +                   \n+  xlab(""Category"")+ggtitle(""Birds of Minnesota"")\n+\n+```\n+In the following snippet, we install the [dplyr](https://www.rdocumentation.org/packages/dplyr/versions/0.7.8) and [lubridate](https://www.rdocumentation.org/packages/lubridate/versions/1.8.0) packages to help manipulate and group data in order to plot a stacked bar chart. First, you group the data by the `Category` of bird and then summarise the `MinLength`, `MaxLength`, `MinBodyMass`,`MaxdyMass`,`MinWingspan`,`MaxWingspan` columns. Then, plot the bar chart using `ggplot2` package and specify the colours for the different category and the labels. \n+\n+![Stacked bar chart](images/stacked-bar-chart.png)\n+\n+This bar chart, however, is unreadable because there is too much non-grouped data. You need to select only the data that you want to plot, so let's look at the length of birds based on their category.\n+\n+Filter your data to include only the bird's category.\n+\n+Since there are many categories, you can display this chart vertically and tweak its height to account for all the data:\n+\n+```r\n+birds_count<-dplyr::count(birds_filtered, Category, sort = TRUE)\n+birds_count$Category <- factor(birds_count$Category, levels = birds_count$Category)\n+ggplot(birds_count,aes(Category,n))+geom_bar(stat=""identity"")+coord_flip()\n+```\n+You first count unique values in the `Category` column and then sort them into a new dataframe `birds_count`.This sorted data is then factored in the same level so that it is plotted in the sorted way. Using `ggplot2` you then plot the data in a bar chart. The `coord_flip()` plots horizontal bars. \n+\n+![category-length](images/category-length.png)\n+\n+This bar chart shows a good view of the number of birds in each category. In a blink of an eye, you see that the largest number of birds in this region are in the Ducks/Geese/Waterfowl category. Minnesota is the 'land of 10,000 lakes' so this isn't surprising!\n+\n+✅ Try some other counts on this dataset. Does anything surprise you?\n+\n+## Comparing data\n+\n+You can try different comparisons of grouped data by creating new axes. Try a comparison of the MaxLength of a bird, based on its category:\n+\n+```r\n+birds_grouped <- birds_filtered %>%\n+  group_by(Category) %>%\n+  summarise(\n+  MaxLength = max(MaxLength, na.rm = T),\n+  MinLength = max(MinLength, na.rm = T)\n+           ) %>%\n+  arrange(Category)\n+  \n+ggplot(birds_grouped,aes(Category,MaxLength))+geom_bar(stat=""identity"")+coord_flip()\n+```\n+We group the `birds_filtered` data by `Category` and then plot a bar graph. \n+\n+![comparing data](images/comparingdata.png)\n+\n+Nothing is surprising here: hummingbirds have the least MaxLength compared to Pelicans or Geese. It's good when data makes logical sense!\n+\n+You can create more interesting visualizations of bar charts by superimposing data. Let's superimpose Minimum and Maximum Length on a given bird category:\n+\n+```r\n+ggplot(data=birds_grouped, aes(x=Category)) +\n+  geom_bar(aes(y=MaxLength), stat=""identity"", position =""identity"",  fill='blue') +\n+  geom_bar(aes(y=MinLength), stat=""identity"", position=""identity"", fill='orange')+\n+  coord_flip()\n+```\n+![super-imposed values](images/superimposed-values.png)\n+\n+## 🚀 Challenge\n+\n+This bird dataset offers a wealth of information about different types of birds within a particular ecosystem. Search around the internet and see if you can find other bird-oriented datasets. Practice building charts and graphs around these birds to discover facts you didn't realize.\n+## [Post-lecture quiz]() To be linked\n+\n+## Review & Self Study\n+\n+This first lesson has given you some information about how to use `ggplot2`to visualize quantities. Do some research around other ways to work with datasets for visualization. Research and lookout for datasets that you could visualize using other packages like [Lattice](https://stat.ethz.ch/R-manual/R-devel/library/lattice/html/Lattice.html) and [Plotly](https://github.com/plotly/plotly.R#readme)\n+\n+## Assignment",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ef,microsoft/Data-Science-For-Beginners,847188629,813103836,jlooper,1450004,2022-02-23T16:52:09+00:00,2022-02-23T16:52:09+00:00,"hi, @Vidushi-Gupta sorry I didn't see this. Yes, I think we have to keep this the same quiz",true,3-Data-Visualization/R/09-visualization-quantities/README.md,NaN,"@@ -0,0 +1,216 @@\n+# Visualizing Quantities\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/sketchnotes/09-Visualizing-Quantities.png)|\n+|:---:|\n+| Visualizing Quantities - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+In this lesson you will explore how to use some of the many available R packages libraries to learn how to create interesting visualizations all around the concept of quantity. Using a cleaned dataset about the birds of Minnesota, you can learn many interesting facts about local wildlife. \n+## [Pre-lecture quiz]() TO BE LINKED",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f0,microsoft/Data-Science-For-Beginners,796755238,764326692,jlooper,1450004,2021-12-07T20:09:12+00:00,2021-12-07T20:09:19+00:00,"hi, let's relink this to the root assignment using `../` please",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f1,microsoft/Data-Science-For-Beginners,796755238,767133932,mertkahramann,73400722,2021-12-11T09:24:27+00:00,2021-12-11T12:35:31+00:00,i suppose it's all good now and i translated another section (base README.md),false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f2,microsoft/Data-Science-For-Beginners,796755238,764326692,jlooper,1450004,2021-12-07T20:09:12+00:00,2021-12-07T20:09:19+00:00,"hi, let's relink this to the root assignment using `../` please",true,1-Introduction/03-defining-data/translations/README.tr.md,74,"@@ -0,0 +1,76 @@\n+# Veriyi Tanımlamak\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../../sketchnotes/03-DefiningData.png)|\n+|:---:|\n+|Veriyi Tanımlamak - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+Veri, keşifler yapmak ve bilinçli kararları desteklemek için kullanılan gerçekler, bilgi, gözlemler ve ölçümlerdir. Bir veri noktası, veri noktalarından oluşan bir yığın olan veri setlerindeki bir birim veridir. Veri setleri genellikle kaynağına veya verinin nereden geldiğine bağlı olarak farklı formatlarda ve yapılarda bulunabilir. Örneğin, bir şirketin aylık kazancı bir hesap çizelgesinde bulunabilecekken bir akıllı saatten gelen saatlik kalp atışı verisi [JSON] (https://stackoverflow.com/a/383699) formatında olabilir.\n+\n+Bu ders veriyi karakteristiklerine ve kaynaklarına göre tanımlama ve sınıflandırma üzerine odaklanmaktadır.\n+\n+## [Ders Öncesi Kısa Sınavı](https://red-water-0103e7a0f.azurestaticapps.net/quiz/4)\n+\n+## Veri nasıl tanımlanır\n+\n+**Ham veri** kaynağından oluşturulduğu şekilde aktarılan ve analiz edilmemiş veya düzenlenmemiş veridir. Bir veri setinde ne olduğunu anlayabilmek için veri setlerinin insanların ve verilerin daha ileri düzeyde analiz etmekte kullanabilecekleri teknolojilerin anlayabileceği bir formatta düzenlenmesi gerekmektedir. Bir veri setinin yapısı veri setinin nasıl düzenlendiğini açıklar ve yapısal, yapısal olmayan ve yarı yapısal olarak sınıflandırılabilir.Bu yapı tipleri değişkenlik gösterebilir ve kaynağa bağlıdır ancak veri setleri nihayetinde bu üç kategoriden birisine uyacaktır.\n+\n+### Nicel veri\n+\n+Nicel veri bir veri setinin içerisindeki sayısal gözlemlerdir ve genel olacak analiz edilebilir, ölçülebilir ve matematiksel olarak kullanılabilirdir. Nicel verinin bazı örnekleri şu şekilde verilebilir: ülkenin nüfusu, kişinin boyu veya şirketin kazancı. Bazı ek analizlerle nicel veri, Hava Kalitesi İndeksi'nin sezonsal eğilimlerini keşfetmek için veya sıradan bir iş günündeki trafiğin en yoğun olduğu zamanları tahmin etmek için kullanılabilir.\n+\n+### Nitel veri\n+Nitel veri veya diğer adıysal kategorik veri, nicel verinin gözlemlerindeki gibi objektif olarak ölçülemeyen verilerdir. Genel olarak ürün veya süreç gibi bir şeyin niteliğini tutan çeşitli formatlardaki subjektif verilerdir. Bazen nitel veri telefon numaraları veya zaman damgaları gibi sayısal olabilir ve genellikle bunlar matematiksel olarak kullanılamaz. Nitel data ile ilgili bazı örnekler: video yorumları, bir arabanın marka ve modeli veya en yakın arkadaşının en sevdiği rengi. Nitel veri, müşterilerin hangi ürünleri en çok sevdiğini görmekte veya bir iş başvurusundaki öz geçmişlerde sıklıkla kullanılan kelimeleri belirlemekte kullanılabilir.\n+\n+### Yapısal veri\n+\n+Yapısal veri, her satırın aynı sütun takımına sahip olduğu satır ve sütunlarla düzenlenmiş veridir. Sütunlar belirli bir tipteki değeri temsil eder ve değerin neyi temsil ettiğini açıklayan bir isimle tanımlanır, satırlar ise gerçek değerleri bulundurur. Sütunlar genellikle değerin doğru bir şekilde sütunu temsil ettiğini güvence altına almak için spesifik bir dizi kurallara ya da değerler üzerinde sınırlandırmalara sahip olur. Örneğin her satırın bir telefon numarası içermesi gerektiği ve numaraların alfabetik karakterleri asla içermemesi gerektiği bir müşteri çizelgesini düşünün. Telefon numarası sütununa asla boş olmaması ve sadece numara içerdiğinden emin olmak için kurallar uygulanmış olabilir.\n+\n+Yapısal verinin bir faydası diğer yapısal verilerle ilişkilendirilerek düzenlenebilir olmasıdır. Ancak veri spesifik bir şekilde düzenlendiği için genel yapısında değişiklikler yapmak oldukça fazla efor gerektirecektir. Örneğin bir müşteri çizelgesine bir email sütunu eklemek, sizin aynı zamanda veri setinde bulunan varolan müşteri satırlarına bu değerleri nasıl ekleyeceğinizi çözmenizi gerektirecektir.\n+\n+Yapısal veriye örnekler: çizelgeler, ilişkisel veritabanları, telefon numaraları, hesap ekstreleri\n+\n+### Yapısal olmayan veri\n+\n+Yapısal olmayan veri genellikle satırlar ve sütunlar kullanılarak kategorize edilemez ve bir format veya takip edilecek kurallar bulundurmazlar. Çünkü yapısal olmayan veriler yapılarında daha az sınırlandırmalar bulundururlar. Yapısal veri setleriyle karşılaştırıldığında yeni bilgi eklemek daha kolaydır. Her 2 dakikada bir barometrik basınç verisini toplayan bir sensör eğer sıcaklığı da ölçüp kaydetmesine izin veren bir güncelleme aldıysa ve eğer veri yapısal değilse mevcut veride değişiklik yapmamıza gerek kalmaz. Ancak bu tip bir veriyi analiz edip incelemek daha uzun süre alabilir. Örneğin sensör verisinden geçen ayki ortalama sıcaklığı bulmak isteyen bir bilim adamını düşünelim. Fakat keşfediyor ki sensör topladığı verilerde bir sayı yerine hatalı olduğunu gösteren ""e"" harfini kaydetmiş, yani bu demek oluyor ki veri eksiktir.\n+\n+Yapısal olmayan veriler üzerine örnekler: metin dosyaları, metin iletisi, video dosyaları\n+\n+### Yarı yapısal\n+\n+Yarı yapısal veri, onu hem yapısal hem de yapısal olmayan verinin bir kombinasyonu yapan özelliklere sahiptir. Genellikle satır ve sütunlar formatına uymaz ancak yapısal olarak kabul edilebilecek bir şekilde düzenlenmiştir ve sabit bir formatı veya bir dizi kuralı takip eder. İyi tanımlanmış bir hiyerarşi veya yeni bilginin kolay entegrasyonuna izin veren daha esnek bir şeyler gibi kaynaklar arasında yapı değişkenlik gösterecektir. Metaveri verilerin nasıl organize edileceğine ve saklanacağına karar vermeye yardımcı olan göstergelerdir ve verinin tipine dayalı olarak çeşitli isimleri olacaktır. Etiketler, elemanlar, varlıklar ve nitelikler bazı yaygın metaveri isimleridir. Örneğin tipik bir email iletisi konuya, mesaj gövdesine ve bir dizi alıcıya sahiptir ve kim tarafından veya ne zaman gönderildiğine göre düzenlenebilir.\n+\n+Yarı yapısal veriye örnekler: HTML, CSV dosyaları, JavaScript Nesne Notasyonu (JSON)\n+\n+## Verinin Kaynakları\n+\n+Bir veri kaynağı verinin oluşturulduğu veya ""yaşadığı"" ilk konumdur ve nasıl ve ne zaman toplandığına göre değişkenlik gösterecektir. Kullanıcı(lar) tarafından oluşturulan veriler birincil veri olarak tanımlanırken ikincil veri dediğimiz veriler genel kullanım için toplanmış verilerin bulunduğu bir kaynaktan gelir. Örneğin bir yağmur ormanındaki gözlemleri toplayan bir gurup bilim adamı birincil kaynak olarak nitelendirilebilirken eğer bu kişiler toplandıkları verileri başka bilim adamlarıyla paylaşmak isterlerse bu verileri kullanacaklara bu veriler ikincil veri olacaktır.\n+\n+Veritabanları yaygın bir kaynaktır ve verileri tutmak ve sürdürülebilirliği sağlamak için bir veritabanı yönetim sistemine bağlıdır. Kullanıcılar verileri araştırmak için sorgular dediğimiz komutları kullanır. Dosya şeklindeki veir kaynakları sesler, görüntüler ve video dosyaları olabileceği gibi Excel gibi hesap çizelgeleri de olabilir. Veritabanlarının ve dosyaların bulunabileceği internet kaynakları verileri barındırmak için yaygın bir kaynaktır. Uygulama programlama arayüzleri (API) programlamacıların harici kullanıcılara internet üzerinden veri paylaşımı için yollar oluşturmaya olanak sağlarken web kazıma işlemi web sitelerinden veri çıkarmaya yarar. [""Veriyle Çalışmak""taki dersler](/2-Working-With-Data) çeşitli veri kaynaklarının nasıl kullanılacağına odaklanmaktadır.\n+\n+## Sonuç\n+\n+Bu derste öğrendiklerimizi gözden geçirelim:\n+\n+- Verinin ne olduğunu\n+- Verinin nasıl tanımlandığını\n+- Verinin nasıl sınıflandırılıp kategorize edildiğini\n+- Verinin nerelerde bulunabileceği öğrendik.\n+\n+## 🚀 Challange\n+\n+Kaggle mükemmel bir açık veri seti kaynağıdır. İlginç birkaç veri seti bulmak ve 3 ila 5 veri setini aşağıdaki kriterlere göre sıralamak için [Veri seti arama aracını](https://www.kaggle.com/datasets) kullanın.\n+\n+Kriterler:\n+\n+- Bu veri nicel midir yoksa nitel midir?\n+- Bu veri yapısal mıdır, yapısal değil midir yoksa yarı yapısal mıdır? \n+\n+## [Ders Sonu Kısa Sınavı](https://red-water-0103e7a0f.azurestaticapps.net/quiz/5)\n+\n+## İnceleme & Öz Çalışma\n+\n+- Bu [Verini Sınıflandır](https://docs.microsoft.com/en-us/learn/modules/choose-storage-approach-in-azure/2-classify-data) başlıklı Microsoft Learn dersi detaylı bir şekilde yapısal, yarı yapısal ve yapısal olmayan verileri ele almaktadır.\n+\n+## Ödev",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f3,microsoft/Data-Science-For-Beginners,796755238,767133932,mertkahramann,73400722,2021-12-11T09:24:27+00:00,2021-12-11T12:35:31+00:00,i suppose it's all good now and i translated another section (base README.md),true,1-Introduction/03-defining-data/translations/README.tr.md,74,"@@ -0,0 +1,76 @@\n+# Veriyi Tanımlamak\n+\n+|![ Sketchnote by [(@sketchthedocs)](https://sketchthedocs.dev) ](../../../sketchnotes/03-DefiningData.png)|\n+|:---:|\n+|Veriyi Tanımlamak - _Sketchnote by [@nitya](https://twitter.com/nitya)_ |\n+\n+Veri, keşifler yapmak ve bilinçli kararları desteklemek için kullanılan gerçekler, bilgi, gözlemler ve ölçümlerdir. Bir veri noktası, veri noktalarından oluşan bir yığın olan veri setlerindeki bir birim veridir. Veri setleri genellikle kaynağına veya verinin nereden geldiğine bağlı olarak farklı formatlarda ve yapılarda bulunabilir. Örneğin, bir şirketin aylık kazancı bir hesap çizelgesinde bulunabilecekken bir akıllı saatten gelen saatlik kalp atışı verisi [JSON] (https://stackoverflow.com/a/383699) formatında olabilir.\n+\n+Bu ders veriyi karakteristiklerine ve kaynaklarına göre tanımlama ve sınıflandırma üzerine odaklanmaktadır.\n+\n+## [Ders Öncesi Kısa Sınavı](https://red-water-0103e7a0f.azurestaticapps.net/quiz/4)\n+\n+## Veri nasıl tanımlanır\n+\n+**Ham veri** kaynağından oluşturulduğu şekilde aktarılan ve analiz edilmemiş veya düzenlenmemiş veridir. Bir veri setinde ne olduğunu anlayabilmek için veri setlerinin insanların ve verilerin daha ileri düzeyde analiz etmekte kullanabilecekleri teknolojilerin anlayabileceği bir formatta düzenlenmesi gerekmektedir. Bir veri setinin yapısı veri setinin nasıl düzenlendiğini açıklar ve yapısal, yapısal olmayan ve yarı yapısal olarak sınıflandırılabilir.Bu yapı tipleri değişkenlik gösterebilir ve kaynağa bağlıdır ancak veri setleri nihayetinde bu üç kategoriden birisine uyacaktır.\n+\n+### Nicel veri\n+\n+Nicel veri bir veri setinin içerisindeki sayısal gözlemlerdir ve genel olacak analiz edilebilir, ölçülebilir ve matematiksel olarak kullanılabilirdir. Nicel verinin bazı örnekleri şu şekilde verilebilir: ülkenin nüfusu, kişinin boyu veya şirketin kazancı. Bazı ek analizlerle nicel veri, Hava Kalitesi İndeksi'nin sezonsal eğilimlerini keşfetmek için veya sıradan bir iş günündeki trafiğin en yoğun olduğu zamanları tahmin etmek için kullanılabilir.\n+\n+### Nitel veri\n+Nitel veri veya diğer adıysal kategorik veri, nicel verinin gözlemlerindeki gibi objektif olarak ölçülemeyen verilerdir. Genel olarak ürün veya süreç gibi bir şeyin niteliğini tutan çeşitli formatlardaki subjektif verilerdir. Bazen nitel veri telefon numaraları veya zaman damgaları gibi sayısal olabilir ve genellikle bunlar matematiksel olarak kullanılamaz. Nitel data ile ilgili bazı örnekler: video yorumları, bir arabanın marka ve modeli veya en yakın arkadaşının en sevdiği rengi. Nitel veri, müşterilerin hangi ürünleri en çok sevdiğini görmekte veya bir iş başvurusundaki öz geçmişlerde sıklıkla kullanılan kelimeleri belirlemekte kullanılabilir.\n+\n+### Yapısal veri\n+\n+Yapısal veri, her satırın aynı sütun takımına sahip olduğu satır ve sütunlarla düzenlenmiş veridir. Sütunlar belirli bir tipteki değeri temsil eder ve değerin neyi temsil ettiğini açıklayan bir isimle tanımlanır, satırlar ise gerçek değerleri bulundurur. Sütunlar genellikle değerin doğru bir şekilde sütunu temsil ettiğini güvence altına almak için spesifik bir dizi kurallara ya da değerler üzerinde sınırlandırmalara sahip olur. Örneğin her satırın bir telefon numarası içermesi gerektiği ve numaraların alfabetik karakterleri asla içermemesi gerektiği bir müşteri çizelgesini düşünün. Telefon numarası sütununa asla boş olmaması ve sadece numara içerdiğinden emin olmak için kurallar uygulanmış olabilir.\n+\n+Yapısal verinin bir faydası diğer yapısal verilerle ilişkilendirilerek düzenlenebilir olmasıdır. Ancak veri spesifik bir şekilde düzenlendiği için genel yapısında değişiklikler yapmak oldukça fazla efor gerektirecektir. Örneğin bir müşteri çizelgesine bir email sütunu eklemek, sizin aynı zamanda veri setinde bulunan varolan müşteri satırlarına bu değerleri nasıl ekleyeceğinizi çözmenizi gerektirecektir.\n+\n+Yapısal veriye örnekler: çizelgeler, ilişkisel veritabanları, telefon numaraları, hesap ekstreleri\n+\n+### Yapısal olmayan veri\n+\n+Yapısal olmayan veri genellikle satırlar ve sütunlar kullanılarak kategorize edilemez ve bir format veya takip edilecek kurallar bulundurmazlar. Çünkü yapısal olmayan veriler yapılarında daha az sınırlandırmalar bulundururlar. Yapısal veri setleriyle karşılaştırıldığında yeni bilgi eklemek daha kolaydır. Her 2 dakikada bir barometrik basınç verisini toplayan bir sensör eğer sıcaklığı da ölçüp kaydetmesine izin veren bir güncelleme aldıysa ve eğer veri yapısal değilse mevcut veride değişiklik yapmamıza gerek kalmaz. Ancak bu tip bir veriyi analiz edip incelemek daha uzun süre alabilir. Örneğin sensör verisinden geçen ayki ortalama sıcaklığı bulmak isteyen bir bilim adamını düşünelim. Fakat keşfediyor ki sensör topladığı verilerde bir sayı yerine hatalı olduğunu gösteren ""e"" harfini kaydetmiş, yani bu demek oluyor ki veri eksiktir.\n+\n+Yapısal olmayan veriler üzerine örnekler: metin dosyaları, metin iletisi, video dosyaları\n+\n+### Yarı yapısal\n+\n+Yarı yapısal veri, onu hem yapısal hem de yapısal olmayan verinin bir kombinasyonu yapan özelliklere sahiptir. Genellikle satır ve sütunlar formatına uymaz ancak yapısal olarak kabul edilebilecek bir şekilde düzenlenmiştir ve sabit bir formatı veya bir dizi kuralı takip eder. İyi tanımlanmış bir hiyerarşi veya yeni bilginin kolay entegrasyonuna izin veren daha esnek bir şeyler gibi kaynaklar arasında yapı değişkenlik gösterecektir. Metaveri verilerin nasıl organize edileceğine ve saklanacağına karar vermeye yardımcı olan göstergelerdir ve verinin tipine dayalı olarak çeşitli isimleri olacaktır. Etiketler, elemanlar, varlıklar ve nitelikler bazı yaygın metaveri isimleridir. Örneğin tipik bir email iletisi konuya, mesaj gövdesine ve bir dizi alıcıya sahiptir ve kim tarafından veya ne zaman gönderildiğine göre düzenlenebilir.\n+\n+Yarı yapısal veriye örnekler: HTML, CSV dosyaları, JavaScript Nesne Notasyonu (JSON)\n+\n+## Verinin Kaynakları\n+\n+Bir veri kaynağı verinin oluşturulduğu veya ""yaşadığı"" ilk konumdur ve nasıl ve ne zaman toplandığına göre değişkenlik gösterecektir. Kullanıcı(lar) tarafından oluşturulan veriler birincil veri olarak tanımlanırken ikincil veri dediğimiz veriler genel kullanım için toplanmış verilerin bulunduğu bir kaynaktan gelir. Örneğin bir yağmur ormanındaki gözlemleri toplayan bir gurup bilim adamı birincil kaynak olarak nitelendirilebilirken eğer bu kişiler toplandıkları verileri başka bilim adamlarıyla paylaşmak isterlerse bu verileri kullanacaklara bu veriler ikincil veri olacaktır.\n+\n+Veritabanları yaygın bir kaynaktır ve verileri tutmak ve sürdürülebilirliği sağlamak için bir veritabanı yönetim sistemine bağlıdır. Kullanıcılar verileri araştırmak için sorgular dediğimiz komutları kullanır. Dosya şeklindeki veir kaynakları sesler, görüntüler ve video dosyaları olabileceği gibi Excel gibi hesap çizelgeleri de olabilir. Veritabanlarının ve dosyaların bulunabileceği internet kaynakları verileri barındırmak için yaygın bir kaynaktır. Uygulama programlama arayüzleri (API) programlamacıların harici kullanıcılara internet üzerinden veri paylaşımı için yollar oluşturmaya olanak sağlarken web kazıma işlemi web sitelerinden veri çıkarmaya yarar. [""Veriyle Çalışmak""taki dersler](/2-Working-With-Data) çeşitli veri kaynaklarının nasıl kullanılacağına odaklanmaktadır.\n+\n+## Sonuç\n+\n+Bu derste öğrendiklerimizi gözden geçirelim:\n+\n+- Verinin ne olduğunu\n+- Verinin nasıl tanımlandığını\n+- Verinin nasıl sınıflandırılıp kategorize edildiğini\n+- Verinin nerelerde bulunabileceği öğrendik.\n+\n+## 🚀 Challange\n+\n+Kaggle mükemmel bir açık veri seti kaynağıdır. İlginç birkaç veri seti bulmak ve 3 ila 5 veri setini aşağıdaki kriterlere göre sıralamak için [Veri seti arama aracını](https://www.kaggle.com/datasets) kullanın.\n+\n+Kriterler:\n+\n+- Bu veri nicel midir yoksa nitel midir?\n+- Bu veri yapısal mıdır, yapısal değil midir yoksa yarı yapısal mıdır? \n+\n+## [Ders Sonu Kısa Sınavı](https://red-water-0103e7a0f.azurestaticapps.net/quiz/5)\n+\n+## İnceleme & Öz Çalışma\n+\n+- Bu [Verini Sınıflandır](https://docs.microsoft.com/en-us/learn/modules/choose-storage-approach-in-azure/2-classify-data) başlıklı Microsoft Learn dersi detaylı bir şekilde yapısal, yarı yapısal ve yapısal olmayan verileri ele almaktadır.\n+\n+## Ödev",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f4,microsoft/Data-Science-For-Beginners,794837347,762656647,jlooper,1450004,2021-12-06T01:53:33+00:00,2021-12-06T01:54:27+00:00,"hi, rather than the full link, it's preferable to add relative links here",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f5,microsoft/Data-Science-For-Beginners,794837347,762656678,jlooper,1450004,2021-12-06T01:53:41+00:00,2021-12-06T01:54:27+00:00,same here,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f6,microsoft/Data-Science-For-Beginners,794837347,763021137,flegaspi700,75757416,2021-12-06T13:48:17+00:00,2021-12-06T13:48:18+00:00,"The relative links returns a Page not found error. I made some modification, so it doesn't have the full link. ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f7,microsoft/Data-Science-For-Beginners,794837347,764327783,jlooper,1450004,2021-12-07T20:10:59+00:00,2021-12-07T20:10:59+00:00,can you link to a README.md file using a relative link?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f8,microsoft/Data-Science-For-Beginners,794837347,762656647,jlooper,1450004,2021-12-06T01:53:33+00:00,2021-12-06T01:54:27+00:00,"hi, rather than the full link, it's preferable to add relative links here",true,4-Data-Science-Lifecycle/15-analyzing/README.md,NaN,"@@ -34,10 +34,10 @@ General querying of the data can help you answer some general questions and theo\n The [`query() `function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) in the Pandas library allows you to select columns and receive simple answers about the data through the rows retrieved.\n \n ## Exploring with Visualizations\n-You don’t have to wait until the data is thoroughly cleaned and analyzed to start creating visualizations. In fact, having a visual representation while exploring can help identify patterns, relationships, and problems in the data. Furthermore, visualizations provide a means of communication with those who are not involved with managing the data and can be an opportunity to share and clarify additional questions that were not addressed in the capture stage. Refer to the [section on Visualizations](3-Data-Visualization) to learn more about some popular ways to explore visually.\n+You don’t have to wait until the data is thoroughly cleaned and analyzed to start creating visualizations. In fact, having a visual representation while exploring can help identify patterns, relationships, and problems in the data. Furthermore, visualizations provide a means of communication with those who are not involved with managing the data and can be an opportunity to share and clarify additional questions that were not addressed in the capture stage. Refer to the [section on Visualizations](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/3-Data-Visualization) to learn more about some popular ways to explore visually.",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303f9,microsoft/Data-Science-For-Beginners,794837347,762656678,jlooper,1450004,2021-12-06T01:53:41+00:00,2021-12-06T01:54:27+00:00,same here,true,4-Data-Science-Lifecycle/15-analyzing/README.md,NaN,"@@ -34,10 +34,10 @@ General querying of the data can help you answer some general questions and theo\n The [`query() `function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) in the Pandas library allows you to select columns and receive simple answers about the data through the rows retrieved.\n \n ## Exploring with Visualizations\n-You don’t have to wait until the data is thoroughly cleaned and analyzed to start creating visualizations. In fact, having a visual representation while exploring can help identify patterns, relationships, and problems in the data. Furthermore, visualizations provide a means of communication with those who are not involved with managing the data and can be an opportunity to share and clarify additional questions that were not addressed in the capture stage. Refer to the [section on Visualizations](3-Data-Visualization) to learn more about some popular ways to explore visually.\n+You don’t have to wait until the data is thoroughly cleaned and analyzed to start creating visualizations. In fact, having a visual representation while exploring can help identify patterns, relationships, and problems in the data. Furthermore, visualizations provide a means of communication with those who are not involved with managing the data and can be an opportunity to share and clarify additional questions that were not addressed in the capture stage. Refer to the [section on Visualizations](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/3-Data-Visualization) to learn more about some popular ways to explore visually.\n \n ## Exploring to identify inconsistencies\n-All the topics in this lesson can help identify missing or inconsistent values, but Pandas provides functions to check for some of these. [isna() or isnull()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isna.html) can check for missing values. One important piece of exploring for these values within your data is to explore why they ended up that way in the first place. This can help you decide on what [actions to take to resolve them](2-Working-With-Data\08-data-preparation\notebook.ipynb).\n+All the topics in this lesson can help identify missing or inconsistent values, but Pandas provides functions to check for some of these. [isna() or isnull()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isna.html) can check for missing values. One important piece of exploring for these values within your data is to explore why they ended up that way in the first place. This can help you decide on what [actions to take to resolve them](https://github.com/microsoft/Data-Science-For-Beginners/blob/main/2-Working-With-Data/08-data-preparation/notebook.ipynb).",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303fa,microsoft/Data-Science-For-Beginners,794837347,763021137,flegaspi700,75757416,2021-12-06T13:48:17+00:00,2021-12-06T13:48:18+00:00,"The relative links returns a Page not found error. I made some modification, so it doesn't have the full link. ",true,4-Data-Science-Lifecycle/15-analyzing/README.md,NaN,"@@ -34,10 +34,10 @@ General querying of the data can help you answer some general questions and theo\n The [`query() `function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) in the Pandas library allows you to select columns and receive simple answers about the data through the rows retrieved.\n \n ## Exploring with Visualizations\n-You don’t have to wait until the data is thoroughly cleaned and analyzed to start creating visualizations. In fact, having a visual representation while exploring can help identify patterns, relationships, and problems in the data. Furthermore, visualizations provide a means of communication with those who are not involved with managing the data and can be an opportunity to share and clarify additional questions that were not addressed in the capture stage. Refer to the [section on Visualizations](3-Data-Visualization) to learn more about some popular ways to explore visually.\n+You don’t have to wait until the data is thoroughly cleaned and analyzed to start creating visualizations. In fact, having a visual representation while exploring can help identify patterns, relationships, and problems in the data. Furthermore, visualizations provide a means of communication with those who are not involved with managing the data and can be an opportunity to share and clarify additional questions that were not addressed in the capture stage. Refer to the [section on Visualizations](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/3-Data-Visualization) to learn more about some popular ways to explore visually.",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303fb,microsoft/Data-Science-For-Beginners,794837347,764327783,jlooper,1450004,2021-12-07T20:10:59+00:00,2021-12-07T20:10:59+00:00,can you link to a README.md file using a relative link?,true,4-Data-Science-Lifecycle/15-analyzing/README.md,NaN,"@@ -34,10 +34,10 @@ General querying of the data can help you answer some general questions and theo\n The [`query() `function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) in the Pandas library allows you to select columns and receive simple answers about the data through the rows retrieved.\n \n ## Exploring with Visualizations\n-You don’t have to wait until the data is thoroughly cleaned and analyzed to start creating visualizations. In fact, having a visual representation while exploring can help identify patterns, relationships, and problems in the data. Furthermore, visualizations provide a means of communication with those who are not involved with managing the data and can be an opportunity to share and clarify additional questions that were not addressed in the capture stage. Refer to the [section on Visualizations](3-Data-Visualization) to learn more about some popular ways to explore visually.\n+You don’t have to wait until the data is thoroughly cleaned and analyzed to start creating visualizations. In fact, having a visual representation while exploring can help identify patterns, relationships, and problems in the data. Furthermore, visualizations provide a means of communication with those who are not involved with managing the data and can be an opportunity to share and clarify additional questions that were not addressed in the capture stage. Refer to the [section on Visualizations](https://github.com/microsoft/Data-Science-For-Beginners/tree/main/3-Data-Visualization) to learn more about some popular ways to explore visually.",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303fc,microsoft/Data-Science-For-Beginners,787572665,757742996,jlooper,1450004,2021-11-27T05:13:27+00:00,2021-11-27T05:13:36+00:00,"There's another typo: \r\n\r\nThere is no doubt that making and building projects is the best way to put your skills and knowledge to the test. In this lesson, we are going to explore two different ways of building a data science project for the prediction of heart failure attacks in Azure ML Studio, through Low code/No code and through the Azure ML SDK as shown in the following schema:",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303fd,microsoft/Data-Science-For-Beginners,787572665,760535095,jlooper,1450004,2021-12-01T20:10:38+00:00,2021-12-01T20:10:38+00:00,@quake2005 can you make the requested edit and I will merge,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303fe,microsoft/Data-Science-For-Beginners,787572665,760647165,Mohamad-Jaallouk,7364193,2021-12-01T23:23:04+00:00,2021-12-01T23:23:04+00:00,@jlooper Sure! I opened a new PR. I will close this one. (Not sure of it's the right way as I could not edit the old one).,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec784303ff,microsoft/Data-Science-For-Beginners,787572665,757742996,jlooper,1450004,2021-11-27T05:13:27+00:00,2021-11-27T05:13:36+00:00,"There's another typo: \r\n\r\nThere is no doubt that making and building projects is the best way to put your skills and knowledge to the test. In this lesson, we are going to explore two different ways of building a data science project for the prediction of heart failure attacks in Azure ML Studio, through Low code/No code and through the Azure ML SDK as shown in the following schema:",true,5-Data-Science-In-Cloud/18-Low-Code/README.md,5,"@@ -50,7 +50,7 @@ Azure ML provides all the tools developers and data scientists need for their ma\n \n ### 1.2 The Heart Failure Prediction Project:\n \n-There is no doubt that making and building projects is the best to put your skills and knowledge to test. In this lesson, we are going to explore two different ways of building a data science project for the prediction of heart failure attacks in Azure ML Studio, through Low code/No code and through the Azure ML SDK as shown in the following schema:\n+There is no doubt that making and building projects is the best way to put your skills and knowledge to test. In this lesson, we are going to explore two different ways of building a data science project for the prediction of heart failure attacks in Azure ML Studio, through Low code/No code and through the Azure ML SDK as shown in the following schema:",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430400,microsoft/Data-Science-For-Beginners,787572665,760535095,jlooper,1450004,2021-12-01T20:10:38+00:00,2021-12-01T20:10:38+00:00,@quake2005 can you make the requested edit and I will merge,true,5-Data-Science-In-Cloud/18-Low-Code/README.md,5,"@@ -50,7 +50,7 @@ Azure ML provides all the tools developers and data scientists need for their ma\n \n ### 1.2 The Heart Failure Prediction Project:\n \n-There is no doubt that making and building projects is the best to put your skills and knowledge to test. In this lesson, we are going to explore two different ways of building a data science project for the prediction of heart failure attacks in Azure ML Studio, through Low code/No code and through the Azure ML SDK as shown in the following schema:\n+There is no doubt that making and building projects is the best way to put your skills and knowledge to test. In this lesson, we are going to explore two different ways of building a data science project for the prediction of heart failure attacks in Azure ML Studio, through Low code/No code and through the Azure ML SDK as shown in the following schema:",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430401,microsoft/Data-Science-For-Beginners,787572665,760647165,Mohamad-Jaallouk,7364193,2021-12-01T23:23:04+00:00,2021-12-01T23:23:04+00:00,@jlooper Sure! I opened a new PR. I will close this one. (Not sure of it's the right way as I could not edit the old one).,true,5-Data-Science-In-Cloud/18-Low-Code/README.md,5,"@@ -50,7 +50,7 @@ Azure ML provides all the tools developers and data scientists need for their ma\n \n ### 1.2 The Heart Failure Prediction Project:\n \n-There is no doubt that making and building projects is the best to put your skills and knowledge to test. In this lesson, we are going to explore two different ways of building a data science project for the prediction of heart failure attacks in Azure ML Studio, through Low code/No code and through the Azure ML SDK as shown in the following schema:\n+There is no doubt that making and building projects is the best way to put your skills and knowledge to test. In this lesson, we are going to explore two different ways of building a data science project for the prediction of heart failure attacks in Azure ML Studio, through Low code/No code and through the Azure ML SDK as shown in the following schema:",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430402,microsoft/Data-Science-For-Beginners,785975006,757743466,jlooper,1450004,2021-11-27T05:20:32+00:00,2021-11-27T05:20:39+00:00,"hi, can you ensure that these links are valid, probably you need relative links back to the English versions",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430403,microsoft/Data-Science-For-Beginners,785975006,759248769,FranciscoImanolSuarez,28679360,2021-11-30T12:53:55+00:00,2021-11-30T12:53:55+00:00,"Hello! Is it ok if I first translate ""05-relational-bases/README.md"" and then link it here?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430404,microsoft/Data-Science-For-Beginners,785975006,762084965,paladique,1414307,2021-12-03T16:37:03+00:00,2021-12-03T16:37:03+00:00,"@FranciscoImanolSuarez that's fine if you'd like to, but let's get this one merged first. I suggest updating the links first (links to lessons 5,6,7,8 will need to be updated in this readme) and then submit your lesson translation in a new PR.  ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430405,microsoft/Data-Science-For-Beginners,785975006,775613185,FranciscoImanolSuarez,28679360,2021-12-27T18:49:35+00:00,2021-12-27T18:49:35+00:00,"Perfect @paladique , unfortunately I can't understand what the error of the links is.\r\n\r\nSorry for the delay in responding, from today I will dedicate more time to this.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430406,microsoft/Data-Science-For-Beginners,785975006,781567982,paladique,1414307,2022-01-10T21:33:38+00:00,2022-01-10T21:33:38+00:00,"No problem, these can take some time and we appreciate the time you've spent on it so far. The English versions can be accessed outside the `translations` directory like this:\r\n\r\n `[Bases de datos relacionales](../05-relational-databases/README.md)`\r\n\r\nLet us know if you have any other questions",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430407,microsoft/Data-Science-For-Beginners,785975006,783040474,FranciscoImanolSuarez,28679360,2022-01-12T12:44:41+00:00,2022-01-12T12:45:00+00:00,Problems solved @paladique ,false,NaN,NaN,NaN,1,1,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430408,microsoft/Data-Science-For-Beginners,785975006,757743466,jlooper,1450004,2021-11-27T05:20:32+00:00,2021-11-27T05:20:39+00:00,"hi, can you ensure that these links are valid, probably you need relative links back to the English versions",true,2-Working-With-Data/translations/README.es.md,NaN,"@@ -0,0 +1,17 @@\n+# Trabajando con datos\n+\n+![Amor por los datos](images/data-love.jpg)\n+> Fotografia de <a href=""https://unsplash.com/@swimstaralex?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Alexander Sinn</a> on <a href=""https://unsplash.com/s/photos/data?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>\n+  \n+En estas lecciones, aprenderás algunas de las formas para manejar datos, también aprenderás a manipularlos y usarlos en aplicaciones. Aprendera sobre bases de datos relacionales y no relacionales así como también almacenar datos en estas. Veremos los fundamentos de Python para poder administrar datos y extraerlos.\n+\n+### Temas\n+\n+1. [Bases de datos relacionales](05-relational-databases/README.md)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430409,microsoft/Data-Science-For-Beginners,785975006,759248769,FranciscoImanolSuarez,28679360,2021-11-30T12:53:55+00:00,2021-11-30T12:53:55+00:00,"Hello! Is it ok if I first translate ""05-relational-bases/README.md"" and then link it here?",true,2-Working-With-Data/translations/README.es.md,NaN,"@@ -0,0 +1,17 @@\n+# Trabajando con datos\n+\n+![Amor por los datos](images/data-love.jpg)\n+> Fotografia de <a href=""https://unsplash.com/@swimstaralex?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Alexander Sinn</a> on <a href=""https://unsplash.com/s/photos/data?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText"">Unsplash</a>\n+  \n+En estas lecciones, aprenderás algunas de las formas para manejar datos, también aprenderás a manipularlos y usarlos en aplicaciones. Aprendera sobre bases de datos relacionales y no relacionales así como también almacenar datos en estas. Veremos los fundamentos de Python para poder administrar datos y extraerlos.\n+\n+### Temas\n+\n+1. [Bases de datos relacionales](05-relational-databases/README.md)",0,0,0,0,0,0,0,microsoft_Data-Science-For-Beginners
6824cd900ce5dcec78430536,microsoft/ML-For-Beginners,874488054,821976688,jlooper,1450004,2022-03-08T19:00:44+00:00,2022-03-08T19:02:42+00:00,all of these links need to be turned to relative paths so they won't 404,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430537,microsoft/ML-For-Beginners,874488054,821977229,jlooper,1450004,2022-03-08T19:01:23+00:00,2022-03-08T19:02:42+00:00,"this will 404, fix link ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430538,microsoft/ML-For-Beginners,874488054,821977401,jlooper,1450004,2022-03-08T19:01:37+00:00,2022-03-08T19:02:42+00:00,malformed markdown,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430539,microsoft/ML-For-Beginners,874488054,821977537,jlooper,1450004,2022-03-08T19:01:49+00:00,2022-03-08T19:02:42+00:00,this will 404,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843053a,microsoft/ML-For-Beginners,874488054,821977987,jlooper,1450004,2022-03-08T19:02:28+00:00,2022-03-08T19:02:42+00:00,this is broken,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843053b,microsoft/ML-For-Beginners,874488054,829092830,jlooper,1450004,2022-03-17T13:03:54+00:00,2022-03-17T13:05:11+00:00,not sure why the links here are going to the Italian translation?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843053c,microsoft/ML-For-Beginners,874488054,829093189,jlooper,1450004,2022-03-17T13:04:12+00:00,2022-03-17T13:05:11+00:00,This is linking to the Italian translation,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843053d,microsoft/ML-For-Beginners,874488054,829094136,jlooper,1450004,2022-03-17T13:05:02+00:00,2022-03-17T13:05:11+00:00,This should link to an animated gif,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843053e,microsoft/ML-For-Beginners,874488054,821976688,jlooper,1450004,2022-03-08T19:00:44+00:00,2022-03-08T19:02:42+00:00,all of these links need to be turned to relative paths so they won't 404,true,translations/README.hi.md,NaN,"@@ -0,0 +1,127 @@\n+[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n+[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n+[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n+[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n+\n+[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n+[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n+[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n+\n+# शुरुआती के लिए मशीन लर्निंग - एक पाठ्यक्रम\n+\n+> 🌍दुनिया भर में यात्रा करें क्योंकि हम विश्व संस्कृतियों के माध्यम से मशीन लर्निंग का पता लगाते हैं 🌍\n+\n+माइक्रोसॉफ्ट के एज़्योर क्लाउड एडवोकेट्स को 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्नता हो रही है **मशीन लर्निंग** के बारे में 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्न हैं। इस पाठ्यक्रम में, आप   प्राथमिक रूप से स्किकिट-लर्न को एक पुस्तकालय के रूप में उपयोग करते हुए और हमारे आगामी 'एआई फॉर बिगिनर्स' पाठ्यक्रम में शामिल गहन शिक्षण से बचने के लिए, जिसे कभी-कभी **क्लासिक मशीन लर्निंग** कहा जाता है, के बारे में जानेंगे। इन पाठों को हमारे साथ जोड़ें ['शुरुआती के लिए डेटा विज्ञान' पाठ्यक्रम](https://aka.ms/datascience-beginners), भी!\n+\n+दुनिया भर में हमारे साथ यात्रा करें क्योंकि हम इन क्लासिक तकनीकों को दुनिया के कई क्षेत्रों के डेटा पर लागू करते हैं। प्रत्येक पाठ में पाठ से पहले और बाद में प्रश्नोत्तरी, पाठ को पूरा करने के लिए लिखित निर्देश, एक समाधान, एक असाइनमेंट, और बहुत कुछ शामिल हैं। हमारी परियोजना-आधारित शिक्षाशास्त्र आपको निर्माण करते समय सीखने की अनुमति देता है, जो नए कौशल को 'छड़ी' करने का एक सिद्ध तरीका है।\n+\n+**✍️ हमारे लेखकों ** जेन लूपर, स्टीफन हॉवेल, फ्रांसेस्का लाज़ेरी, टोमोमी इमुरा, कैसी ब्रेवियू, दिमित्री सोशनिकोव, क्रिस नोरिंग, अनिर्बान मुखर्जी, ओरनेला अल्टुनियन और एमी बॉयड को हार्दिक धन्यवाद।\n+\n+**🎨 हमारे चित्रकारों ** तोमोमी इमुरा, दासानी मदिपल्ली और जेन लूपर को भी धन्यवाद\n+\n+**🙏 हमारे Microsoft छात्र राजदूत लेखकों, समीक्षकों और सामग्री योगदानकर्ताओं के लिए विशेष धन्यवाद **, विशेष रूप से रिषित डागली, मुहम्मद साकिब खान इनान, रोहन राज, अलेक्जेंड्रू पेट्रेस्कु, अभिषेक जायसवाल, नवरीन तबस्सुम, इओन समुइला, और स्निग्धा अग्रवाल\n+\n+**🤩 हमारे R पाठों के लिए Microsoft छात्र राजदूत एरिक वंजाउ का अतिरिक्त आभार!**\n+\n+---\n+\n+# शुरू करना\n+\n+**विद्यार्थी**, इस पाठ्यक्रम का उपयोग करने के लिए, संपूर्ण रेपो को अपने स्वयं के गिटहब खाते में फोर्क करें और अभ्यास स्वयं या समूह के साथ पूरा करें:\n+\n+- प्री-लेक्चर क्विज से शुरुआत करें।\n+- व्याख्यान पढ़ें और गतिविधियों को पूरा करें, प्रत्येक ज्ञान जांच पर रुकें और प्रतिबिंबित करें।\n+- समाधान कोड चलाने के बजाय पाठों को समझकर प्रोजेक्ट बनाने का प्रयास करें; हालांकि वह कोड प्रत्येक परियोजना-उन्मुख पाठ में `/ समाधान` फ़ोल्डर में उपलब्ध है।\n+- व्याख्यान के बाद प्रश्नोत्तरी लें।\n+- चुनौती को पूरा करें।\n+- असाइनमेंट पूरा करें।\n+- एक पाठ समूह पूरा करने के बाद, [चर्चा बोर्ड](https://github.com/microsoft/ML-For-Beginners/discussions) पर जाएँ और उपयुक्त PAT रूब्रिक भरकर ""ज़ोर से सीखें""। एक 'पीएटी' एक प्रगति आकलन उपकरण है जो एक रूब्रिक है जिसे आप अपने सीखने को आगे बढ़ाने के लिए भरते हैं। आप अन्य पीएटी पर भी प्रतिक्रिया कर सकते हैं ताकि हम एक साथ सीख सकें।\n+\n+> आगे के अध्ययन के लिए, हम इन [माइक्रोसॉफ्ट लर्न](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) मॉड्यूल और सीखने के रास्तों का अनुसरण करने की सलाह देते हैं।.\n+\n+**शिक्षक**, हमारे पास [कुछ सुझाव शामिल हैं] (for-teachers.md) इस पाठ्यक्रम का उपयोग कैसे करें।\n+---\n+\n+## टीम से मिलो\n+\n+[![प्रोमो वीडियो](ml.gif)](https://youtu.be/Tj1XWrDSYJU ""Promo video"")\n+\n+**Gif by** [मोहित जैसल](https://linkedin.com/in/mohitjaisal)\n+\n+> 🎥 परियोजना और इसे बनाने वाले लोगों के बारे में वीडियो के लिए ऊपर की छवि पर क्लिक करें!\n+---\n+\n+## शिक्षा शास्त्र\n+\n+इस पाठ्यक्रम का निर्माण करते समय हमने दो शैक्षणिक सिद्धांतों को चुना है: यह सुनिश्चित करना कि यह व्यावहारिक रूप से **परियोजना-आधारित** है और इसमें **लगातार प्रश्नोत्तरी** शामिल हैं। इसके अलावा, इस पाठ्यक्रम में एक समान **थीम** है जो इसे एकता प्रदान करता है।\n+\n+यह सुनिश्चित करके कि सामग्री परियोजनाओं के साथ संरेखित होती है, छात्रों के लिए प्रक्रिया को और अधिक आकर्षक बनाया जाता है और अवधारणाओं के प्रतिधारण को बढ़ाया जाएगा। इसके अलावा, कक्षा से पहले एक कम-दांव प्रश्नोत्तरी छात्र के विषय को सीखने के इरादे को निर्धारित करती है, जबकि कक्षा के बाद दूसरी प्रश्नोत्तरी आगे प्रतिधारण सुनिश्चित करती है। इस पाठ्यक्रम को लचीला और मजेदार बनाने के लिए डिज़ाइन किया गया था और इसे पूर्ण या आंशिक रूप से लिया जा सकता है। परियोजनाएं छोटी शुरू होती हैं और 12-सप्ताह के चक्र के अंत तक तेजी से जटिल हो जाती हैं। इस पाठ्यक्रम में एमएल के वास्तविक-विश्व अनुप्रयोगों पर एक पोस्टस्क्रिप्ट भी शामिल है, जिसका उपयोग अतिरिक्त क्रेडिट के रूप में या चर्चा के आधार के रूप में किया जा सकता है।\n+\n+> हमारी [आचार संहिता](CODE_OF_CONDUCT.md), [योगदान](CONTRIBUTING.md), और [अनुवाद](TRANSLATIONS.md) दिशानिर्देश खोजें। हम आपकी रचनात्मक प्रतिक्रिया का स्वागत करते हैं!\n+\n+## प्रत्येक पाठ में शामिल हैं:\n+\n+- वैकल्पिक स्केचनोट\n+- वैकल्पिक पूरक वीडियो\n+- पूर्व व्याख्यान वार्मअप प्रश्नोत्तरी\n+- लिखित पाठ\n+- प्रोजेक्ट-आधारित पाठों के लिए, प्रोजेक्ट बनाने के तरीके के बारे में चरण-दर-चरण मार्गदर्शिका\n+- ज्ञान जांच\n+- एक चुनौती\n+- पूरक पठन\n+- कार्यभार\n+- व्याख्यान के बाद प्रश्नोत्तरी\n+\n+> **भाषाओं के बारे में एक नोट**: ये पाठ मुख्य रूप से पायथन में लिखे गए हैं, लेकिन कई आर में भी उपलब्ध हैं। एक आर पाठ को पूरा करने के लिए, `/ समाधान` फ़ोल्डर में जाएं और आर पाठ देखें। उनमें एक .rmd एक्सटेंशन शामिल है जो एक **R मार्कडाउन** फ़ाइल का प्रतिनिधित्व करता है जिसे केवल `कोड चंक्स` (आर या अन्य भाषाओं के) के एम्बेडिंग के रूप में परिभाषित किया जा सकता है और एक `वाईएएमएल हेडर` (जो इस तरह के आउटपुट को प्रारूपित करने का मार्गदर्शन करता है) पीडीएफ के रूप में) एक `मार्कडाउन दस्तावेज़` में। जैसे, यह डेटा विज्ञान के लिए एक अनुकरणीय संलेखन ढांचे के रूप में कार्य करता है क्योंकि यह आपको अपने कोड, इसके आउटपुट और आपके विचारों को मार्कडाउन में लिखने की अनुमति देकर आपको संयोजित करने की अनुमति देता है। इसके अलावा, आर मार्कडाउन दस्तावेजों को पीडीएफ, एचटीएमएल या वर्ड जैसे आउटपुट स्वरूपों में प्रस्तुत किया जा सकता है।\n+\n+> **क्विज़ के बारे में एक नोट**: सभी क्विज़ शामिल हैं [इस ऐप में](https://white-water-09ec41f0f.azurestaticapps.net/), प्रत्येक तीन प्रश्नों के कुल 52 क्विज़ के लिए। वे पाठों के भीतर से जुड़े हुए हैं लेकिन प्रश्नोत्तरी ऐप को स्थानीय रूप से चलाया जा सकता है; `क्विज़-ऐप` फ़ोल्डर में दिए गए निर्देशों का पालन करें।\n+\n+| पाठ संख्या |                             विषय                             |                  पाठ समूहन                  | सीखने के मकसद                                                                                                            |                                                             जुड़ा हुआ पाठ                                                              |                        लेखक                        |\n+| :-----------: | :------------------------------------------------------------: | :-------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------: |\n+| 01 | मशीन लर्निंग का परिचय | [परिचय](1-Introduction/README.md)  | मशीन लर्निंग के पीछे की बुनियादी अवधारणाओं को जानें | [पाठ](1-Introduction/1-intro-to-ML/README.md)   | मुहम्मद |",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843053f,microsoft/ML-For-Beginners,874488054,821977229,jlooper,1450004,2022-03-08T19:01:23+00:00,2022-03-08T19:02:42+00:00,"this will 404, fix link ",true,translations/README.hi.md,NaN,"@@ -0,0 +1,127 @@\n+[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n+[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n+[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n+[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n+\n+[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n+[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n+[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n+\n+# शुरुआती के लिए मशीन लर्निंग - एक पाठ्यक्रम\n+\n+> 🌍दुनिया भर में यात्रा करें क्योंकि हम विश्व संस्कृतियों के माध्यम से मशीन लर्निंग का पता लगाते हैं 🌍\n+\n+माइक्रोसॉफ्ट के एज़्योर क्लाउड एडवोकेट्स को 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्नता हो रही है **मशीन लर्निंग** के बारे में 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्न हैं। इस पाठ्यक्रम में, आप   प्राथमिक रूप से स्किकिट-लर्न को एक पुस्तकालय के रूप में उपयोग करते हुए और हमारे आगामी 'एआई फॉर बिगिनर्स' पाठ्यक्रम में शामिल गहन शिक्षण से बचने के लिए, जिसे कभी-कभी **क्लासिक मशीन लर्निंग** कहा जाता है, के बारे में जानेंगे। इन पाठों को हमारे साथ जोड़ें ['शुरुआती के लिए डेटा विज्ञान' पाठ्यक्रम](https://aka.ms/datascience-beginners), भी!\n+\n+दुनिया भर में हमारे साथ यात्रा करें क्योंकि हम इन क्लासिक तकनीकों को दुनिया के कई क्षेत्रों के डेटा पर लागू करते हैं। प्रत्येक पाठ में पाठ से पहले और बाद में प्रश्नोत्तरी, पाठ को पूरा करने के लिए लिखित निर्देश, एक समाधान, एक असाइनमेंट, और बहुत कुछ शामिल हैं। हमारी परियोजना-आधारित शिक्षाशास्त्र आपको निर्माण करते समय सीखने की अनुमति देता है, जो नए कौशल को 'छड़ी' करने का एक सिद्ध तरीका है।\n+\n+**✍️ हमारे लेखकों ** जेन लूपर, स्टीफन हॉवेल, फ्रांसेस्का लाज़ेरी, टोमोमी इमुरा, कैसी ब्रेवियू, दिमित्री सोशनिकोव, क्रिस नोरिंग, अनिर्बान मुखर्जी, ओरनेला अल्टुनियन और एमी बॉयड को हार्दिक धन्यवाद।\n+\n+**🎨 हमारे चित्रकारों ** तोमोमी इमुरा, दासानी मदिपल्ली और जेन लूपर को भी धन्यवाद\n+\n+**🙏 हमारे Microsoft छात्र राजदूत लेखकों, समीक्षकों और सामग्री योगदानकर्ताओं के लिए विशेष धन्यवाद **, विशेष रूप से रिषित डागली, मुहम्मद साकिब खान इनान, रोहन राज, अलेक्जेंड्रू पेट्रेस्कु, अभिषेक जायसवाल, नवरीन तबस्सुम, इओन समुइला, और स्निग्धा अग्रवाल\n+\n+**🤩 हमारे R पाठों के लिए Microsoft छात्र राजदूत एरिक वंजाउ का अतिरिक्त आभार!**\n+\n+---\n+\n+# शुरू करना\n+\n+**विद्यार्थी**, इस पाठ्यक्रम का उपयोग करने के लिए, संपूर्ण रेपो को अपने स्वयं के गिटहब खाते में फोर्क करें और अभ्यास स्वयं या समूह के साथ पूरा करें:\n+\n+- प्री-लेक्चर क्विज से शुरुआत करें।\n+- व्याख्यान पढ़ें और गतिविधियों को पूरा करें, प्रत्येक ज्ञान जांच पर रुकें और प्रतिबिंबित करें।\n+- समाधान कोड चलाने के बजाय पाठों को समझकर प्रोजेक्ट बनाने का प्रयास करें; हालांकि वह कोड प्रत्येक परियोजना-उन्मुख पाठ में `/ समाधान` फ़ोल्डर में उपलब्ध है।\n+- व्याख्यान के बाद प्रश्नोत्तरी लें।\n+- चुनौती को पूरा करें।\n+- असाइनमेंट पूरा करें।\n+- एक पाठ समूह पूरा करने के बाद, [चर्चा बोर्ड](https://github.com/microsoft/ML-For-Beginners/discussions) पर जाएँ और उपयुक्त PAT रूब्रिक भरकर ""ज़ोर से सीखें""। एक 'पीएटी' एक प्रगति आकलन उपकरण है जो एक रूब्रिक है जिसे आप अपने सीखने को आगे बढ़ाने के लिए भरते हैं। आप अन्य पीएटी पर भी प्रतिक्रिया कर सकते हैं ताकि हम एक साथ सीख सकें।\n+\n+> आगे के अध्ययन के लिए, हम इन [माइक्रोसॉफ्ट लर्न](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) मॉड्यूल और सीखने के रास्तों का अनुसरण करने की सलाह देते हैं।.\n+\n+**शिक्षक**, हमारे पास [कुछ सुझाव शामिल हैं] (for-teachers.md) इस पाठ्यक्रम का उपयोग कैसे करें।\n+---\n+\n+## टीम से मिलो\n+\n+[![प्रोमो वीडियो](ml.gif)](https://youtu.be/Tj1XWrDSYJU ""Promo video"")\n+\n+**Gif by** [मोहित जैसल](https://linkedin.com/in/mohitjaisal)\n+\n+> 🎥 परियोजना और इसे बनाने वाले लोगों के बारे में वीडियो के लिए ऊपर की छवि पर क्लिक करें!\n+---\n+\n+## शिक्षा शास्त्र\n+\n+इस पाठ्यक्रम का निर्माण करते समय हमने दो शैक्षणिक सिद्धांतों को चुना है: यह सुनिश्चित करना कि यह व्यावहारिक रूप से **परियोजना-आधारित** है और इसमें **लगातार प्रश्नोत्तरी** शामिल हैं। इसके अलावा, इस पाठ्यक्रम में एक समान **थीम** है जो इसे एकता प्रदान करता है।\n+\n+यह सुनिश्चित करके कि सामग्री परियोजनाओं के साथ संरेखित होती है, छात्रों के लिए प्रक्रिया को और अधिक आकर्षक बनाया जाता है और अवधारणाओं के प्रतिधारण को बढ़ाया जाएगा। इसके अलावा, कक्षा से पहले एक कम-दांव प्रश्नोत्तरी छात्र के विषय को सीखने के इरादे को निर्धारित करती है, जबकि कक्षा के बाद दूसरी प्रश्नोत्तरी आगे प्रतिधारण सुनिश्चित करती है। इस पाठ्यक्रम को लचीला और मजेदार बनाने के लिए डिज़ाइन किया गया था और इसे पूर्ण या आंशिक रूप से लिया जा सकता है। परियोजनाएं छोटी शुरू होती हैं और 12-सप्ताह के चक्र के अंत तक तेजी से जटिल हो जाती हैं। इस पाठ्यक्रम में एमएल के वास्तविक-विश्व अनुप्रयोगों पर एक पोस्टस्क्रिप्ट भी शामिल है, जिसका उपयोग अतिरिक्त क्रेडिट के रूप में या चर्चा के आधार के रूप में किया जा सकता है।\n+\n+> हमारी [आचार संहिता](CODE_OF_CONDUCT.md), [योगदान](CONTRIBUTING.md), और [अनुवाद](TRANSLATIONS.md) दिशानिर्देश खोजें। हम आपकी रचनात्मक प्रतिक्रिया का स्वागत करते हैं!",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430540,microsoft/ML-For-Beginners,874488054,821977401,jlooper,1450004,2022-03-08T19:01:37+00:00,2022-03-08T19:02:42+00:00,malformed markdown,true,translations/README.hi.md,NaN,"@@ -0,0 +1,127 @@\n+[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n+[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n+[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n+[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n+\n+[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n+[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n+[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n+\n+# शुरुआती के लिए मशीन लर्निंग - एक पाठ्यक्रम\n+\n+> 🌍दुनिया भर में यात्रा करें क्योंकि हम विश्व संस्कृतियों के माध्यम से मशीन लर्निंग का पता लगाते हैं 🌍\n+\n+माइक्रोसॉफ्ट के एज़्योर क्लाउड एडवोकेट्स को 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्नता हो रही है **मशीन लर्निंग** के बारे में 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्न हैं। इस पाठ्यक्रम में, आप   प्राथमिक रूप से स्किकिट-लर्न को एक पुस्तकालय के रूप में उपयोग करते हुए और हमारे आगामी 'एआई फॉर बिगिनर्स' पाठ्यक्रम में शामिल गहन शिक्षण से बचने के लिए, जिसे कभी-कभी **क्लासिक मशीन लर्निंग** कहा जाता है, के बारे में जानेंगे। इन पाठों को हमारे साथ जोड़ें ['शुरुआती के लिए डेटा विज्ञान' पाठ्यक्रम](https://aka.ms/datascience-beginners), भी!\n+\n+दुनिया भर में हमारे साथ यात्रा करें क्योंकि हम इन क्लासिक तकनीकों को दुनिया के कई क्षेत्रों के डेटा पर लागू करते हैं। प्रत्येक पाठ में पाठ से पहले और बाद में प्रश्नोत्तरी, पाठ को पूरा करने के लिए लिखित निर्देश, एक समाधान, एक असाइनमेंट, और बहुत कुछ शामिल हैं। हमारी परियोजना-आधारित शिक्षाशास्त्र आपको निर्माण करते समय सीखने की अनुमति देता है, जो नए कौशल को 'छड़ी' करने का एक सिद्ध तरीका है।\n+\n+**✍️ हमारे लेखकों ** जेन लूपर, स्टीफन हॉवेल, फ्रांसेस्का लाज़ेरी, टोमोमी इमुरा, कैसी ब्रेवियू, दिमित्री सोशनिकोव, क्रिस नोरिंग, अनिर्बान मुखर्जी, ओरनेला अल्टुनियन और एमी बॉयड को हार्दिक धन्यवाद।\n+\n+**🎨 हमारे चित्रकारों ** तोमोमी इमुरा, दासानी मदिपल्ली और जेन लूपर को भी धन्यवाद",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430541,microsoft/ML-For-Beginners,874488054,821977537,jlooper,1450004,2022-03-08T19:01:49+00:00,2022-03-08T19:02:42+00:00,this will 404,true,translations/README.hi.md,NaN,"@@ -0,0 +1,127 @@\n+[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n+[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n+[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n+[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n+\n+[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n+[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n+[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n+\n+# शुरुआती के लिए मशीन लर्निंग - एक पाठ्यक्रम\n+\n+> 🌍दुनिया भर में यात्रा करें क्योंकि हम विश्व संस्कृतियों के माध्यम से मशीन लर्निंग का पता लगाते हैं 🌍\n+\n+माइक्रोसॉफ्ट के एज़्योर क्लाउड एडवोकेट्स को 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्नता हो रही है **मशीन लर्निंग** के बारे में 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्न हैं। इस पाठ्यक्रम में, आप   प्राथमिक रूप से स्किकिट-लर्न को एक पुस्तकालय के रूप में उपयोग करते हुए और हमारे आगामी 'एआई फॉर बिगिनर्स' पाठ्यक्रम में शामिल गहन शिक्षण से बचने के लिए, जिसे कभी-कभी **क्लासिक मशीन लर्निंग** कहा जाता है, के बारे में जानेंगे। इन पाठों को हमारे साथ जोड़ें ['शुरुआती के लिए डेटा विज्ञान' पाठ्यक्रम](https://aka.ms/datascience-beginners), भी!\n+\n+दुनिया भर में हमारे साथ यात्रा करें क्योंकि हम इन क्लासिक तकनीकों को दुनिया के कई क्षेत्रों के डेटा पर लागू करते हैं। प्रत्येक पाठ में पाठ से पहले और बाद में प्रश्नोत्तरी, पाठ को पूरा करने के लिए लिखित निर्देश, एक समाधान, एक असाइनमेंट, और बहुत कुछ शामिल हैं। हमारी परियोजना-आधारित शिक्षाशास्त्र आपको निर्माण करते समय सीखने की अनुमति देता है, जो नए कौशल को 'छड़ी' करने का एक सिद्ध तरीका है।\n+\n+**✍️ हमारे लेखकों ** जेन लूपर, स्टीफन हॉवेल, फ्रांसेस्का लाज़ेरी, टोमोमी इमुरा, कैसी ब्रेवियू, दिमित्री सोशनिकोव, क्रिस नोरिंग, अनिर्बान मुखर्जी, ओरनेला अल्टुनियन और एमी बॉयड को हार्दिक धन्यवाद।\n+\n+**🎨 हमारे चित्रकारों ** तोमोमी इमुरा, दासानी मदिपल्ली और जेन लूपर को भी धन्यवाद\n+\n+**🙏 हमारे Microsoft छात्र राजदूत लेखकों, समीक्षकों और सामग्री योगदानकर्ताओं के लिए विशेष धन्यवाद **, विशेष रूप से रिषित डागली, मुहम्मद साकिब खान इनान, रोहन राज, अलेक्जेंड्रू पेट्रेस्कु, अभिषेक जायसवाल, नवरीन तबस्सुम, इओन समुइला, और स्निग्धा अग्रवाल\n+\n+**🤩 हमारे R पाठों के लिए Microsoft छात्र राजदूत एरिक वंजाउ का अतिरिक्त आभार!**\n+\n+---\n+\n+# शुरू करना\n+\n+**विद्यार्थी**, इस पाठ्यक्रम का उपयोग करने के लिए, संपूर्ण रेपो को अपने स्वयं के गिटहब खाते में फोर्क करें और अभ्यास स्वयं या समूह के साथ पूरा करें:\n+\n+- प्री-लेक्चर क्विज से शुरुआत करें।\n+- व्याख्यान पढ़ें और गतिविधियों को पूरा करें, प्रत्येक ज्ञान जांच पर रुकें और प्रतिबिंबित करें।\n+- समाधान कोड चलाने के बजाय पाठों को समझकर प्रोजेक्ट बनाने का प्रयास करें; हालांकि वह कोड प्रत्येक परियोजना-उन्मुख पाठ में `/ समाधान` फ़ोल्डर में उपलब्ध है।\n+- व्याख्यान के बाद प्रश्नोत्तरी लें।\n+- चुनौती को पूरा करें।\n+- असाइनमेंट पूरा करें।\n+- एक पाठ समूह पूरा करने के बाद, [चर्चा बोर्ड](https://github.com/microsoft/ML-For-Beginners/discussions) पर जाएँ और उपयुक्त PAT रूब्रिक भरकर ""ज़ोर से सीखें""। एक 'पीएटी' एक प्रगति आकलन उपकरण है जो एक रूब्रिक है जिसे आप अपने सीखने को आगे बढ़ाने के लिए भरते हैं। आप अन्य पीएटी पर भी प्रतिक्रिया कर सकते हैं ताकि हम एक साथ सीख सकें।\n+\n+> आगे के अध्ययन के लिए, हम इन [माइक्रोसॉफ्ट लर्न](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) मॉड्यूल और सीखने के रास्तों का अनुसरण करने की सलाह देते हैं।.\n+\n+**शिक्षक**, हमारे पास [कुछ सुझाव शामिल हैं] (for-teachers.md) इस पाठ्यक्रम का उपयोग कैसे करें।",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430542,microsoft/ML-For-Beginners,874488054,821977987,jlooper,1450004,2022-03-08T19:02:28+00:00,2022-03-08T19:02:42+00:00,this is broken,true,translations/README.hi.md,NaN,"@@ -0,0 +1,127 @@\n+[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n+[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n+[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n+[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n+\n+[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n+[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n+[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n+\n+# शुरुआती के लिए मशीन लर्निंग - एक पाठ्यक्रम\n+\n+> 🌍दुनिया भर में यात्रा करें क्योंकि हम विश्व संस्कृतियों के माध्यम से मशीन लर्निंग का पता लगाते हैं 🌍\n+\n+माइक्रोसॉफ्ट के एज़्योर क्लाउड एडवोकेट्स को 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्नता हो रही है **मशीन लर्निंग** के बारे में 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्न हैं। इस पाठ्यक्रम में, आप   प्राथमिक रूप से स्किकिट-लर्न को एक पुस्तकालय के रूप में उपयोग करते हुए और हमारे आगामी 'एआई फॉर बिगिनर्स' पाठ्यक्रम में शामिल गहन शिक्षण से बचने के लिए, जिसे कभी-कभी **क्लासिक मशीन लर्निंग** कहा जाता है, के बारे में जानेंगे। इन पाठों को हमारे साथ जोड़ें ['शुरुआती के लिए डेटा विज्ञान' पाठ्यक्रम](https://aka.ms/datascience-beginners), भी!\n+\n+दुनिया भर में हमारे साथ यात्रा करें क्योंकि हम इन क्लासिक तकनीकों को दुनिया के कई क्षेत्रों के डेटा पर लागू करते हैं। प्रत्येक पाठ में पाठ से पहले और बाद में प्रश्नोत्तरी, पाठ को पूरा करने के लिए लिखित निर्देश, एक समाधान, एक असाइनमेंट, और बहुत कुछ शामिल हैं। हमारी परियोजना-आधारित शिक्षाशास्त्र आपको निर्माण करते समय सीखने की अनुमति देता है, जो नए कौशल को 'छड़ी' करने का एक सिद्ध तरीका है।\n+\n+**✍️ हमारे लेखकों ** जेन लूपर, स्टीफन हॉवेल, फ्रांसेस्का लाज़ेरी, टोमोमी इमुरा, कैसी ब्रेवियू, दिमित्री सोशनिकोव, क्रिस नोरिंग, अनिर्बान मुखर्जी, ओरनेला अल्टुनियन और एमी बॉयड को हार्दिक धन्यवाद।\n+\n+**🎨 हमारे चित्रकारों ** तोमोमी इमुरा, दासानी मदिपल्ली और जेन लूपर को भी धन्यवाद\n+\n+**🙏 हमारे Microsoft छात्र राजदूत लेखकों, समीक्षकों और सामग्री योगदानकर्ताओं के लिए विशेष धन्यवाद **, विशेष रूप से रिषित डागली, मुहम्मद साकिब खान इनान, रोहन राज, अलेक्जेंड्रू पेट्रेस्कु, अभिषेक जायसवाल, नवरीन तबस्सुम, इओन समुइला, और स्निग्धा अग्रवाल\n+\n+**🤩 हमारे R पाठों के लिए Microsoft छात्र राजदूत एरिक वंजाउ का अतिरिक्त आभार!**\n+\n+---\n+\n+# शुरू करना\n+\n+**विद्यार्थी**, इस पाठ्यक्रम का उपयोग करने के लिए, संपूर्ण रेपो को अपने स्वयं के गिटहब खाते में फोर्क करें और अभ्यास स्वयं या समूह के साथ पूरा करें:\n+\n+- प्री-लेक्चर क्विज से शुरुआत करें।\n+- व्याख्यान पढ़ें और गतिविधियों को पूरा करें, प्रत्येक ज्ञान जांच पर रुकें और प्रतिबिंबित करें।\n+- समाधान कोड चलाने के बजाय पाठों को समझकर प्रोजेक्ट बनाने का प्रयास करें; हालांकि वह कोड प्रत्येक परियोजना-उन्मुख पाठ में `/ समाधान` फ़ोल्डर में उपलब्ध है।\n+- व्याख्यान के बाद प्रश्नोत्तरी लें।\n+- चुनौती को पूरा करें।\n+- असाइनमेंट पूरा करें।\n+- एक पाठ समूह पूरा करने के बाद, [चर्चा बोर्ड](https://github.com/microsoft/ML-For-Beginners/discussions) पर जाएँ और उपयुक्त PAT रूब्रिक भरकर ""ज़ोर से सीखें""। एक 'पीएटी' एक प्रगति आकलन उपकरण है जो एक रूब्रिक है जिसे आप अपने सीखने को आगे बढ़ाने के लिए भरते हैं। आप अन्य पीएटी पर भी प्रतिक्रिया कर सकते हैं ताकि हम एक साथ सीख सकें।\n+\n+> आगे के अध्ययन के लिए, हम इन [माइक्रोसॉफ्ट लर्न](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) मॉड्यूल और सीखने के रास्तों का अनुसरण करने की सलाह देते हैं।.\n+\n+**शिक्षक**, हमारे पास [कुछ सुझाव शामिल हैं] (for-teachers.md) इस पाठ्यक्रम का उपयोग कैसे करें।\n+---\n+\n+## टीम से मिलो\n+\n+[![प्रोमो वीडियो](ml.gif)](https://youtu.be/Tj1XWrDSYJU ""Promo video"")",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430543,microsoft/ML-For-Beginners,874488054,829092830,jlooper,1450004,2022-03-17T13:03:54+00:00,2022-03-17T13:05:11+00:00,not sure why the links here are going to the Italian translation?,true,translations/README.hi.md,NaN,"@@ -0,0 +1,127 @@\n+[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n+[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n+[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n+[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n+\n+[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n+[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n+[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n+\n+# शुरुआती के लिए मशीन लर्निंग - एक पाठ्यक्रम\n+\n+> 🌍दुनिया भर में यात्रा करें क्योंकि हम विश्व संस्कृतियों के माध्यम से मशीन लर्निंग का पता लगाते हैं 🌍\n+\n+माइक्रोसॉफ्ट के एज़्योर क्लाउड एडवोकेट्स को 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्नता हो रही है **मशीन लर्निंग** के बारे में 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्न हैं। इस पाठ्यक्रम में, आप   प्राथमिक रूप से स्किकिट-लर्न को एक पुस्तकालय के रूप में उपयोग करते हुए और हमारे आगामी 'एआई फॉर बिगिनर्स' पाठ्यक्रम में शामिल गहन शिक्षण से बचने के लिए, जिसे कभी-कभी **क्लासिक मशीन लर्निंग** कहा जाता है, के बारे में जानेंगे। इन पाठों को हमारे साथ जोड़ें ['शुरुआती के लिए डेटा विज्ञान' पाठ्यक्रम](https://aka.ms/datascience-beginners), भी!\n+\n+दुनिया भर में हमारे साथ यात्रा करें क्योंकि हम इन क्लासिक तकनीकों को दुनिया के कई क्षेत्रों के डेटा पर लागू करते हैं। प्रत्येक पाठ में पाठ से पहले और बाद में प्रश्नोत्तरी, पाठ को पूरा करने के लिए लिखित निर्देश, एक समाधान, एक असाइनमेंट, और बहुत कुछ शामिल हैं। हमारी परियोजना-आधारित शिक्षाशास्त्र आपको निर्माण करते समय सीखने की अनुमति देता है, जो नए कौशल को 'छड़ी' करने का एक सिद्ध तरीका है।\n+\n+**✍️ हमारे लेखकों** जेन लूपर, स्टीफन हॉवेल, फ्रांसेस्का लाज़ेरी, टोमोमी इमुरा, कैसी ब्रेवियू, दिमित्री सोशनिकोव, क्रिस नोरिंग, अनिर्बान मुखर्जी, ओरनेला अल्टुनियन और एमी बॉयड को हार्दिक धन्यवाद।\n+\n+**🎨 हमारे चित्रकारों** तोमोमी इमुरा, दासानी मदिपल्ली और जेन लूपर को भी धन्यवाद\n+\n+**🙏 हमारे Microsoft छात्र राजदूत लेखकों, समीक्षकों और सामग्री योगदानकर्ताओं के लिए विशेष धन्यवाद**, विशेष रूप से रिषित डागली, मुहम्मद साकिब खान इनान, रोहन राज, अलेक्जेंड्रू पेट्रेस्कु, अभिषेक जायसवाल, नवरीन तबस्सुम, इओन समुइला, और स्निग्धा अग्रवाल\n+\n+**🤩 हमारे R पाठों के लिए Microsoft छात्र राजदूत एरिक वंजाउ का अतिरिक्त आभार!**\n+\n+---\n+\n+# शुरू करना\n+\n+**विद्यार्थी**, इस पाठ्यक्रम का उपयोग करने के लिए, संपूर्ण रेपो को अपने स्वयं के गिटहब खाते में फोर्क करें और अभ्यास स्वयं या समूह के साथ पूरा करें:\n+\n+- प्री-लेक्चर क्विज से शुरुआत करें।\n+- व्याख्यान पढ़ें और गतिविधियों को पूरा करें, प्रत्येक ज्ञान जांच पर रुकें और प्रतिबिंबित करें।\n+- समाधान कोड चलाने के बजाय पाठों को समझकर प्रोजेक्ट बनाने का प्रयास करें; हालांकि वह कोड प्रत्येक परियोजना-उन्मुख पाठ में `/ समाधान` फ़ोल्डर में उपलब्ध है।\n+- व्याख्यान के बाद प्रश्नोत्तरी लें।\n+- चुनौती को पूरा करें।\n+- असाइनमेंट पूरा करें।\n+- एक पाठ समूह पूरा करने के बाद, [चर्चा बोर्ड](https://github.com/microsoft/ML-For-Beginners/discussions) पर जाएँ और उपयुक्त PAT रूब्रिक भरकर ""ज़ोर से सीखें""। एक 'पीएटी' एक प्रगति आकलन उपकरण है जो एक रूब्रिक है जिसे आप अपने सीखने को आगे बढ़ाने के लिए भरते हैं। आप अन्य पीएटी पर भी प्रतिक्रिया कर सकते हैं ताकि हम एक साथ सीख सकें।\n+\n+> आगे के अध्ययन के लिए, हम इन [माइक्रोसॉफ्ट लर्न](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) मॉड्यूल और सीखने के रास्तों का अनुसरण करने की सलाह देते हैं।.\n+\n+**शिक्षक**, हमारे पास [कुछ सुझाव शामिल हैं](../for-teachers.md) इस पाठ्यक्रम का उपयोग कैसे करें।\n+---\n+\n+## टीम से मिलो\n+\n+[![प्रोमो वीडियो](../ml-for-beginners.png)](https://youtu.be/Tj1XWrDSYJU ""Promo video"")\n+\n+**Gif by** [मोहित जैसल](https://linkedin.com/in/mohitjaisal)\n+\n+> 🎥 परियोजना और इसे बनाने वाले लोगों के बारे में वीडियो के लिए ऊपर की छवि पर क्लिक करें!\n+---\n+\n+## शिक्षा शास्त्र\n+\n+इस पाठ्यक्रम का निर्माण करते समय हमने दो शैक्षणिक सिद्धांतों को चुना है: यह सुनिश्चित करना कि यह व्यावहारिक रूप से **परियोजना-आधारित** है और इसमें **लगातार प्रश्नोत्तरी** शामिल हैं। इसके अलावा, इस पाठ्यक्रम में एक समान **थीम** है जो इसे एकता प्रदान करता है।\n+\n+यह सुनिश्चित करके कि सामग्री परियोजनाओं के साथ संरेखित होती है, छात्रों के लिए प्रक्रिया को और अधिक आकर्षक बनाया जाता है और अवधारणाओं के प्रतिधारण को बढ़ाया जाएगा। इसके अलावा, कक्षा से पहले एक कम-दांव प्रश्नोत्तरी छात्र के विषय को सीखने के इरादे को निर्धारित करती है, जबकि कक्षा के बाद दूसरी प्रश्नोत्तरी आगे प्रतिधारण सुनिश्चित करती है। इस पाठ्यक्रम को लचीला और मजेदार बनाने के लिए डिज़ाइन किया गया था और इसे पूर्ण या आंशिक रूप से लिया जा सकता है। परियोजनाएं छोटी शुरू होती हैं और 12-सप्ताह के चक्र के अंत तक तेजी से जटिल हो जाती हैं। इस पाठ्यक्रम में एमएल के वास्तविक-विश्व अनुप्रयोगों पर एक पोस्टस्क्रिप्ट भी शामिल है, जिसका उपयोग अतिरिक्त क्रेडिट के रूप में या चर्चा के आधार के रूप में किया जा सकता है।\n+\n+> हमारी [आचार संहिता](../CODE_OF_CONDUCT.md), [योगदान](../CONTRIBUTING.md), और [अनुवाद](../TRANSLATIONS.md) दिशानिर्देश खोजें। हम आपकी रचनात्मक प्रतिक्रिया का स्वागत करते हैं!\n+\n+## प्रत्येक पाठ में शामिल हैं:\n+\n+- वैकल्पिक स्केचनोट\n+- वैकल्पिक पूरक वीडियो\n+- पूर्व व्याख्यान वार्मअप प्रश्नोत्तरी\n+- लिखित पाठ\n+- प्रोजेक्ट-आधारित पाठों के लिए, प्रोजेक्ट बनाने के तरीके के बारे में चरण-दर-चरण मार्गदर्शिका\n+- ज्ञान जांच\n+- एक चुनौती\n+- पूरक पठन\n+- कार्यभार\n+- व्याख्यान के बाद प्रश्नोत्तरी\n+\n+> **भाषाओं के बारे में एक नोट**: ये पाठ मुख्य रूप से पायथन में लिखे गए हैं, लेकिन कई आर में भी उपलब्ध हैं। एक आर पाठ को पूरा करने के लिए, `/ समाधान` फ़ोल्डर में जाएं और आर पाठ देखें। उनमें एक .rmd एक्सटेंशन शामिल है जो एक **R मार्कडाउन** फ़ाइल का प्रतिनिधित्व करता है जिसे केवल `कोड चंक्स` (आर या अन्य भाषाओं के) के एम्बेडिंग के रूप में परिभाषित किया जा सकता है और एक `वाईएएमएल हेडर` (जो इस तरह के आउटपुट को प्रारूपित करने का मार्गदर्शन करता है) पीडीएफ के रूप में) एक `मार्कडाउन दस्तावेज़` में। जैसे, यह डेटा विज्ञान के लिए एक अनुकरणीय संलेखन ढांचे के रूप में कार्य करता है क्योंकि यह आपको अपने कोड, इसके आउटपुट और आपके विचारों को मार्कडाउन में लिखने की अनुमति देकर आपको संयोजित करने की अनुमति देता है। इसके अलावा, आर मार्कडाउन दस्तावेजों को पीडीएफ, एचटीएमएल या वर्ड जैसे आउटपुट स्वरूपों में प्रस्तुत किया जा सकता है।\n+\n+> **क्विज़ के बारे में एक नोट**: सभी क्विज़ शामिल हैं [इस ऐप में](https://white-water-09ec41f0f.azurestaticapps.net/), प्रत्येक तीन प्रश्नों के कुल 52 क्विज़ के लिए। वे पाठों के भीतर से जुड़े हुए हैं लेकिन प्रश्नोत्तरी ऐप को स्थानीय रूप से चलाया जा सकता है; `क्विज़-ऐप` फ़ोल्डर में दिए गए निर्देशों का पालन करें।\n+\n+| पाठ संख्या |                             विषय                             |                  पाठ समूहन                  | सीखने के मकसद                                                                                                            |                                                             जुड़ा हुआ पाठ                                                              |                        लेखक                        |\n+| :-----------: | :------------------------------------------------------------: | :-------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------: |\n+| 01 | मशीन लर्निंग का परिचय | [परिचय](../1-Introduction/translations/README.it.md)  | मशीन लर्निंग के पीछे की बुनियादी अवधारणाओं को जानें | [पाठ](../1-Introduction/1-intro-to-ML/README.md)   | मुहम्मद |\n+| 02 | मशीन लर्निंग का इतिहास | [परिचय](../1-Introduction/translations/README.it.md)  | इस क्षेत्र में अंतर्निहित इतिहास को जानें | [पाठ](../1-Introduction/2-history-of-ML/README.md)   | जेन और एमी |\n+| 03 | निष्पक्षता और मशीन लर्निंग | [परिचय](../1-Introduction/translations/README.it.md)  | निष्पक्षता के आसपास कौन से महत्वपूर्ण दार्शनिक मुद्दे हैं जिन पर छात्रों को एमएल मॉडल बनाते और लागू करते समय विचार करना चाहिए? | [पाठ](../1-Introduction/3-fairness/README.md) | तोमोमी |",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430544,microsoft/ML-For-Beginners,874488054,829093189,jlooper,1450004,2022-03-17T13:04:12+00:00,2022-03-17T13:05:11+00:00,This is linking to the Italian translation,true,translations/README.hi.md,NaN,"@@ -0,0 +1,127 @@\n+[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n+[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n+[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n+[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n+\n+[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n+[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n+[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n+\n+# शुरुआती के लिए मशीन लर्निंग - एक पाठ्यक्रम\n+\n+> 🌍दुनिया भर में यात्रा करें क्योंकि हम विश्व संस्कृतियों के माध्यम से मशीन लर्निंग का पता लगाते हैं 🌍\n+\n+माइक्रोसॉफ्ट के एज़्योर क्लाउड एडवोकेट्स को 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्नता हो रही है **मशीन लर्निंग** के बारे में 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्न हैं। इस पाठ्यक्रम में, आप   प्राथमिक रूप से स्किकिट-लर्न को एक पुस्तकालय के रूप में उपयोग करते हुए और हमारे आगामी 'एआई फॉर बिगिनर्स' पाठ्यक्रम में शामिल गहन शिक्षण से बचने के लिए, जिसे कभी-कभी **क्लासिक मशीन लर्निंग** कहा जाता है, के बारे में जानेंगे। इन पाठों को हमारे साथ जोड़ें ['शुरुआती के लिए डेटा विज्ञान' पाठ्यक्रम](https://aka.ms/datascience-beginners), भी!\n+\n+दुनिया भर में हमारे साथ यात्रा करें क्योंकि हम इन क्लासिक तकनीकों को दुनिया के कई क्षेत्रों के डेटा पर लागू करते हैं। प्रत्येक पाठ में पाठ से पहले और बाद में प्रश्नोत्तरी, पाठ को पूरा करने के लिए लिखित निर्देश, एक समाधान, एक असाइनमेंट, और बहुत कुछ शामिल हैं। हमारी परियोजना-आधारित शिक्षाशास्त्र आपको निर्माण करते समय सीखने की अनुमति देता है, जो नए कौशल को 'छड़ी' करने का एक सिद्ध तरीका है।\n+\n+**✍️ हमारे लेखकों** जेन लूपर, स्टीफन हॉवेल, फ्रांसेस्का लाज़ेरी, टोमोमी इमुरा, कैसी ब्रेवियू, दिमित्री सोशनिकोव, क्रिस नोरिंग, अनिर्बान मुखर्जी, ओरनेला अल्टुनियन और एमी बॉयड को हार्दिक धन्यवाद।\n+\n+**🎨 हमारे चित्रकारों** तोमोमी इमुरा, दासानी मदिपल्ली और जेन लूपर को भी धन्यवाद\n+\n+**🙏 हमारे Microsoft छात्र राजदूत लेखकों, समीक्षकों और सामग्री योगदानकर्ताओं के लिए विशेष धन्यवाद**, विशेष रूप से रिषित डागली, मुहम्मद साकिब खान इनान, रोहन राज, अलेक्जेंड्रू पेट्रेस्कु, अभिषेक जायसवाल, नवरीन तबस्सुम, इओन समुइला, और स्निग्धा अग्रवाल\n+\n+**🤩 हमारे R पाठों के लिए Microsoft छात्र राजदूत एरिक वंजाउ का अतिरिक्त आभार!**\n+\n+---\n+\n+# शुरू करना\n+\n+**विद्यार्थी**, इस पाठ्यक्रम का उपयोग करने के लिए, संपूर्ण रेपो को अपने स्वयं के गिटहब खाते में फोर्क करें और अभ्यास स्वयं या समूह के साथ पूरा करें:\n+\n+- प्री-लेक्चर क्विज से शुरुआत करें।\n+- व्याख्यान पढ़ें और गतिविधियों को पूरा करें, प्रत्येक ज्ञान जांच पर रुकें और प्रतिबिंबित करें।\n+- समाधान कोड चलाने के बजाय पाठों को समझकर प्रोजेक्ट बनाने का प्रयास करें; हालांकि वह कोड प्रत्येक परियोजना-उन्मुख पाठ में `/ समाधान` फ़ोल्डर में उपलब्ध है।\n+- व्याख्यान के बाद प्रश्नोत्तरी लें।\n+- चुनौती को पूरा करें।\n+- असाइनमेंट पूरा करें।\n+- एक पाठ समूह पूरा करने के बाद, [चर्चा बोर्ड](https://github.com/microsoft/ML-For-Beginners/discussions) पर जाएँ और उपयुक्त PAT रूब्रिक भरकर ""ज़ोर से सीखें""। एक 'पीएटी' एक प्रगति आकलन उपकरण है जो एक रूब्रिक है जिसे आप अपने सीखने को आगे बढ़ाने के लिए भरते हैं। आप अन्य पीएटी पर भी प्रतिक्रिया कर सकते हैं ताकि हम एक साथ सीख सकें।\n+\n+> आगे के अध्ययन के लिए, हम इन [माइक्रोसॉफ्ट लर्न](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) मॉड्यूल और सीखने के रास्तों का अनुसरण करने की सलाह देते हैं।.\n+\n+**शिक्षक**, हमारे पास [कुछ सुझाव शामिल हैं](../for-teachers.md) इस पाठ्यक्रम का उपयोग कैसे करें।\n+---\n+\n+## टीम से मिलो\n+\n+[![प्रोमो वीडियो](../ml-for-beginners.png)](https://youtu.be/Tj1XWrDSYJU ""Promo video"")\n+\n+**Gif by** [मोहित जैसल](https://linkedin.com/in/mohitjaisal)\n+\n+> 🎥 परियोजना और इसे बनाने वाले लोगों के बारे में वीडियो के लिए ऊपर की छवि पर क्लिक करें!\n+---\n+\n+## शिक्षा शास्त्र\n+\n+इस पाठ्यक्रम का निर्माण करते समय हमने दो शैक्षणिक सिद्धांतों को चुना है: यह सुनिश्चित करना कि यह व्यावहारिक रूप से **परियोजना-आधारित** है और इसमें **लगातार प्रश्नोत्तरी** शामिल हैं। इसके अलावा, इस पाठ्यक्रम में एक समान **थीम** है जो इसे एकता प्रदान करता है।\n+\n+यह सुनिश्चित करके कि सामग्री परियोजनाओं के साथ संरेखित होती है, छात्रों के लिए प्रक्रिया को और अधिक आकर्षक बनाया जाता है और अवधारणाओं के प्रतिधारण को बढ़ाया जाएगा। इसके अलावा, कक्षा से पहले एक कम-दांव प्रश्नोत्तरी छात्र के विषय को सीखने के इरादे को निर्धारित करती है, जबकि कक्षा के बाद दूसरी प्रश्नोत्तरी आगे प्रतिधारण सुनिश्चित करती है। इस पाठ्यक्रम को लचीला और मजेदार बनाने के लिए डिज़ाइन किया गया था और इसे पूर्ण या आंशिक रूप से लिया जा सकता है। परियोजनाएं छोटी शुरू होती हैं और 12-सप्ताह के चक्र के अंत तक तेजी से जटिल हो जाती हैं। इस पाठ्यक्रम में एमएल के वास्तविक-विश्व अनुप्रयोगों पर एक पोस्टस्क्रिप्ट भी शामिल है, जिसका उपयोग अतिरिक्त क्रेडिट के रूप में या चर्चा के आधार के रूप में किया जा सकता है।\n+\n+> हमारी [आचार संहिता](../CODE_OF_CONDUCT.md), [योगदान](../CONTRIBUTING.md), और [अनुवाद](../TRANSLATIONS.md) दिशानिर्देश खोजें। हम आपकी रचनात्मक प्रतिक्रिया का स्वागत करते हैं!\n+\n+## प्रत्येक पाठ में शामिल हैं:\n+\n+- वैकल्पिक स्केचनोट\n+- वैकल्पिक पूरक वीडियो\n+- पूर्व व्याख्यान वार्मअप प्रश्नोत्तरी\n+- लिखित पाठ\n+- प्रोजेक्ट-आधारित पाठों के लिए, प्रोजेक्ट बनाने के तरीके के बारे में चरण-दर-चरण मार्गदर्शिका\n+- ज्ञान जांच\n+- एक चुनौती\n+- पूरक पठन\n+- कार्यभार\n+- व्याख्यान के बाद प्रश्नोत्तरी\n+\n+> **भाषाओं के बारे में एक नोट**: ये पाठ मुख्य रूप से पायथन में लिखे गए हैं, लेकिन कई आर में भी उपलब्ध हैं। एक आर पाठ को पूरा करने के लिए, `/ समाधान` फ़ोल्डर में जाएं और आर पाठ देखें। उनमें एक .rmd एक्सटेंशन शामिल है जो एक **R मार्कडाउन** फ़ाइल का प्रतिनिधित्व करता है जिसे केवल `कोड चंक्स` (आर या अन्य भाषाओं के) के एम्बेडिंग के रूप में परिभाषित किया जा सकता है और एक `वाईएएमएल हेडर` (जो इस तरह के आउटपुट को प्रारूपित करने का मार्गदर्शन करता है) पीडीएफ के रूप में) एक `मार्कडाउन दस्तावेज़` में। जैसे, यह डेटा विज्ञान के लिए एक अनुकरणीय संलेखन ढांचे के रूप में कार्य करता है क्योंकि यह आपको अपने कोड, इसके आउटपुट और आपके विचारों को मार्कडाउन में लिखने की अनुमति देकर आपको संयोजित करने की अनुमति देता है। इसके अलावा, आर मार्कडाउन दस्तावेजों को पीडीएफ, एचटीएमएल या वर्ड जैसे आउटपुट स्वरूपों में प्रस्तुत किया जा सकता है।\n+\n+> **क्विज़ के बारे में एक नोट**: सभी क्विज़ शामिल हैं [इस ऐप में](https://white-water-09ec41f0f.azurestaticapps.net/), प्रत्येक तीन प्रश्नों के कुल 52 क्विज़ के लिए। वे पाठों के भीतर से जुड़े हुए हैं लेकिन प्रश्नोत्तरी ऐप को स्थानीय रूप से चलाया जा सकता है; `क्विज़-ऐप` फ़ोल्डर में दिए गए निर्देशों का पालन करें।\n+\n+| पाठ संख्या |                             विषय                             |                  पाठ समूहन                  | सीखने के मकसद                                                                                                            |                                                             जुड़ा हुआ पाठ                                                              |                        लेखक                        |\n+| :-----------: | :------------------------------------------------------------: | :-------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------: |\n+| 01 | मशीन लर्निंग का परिचय | [परिचय](../1-Introduction/translations/README.it.md)  | मशीन लर्निंग के पीछे की बुनियादी अवधारणाओं को जानें | [पाठ](../1-Introduction/1-intro-to-ML/README.md)   | मुहम्मद |",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430545,microsoft/ML-For-Beginners,874488054,829094136,jlooper,1450004,2022-03-17T13:05:02+00:00,2022-03-17T13:05:11+00:00,This should link to an animated gif,true,translations/README.hi.md,NaN,"@@ -0,0 +1,127 @@\n+[![GitHub license](https://img.shields.io/github/license/microsoft/ML-For-Beginners.svg)](https://github.com/microsoft/ML-For-Beginners/blob/master/LICENSE)\n+[![GitHub contributors](https://img.shields.io/github/contributors/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/graphs/contributors/)\n+[![GitHub issues](https://img.shields.io/github/issues/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/issues/)\n+[![GitHub pull-requests](https://img.shields.io/github/issues-pr/microsoft/ML-For-Beginners.svg)](https://GitHub.com/microsoft/ML-For-Beginners/pulls/)\n+[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)\n+\n+[![GitHub watchers](https://img.shields.io/github/watchers/microsoft/ML-For-Beginners.svg?style=social&label=Watch)](https://GitHub.com/microsoft/ML-For-Beginners/watchers/)\n+[![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n+[![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n+\n+# शुरुआती के लिए मशीन लर्निंग - एक पाठ्यक्रम\n+\n+> 🌍दुनिया भर में यात्रा करें क्योंकि हम विश्व संस्कृतियों के माध्यम से मशीन लर्निंग का पता लगाते हैं 🌍\n+\n+माइक्रोसॉफ्ट के एज़्योर क्लाउड एडवोकेट्स को 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्नता हो रही है **मशीन लर्निंग** के बारे में 12-सप्ताह, 26-पाठ पाठ्यक्रम की पेशकश करके प्रसन्न हैं। इस पाठ्यक्रम में, आप   प्राथमिक रूप से स्किकिट-लर्न को एक पुस्तकालय के रूप में उपयोग करते हुए और हमारे आगामी 'एआई फॉर बिगिनर्स' पाठ्यक्रम में शामिल गहन शिक्षण से बचने के लिए, जिसे कभी-कभी **क्लासिक मशीन लर्निंग** कहा जाता है, के बारे में जानेंगे। इन पाठों को हमारे साथ जोड़ें ['शुरुआती के लिए डेटा विज्ञान' पाठ्यक्रम](https://aka.ms/datascience-beginners), भी!\n+\n+दुनिया भर में हमारे साथ यात्रा करें क्योंकि हम इन क्लासिक तकनीकों को दुनिया के कई क्षेत्रों के डेटा पर लागू करते हैं। प्रत्येक पाठ में पाठ से पहले और बाद में प्रश्नोत्तरी, पाठ को पूरा करने के लिए लिखित निर्देश, एक समाधान, एक असाइनमेंट, और बहुत कुछ शामिल हैं। हमारी परियोजना-आधारित शिक्षाशास्त्र आपको निर्माण करते समय सीखने की अनुमति देता है, जो नए कौशल को 'छड़ी' करने का एक सिद्ध तरीका है।\n+\n+**✍️ हमारे लेखकों** जेन लूपर, स्टीफन हॉवेल, फ्रांसेस्का लाज़ेरी, टोमोमी इमुरा, कैसी ब्रेवियू, दिमित्री सोशनिकोव, क्रिस नोरिंग, अनिर्बान मुखर्जी, ओरनेला अल्टुनियन और एमी बॉयड को हार्दिक धन्यवाद।\n+\n+**🎨 हमारे चित्रकारों** तोमोमी इमुरा, दासानी मदिपल्ली और जेन लूपर को भी धन्यवाद\n+\n+**🙏 हमारे Microsoft छात्र राजदूत लेखकों, समीक्षकों और सामग्री योगदानकर्ताओं के लिए विशेष धन्यवाद**, विशेष रूप से रिषित डागली, मुहम्मद साकिब खान इनान, रोहन राज, अलेक्जेंड्रू पेट्रेस्कु, अभिषेक जायसवाल, नवरीन तबस्सुम, इओन समुइला, और स्निग्धा अग्रवाल\n+\n+**🤩 हमारे R पाठों के लिए Microsoft छात्र राजदूत एरिक वंजाउ का अतिरिक्त आभार!**\n+\n+---\n+\n+# शुरू करना\n+\n+**विद्यार्थी**, इस पाठ्यक्रम का उपयोग करने के लिए, संपूर्ण रेपो को अपने स्वयं के गिटहब खाते में फोर्क करें और अभ्यास स्वयं या समूह के साथ पूरा करें:\n+\n+- प्री-लेक्चर क्विज से शुरुआत करें।\n+- व्याख्यान पढ़ें और गतिविधियों को पूरा करें, प्रत्येक ज्ञान जांच पर रुकें और प्रतिबिंबित करें।\n+- समाधान कोड चलाने के बजाय पाठों को समझकर प्रोजेक्ट बनाने का प्रयास करें; हालांकि वह कोड प्रत्येक परियोजना-उन्मुख पाठ में `/ समाधान` फ़ोल्डर में उपलब्ध है।\n+- व्याख्यान के बाद प्रश्नोत्तरी लें।\n+- चुनौती को पूरा करें।\n+- असाइनमेंट पूरा करें।\n+- एक पाठ समूह पूरा करने के बाद, [चर्चा बोर्ड](https://github.com/microsoft/ML-For-Beginners/discussions) पर जाएँ और उपयुक्त PAT रूब्रिक भरकर ""ज़ोर से सीखें""। एक 'पीएटी' एक प्रगति आकलन उपकरण है जो एक रूब्रिक है जिसे आप अपने सीखने को आगे बढ़ाने के लिए भरते हैं। आप अन्य पीएटी पर भी प्रतिक्रिया कर सकते हैं ताकि हम एक साथ सीख सकें।\n+\n+> आगे के अध्ययन के लिए, हम इन [माइक्रोसॉफ्ट लर्न](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) मॉड्यूल और सीखने के रास्तों का अनुसरण करने की सलाह देते हैं।.\n+\n+**शिक्षक**, हमारे पास [कुछ सुझाव शामिल हैं](../for-teachers.md) इस पाठ्यक्रम का उपयोग कैसे करें।\n+---\n+\n+## टीम से मिलो\n+\n+[![प्रोमो वीडियो](../ml-for-beginners.png)](https://youtu.be/Tj1XWrDSYJU ""Promo video"")",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430546,microsoft/ML-For-Beginners,861730997,813327603,HugoCL,39707889,2022-02-23T21:20:21+00:00,2022-02-23T21:20:21+00:00,"Missing translation, it can be translated to ""Se verá así:""",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430547,microsoft/ML-For-Beginners,861730997,813329085,HugoCL,39707889,2022-02-23T21:22:28+00:00,2022-02-23T21:30:52+00:00,"Small typo, should be ""¡Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!""",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430548,microsoft/ML-For-Beginners,861730997,813331547,HugoCL,39707889,2022-02-23T21:25:46+00:00,2022-02-23T21:30:34+00:00,"Small typo, should be ""Un **árbol** de decisión podría funcionar, o la regresión logística para datos multiclase.""",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430549,microsoft/ML-For-Beginners,861730997,813333245,HugoCL,39707889,2022-02-23T21:28:03+00:00,2022-02-23T21:30:23+00:00,"To sound more natural, it can say ""¿Puedes explicar por qué el modelo está muy seguro **de que** esta es una cocina India?"" ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843054a,microsoft/ML-For-Beginners,861730997,813334275,HugoCL,39707889,2022-02-23T21:29:35+00:00,2022-02-23T21:29:36+00:00,"I think it can be ""Indaga un poco más en las **matemáticas** detrás de la regresión logística en...""",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843054b,microsoft/ML-For-Beginners,861730997,813342081,hereje,3022824,2022-02-23T21:39:41+00:00,2022-02-23T21:39:41+00:00,Thanks!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843054c,microsoft/ML-For-Beginners,861730997,813343806,HugoCL,39707889,2022-02-23T21:41:24+00:00,2022-02-23T21:41:25+00:00,Thanks to you Angel!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843054d,microsoft/ML-For-Beginners,861730997,813344451,hereje,3022824,2022-02-23T21:42:09+00:00,2022-02-23T21:42:09+00:00,Done!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843054e,microsoft/ML-For-Beginners,861730997,813344648,hereje,3022824,2022-02-23T21:42:22+00:00,2022-02-23T21:42:22+00:00,Done!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843054f,microsoft/ML-For-Beginners,861730997,813344932,hereje,3022824,2022-02-23T21:42:39+00:00,2022-02-23T21:42:39+00:00,Thanks for your help!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430550,microsoft/ML-For-Beginners,861730997,813327603,HugoCL,39707889,2022-02-23T21:20:21+00:00,2022-02-23T21:20:21+00:00,"Missing translation, it can be translated to ""Se verá así:""",true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430551,microsoft/ML-For-Beginners,861730997,813329085,HugoCL,39707889,2022-02-23T21:22:28+00:00,2022-02-23T21:30:52+00:00,"Small typo, should be ""¡Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!""",true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:\n+\n+    ```output\n+    0    indian\n+    1    indian\n+    2    indian\n+    3    indian\n+    4    indian\n+    Name: cuisine, dtype: object\n+    ```\n+\n+1. Elimina la columna `Unnamed: 0` y la columna `cuisine`, llamando a `drop()`. Guarda el resto de los datos como características entrenables:\n+\n+    ```python\n+    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n+    cuisines_feature_df.head()\n+    ```\n+\n+    Tus características lucen así:\n+\n+|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n+| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n+|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n+\n+¡Ahora ya estás listo para entrenar tu modelo!\n+\n+## Eligiendo tu clasificador\n+\n+Ahora que tus datos están limpios y listos para entrenamiento, tienes que decidir qué algoritmo usar para hacer el trabajo.\n+\n+Scikit-learn agrupa clasificaciones bajo aprendizaje supervisado, y en esa categoría encontrarás muchas formas de clasificar. [La variedad](https://scikit-learn.org/stable/supervised_learning.html) es bastante abrumadora a primera vista. Los siguientes métodos incluyen técnicas de clasificación:\n+\n+- Modelos lineales\n+- Máquinas de vectores de soporte\n+- Descenso de gradiente estocástico\n+- Vecinos más cercanos\n+- Procesos Gaussianos\n+- Árboles de decisión\n+- Métodos de conjunto (clasificador de votos)\n+- Algoritmos multiclase y multisalida (clasificación multiclase y multietiqueta, clasificación multiclase-multisalida)\n+\n+> También puedes usar [redes neuronales para clasificar los datos](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), pero eso está fuera del alcance de esta lección.\n+\n+### ¿Qué clasificador usar?\n+\n+Así que, ¿qué clasificador deberías elegir? A menudo, el ejecutar varios y buscar un buen resultado es una forma de probar. Scikit-lean ofrece una [comparación lado a lado](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) en un conjunto de datos creado, comparando KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB y QuadraticDiscrinationAnalysis, mostrando los resultados visualizados:\n+\n+![Comparación de clasificadores](../images/comparison.png)\n+> Gráficos generados en la documentación de Scikit-learn\n+\n+> AutoML resuelve este problema de forma pulcra al ejecutar estas comparaciones en la nube, permitiéndote elegir el mejor algoritmo para tus datos. Pruébalo [aquí](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-15963-cxa)\n+\n+### Un mejor enfoque\n+\n+Una mejor forma a estar adivinando, es seguir las ideas de esta [hoja de trucos de ML](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa). Aquí, descubrimos que, para nuestro problema multiclase, tenemos algunas opciones:\n+\n+![Hoja de trucos para problemas multiclase](../images/cheatsheet.png)\n+> Una sección de la hoja de trucos de algoritmos de Microsoft, detallando opciones de clasificación multiclase.\n+\n+✅ !Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430552,microsoft/ML-For-Beginners,861730997,813331547,HugoCL,39707889,2022-02-23T21:25:46+00:00,2022-02-23T21:30:34+00:00,"Small typo, should be ""Un **árbol** de decisión podría funcionar, o la regresión logística para datos multiclase.""",true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:\n+\n+    ```output\n+    0    indian\n+    1    indian\n+    2    indian\n+    3    indian\n+    4    indian\n+    Name: cuisine, dtype: object\n+    ```\n+\n+1. Elimina la columna `Unnamed: 0` y la columna `cuisine`, llamando a `drop()`. Guarda el resto de los datos como características entrenables:\n+\n+    ```python\n+    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n+    cuisines_feature_df.head()\n+    ```\n+\n+    Tus características lucen así:\n+\n+|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n+| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n+|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n+\n+¡Ahora ya estás listo para entrenar tu modelo!\n+\n+## Eligiendo tu clasificador\n+\n+Ahora que tus datos están limpios y listos para entrenamiento, tienes que decidir qué algoritmo usar para hacer el trabajo.\n+\n+Scikit-learn agrupa clasificaciones bajo aprendizaje supervisado, y en esa categoría encontrarás muchas formas de clasificar. [La variedad](https://scikit-learn.org/stable/supervised_learning.html) es bastante abrumadora a primera vista. Los siguientes métodos incluyen técnicas de clasificación:\n+\n+- Modelos lineales\n+- Máquinas de vectores de soporte\n+- Descenso de gradiente estocástico\n+- Vecinos más cercanos\n+- Procesos Gaussianos\n+- Árboles de decisión\n+- Métodos de conjunto (clasificador de votos)\n+- Algoritmos multiclase y multisalida (clasificación multiclase y multietiqueta, clasificación multiclase-multisalida)\n+\n+> También puedes usar [redes neuronales para clasificar los datos](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), pero eso está fuera del alcance de esta lección.\n+\n+### ¿Qué clasificador usar?\n+\n+Así que, ¿qué clasificador deberías elegir? A menudo, el ejecutar varios y buscar un buen resultado es una forma de probar. Scikit-lean ofrece una [comparación lado a lado](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) en un conjunto de datos creado, comparando KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB y QuadraticDiscrinationAnalysis, mostrando los resultados visualizados:\n+\n+![Comparación de clasificadores](../images/comparison.png)\n+> Gráficos generados en la documentación de Scikit-learn\n+\n+> AutoML resuelve este problema de forma pulcra al ejecutar estas comparaciones en la nube, permitiéndote elegir el mejor algoritmo para tus datos. Pruébalo [aquí](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-15963-cxa)\n+\n+### Un mejor enfoque\n+\n+Una mejor forma a estar adivinando, es seguir las ideas de esta [hoja de trucos de ML](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa). Aquí, descubrimos que, para nuestro problema multiclase, tenemos algunas opciones:\n+\n+![Hoja de trucos para problemas multiclase](../images/cheatsheet.png)\n+> Una sección de la hoja de trucos de algoritmos de Microsoft, detallando opciones de clasificación multiclase.\n+\n+✅ !Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!\n+\n+### Razonamiento\n+\n+Veamos si podemos razonar nuestro camino a través de diferentes enfoques dadas las restricciones que tenemos:\n+\n+- **Las redes neuronales son muy pesadas**. Dado nuestro conjunto de datos limpio aunque mínimo, y el hecho que estamos ejecutando el entrenamiento de forma local vía los notebooks, las redes neuronales son demasiado pesadas para esta tarea.\n+- **Sin clasificador de dos clases**. No usamos clasificador de dos clases, por lo que descarta un uno-contra-todos.\n+- **El árbol de decisión o la regresión logística podría funcionar**. Un árbolo de decisión podría funcionar, o la regresión logística para datos multiclase.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430553,microsoft/ML-For-Beginners,861730997,813333245,HugoCL,39707889,2022-02-23T21:28:03+00:00,2022-02-23T21:30:23+00:00,"To sound more natural, it can say ""¿Puedes explicar por qué el modelo está muy seguro **de que** esta es una cocina India?"" ",true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:\n+\n+    ```output\n+    0    indian\n+    1    indian\n+    2    indian\n+    3    indian\n+    4    indian\n+    Name: cuisine, dtype: object\n+    ```\n+\n+1. Elimina la columna `Unnamed: 0` y la columna `cuisine`, llamando a `drop()`. Guarda el resto de los datos como características entrenables:\n+\n+    ```python\n+    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n+    cuisines_feature_df.head()\n+    ```\n+\n+    Tus características lucen así:\n+\n+|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n+| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n+|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n+\n+¡Ahora ya estás listo para entrenar tu modelo!\n+\n+## Eligiendo tu clasificador\n+\n+Ahora que tus datos están limpios y listos para entrenamiento, tienes que decidir qué algoritmo usar para hacer el trabajo.\n+\n+Scikit-learn agrupa clasificaciones bajo aprendizaje supervisado, y en esa categoría encontrarás muchas formas de clasificar. [La variedad](https://scikit-learn.org/stable/supervised_learning.html) es bastante abrumadora a primera vista. Los siguientes métodos incluyen técnicas de clasificación:\n+\n+- Modelos lineales\n+- Máquinas de vectores de soporte\n+- Descenso de gradiente estocástico\n+- Vecinos más cercanos\n+- Procesos Gaussianos\n+- Árboles de decisión\n+- Métodos de conjunto (clasificador de votos)\n+- Algoritmos multiclase y multisalida (clasificación multiclase y multietiqueta, clasificación multiclase-multisalida)\n+\n+> También puedes usar [redes neuronales para clasificar los datos](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), pero eso está fuera del alcance de esta lección.\n+\n+### ¿Qué clasificador usar?\n+\n+Así que, ¿qué clasificador deberías elegir? A menudo, el ejecutar varios y buscar un buen resultado es una forma de probar. Scikit-lean ofrece una [comparación lado a lado](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) en un conjunto de datos creado, comparando KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB y QuadraticDiscrinationAnalysis, mostrando los resultados visualizados:\n+\n+![Comparación de clasificadores](../images/comparison.png)\n+> Gráficos generados en la documentación de Scikit-learn\n+\n+> AutoML resuelve este problema de forma pulcra al ejecutar estas comparaciones en la nube, permitiéndote elegir el mejor algoritmo para tus datos. Pruébalo [aquí](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-15963-cxa)\n+\n+### Un mejor enfoque\n+\n+Una mejor forma a estar adivinando, es seguir las ideas de esta [hoja de trucos de ML](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa). Aquí, descubrimos que, para nuestro problema multiclase, tenemos algunas opciones:\n+\n+![Hoja de trucos para problemas multiclase](../images/cheatsheet.png)\n+> Una sección de la hoja de trucos de algoritmos de Microsoft, detallando opciones de clasificación multiclase.\n+\n+✅ !Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!\n+\n+### Razonamiento\n+\n+Veamos si podemos razonar nuestro camino a través de diferentes enfoques dadas las restricciones que tenemos:\n+\n+- **Las redes neuronales son muy pesadas**. Dado nuestro conjunto de datos limpio aunque mínimo, y el hecho que estamos ejecutando el entrenamiento de forma local vía los notebooks, las redes neuronales son demasiado pesadas para esta tarea.\n+- **Sin clasificador de dos clases**. No usamos clasificador de dos clases, por lo que descarta un uno-contra-todos.\n+- **El árbol de decisión o la regresión logística podría funcionar**. Un árbolo de decisión podría funcionar, o la regresión logística para datos multiclase.\n+- **Los árboles de decisión potenciados multiclase resuelven un problema diferente**. El árbol de decisión potenciado multiclase es el más adecuado para tareas no paramétricas, por ejemplo, las tareas designadas para construir clasificaciones, por lo que no es útil para nosotros.\n+\n+### Usando Scikit-learn\n+\n+Usaremos Scikit-learn para analizar nuestros datos. Sin embargo, hay varias formas de usar la regresión logística en Scikit-learn. Da un vistazo a los [parámetros a pasar](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  \n+\n+En esencia, hay dos parámetros importantes - `multi_class` y `solver` - que necesitamos especificar, cuando le pedimos a Scikit-learn realice una regresión logística. El valor `multi_class` aplica cierto comportamiento. El valor del solucionador (`solver`) es el algoritmo a usar. No todos los solucionadores pueden ser emparejados con todos los valores `multi_class`.\n+\n+De acuerdo a la documentación, en el caso multiclase, el algoritmo de entrenamiento:\n+\n+- **Usa el esquema uno contra el resto (OvsR)**, si la opción `multi_class` se configura a `ovr`\n+- **Usa la pérdida de entropía cruzada**, si la opción `multi_class` se configura a `multinomial` (Actualmente la opción `multinomial` es soportada sólo por los solucionadores ‘lbfgs’, ‘sag’, ‘saga’ y ‘newton-cg’.).\n+\n+> 🎓 Aquí, el 'esquema' puede ser 'ovr' (one-vs-rest) o 'multinomial'. Ya que la regresión logística está diseñada realmente para soportar la clasificación binaria, estos esquemas te permiten manejar mejor las tareas de clasificación multiclase [fuente](https://machinelea\n+rningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/).\n+\n+> 🎓 El 'solucionador' es definido como ""el algoritmo a usar en el problema de optimización"" [fuente](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).\n+\n+\n+Scikit-learn ofrece esta tabla para explicar como los solucionadores manejan distintos desafíos presentados por distintas clases de datos estructurados:\n+\n+![solucionadores](../images/solvers.png)\n+\n+## Ejercicio - divide los datos\n+\n+Nos podemos enfocar en la regresión logística para nuestra primer prueba de entrenamiento ya que recién aprendiste sobre esto último en la lección anterior.\n+Divide tus datos en los grupos 'training' y 'testing' al llamar a `train_test_split()`:\n+\n+```python\n+X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n+```\n+\n+## Ejercicio - aplica la regresión logística\n+\n+Ya que estás usando un caso multiclase, necesitas elegir qué _esquema_ usar y qué _solucionador_ configurar. Usa LogisticRegression con un ajuste multiclase y el solucionador **liblinear** para entrenar.\n+\n+1. Crea un regresión logística con un multi_class configurado a `ovr` y el solucionador ajustado a `liblinear`:\n+\n+    ```python\n+    lr = LogisticRegression(multi_class='ovr',solver='liblinear')\n+    model = lr.fit(X_train, np.ravel(y_train))\n+    \n+    accuracy = model.score(X_test, y_test)\n+    print (""Accuracy is {}"".format(accuracy))\n+    ```\n+\n+    ✅ Prueba un solucionador diferente como `lbfgs`, el cual suele ser configurado por defecto\n+\n+    > Nota, usa la función de Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) para aplanar tus datos cuando sea necesario.\n+\n+    ¡La precisión es buena por enciam del **80%*!\n+\n+1. Puedes ver este modelo en acción al probar una fila de datos (#50):\n+\n+    ```python\n+    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')\n+    print(f'cuisine: {y_test.iloc[50]}')\n+    ```\n+\n+    El resultado es impreso:\n+\n+   ```output\n+   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')\n+   cuisine: indian\n+   ```\n+\n+   ✅ Prueba un número de fila distinto y revisa los resultados\n+\n+1. Indagando más, puedes revisar la precisión de esta predicción:\n+\n+    ```python\n+    test= X_test.iloc[50].values.reshape(-1, 1).T\n+    proba = model.predict_proba(test)\n+    classes = model.classes_\n+    resultdf = pd.DataFrame(data=proba, columns=classes)\n+    \n+    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])\n+    topPrediction.head()\n+    ```\n+\n+    El resultado es impreso - La cocina India es su mejor conjetura, con buena probabilidad:\n+\n+    |          |        0 |\n+    | -------: | -------: |\n+    |   indian | 0.715851 |\n+    |  chinese | 0.229475 |\n+    | japanese | 0.029763 |\n+    |   korean | 0.017277 |\n+    |     thai | 0.007634 |\n+\n+    ✅ ¿Puedes explicar por qué el modelo está muy seguro esta es una cocina India?",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430554,microsoft/ML-For-Beginners,861730997,813334275,HugoCL,39707889,2022-02-23T21:29:35+00:00,2022-02-23T21:29:36+00:00,"I think it can be ""Indaga un poco más en las **matemáticas** detrás de la regresión logística en...""",true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:\n+\n+    ```output\n+    0    indian\n+    1    indian\n+    2    indian\n+    3    indian\n+    4    indian\n+    Name: cuisine, dtype: object\n+    ```\n+\n+1. Elimina la columna `Unnamed: 0` y la columna `cuisine`, llamando a `drop()`. Guarda el resto de los datos como características entrenables:\n+\n+    ```python\n+    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n+    cuisines_feature_df.head()\n+    ```\n+\n+    Tus características lucen así:\n+\n+|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n+| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n+|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n+\n+¡Ahora ya estás listo para entrenar tu modelo!\n+\n+## Eligiendo tu clasificador\n+\n+Ahora que tus datos están limpios y listos para entrenamiento, tienes que decidir qué algoritmo usar para hacer el trabajo.\n+\n+Scikit-learn agrupa clasificaciones bajo aprendizaje supervisado, y en esa categoría encontrarás muchas formas de clasificar. [La variedad](https://scikit-learn.org/stable/supervised_learning.html) es bastante abrumadora a primera vista. Los siguientes métodos incluyen técnicas de clasificación:\n+\n+- Modelos lineales\n+- Máquinas de vectores de soporte\n+- Descenso de gradiente estocástico\n+- Vecinos más cercanos\n+- Procesos Gaussianos\n+- Árboles de decisión\n+- Métodos de conjunto (clasificador de votos)\n+- Algoritmos multiclase y multisalida (clasificación multiclase y multietiqueta, clasificación multiclase-multisalida)\n+\n+> También puedes usar [redes neuronales para clasificar los datos](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), pero eso está fuera del alcance de esta lección.\n+\n+### ¿Qué clasificador usar?\n+\n+Así que, ¿qué clasificador deberías elegir? A menudo, el ejecutar varios y buscar un buen resultado es una forma de probar. Scikit-lean ofrece una [comparación lado a lado](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) en un conjunto de datos creado, comparando KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB y QuadraticDiscrinationAnalysis, mostrando los resultados visualizados:\n+\n+![Comparación de clasificadores](../images/comparison.png)\n+> Gráficos generados en la documentación de Scikit-learn\n+\n+> AutoML resuelve este problema de forma pulcra al ejecutar estas comparaciones en la nube, permitiéndote elegir el mejor algoritmo para tus datos. Pruébalo [aquí](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-15963-cxa)\n+\n+### Un mejor enfoque\n+\n+Una mejor forma a estar adivinando, es seguir las ideas de esta [hoja de trucos de ML](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa). Aquí, descubrimos que, para nuestro problema multiclase, tenemos algunas opciones:\n+\n+![Hoja de trucos para problemas multiclase](../images/cheatsheet.png)\n+> Una sección de la hoja de trucos de algoritmos de Microsoft, detallando opciones de clasificación multiclase.\n+\n+✅ !Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!\n+\n+### Razonamiento\n+\n+Veamos si podemos razonar nuestro camino a través de diferentes enfoques dadas las restricciones que tenemos:\n+\n+- **Las redes neuronales son muy pesadas**. Dado nuestro conjunto de datos limpio aunque mínimo, y el hecho que estamos ejecutando el entrenamiento de forma local vía los notebooks, las redes neuronales son demasiado pesadas para esta tarea.\n+- **Sin clasificador de dos clases**. No usamos clasificador de dos clases, por lo que descarta un uno-contra-todos.\n+- **El árbol de decisión o la regresión logística podría funcionar**. Un árbolo de decisión podría funcionar, o la regresión logística para datos multiclase.\n+- **Los árboles de decisión potenciados multiclase resuelven un problema diferente**. El árbol de decisión potenciado multiclase es el más adecuado para tareas no paramétricas, por ejemplo, las tareas designadas para construir clasificaciones, por lo que no es útil para nosotros.\n+\n+### Usando Scikit-learn\n+\n+Usaremos Scikit-learn para analizar nuestros datos. Sin embargo, hay varias formas de usar la regresión logística en Scikit-learn. Da un vistazo a los [parámetros a pasar](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  \n+\n+En esencia, hay dos parámetros importantes - `multi_class` y `solver` - que necesitamos especificar, cuando le pedimos a Scikit-learn realice una regresión logística. El valor `multi_class` aplica cierto comportamiento. El valor del solucionador (`solver`) es el algoritmo a usar. No todos los solucionadores pueden ser emparejados con todos los valores `multi_class`.\n+\n+De acuerdo a la documentación, en el caso multiclase, el algoritmo de entrenamiento:\n+\n+- **Usa el esquema uno contra el resto (OvsR)**, si la opción `multi_class` se configura a `ovr`\n+- **Usa la pérdida de entropía cruzada**, si la opción `multi_class` se configura a `multinomial` (Actualmente la opción `multinomial` es soportada sólo por los solucionadores ‘lbfgs’, ‘sag’, ‘saga’ y ‘newton-cg’.).\n+\n+> 🎓 Aquí, el 'esquema' puede ser 'ovr' (one-vs-rest) o 'multinomial'. Ya que la regresión logística está diseñada realmente para soportar la clasificación binaria, estos esquemas te permiten manejar mejor las tareas de clasificación multiclase [fuente](https://machinelea\n+rningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/).\n+\n+> 🎓 El 'solucionador' es definido como ""el algoritmo a usar en el problema de optimización"" [fuente](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).\n+\n+\n+Scikit-learn ofrece esta tabla para explicar como los solucionadores manejan distintos desafíos presentados por distintas clases de datos estructurados:\n+\n+![solucionadores](../images/solvers.png)\n+\n+## Ejercicio - divide los datos\n+\n+Nos podemos enfocar en la regresión logística para nuestra primer prueba de entrenamiento ya que recién aprendiste sobre esto último en la lección anterior.\n+Divide tus datos en los grupos 'training' y 'testing' al llamar a `train_test_split()`:\n+\n+```python\n+X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n+```\n+\n+## Ejercicio - aplica la regresión logística\n+\n+Ya que estás usando un caso multiclase, necesitas elegir qué _esquema_ usar y qué _solucionador_ configurar. Usa LogisticRegression con un ajuste multiclase y el solucionador **liblinear** para entrenar.\n+\n+1. Crea un regresión logística con un multi_class configurado a `ovr` y el solucionador ajustado a `liblinear`:\n+\n+    ```python\n+    lr = LogisticRegression(multi_class='ovr',solver='liblinear')\n+    model = lr.fit(X_train, np.ravel(y_train))\n+    \n+    accuracy = model.score(X_test, y_test)\n+    print (""Accuracy is {}"".format(accuracy))\n+    ```\n+\n+    ✅ Prueba un solucionador diferente como `lbfgs`, el cual suele ser configurado por defecto\n+\n+    > Nota, usa la función de Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) para aplanar tus datos cuando sea necesario.\n+\n+    ¡La precisión es buena por enciam del **80%*!\n+\n+1. Puedes ver este modelo en acción al probar una fila de datos (#50):\n+\n+    ```python\n+    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')\n+    print(f'cuisine: {y_test.iloc[50]}')\n+    ```\n+\n+    El resultado es impreso:\n+\n+   ```output\n+   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')\n+   cuisine: indian\n+   ```\n+\n+   ✅ Prueba un número de fila distinto y revisa los resultados\n+\n+1. Indagando más, puedes revisar la precisión de esta predicción:\n+\n+    ```python\n+    test= X_test.iloc[50].values.reshape(-1, 1).T\n+    proba = model.predict_proba(test)\n+    classes = model.classes_\n+    resultdf = pd.DataFrame(data=proba, columns=classes)\n+    \n+    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])\n+    topPrediction.head()\n+    ```\n+\n+    El resultado es impreso - La cocina India es su mejor conjetura, con buena probabilidad:\n+\n+    |          |        0 |\n+    | -------: | -------: |\n+    |   indian | 0.715851 |\n+    |  chinese | 0.229475 |\n+    | japanese | 0.029763 |\n+    |   korean | 0.017277 |\n+    |     thai | 0.007634 |\n+\n+    ✅ ¿Puedes explicar por qué el modelo está muy seguro esta es una cocina India?\n+\n+1. Obtén mayor detalle al imprimir un reporte de clasificación, como lo hiciste en las lecciones de regresión:\n+\n+    ```python\n+    y_pred = model.predict(X_test)\n+    print(classification_report(y_test,y_pred))\n+    ```\n+\n+    |              | precision | recall | f1-score | support |\n+    | ------------ | --------- | ------ | -------- | ------- |\n+    | chinese      | 0.73      | 0.71   | 0.72     | 229     |\n+    | indian       | 0.91      | 0.93   | 0.92     | 254     |\n+    | japanese     | 0.70      | 0.75   | 0.72     | 220     |\n+    | korean       | 0.86      | 0.76   | 0.81     | 242     |\n+    | thai         | 0.79      | 0.85   | 0.82     | 254     |\n+    | accuracy     | 0.80      | 1199   |          |         |\n+    | macro avg    | 0.80      | 0.80   | 0.80     | 1199    |\n+    | weighted avg | 0.80      | 0.80   | 0.80     | 1199    |\n+\n+## 🚀Desafío\n+\n+En esta lección, usaste tus datos limpios para construir un modelo de aprendizaje automático que puede predecir una cocina nacional basado en una serie de ingredientes. Toma un tiempo para leer las diversas opciones que provee Scikit-learn para clasificar los datos. Profundiza en el concepto de 'solucionador' para comprender que sucede detrás de escena.\n+\n+## [Examen posterior a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/22/)\n+\n+## Revisión y autoestudio\n+\n+Indaga un poco más en las matemática detrás de la regresión logística en [esta lección](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430555,microsoft/ML-For-Beginners,861730997,813342081,hereje,3022824,2022-02-23T21:39:41+00:00,2022-02-23T21:39:41+00:00,Thanks!,true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:\n+\n+    ```output\n+    0    indian\n+    1    indian\n+    2    indian\n+    3    indian\n+    4    indian\n+    Name: cuisine, dtype: object\n+    ```\n+\n+1. Elimina la columna `Unnamed: 0` y la columna `cuisine`, llamando a `drop()`. Guarda el resto de los datos como características entrenables:\n+\n+    ```python\n+    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n+    cuisines_feature_df.head()\n+    ```\n+\n+    Tus características lucen así:\n+\n+|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n+| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n+|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n+\n+¡Ahora ya estás listo para entrenar tu modelo!\n+\n+## Eligiendo tu clasificador\n+\n+Ahora que tus datos están limpios y listos para entrenamiento, tienes que decidir qué algoritmo usar para hacer el trabajo.\n+\n+Scikit-learn agrupa clasificaciones bajo aprendizaje supervisado, y en esa categoría encontrarás muchas formas de clasificar. [La variedad](https://scikit-learn.org/stable/supervised_learning.html) es bastante abrumadora a primera vista. Los siguientes métodos incluyen técnicas de clasificación:\n+\n+- Modelos lineales\n+- Máquinas de vectores de soporte\n+- Descenso de gradiente estocástico\n+- Vecinos más cercanos\n+- Procesos Gaussianos\n+- Árboles de decisión\n+- Métodos de conjunto (clasificador de votos)\n+- Algoritmos multiclase y multisalida (clasificación multiclase y multietiqueta, clasificación multiclase-multisalida)\n+\n+> También puedes usar [redes neuronales para clasificar los datos](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), pero eso está fuera del alcance de esta lección.\n+\n+### ¿Qué clasificador usar?\n+\n+Así que, ¿qué clasificador deberías elegir? A menudo, el ejecutar varios y buscar un buen resultado es una forma de probar. Scikit-lean ofrece una [comparación lado a lado](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) en un conjunto de datos creado, comparando KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB y QuadraticDiscrinationAnalysis, mostrando los resultados visualizados:\n+\n+![Comparación de clasificadores](../images/comparison.png)\n+> Gráficos generados en la documentación de Scikit-learn\n+\n+> AutoML resuelve este problema de forma pulcra al ejecutar estas comparaciones en la nube, permitiéndote elegir el mejor algoritmo para tus datos. Pruébalo [aquí](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-15963-cxa)\n+\n+### Un mejor enfoque\n+\n+Una mejor forma a estar adivinando, es seguir las ideas de esta [hoja de trucos de ML](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa). Aquí, descubrimos que, para nuestro problema multiclase, tenemos algunas opciones:\n+\n+![Hoja de trucos para problemas multiclase](../images/cheatsheet.png)\n+> Una sección de la hoja de trucos de algoritmos de Microsoft, detallando opciones de clasificación multiclase.\n+\n+✅ !Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!\n+\n+### Razonamiento\n+\n+Veamos si podemos razonar nuestro camino a través de diferentes enfoques dadas las restricciones que tenemos:\n+\n+- **Las redes neuronales son muy pesadas**. Dado nuestro conjunto de datos limpio aunque mínimo, y el hecho que estamos ejecutando el entrenamiento de forma local vía los notebooks, las redes neuronales son demasiado pesadas para esta tarea.\n+- **Sin clasificador de dos clases**. No usamos clasificador de dos clases, por lo que descarta un uno-contra-todos.\n+- **El árbol de decisión o la regresión logística podría funcionar**. Un árbolo de decisión podría funcionar, o la regresión logística para datos multiclase.\n+- **Los árboles de decisión potenciados multiclase resuelven un problema diferente**. El árbol de decisión potenciado multiclase es el más adecuado para tareas no paramétricas, por ejemplo, las tareas designadas para construir clasificaciones, por lo que no es útil para nosotros.\n+\n+### Usando Scikit-learn\n+\n+Usaremos Scikit-learn para analizar nuestros datos. Sin embargo, hay varias formas de usar la regresión logística en Scikit-learn. Da un vistazo a los [parámetros a pasar](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  \n+\n+En esencia, hay dos parámetros importantes - `multi_class` y `solver` - que necesitamos especificar, cuando le pedimos a Scikit-learn realice una regresión logística. El valor `multi_class` aplica cierto comportamiento. El valor del solucionador (`solver`) es el algoritmo a usar. No todos los solucionadores pueden ser emparejados con todos los valores `multi_class`.\n+\n+De acuerdo a la documentación, en el caso multiclase, el algoritmo de entrenamiento:\n+\n+- **Usa el esquema uno contra el resto (OvsR)**, si la opción `multi_class` se configura a `ovr`\n+- **Usa la pérdida de entropía cruzada**, si la opción `multi_class` se configura a `multinomial` (Actualmente la opción `multinomial` es soportada sólo por los solucionadores ‘lbfgs’, ‘sag’, ‘saga’ y ‘newton-cg’.).\n+\n+> 🎓 Aquí, el 'esquema' puede ser 'ovr' (one-vs-rest) o 'multinomial'. Ya que la regresión logística está diseñada realmente para soportar la clasificación binaria, estos esquemas te permiten manejar mejor las tareas de clasificación multiclase [fuente](https://machinelea\n+rningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/).\n+\n+> 🎓 El 'solucionador' es definido como ""el algoritmo a usar en el problema de optimización"" [fuente](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).\n+\n+\n+Scikit-learn ofrece esta tabla para explicar como los solucionadores manejan distintos desafíos presentados por distintas clases de datos estructurados:\n+\n+![solucionadores](../images/solvers.png)\n+\n+## Ejercicio - divide los datos\n+\n+Nos podemos enfocar en la regresión logística para nuestra primer prueba de entrenamiento ya que recién aprendiste sobre esto último en la lección anterior.\n+Divide tus datos en los grupos 'training' y 'testing' al llamar a `train_test_split()`:\n+\n+```python\n+X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n+```\n+\n+## Ejercicio - aplica la regresión logística\n+\n+Ya que estás usando un caso multiclase, necesitas elegir qué _esquema_ usar y qué _solucionador_ configurar. Usa LogisticRegression con un ajuste multiclase y el solucionador **liblinear** para entrenar.\n+\n+1. Crea un regresión logística con un multi_class configurado a `ovr` y el solucionador ajustado a `liblinear`:\n+\n+    ```python\n+    lr = LogisticRegression(multi_class='ovr',solver='liblinear')\n+    model = lr.fit(X_train, np.ravel(y_train))\n+    \n+    accuracy = model.score(X_test, y_test)\n+    print (""Accuracy is {}"".format(accuracy))\n+    ```\n+\n+    ✅ Prueba un solucionador diferente como `lbfgs`, el cual suele ser configurado por defecto\n+\n+    > Nota, usa la función de Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) para aplanar tus datos cuando sea necesario.\n+\n+    ¡La precisión es buena por enciam del **80%*!\n+\n+1. Puedes ver este modelo en acción al probar una fila de datos (#50):\n+\n+    ```python\n+    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')\n+    print(f'cuisine: {y_test.iloc[50]}')\n+    ```\n+\n+    El resultado es impreso:\n+\n+   ```output\n+   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')\n+   cuisine: indian\n+   ```\n+\n+   ✅ Prueba un número de fila distinto y revisa los resultados\n+\n+1. Indagando más, puedes revisar la precisión de esta predicción:\n+\n+    ```python\n+    test= X_test.iloc[50].values.reshape(-1, 1).T\n+    proba = model.predict_proba(test)\n+    classes = model.classes_\n+    resultdf = pd.DataFrame(data=proba, columns=classes)\n+    \n+    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])\n+    topPrediction.head()\n+    ```\n+\n+    El resultado es impreso - La cocina India es su mejor conjetura, con buena probabilidad:\n+\n+    |          |        0 |\n+    | -------: | -------: |\n+    |   indian | 0.715851 |\n+    |  chinese | 0.229475 |\n+    | japanese | 0.029763 |\n+    |   korean | 0.017277 |\n+    |     thai | 0.007634 |\n+\n+    ✅ ¿Puedes explicar por qué el modelo está muy seguro esta es una cocina India?\n+\n+1. Obtén mayor detalle al imprimir un reporte de clasificación, como lo hiciste en las lecciones de regresión:\n+\n+    ```python\n+    y_pred = model.predict(X_test)\n+    print(classification_report(y_test,y_pred))\n+    ```\n+\n+    |              | precision | recall | f1-score | support |\n+    | ------------ | --------- | ------ | -------- | ------- |\n+    | chinese      | 0.73      | 0.71   | 0.72     | 229     |\n+    | indian       | 0.91      | 0.93   | 0.92     | 254     |\n+    | japanese     | 0.70      | 0.75   | 0.72     | 220     |\n+    | korean       | 0.86      | 0.76   | 0.81     | 242     |\n+    | thai         | 0.79      | 0.85   | 0.82     | 254     |\n+    | accuracy     | 0.80      | 1199   |          |         |\n+    | macro avg    | 0.80      | 0.80   | 0.80     | 1199    |\n+    | weighted avg | 0.80      | 0.80   | 0.80     | 1199    |\n+\n+## 🚀Desafío\n+\n+En esta lección, usaste tus datos limpios para construir un modelo de aprendizaje automático que puede predecir una cocina nacional basado en una serie de ingredientes. Toma un tiempo para leer las diversas opciones que provee Scikit-learn para clasificar los datos. Profundiza en el concepto de 'solucionador' para comprender que sucede detrás de escena.\n+\n+## [Examen posterior a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/22/)\n+\n+## Revisión y autoestudio\n+\n+Indaga un poco más en las matemática detrás de la regresión logística en [esta lección](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430556,microsoft/ML-For-Beginners,861730997,813343806,HugoCL,39707889,2022-02-23T21:41:24+00:00,2022-02-23T21:41:25+00:00,Thanks to you Angel!,true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:\n+\n+    ```output\n+    0    indian\n+    1    indian\n+    2    indian\n+    3    indian\n+    4    indian\n+    Name: cuisine, dtype: object\n+    ```\n+\n+1. Elimina la columna `Unnamed: 0` y la columna `cuisine`, llamando a `drop()`. Guarda el resto de los datos como características entrenables:\n+\n+    ```python\n+    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n+    cuisines_feature_df.head()\n+    ```\n+\n+    Tus características lucen así:\n+\n+|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n+| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n+|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n+\n+¡Ahora ya estás listo para entrenar tu modelo!\n+\n+## Eligiendo tu clasificador\n+\n+Ahora que tus datos están limpios y listos para entrenamiento, tienes que decidir qué algoritmo usar para hacer el trabajo.\n+\n+Scikit-learn agrupa clasificaciones bajo aprendizaje supervisado, y en esa categoría encontrarás muchas formas de clasificar. [La variedad](https://scikit-learn.org/stable/supervised_learning.html) es bastante abrumadora a primera vista. Los siguientes métodos incluyen técnicas de clasificación:\n+\n+- Modelos lineales\n+- Máquinas de vectores de soporte\n+- Descenso de gradiente estocástico\n+- Vecinos más cercanos\n+- Procesos Gaussianos\n+- Árboles de decisión\n+- Métodos de conjunto (clasificador de votos)\n+- Algoritmos multiclase y multisalida (clasificación multiclase y multietiqueta, clasificación multiclase-multisalida)\n+\n+> También puedes usar [redes neuronales para clasificar los datos](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), pero eso está fuera del alcance de esta lección.\n+\n+### ¿Qué clasificador usar?\n+\n+Así que, ¿qué clasificador deberías elegir? A menudo, el ejecutar varios y buscar un buen resultado es una forma de probar. Scikit-lean ofrece una [comparación lado a lado](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) en un conjunto de datos creado, comparando KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB y QuadraticDiscrinationAnalysis, mostrando los resultados visualizados:\n+\n+![Comparación de clasificadores](../images/comparison.png)\n+> Gráficos generados en la documentación de Scikit-learn\n+\n+> AutoML resuelve este problema de forma pulcra al ejecutar estas comparaciones en la nube, permitiéndote elegir el mejor algoritmo para tus datos. Pruébalo [aquí](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-15963-cxa)\n+\n+### Un mejor enfoque\n+\n+Una mejor forma a estar adivinando, es seguir las ideas de esta [hoja de trucos de ML](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa). Aquí, descubrimos que, para nuestro problema multiclase, tenemos algunas opciones:\n+\n+![Hoja de trucos para problemas multiclase](../images/cheatsheet.png)\n+> Una sección de la hoja de trucos de algoritmos de Microsoft, detallando opciones de clasificación multiclase.\n+\n+✅ !Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!\n+\n+### Razonamiento\n+\n+Veamos si podemos razonar nuestro camino a través de diferentes enfoques dadas las restricciones que tenemos:\n+\n+- **Las redes neuronales son muy pesadas**. Dado nuestro conjunto de datos limpio aunque mínimo, y el hecho que estamos ejecutando el entrenamiento de forma local vía los notebooks, las redes neuronales son demasiado pesadas para esta tarea.\n+- **Sin clasificador de dos clases**. No usamos clasificador de dos clases, por lo que descarta un uno-contra-todos.\n+- **El árbol de decisión o la regresión logística podría funcionar**. Un árbolo de decisión podría funcionar, o la regresión logística para datos multiclase.\n+- **Los árboles de decisión potenciados multiclase resuelven un problema diferente**. El árbol de decisión potenciado multiclase es el más adecuado para tareas no paramétricas, por ejemplo, las tareas designadas para construir clasificaciones, por lo que no es útil para nosotros.\n+\n+### Usando Scikit-learn\n+\n+Usaremos Scikit-learn para analizar nuestros datos. Sin embargo, hay varias formas de usar la regresión logística en Scikit-learn. Da un vistazo a los [parámetros a pasar](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).  \n+\n+En esencia, hay dos parámetros importantes - `multi_class` y `solver` - que necesitamos especificar, cuando le pedimos a Scikit-learn realice una regresión logística. El valor `multi_class` aplica cierto comportamiento. El valor del solucionador (`solver`) es el algoritmo a usar. No todos los solucionadores pueden ser emparejados con todos los valores `multi_class`.\n+\n+De acuerdo a la documentación, en el caso multiclase, el algoritmo de entrenamiento:\n+\n+- **Usa el esquema uno contra el resto (OvsR)**, si la opción `multi_class` se configura a `ovr`\n+- **Usa la pérdida de entropía cruzada**, si la opción `multi_class` se configura a `multinomial` (Actualmente la opción `multinomial` es soportada sólo por los solucionadores ‘lbfgs’, ‘sag’, ‘saga’ y ‘newton-cg’.).\n+\n+> 🎓 Aquí, el 'esquema' puede ser 'ovr' (one-vs-rest) o 'multinomial'. Ya que la regresión logística está diseñada realmente para soportar la clasificación binaria, estos esquemas te permiten manejar mejor las tareas de clasificación multiclase [fuente](https://machinelea\n+rningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/).\n+\n+> 🎓 El 'solucionador' es definido como ""el algoritmo a usar en el problema de optimización"" [fuente](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regressio#sklearn.linear_model.LogisticRegression).\n+\n+\n+Scikit-learn ofrece esta tabla para explicar como los solucionadores manejan distintos desafíos presentados por distintas clases de datos estructurados:\n+\n+![solucionadores](../images/solvers.png)\n+\n+## Ejercicio - divide los datos\n+\n+Nos podemos enfocar en la regresión logística para nuestra primer prueba de entrenamiento ya que recién aprendiste sobre esto último en la lección anterior.\n+Divide tus datos en los grupos 'training' y 'testing' al llamar a `train_test_split()`:\n+\n+```python\n+X_train, X_test, y_train, y_test = train_test_split(cuisines_feature_df, cuisines_label_df, test_size=0.3)\n+```\n+\n+## Ejercicio - aplica la regresión logística\n+\n+Ya que estás usando un caso multiclase, necesitas elegir qué _esquema_ usar y qué _solucionador_ configurar. Usa LogisticRegression con un ajuste multiclase y el solucionador **liblinear** para entrenar.\n+\n+1. Crea un regresión logística con un multi_class configurado a `ovr` y el solucionador ajustado a `liblinear`:\n+\n+    ```python\n+    lr = LogisticRegression(multi_class='ovr',solver='liblinear')\n+    model = lr.fit(X_train, np.ravel(y_train))\n+    \n+    accuracy = model.score(X_test, y_test)\n+    print (""Accuracy is {}"".format(accuracy))\n+    ```\n+\n+    ✅ Prueba un solucionador diferente como `lbfgs`, el cual suele ser configurado por defecto\n+\n+    > Nota, usa la función de Pandas [`ravel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.ravel.html) para aplanar tus datos cuando sea necesario.\n+\n+    ¡La precisión es buena por enciam del **80%*!\n+\n+1. Puedes ver este modelo en acción al probar una fila de datos (#50):\n+\n+    ```python\n+    print(f'ingredients: {X_test.iloc[50][X_test.iloc[50]!=0].keys()}')\n+    print(f'cuisine: {y_test.iloc[50]}')\n+    ```\n+\n+    El resultado es impreso:\n+\n+   ```output\n+   ingredients: Index(['cilantro', 'onion', 'pea', 'potato', 'tomato', 'vegetable_oil'], dtype='object')\n+   cuisine: indian\n+   ```\n+\n+   ✅ Prueba un número de fila distinto y revisa los resultados\n+\n+1. Indagando más, puedes revisar la precisión de esta predicción:\n+\n+    ```python\n+    test= X_test.iloc[50].values.reshape(-1, 1).T\n+    proba = model.predict_proba(test)\n+    classes = model.classes_\n+    resultdf = pd.DataFrame(data=proba, columns=classes)\n+    \n+    topPrediction = resultdf.T.sort_values(by=[0], ascending = [False])\n+    topPrediction.head()\n+    ```\n+\n+    El resultado es impreso - La cocina India es su mejor conjetura, con buena probabilidad:\n+\n+    |          |        0 |\n+    | -------: | -------: |\n+    |   indian | 0.715851 |\n+    |  chinese | 0.229475 |\n+    | japanese | 0.029763 |\n+    |   korean | 0.017277 |\n+    |     thai | 0.007634 |\n+\n+    ✅ ¿Puedes explicar por qué el modelo está muy seguro esta es una cocina India?\n+\n+1. Obtén mayor detalle al imprimir un reporte de clasificación, como lo hiciste en las lecciones de regresión:\n+\n+    ```python\n+    y_pred = model.predict(X_test)\n+    print(classification_report(y_test,y_pred))\n+    ```\n+\n+    |              | precision | recall | f1-score | support |\n+    | ------------ | --------- | ------ | -------- | ------- |\n+    | chinese      | 0.73      | 0.71   | 0.72     | 229     |\n+    | indian       | 0.91      | 0.93   | 0.92     | 254     |\n+    | japanese     | 0.70      | 0.75   | 0.72     | 220     |\n+    | korean       | 0.86      | 0.76   | 0.81     | 242     |\n+    | thai         | 0.79      | 0.85   | 0.82     | 254     |\n+    | accuracy     | 0.80      | 1199   |          |         |\n+    | macro avg    | 0.80      | 0.80   | 0.80     | 1199    |\n+    | weighted avg | 0.80      | 0.80   | 0.80     | 1199    |\n+\n+## 🚀Desafío\n+\n+En esta lección, usaste tus datos limpios para construir un modelo de aprendizaje automático que puede predecir una cocina nacional basado en una serie de ingredientes. Toma un tiempo para leer las diversas opciones que provee Scikit-learn para clasificar los datos. Profundiza en el concepto de 'solucionador' para comprender que sucede detrás de escena.\n+\n+## [Examen posterior a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/22/)\n+\n+## Revisión y autoestudio\n+\n+Indaga un poco más en las matemática detrás de la regresión logística en [esta lección](https://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2006.pdf)",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430557,microsoft/ML-For-Beginners,861730997,813344451,hereje,3022824,2022-02-23T21:42:09+00:00,2022-02-23T21:42:09+00:00,Done!,true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430558,microsoft/ML-For-Beginners,861730997,813344648,hereje,3022824,2022-02-23T21:42:22+00:00,2022-02-23T21:42:22+00:00,Done!,true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:\n+\n+    ```output\n+    0    indian\n+    1    indian\n+    2    indian\n+    3    indian\n+    4    indian\n+    Name: cuisine, dtype: object\n+    ```\n+\n+1. Elimina la columna `Unnamed: 0` y la columna `cuisine`, llamando a `drop()`. Guarda el resto de los datos como características entrenables:\n+\n+    ```python\n+    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n+    cuisines_feature_df.head()\n+    ```\n+\n+    Tus características lucen así:\n+\n+|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n+| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n+|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n+\n+¡Ahora ya estás listo para entrenar tu modelo!\n+\n+## Eligiendo tu clasificador\n+\n+Ahora que tus datos están limpios y listos para entrenamiento, tienes que decidir qué algoritmo usar para hacer el trabajo.\n+\n+Scikit-learn agrupa clasificaciones bajo aprendizaje supervisado, y en esa categoría encontrarás muchas formas de clasificar. [La variedad](https://scikit-learn.org/stable/supervised_learning.html) es bastante abrumadora a primera vista. Los siguientes métodos incluyen técnicas de clasificación:\n+\n+- Modelos lineales\n+- Máquinas de vectores de soporte\n+- Descenso de gradiente estocástico\n+- Vecinos más cercanos\n+- Procesos Gaussianos\n+- Árboles de decisión\n+- Métodos de conjunto (clasificador de votos)\n+- Algoritmos multiclase y multisalida (clasificación multiclase y multietiqueta, clasificación multiclase-multisalida)\n+\n+> También puedes usar [redes neuronales para clasificar los datos](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), pero eso está fuera del alcance de esta lección.\n+\n+### ¿Qué clasificador usar?\n+\n+Así que, ¿qué clasificador deberías elegir? A menudo, el ejecutar varios y buscar un buen resultado es una forma de probar. Scikit-lean ofrece una [comparación lado a lado](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) en un conjunto de datos creado, comparando KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB y QuadraticDiscrinationAnalysis, mostrando los resultados visualizados:\n+\n+![Comparación de clasificadores](../images/comparison.png)\n+> Gráficos generados en la documentación de Scikit-learn\n+\n+> AutoML resuelve este problema de forma pulcra al ejecutar estas comparaciones en la nube, permitiéndote elegir el mejor algoritmo para tus datos. Pruébalo [aquí](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-15963-cxa)\n+\n+### Un mejor enfoque\n+\n+Una mejor forma a estar adivinando, es seguir las ideas de esta [hoja de trucos de ML](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa). Aquí, descubrimos que, para nuestro problema multiclase, tenemos algunas opciones:\n+\n+![Hoja de trucos para problemas multiclase](../images/cheatsheet.png)\n+> Una sección de la hoja de trucos de algoritmos de Microsoft, detallando opciones de clasificación multiclase.\n+\n+✅ !Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430559,microsoft/ML-For-Beginners,861730997,813344932,hereje,3022824,2022-02-23T21:42:39+00:00,2022-02-23T21:42:39+00:00,Thanks for your help!,true,4-Classification/2-Classifiers-1/translations/README.es.md,NaN,"@@ -0,0 +1,245 @@\n+# Clasificadores de cocina 1\n+\n+En esta lección, usarás el conjunto de datos que guardaste en la última lección llena de equilibrio, datos limpios todo sobre cocinas.\n+\n+Usarás este conjunto de datos con una variedad de clasificadores para _predecir una cocina nacional dada basado en un grupo de ingredientes_. Mientras lo haces, aprenderás más acerca de algunas formas en que los algoritmos pueden ser aprovechados para las tareas de clasificación.\n+\n+## [Examen previo a la lección](https://white-water-09ec41f0f.azurestaticapps.net/quiz/21/)\n+\n+# Preparación\n+\n+Asumiendo que completaste la [Lección 1](../../1-Introduction/translations/README.es.md), asegura que existe un archivo _cleaned_cuisines.csv_  en el directorio raíz `/data` para estas cuatro lecciones.\n+\n+## Ejercicio - predice una cocina nacional\n+\n+1. Trabaja en el directorio _notebook.ipynb_ de la lección, importa ese archivo junto con la biblioteca Pandas:\n+\n+    ```python\n+    import pandas as pd\n+    cuisines_df = pd.read_csv(""../data/cleaned_cuisines.csv"")\n+    cuisines_df.head()\n+    ```\n+\n+    Los datos lucen así:\n+\n+|     | Unnamed: 0 | cuisine | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood | yam | yeast | yogurt | zucchini |\n+| --- | ---------- | ------- | ------ | -------- | ----- | ---------- | ----- | ------------ | ------- | -------- | --- | ------- | ----------- | ---------- | ----------------------- | ---- | ---- | --- | ----- | ------ | -------- |\n+| 0   | 0          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 1   | 1          | indian  | 1      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 2   | 2          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 3   | 3          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 0      | 0        |\n+| 4   | 4          | indian  | 0      | 0        | 0     | 0          | 0     | 0            | 0       | 0        | ... | 0       | 0           | 0          | 0                       | 0    | 0    | 0   | 0     | 1      | 0        |\n+  \n+\n+1. Ahora, importa varias bibliotecas más:\n+\n+    ```python\n+    from sklearn.linear_model import LogisticRegression\n+    from sklearn.model_selection import train_test_split, cross_val_score\n+    from sklearn.metrics import accuracy_score,precision_score,confusion_matrix,classification_report, precision_recall_curve\n+    from sklearn.svm import SVC\n+    import numpy as np\n+    ```\n+\n+1. Divide las coordenadas X e Y en dos diferentes dataframes para entrenar. `cuisine` puede ser el dataframe de las etiquetas:\n+\n+    ```python\n+    cuisines_label_df = cuisines_df['cuisine']\n+    cuisines_label_df.head()\n+    ```\n+\n+    It will look like this:\n+\n+    ```output\n+    0    indian\n+    1    indian\n+    2    indian\n+    3    indian\n+    4    indian\n+    Name: cuisine, dtype: object\n+    ```\n+\n+1. Elimina la columna `Unnamed: 0` y la columna `cuisine`, llamando a `drop()`. Guarda el resto de los datos como características entrenables:\n+\n+    ```python\n+    cuisines_feature_df = cuisines_df.drop(['Unnamed: 0', 'cuisine'], axis=1)\n+    cuisines_feature_df.head()\n+    ```\n+\n+    Tus características lucen así:\n+\n+|      | almond | angelica | anise | anise_seed | apple | apple_brandy | apricot | armagnac | artemisia | artichoke |  ... | whiskey | white_bread | white_wine | whole_grain_wheat_flour | wine | wood |  yam | yeast | yogurt | zucchini |\n+| ---: | -----: | -------: | ----: | ---------: | ----: | -----------: | ------: | -------: | --------: | --------: | ---: | ------: | ----------: | ---------: | ----------------------: | ---: | ---: | ---: | ----: | -----: | -------: |\n+|    0 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    1 |      1 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    2 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    3 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      0 |        0 | 0 |\n+|    4 |      0 |        0 |     0 |          0 |     0 |            0 |       0 |        0 |         0 |         0 |  ... |       0 |           0 |          0 |                       0 |    0 |    0 |    0 |     0 |      1 |        0 | 0 |\n+\n+¡Ahora ya estás listo para entrenar tu modelo!\n+\n+## Eligiendo tu clasificador\n+\n+Ahora que tus datos están limpios y listos para entrenamiento, tienes que decidir qué algoritmo usar para hacer el trabajo.\n+\n+Scikit-learn agrupa clasificaciones bajo aprendizaje supervisado, y en esa categoría encontrarás muchas formas de clasificar. [La variedad](https://scikit-learn.org/stable/supervised_learning.html) es bastante abrumadora a primera vista. Los siguientes métodos incluyen técnicas de clasificación:\n+\n+- Modelos lineales\n+- Máquinas de vectores de soporte\n+- Descenso de gradiente estocástico\n+- Vecinos más cercanos\n+- Procesos Gaussianos\n+- Árboles de decisión\n+- Métodos de conjunto (clasificador de votos)\n+- Algoritmos multiclase y multisalida (clasificación multiclase y multietiqueta, clasificación multiclase-multisalida)\n+\n+> También puedes usar [redes neuronales para clasificar los datos](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), pero eso está fuera del alcance de esta lección.\n+\n+### ¿Qué clasificador usar?\n+\n+Así que, ¿qué clasificador deberías elegir? A menudo, el ejecutar varios y buscar un buen resultado es una forma de probar. Scikit-lean ofrece una [comparación lado a lado](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) en un conjunto de datos creado, comparando KNeighbors, SVC two ways, GaussianProcessClassifier, DecisionTreeClassifier, RandomForestClassifier, MLPClassifier, AdaBoostClassifier, GaussianNB y QuadraticDiscrinationAnalysis, mostrando los resultados visualizados:\n+\n+![Comparación de clasificadores](../images/comparison.png)\n+> Gráficos generados en la documentación de Scikit-learn\n+\n+> AutoML resuelve este problema de forma pulcra al ejecutar estas comparaciones en la nube, permitiéndote elegir el mejor algoritmo para tus datos. Pruébalo [aquí](https://docs.microsoft.com/learn/modules/automate-model-selection-with-azure-automl/?WT.mc_id=academic-15963-cxa)\n+\n+### Un mejor enfoque\n+\n+Una mejor forma a estar adivinando, es seguir las ideas de esta [hoja de trucos de ML](https://docs.microsoft.com/azure/machine-learning/algorithm-cheat-sheet?WT.mc_id=academic-15963-cxa). Aquí, descubrimos que, para nuestro problema multiclase, tenemos algunas opciones:\n+\n+![Hoja de trucos para problemas multiclase](../images/cheatsheet.png)\n+> Una sección de la hoja de trucos de algoritmos de Microsoft, detallando opciones de clasificación multiclase.\n+\n+✅ !Descarga esta hoja de trucos, imprímela y cuélgala en tu pared!\n+\n+### Razonamiento\n+\n+Veamos si podemos razonar nuestro camino a través de diferentes enfoques dadas las restricciones que tenemos:\n+\n+- **Las redes neuronales son muy pesadas**. Dado nuestro conjunto de datos limpio aunque mínimo, y el hecho que estamos ejecutando el entrenamiento de forma local vía los notebooks, las redes neuronales son demasiado pesadas para esta tarea.\n+- **Sin clasificador de dos clases**. No usamos clasificador de dos clases, por lo que descarta un uno-contra-todos.\n+- **El árbol de decisión o la regresión logística podría funcionar**. Un árbolo de decisión podría funcionar, o la regresión logística para datos multiclase.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843055a,microsoft/ML-For-Beginners,860075241,813131025,Stevengez,53009062,2022-02-23T17:19:39+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\nLos defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente la biblioteca Scikit-learn y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843055b,microsoft/ML-For-Beginners,860075241,813139719,Stevengez,53009062,2022-02-23T17:29:14+00:00,2022-02-23T18:10:48+00:00,```suggestion\r\n- Intenta crear los proyectos al comprender las lecciones en lugar de ejecutar el código de la solución; sin embargo ese código se encuentra disponible en los directorios `/solution` en cada lección orientada a proyecto.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843055c,microsoft/ML-For-Beginners,860075241,813150907,Stevengez,53009062,2022-02-23T17:41:32+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\n- Después de completar un grupo de lecciones, visita el [tablero de discusión](https://github.com/microsoft/ML-For-Beginners/discussions) y ""aprende en voz alta"" llenando la rúbrica PAT apropiada. Un 'PAT' es una herramienta de evaluación del progreso que es una rúbrica la cual llenas para avanzar en tu aprendizaje. También puede reaccionar a otros PATs y así aprender juntos.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843055d,microsoft/ML-For-Beginners,860075241,813154252,Stevengez,53009062,2022-02-23T17:45:26+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\n> Para aprender más, recomendamos seguir estos módulos y rutas de aprendizaje de [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa).\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843055e,microsoft/ML-For-Beginners,860075241,813161369,Stevengez,53009062,2022-02-23T17:53:41+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\nAl asegurar que el contenido se alinea con los proyectos, el proceso se hace más atractivo par los estudiantes y la retención de conceptos incrementará. Además, un pequeño examen antes de cada clase para establecer la intención del estudiante de aprender un tema, mientras el segundo examen después de la clase asegura una mayor retención. Este plan de estudios fue diseñado para ser flexible y divertido y puede se tomado en su totalidad o en partes. Los proyectos comienzan pequeños y van incrementando en complejidad al final del ciclo de 12 semanas. Este plan de estudios también incluye una nota al final sobre aplicaciones del aprendizaje automático en el mundo real, la cual puede ser usada como cŕeditos extra o como base para discusión.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843055f,microsoft/ML-For-Beginners,860075241,813169642,Stevengez,53009062,2022-02-23T18:03:18+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\n> **Una nota acerca de los exámenes**: Todos los exámenes están contenidos [en esta app](https://white-water-09ec41f0f.azurestaticapps.net/), para un total de 52 exámenes de 3 preguntas cada uno, Ellos están vinculados dentro de las lecciones pero la aplicación de exámenes puede ser ejecutada localmente; sigue las instrucciones en el directorio `quiz-app`.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430560,microsoft/ML-For-Beginners,860075241,813177965,abrildur,71578178,2022-02-23T18:13:52+00:00,2022-02-23T22:07:17+00:00,"Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'.  ¡También acompaña estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430561,microsoft/ML-For-Beginners,860075241,813200355,abrildur,71578178,2022-02-23T18:39:55+00:00,2022-02-23T22:07:17+00:00,"Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye:\r\n\r\n- Exámenes previos y posteriores a cada lección\r\n- Instrucciones escritas para completar dicha lección\r\n- Una solución\r\n- Una tarea y más. \r\n\r\nNuestra enseñanza basada en proyectos, te permite aprender mientras practicas; esta es una forma aprobada para que adquieras nuevas habilidades.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430562,microsoft/ML-For-Beginners,860075241,813393172,hereje,3022824,2022-02-23T22:48:08+00:00,2022-02-23T22:48:08+00:00,Done!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430563,microsoft/ML-For-Beginners,860075241,813393326,hereje,3022824,2022-02-23T22:48:22+00:00,2022-02-23T22:48:22+00:00,Done!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430564,microsoft/ML-For-Beginners,860075241,813131025,Stevengez,53009062,2022-02-23T17:19:39+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\nLos defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente la biblioteca Scikit-learn y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\r\n```",true,translations/README.es.md,NaN,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430565,microsoft/ML-For-Beginners,860075241,813139719,Stevengez,53009062,2022-02-23T17:29:14+00:00,2022-02-23T18:10:48+00:00,```suggestion\r\n- Intenta crear los proyectos al comprender las lecciones en lugar de ejecutar el código de la solución; sin embargo ese código se encuentra disponible en los directorios `/solution` en cada lección orientada a proyecto.\r\n```,true,translations/README.es.md,NaN,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n \n-Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n+Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye exámenes previos y posteriores a la lección, instrucciones escritas para completar la lección, una solución, una asignación y más. Nuestra pedagogía basa en proyectos te permite aprender mientras construyes, una forma probada para que las nuevas habilidades se te 'peguen'.\n \n-**✍️ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd\n+**✍️ Agradecimientos encarecidos** a Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, y Amy Boyd.\n \n-**🎨 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n+**🎨 Gracias también a nuestros ilustradores** Tomomi Imura, Dasani Madipalli, y Jen Looper.\n \n-**🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n+**🙏 Agradecimientos especiales 🙏 a nuestros  autores de Embajadores Estudiantiles de Microsoft, revisores, y colaboradores de contenido**, notablemente a Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, y Snigdha Agarwal.\n \n-**🤩 Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!**\n+**🤩 ¡Agradecimiento adicional al embajador estudiantil de Microsoft Eric Wanjau por nuestras lecciones de R!**\n \n ---\n \n-# Getting Started\n+# Empezando\n \n-**Students**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n+**Estudiantes**, para usar este plan de estudios, realiza un fork del repositorio completo en tu cuenta de Github y completa los ejercicios por ti mismo o en grupo:\n \n-- Start with a pre-lecture quiz.\n-- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n-- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n-- Take the post-lecture quiz.\n-- Complete the challenge.\n-- Complete the assignment.\n-- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and ""learn out loud"" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n+- Comienza con un examen previo a la lección.\n+- Lee las lecciones y completa las actividades, pausando y reflexionando en cada verificación de conocimiento.\n+- Intenta crear los proyectos al comprender las lecciones en lugar de ejecutar la solución en código; sin embargo ese código se encuentra disponible en los directorios `/solution` en cada lección orientada a proyecto.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430566,microsoft/ML-For-Beginners,860075241,813150907,Stevengez,53009062,2022-02-23T17:41:32+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\n- Después de completar un grupo de lecciones, visita el [tablero de discusión](https://github.com/microsoft/ML-For-Beginners/discussions) y ""aprende en voz alta"" llenando la rúbrica PAT apropiada. Un 'PAT' es una herramienta de evaluación del progreso que es una rúbrica la cual llenas para avanzar en tu aprendizaje. También puede reaccionar a otros PATs y así aprender juntos.\r\n```",true,translations/README.es.md,NaN,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n \n-Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n+Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye exámenes previos y posteriores a la lección, instrucciones escritas para completar la lección, una solución, una asignación y más. Nuestra pedagogía basa en proyectos te permite aprender mientras construyes, una forma probada para que las nuevas habilidades se te 'peguen'.\n \n-**✍️ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd\n+**✍️ Agradecimientos encarecidos** a Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, y Amy Boyd.\n \n-**🎨 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n+**🎨 Gracias también a nuestros ilustradores** Tomomi Imura, Dasani Madipalli, y Jen Looper.\n \n-**🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n+**🙏 Agradecimientos especiales 🙏 a nuestros  autores de Embajadores Estudiantiles de Microsoft, revisores, y colaboradores de contenido**, notablemente a Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, y Snigdha Agarwal.\n \n-**🤩 Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!**\n+**🤩 ¡Agradecimiento adicional al embajador estudiantil de Microsoft Eric Wanjau por nuestras lecciones de R!**\n \n ---\n \n-# Getting Started\n+# Empezando\n \n-**Students**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n+**Estudiantes**, para usar este plan de estudios, realiza un fork del repositorio completo en tu cuenta de Github y completa los ejercicios por ti mismo o en grupo:\n \n-- Start with a pre-lecture quiz.\n-- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n-- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n-- Take the post-lecture quiz.\n-- Complete the challenge.\n-- Complete the assignment.\n-- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and ""learn out loud"" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n+- Comienza con un examen previo a la lección.\n+- Lee las lecciones y completa las actividades, pausando y reflexionando en cada verificación de conocimiento.\n+- Intenta crear los proyectos al comprender las lecciones en lugar de ejecutar la solución en código; sin embargo ese código se encuentra disponible en los directorios `/solution` en cada lección orientada a proyecto.\n+- Realiza el examen posterior a la lección.\n+- Completa el desafío.\n+- Completa la asignación.\n+- Después de completar un grupo de lecciones, visita el [tablero de discusión](https://github.com/microsoft/ML-For-Beginners/discussions) y ""aprende en voz alta"" al llenar la rúbrica PAT apropiada. Un 'PAT' es una herramienta de evaluación del progreso que es una rúbrica la cual llenas para avanzar en tu aprendizaje. También puede reaccionar a otros PATs y así aprender juntos.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430567,microsoft/ML-For-Beginners,860075241,813154252,Stevengez,53009062,2022-02-23T17:45:26+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\n> Para aprender más, recomendamos seguir estos módulos y rutas de aprendizaje de [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa).\r\n```",true,translations/README.es.md,NaN,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n \n-Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n+Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye exámenes previos y posteriores a la lección, instrucciones escritas para completar la lección, una solución, una asignación y más. Nuestra pedagogía basa en proyectos te permite aprender mientras construyes, una forma probada para que las nuevas habilidades se te 'peguen'.\n \n-**✍️ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd\n+**✍️ Agradecimientos encarecidos** a Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, y Amy Boyd.\n \n-**🎨 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n+**🎨 Gracias también a nuestros ilustradores** Tomomi Imura, Dasani Madipalli, y Jen Looper.\n \n-**🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n+**🙏 Agradecimientos especiales 🙏 a nuestros  autores de Embajadores Estudiantiles de Microsoft, revisores, y colaboradores de contenido**, notablemente a Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, y Snigdha Agarwal.\n \n-**🤩 Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!**\n+**🤩 ¡Agradecimiento adicional al embajador estudiantil de Microsoft Eric Wanjau por nuestras lecciones de R!**\n \n ---\n \n-# Getting Started\n+# Empezando\n \n-**Students**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n+**Estudiantes**, para usar este plan de estudios, realiza un fork del repositorio completo en tu cuenta de Github y completa los ejercicios por ti mismo o en grupo:\n \n-- Start with a pre-lecture quiz.\n-- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n-- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n-- Take the post-lecture quiz.\n-- Complete the challenge.\n-- Complete the assignment.\n-- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and ""learn out loud"" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n+- Comienza con un examen previo a la lección.\n+- Lee las lecciones y completa las actividades, pausando y reflexionando en cada verificación de conocimiento.\n+- Intenta crear los proyectos al comprender las lecciones en lugar de ejecutar la solución en código; sin embargo ese código se encuentra disponible en los directorios `/solution` en cada lección orientada a proyecto.\n+- Realiza el examen posterior a la lección.\n+- Completa el desafío.\n+- Completa la asignación.\n+- Después de completar un grupo de lecciones, visita el [tablero de discusión](https://github.com/microsoft/ML-For-Beginners/discussions) y ""aprende en voz alta"" al llenar la rúbrica PAT apropiada. Un 'PAT' es una herramienta de evaluación del progreso que es una rúbrica la cual llenas para avanzar en tu aprendizaje. También puede reaccionar a otros PATs y así aprender juntos.\n \n-> For further study, we recommend following these [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) modules and learning paths.\n+> Para mayor estudio, recomendamos seguir estos módulos y rutas de aprendizaje de [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa).",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430568,microsoft/ML-For-Beginners,860075241,813161369,Stevengez,53009062,2022-02-23T17:53:41+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\nAl asegurar que el contenido se alinea con los proyectos, el proceso se hace más atractivo par los estudiantes y la retención de conceptos incrementará. Además, un pequeño examen antes de cada clase para establecer la intención del estudiante de aprender un tema, mientras el segundo examen después de la clase asegura una mayor retención. Este plan de estudios fue diseñado para ser flexible y divertido y puede se tomado en su totalidad o en partes. Los proyectos comienzan pequeños y van incrementando en complejidad al final del ciclo de 12 semanas. Este plan de estudios también incluye una nota al final sobre aplicaciones del aprendizaje automático en el mundo real, la cual puede ser usada como cŕeditos extra o como base para discusión.\r\n```",true,translations/README.es.md,NaN,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n \n-Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n+Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye exámenes previos y posteriores a la lección, instrucciones escritas para completar la lección, una solución, una asignación y más. Nuestra pedagogía basa en proyectos te permite aprender mientras construyes, una forma probada para que las nuevas habilidades se te 'peguen'.\n \n-**✍️ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd\n+**✍️ Agradecimientos encarecidos** a Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, y Amy Boyd.\n \n-**🎨 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n+**🎨 Gracias también a nuestros ilustradores** Tomomi Imura, Dasani Madipalli, y Jen Looper.\n \n-**🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n+**🙏 Agradecimientos especiales 🙏 a nuestros  autores de Embajadores Estudiantiles de Microsoft, revisores, y colaboradores de contenido**, notablemente a Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, y Snigdha Agarwal.\n \n-**🤩 Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!**\n+**🤩 ¡Agradecimiento adicional al embajador estudiantil de Microsoft Eric Wanjau por nuestras lecciones de R!**\n \n ---\n \n-# Getting Started\n+# Empezando\n \n-**Students**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n+**Estudiantes**, para usar este plan de estudios, realiza un fork del repositorio completo en tu cuenta de Github y completa los ejercicios por ti mismo o en grupo:\n \n-- Start with a pre-lecture quiz.\n-- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n-- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n-- Take the post-lecture quiz.\n-- Complete the challenge.\n-- Complete the assignment.\n-- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and ""learn out loud"" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n+- Comienza con un examen previo a la lección.\n+- Lee las lecciones y completa las actividades, pausando y reflexionando en cada verificación de conocimiento.\n+- Intenta crear los proyectos al comprender las lecciones en lugar de ejecutar la solución en código; sin embargo ese código se encuentra disponible en los directorios `/solution` en cada lección orientada a proyecto.\n+- Realiza el examen posterior a la lección.\n+- Completa el desafío.\n+- Completa la asignación.\n+- Después de completar un grupo de lecciones, visita el [tablero de discusión](https://github.com/microsoft/ML-For-Beginners/discussions) y ""aprende en voz alta"" al llenar la rúbrica PAT apropiada. Un 'PAT' es una herramienta de evaluación del progreso que es una rúbrica la cual llenas para avanzar en tu aprendizaje. También puede reaccionar a otros PATs y así aprender juntos.\n \n-> For further study, we recommend following these [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) modules and learning paths.\n+> Para mayor estudio, recomendamos seguir estos módulos y rutas de aprendizaje de [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa).\n \n-**Teachers**, we have [included some suggestions](for-teachers.md) on how to use this curriculum.\n+**Profesores**, hemos [incluido algunas sugerencias](../for-teachers.md) de cómo usar este plan de estudios.\n \n ---\n \n-## Meet the Team\n+## Conoce al equipo\n \n-[![Promo video](ml.gif)](https://youtu.be/Tj1XWrDSYJU ""Promo video"")\n+[![Video promocional](../ml.gif)](https://youtu.be/Tj1XWrDSYJU ""Video promocional"")\n \n-**Gif by** [Mohit Jaisal](https://linkedin.com/in/mohitjaisal)\n+**Gif de** [Mohit Jaisal](https://linkedin.com/in/mohitjaisal)\n \n-> 🎥 Click the image above for a video about the project and the folks who created it!\n+> 🎥 ¡Da clic a la imagen de arriba para ver un video acerca del proyecto y la gente que lo creó!\n \n ---\n \n-## Pedagogy\n+## Pedagogía\n \n-We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on **project-based** and that it includes **frequent quizzes**. In addition, this curriculum has a common **theme** to give it cohesion.\n+Hemos elegido dos principios pedagógicos mientras construimos este plan de estudios: asegurar que es práctico **basado en proyectos** y que incluye **exámenes frecuentes**. Además, este plan de estudios tiene un **tema** común para darle cohesión.\n \n-By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.\n+Al asegurar que el contenido se alinea con los proyectos, el proceso se hace más atractivo par los estudiantes y la retención de conceptos incrementará. Además, un examen de apuestas bajas antes de una clase establece la intención del estudiante hacia aprender un tema, mientras el segundo examen después de la clase asegura una mayor retención. Este plan de estudios fue diseñado para ser flexible y divertido y puede se tomado en su totalidad o en partes. Los proyectos comienzan pequeños y van incrementando en complejidad al final del ciclo de 12 semanas. Este plan de estudios también incluye una nota al final de aplicaciones del aprendizaje automático en el mundo real, la cual puede ser usada como cŕeditos extra o como base para discusión.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430569,microsoft/ML-For-Beginners,860075241,813169642,Stevengez,53009062,2022-02-23T18:03:18+00:00,2022-02-23T18:10:48+00:00,"```suggestion\r\n> **Una nota acerca de los exámenes**: Todos los exámenes están contenidos [en esta app](https://white-water-09ec41f0f.azurestaticapps.net/), para un total de 52 exámenes de 3 preguntas cada uno, Ellos están vinculados dentro de las lecciones pero la aplicación de exámenes puede ser ejecutada localmente; sigue las instrucciones en el directorio `quiz-app`.\r\n```",true,translations/README.es.md,NaN,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n \n-Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n+Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye exámenes previos y posteriores a la lección, instrucciones escritas para completar la lección, una solución, una asignación y más. Nuestra pedagogía basa en proyectos te permite aprender mientras construyes, una forma probada para que las nuevas habilidades se te 'peguen'.\n \n-**✍️ Hearty thanks to our authors** Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, and Amy Boyd\n+**✍️ Agradecimientos encarecidos** a Jen Looper, Stephen Howell, Francesca Lazzeri, Tomomi Imura, Cassie Breviu, Dmitry Soshnikov, Chris Noring, Anirban Mukherjee, Ornella Altunyan, y Amy Boyd.\n \n-**🎨 Thanks as well to our illustrators** Tomomi Imura, Dasani Madipalli, and Jen Looper\n+**🎨 Gracias también a nuestros ilustradores** Tomomi Imura, Dasani Madipalli, y Jen Looper.\n \n-**🙏 Special thanks 🙏 to our Microsoft Student Ambassador authors, reviewers, and content contributors**, notably Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, and Snigdha Agarwal\n+**🙏 Agradecimientos especiales 🙏 a nuestros  autores de Embajadores Estudiantiles de Microsoft, revisores, y colaboradores de contenido**, notablemente a Rishit Dagli, Muhammad Sakib Khan Inan, Rohan Raj, Alexandru Petrescu, Abhishek Jaiswal, Nawrin Tabassum, Ioan Samuila, y Snigdha Agarwal.\n \n-**🤩 Extra gratitude to Microsoft Student Ambassador Eric Wanjau for our R lessons!**\n+**🤩 ¡Agradecimiento adicional al embajador estudiantil de Microsoft Eric Wanjau por nuestras lecciones de R!**\n \n ---\n \n-# Getting Started\n+# Empezando\n \n-**Students**, to use this curriculum, fork the entire repo to your own GitHub account and complete the exercises on your own or with a group:\n+**Estudiantes**, para usar este plan de estudios, realiza un fork del repositorio completo en tu cuenta de Github y completa los ejercicios por ti mismo o en grupo:\n \n-- Start with a pre-lecture quiz.\n-- Read the lecture and complete the activities, pausing and reflecting at each knowledge check.\n-- Try to create the projects by comprehending the lessons rather than running the solution code; however that code is available in the `/solution` folders in each project-oriented lesson.\n-- Take the post-lecture quiz.\n-- Complete the challenge.\n-- Complete the assignment.\n-- After completing a lesson group, visit the [Discussion Board](https://github.com/microsoft/ML-For-Beginners/discussions) and ""learn out loud"" by filling out the appropriate PAT rubric. A 'PAT' is a Progress Assessment Tool that is a rubric you fill out to further your learning. You can also react to other PATs so we can learn together.\n+- Comienza con un examen previo a la lección.\n+- Lee las lecciones y completa las actividades, pausando y reflexionando en cada verificación de conocimiento.\n+- Intenta crear los proyectos al comprender las lecciones en lugar de ejecutar la solución en código; sin embargo ese código se encuentra disponible en los directorios `/solution` en cada lección orientada a proyecto.\n+- Realiza el examen posterior a la lección.\n+- Completa el desafío.\n+- Completa la asignación.\n+- Después de completar un grupo de lecciones, visita el [tablero de discusión](https://github.com/microsoft/ML-For-Beginners/discussions) y ""aprende en voz alta"" al llenar la rúbrica PAT apropiada. Un 'PAT' es una herramienta de evaluación del progreso que es una rúbrica la cual llenas para avanzar en tu aprendizaje. También puede reaccionar a otros PATs y así aprender juntos.\n \n-> For further study, we recommend following these [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa) modules and learning paths.\n+> Para mayor estudio, recomendamos seguir estos módulos y rutas de aprendizaje de [Microsoft Learn](https://docs.microsoft.com/en-us/users/jenlooper-2911/collections/k7o7tg1gp306q4?WT.mc_id=academic-15963-cxa).\n \n-**Teachers**, we have [included some suggestions](for-teachers.md) on how to use this curriculum.\n+**Profesores**, hemos [incluido algunas sugerencias](../for-teachers.md) de cómo usar este plan de estudios.\n \n ---\n \n-## Meet the Team\n+## Conoce al equipo\n \n-[![Promo video](ml.gif)](https://youtu.be/Tj1XWrDSYJU ""Promo video"")\n+[![Video promocional](../ml.gif)](https://youtu.be/Tj1XWrDSYJU ""Video promocional"")\n \n-**Gif by** [Mohit Jaisal](https://linkedin.com/in/mohitjaisal)\n+**Gif de** [Mohit Jaisal](https://linkedin.com/in/mohitjaisal)\n \n-> 🎥 Click the image above for a video about the project and the folks who created it!\n+> 🎥 ¡Da clic a la imagen de arriba para ver un video acerca del proyecto y la gente que lo creó!\n \n ---\n \n-## Pedagogy\n+## Pedagogía\n \n-We have chosen two pedagogical tenets while building this curriculum: ensuring that it is hands-on **project-based** and that it includes **frequent quizzes**. In addition, this curriculum has a common **theme** to give it cohesion.\n+Hemos elegido dos principios pedagógicos mientras construimos este plan de estudios: asegurar que es práctico **basado en proyectos** y que incluye **exámenes frecuentes**. Además, este plan de estudios tiene un **tema** común para darle cohesión.\n \n-By ensuring that the content aligns with projects, the process is made more engaging for students and retention of concepts will be augmented. In addition, a low-stakes quiz before a class sets the intention of the student towards learning a topic, while a second quiz after class ensures further retention. This curriculum was designed to be flexible and fun and can be taken in whole or in part. The projects start small and become increasingly complex by the end of the 12-week cycle. This curriculum also includes a postscript on real-world applications of ML, which can be used as extra credit or as a basis for discussion.\n+Al asegurar que el contenido se alinea con los proyectos, el proceso se hace más atractivo par los estudiantes y la retención de conceptos incrementará. Además, un examen de apuestas bajas antes de una clase establece la intención del estudiante hacia aprender un tema, mientras el segundo examen después de la clase asegura una mayor retención. Este plan de estudios fue diseñado para ser flexible y divertido y puede se tomado en su totalidad o en partes. Los proyectos comienzan pequeños y van incrementando en complejidad al final del ciclo de 12 semanas. Este plan de estudios también incluye una nota al final de aplicaciones del aprendizaje automático en el mundo real, la cual puede ser usada como cŕeditos extra o como base para discusión.\n \n-> Find our [Code of Conduct](CODE_OF_CONDUCT.md), [Contributing](CONTRIBUTING.md), and [Translation](TRANSLATIONS.md) guidelines. We welcome your constructive feedback!\n+> Encuentra nuestros lineamientos de [Código de conducta](../CODE_OF_CONDUCT.md), [Contribución](../CONTRIBUTING.md), y [Traducción](../TRANSLATIONS.md). ¡Son bienvenidos tus comentarios constructivos!\n \n-## Each lesson includes:\n+## Cada lección incluye:\n \n-- optional sketchnote\n-- optional supplemental video\n-- pre-lecture warmup quiz\n-- written lesson\n-- for project-based lessons, step-by-step guides on how to build the project\n-- knowledge checks\n-- a challenge\n-- supplemental reading\n-- assignment\n-- post-lecture quiz\n+- boceto opcional\n+- video suplementario opcional\n+- examen de calentamiento previo a la lección\n+- lección escrita\n+- para lecciones basadas en proyectos, guías paso a paso de cómo construir el proyecto\n+- verificaciones de conocimiento\n+- un desafío\n+- lectura suplementaria\n+- asignación\n+- examen posterior a la lección\n \n-> **A note about languages**: These lessons are primarily written in Python, but many are also available in R. To complete an R lesson, go to the `/solution` folder and look for R lessons. They include an .rmd extension that represents an **R Markdown** file which can be simply defined as an embedding of `code chunks` (of R or other languages) and a `YAML header` (that guides how to format outputs such as PDF) in a `Markdown document`. As such, it serves as an exemplary authoring framework for data science since it allows you to combine your code, its output, and your thoughts by allowing you to write them down in Markdown. Moreover, R Markdown documents can be rendered to output formats such as PDF, HTML, or Word.\n+> **Una nota acerca de los lenguajes**: Estas lecciones están escritas principalmente en Python, pero muchas también están disponibles en R. Para completar una lección en R, ve al directorio `/solution` y busca las lecciones. Ellas incluyen una extensión .rmd que representa un archivo **Markdown R** el cual puede ser definido simplemente como `porciones de código` embebido (de R u otros lenguajes) y un `encabezado YAML` (que guía cómo dar formato a las salidas, por ejemplo PDF) en un `documento Markdown`. Como tal, este sirve como un framework ejemplar de autoría para la ciencia de datos ya que permite combinar tu código, su salida, y tus pensamientos al permitirte escribirlos en Markdown. Es más, los documentos Markdown R pueden ser representados como formatos de salida tal como PDF, HTML, o Word.\n \n-> **A note about quizzes**: All quizzes are contained [in this app](https://white-water-09ec41f0f.azurestaticapps.net/), for 52 total quizzes of three questions each. They are linked from within the lessons but the quiz app can be run locally; follow the instruction in the `quiz-app` folder.\n+> **Una nota acerca de los exámenes**: Todos los exámenes están contenidos [en esta app](https://white-water-09ec41f0f.azurestaticapps.net/), por un total de 52 exámenes de 3 preguntas cada uno, Ellos están vinculados dentro de las lecciones pero la aplicación de exámenes puede ser ejecutada localmente; sigue las instrucciones en el directorio `quiz-app`.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843056a,microsoft/ML-For-Beginners,860075241,813177965,abrildur,71578178,2022-02-23T18:13:52+00:00,2022-02-23T22:07:17+00:00,"Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'.  ¡También acompaña estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!",true,translations/README.es.md,16,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n ",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843056b,microsoft/ML-For-Beginners,860075241,813200355,abrildur,71578178,2022-02-23T18:39:55+00:00,2022-02-23T22:07:17+00:00,"Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye:\r\n\r\n- Exámenes previos y posteriores a cada lección\r\n- Instrucciones escritas para completar dicha lección\r\n- Una solución\r\n- Una tarea y más. \r\n\r\nNuestra enseñanza basada en proyectos, te permite aprender mientras practicas; esta es una forma aprobada para que adquieras nuevas habilidades.",true,translations/README.es.md,25,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n \n-Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n+Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye exámenes previos y posteriores a la lección, instrucciones escritas para completar la lección, una solución, una asignación y más. Nuestra pedagogía basa en proyectos te permite aprender mientras construyes, una forma probada para que las nuevas habilidades se te 'peguen'.\n ",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843056c,microsoft/ML-For-Beginners,860075241,813393172,hereje,3022824,2022-02-23T22:48:08+00:00,2022-02-23T22:48:08+00:00,Done!,true,translations/README.es.md,16,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n ",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843056d,microsoft/ML-For-Beginners,860075241,813393326,hereje,3022824,2022-02-23T22:48:22+00:00,2022-02-23T22:48:22+00:00,Done!,true,translations/README.es.md,25,"@@ -8,122 +8,119 @@\n [![GitHub forks](https://img.shields.io/github/forks/microsoft/ML-For-Beginners.svg?style=social&label=Fork)](https://GitHub.com/microsoft/ML-For-Beginners/network/)\n [![GitHub stars](https://img.shields.io/github/stars/microsoft/ML-For-Beginners.svg?style=social&label=Star)](https://GitHub.com/microsoft/ML-For-Beginners/stargazers/)\n \n-# Machine Learning for Beginners - A Curriculum\n+# Aprendizaje automático para principiantes - Un plan de estudios\n \n-> 🌍 Travel around the world as we explore Machine Learning by means of world cultures 🌍\n+> 🌍 Viaja alrededor del mundo mientras exploramos el aprendizaje automático a través de las culturas del mundo 🌍\n \n-Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about **Machine Learning**. In this curriculum, you will learn about what is sometimes called **classic machine learning**, using primarily Scikit-learn as a library and avoiding deep learning, which is covered in our forthcoming 'AI for Beginners' curriculum. Pair these lessons with our ['Data Science for Beginners' curriculum](https://aka.ms/datascience-beginners), as well!\n+Los defensores de Azure Cloud en Microsoft están complacidos en ofrecer un plan de estudios de 12 semanas, con 26 lecciones, todas acerca de **aprendizaje automático**. En este plan de estudios, aprenderás acerca de lo que algunas veces es llamado **aprendizaje automático clásico**, usando principalmente Scikit-learn como una biblioteca y evitando el aprendizaje profundo, el cual se aborda en nuestro próximo plan de estudios 'IA para principiantes'. ¡También empareja estas lecciones con nuestro [plan de estudios 'Ciencia de Datos para principiantes'](https://aka.ms/datascience-beginners)!\n \n-Travel with us around the world as we apply these classic techniques to data from many areas of the world. Each lesson includes pre- and post-lesson quizzes, written instructions to complete the lesson, a solution, an assignment, and more. Our project-based pedagogy allows you to learn while building, a proven way for new skills to 'stick'.\n+Viaja con nosotros alrededor del mundo mientras aplicamos estas técnicas clásicas a los datos de distintas áreas del mundo. Cada lección incluye exámenes previos y posteriores a la lección, instrucciones escritas para completar la lección, una solución, una asignación y más. Nuestra pedagogía basa en proyectos te permite aprender mientras construyes, una forma probada para que las nuevas habilidades se te 'peguen'.\n ",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843056e,microsoft/ML-For-Beginners,859063464,813187254,Stevengez,53009062,2022-02-23T18:25:17+00:00,2022-02-23T18:39:15+00:00,"```suggestion\r\nEl truco principal de este problema es, no obstante, que el motor del carrito no es lo suficientemente potente para escalar la montaña en una sola pasada. Por lo tanto, la única forma de lograrlo es conducir hacia atrás y adelante para generar impulso.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843056f,microsoft/ML-For-Beginners,859063464,813190105,Stevengez,53009062,2022-02-23T18:28:34+00:00,2022-02-23T18:39:15+00:00,```suggestion\r\nEl sistema de recompensas para el carrito de montaña es engañoso:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430570,microsoft/ML-For-Beginners,859063464,813193111,Stevengez,53009062,2022-02-23T18:31:33+00:00,2022-02-23T18:39:15+00:00,```suggestion\r\n* La recompensa de 0 es otorgada si el agente alcanzó la bandera (posición = 0.5) en la cima de la montaña.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430571,microsoft/ML-For-Beginners,859063464,813196248,Stevengez,53009062,2022-02-23T18:34:40+00:00,2022-02-23T18:39:15+00:00,```suggestion\r\n> **Nota**: Es probable que ajustar los hiperparámetros sea necesario para hacer que el algoritmo converja.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430572,microsoft/ML-For-Beginners,859063464,813198311,Stevengez,53009062,2022-02-23T18:37:22+00:00,2022-02-23T18:40:06+00:00,"```suggestion\r\n|          | El algoritmo Q-Learning se adaptó de forma exitosa a partir del ejemplo CartPole, con mínimas modificaciones al código, el cual es capaz de resolver el problema al capturar la bandera con menos de 200 pasos. | Se adoptó un nuevo algoritmo Q-Learning de internet, pero está bien documentado; o se adoptó el algoritmo existente, pero no cumple los resultados deseados | El estudiante no fue capaz de adoptar ningún algoritmo de forma exitosa, pero ha hecho pasos substanciales hacia la solución (implementó la discretización de estado, la estructura de datos de Q-Table, etc.) |\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430573,microsoft/ML-For-Beginners,859063464,813187254,Stevengez,53009062,2022-02-23T18:25:17+00:00,2022-02-23T18:39:15+00:00,"```suggestion\r\nEl truco principal de este problema es, no obstante, que el motor del carrito no es lo suficientemente potente para escalar la montaña en una sola pasada. Por lo tanto, la única forma de lograrlo es conducir hacia atrás y adelante para generar impulso.\r\n```",true,8-Reinforcement/2-Gym/translations/assignment.es.md,NaN,"@@ -1,43 +1,44 @@\n-# Train Mountain Car\n+# Entrena el carrito de montaña\n \n-[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n+[OpenAI Gym](http://gym.openai.com) ha sido diseñado de tal forma que todos los ambientes proveen la misma API - esto es, los mismos métodos `reset`, `step` y `render`, y las mismas abstracciones de **action space** y **observation space**. Así sería posible adaptar los mismos algoritmos de aprendizaje reforzado a diferentes ambientes con mínimos cambios al código.\n \n-## A Mountain Car Environment\n+## Un ambiente de carrito de montaña\n \n-[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n+El [ambiente de carrito de montaña](https://gym.openai.com/envs/MountainCar-v0/) contiene un carrito atrapado en un valle:\n \n-<img src=""images/mountaincar.png"" width=""300""/>\n+<img src=""../images/mountaincar.png"" width=""300""/>\n \n-The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n+El objetivo es salir del valle y capturar la bandera, al hacer a cada paso una de las siguientes acciones:\n \n-| Value | Meaning |\n+| Valor | Significado |\n |---|---|\n-| 0 | Accelerate to the left |\n-| 1 | Do not accelerate |\n-| 2 | Accelerate to the right |\n+| 0 | Acelerar a la izquierda |\n+| 1 | No acelerar |\n+| 2 | Acelerar a la derecha |\n \n-The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n+El truco principal de este problema es, no obstante, que el motor del carrito no es lo suficientemente potente para escalar la montaña en una sola pasada. Por lo tanto, la única forma de lograrlo es conducir de hacia atrás y  adelante para generar impulso.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430574,microsoft/ML-For-Beginners,859063464,813190105,Stevengez,53009062,2022-02-23T18:28:34+00:00,2022-02-23T18:39:15+00:00,```suggestion\r\nEl sistema de recompensas para el carrito de montaña es engañoso:\r\n```,true,8-Reinforcement/2-Gym/translations/assignment.es.md,NaN,"@@ -1,43 +1,44 @@\n-# Train Mountain Car\n+# Entrena el carrito de montaña\n \n-[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n+[OpenAI Gym](http://gym.openai.com) ha sido diseñado de tal forma que todos los ambientes proveen la misma API - esto es, los mismos métodos `reset`, `step` y `render`, y las mismas abstracciones de **action space** y **observation space**. Así sería posible adaptar los mismos algoritmos de aprendizaje reforzado a diferentes ambientes con mínimos cambios al código.\n \n-## A Mountain Car Environment\n+## Un ambiente de carrito de montaña\n \n-[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n+El [ambiente de carrito de montaña](https://gym.openai.com/envs/MountainCar-v0/) contiene un carrito atrapado en un valle:\n \n-<img src=""images/mountaincar.png"" width=""300""/>\n+<img src=""../images/mountaincar.png"" width=""300""/>\n \n-The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n+El objetivo es salir del valle y capturar la bandera, al hacer a cada paso una de las siguientes acciones:\n \n-| Value | Meaning |\n+| Valor | Significado |\n |---|---|\n-| 0 | Accelerate to the left |\n-| 1 | Do not accelerate |\n-| 2 | Accelerate to the right |\n+| 0 | Acelerar a la izquierda |\n+| 1 | No acelerar |\n+| 2 | Acelerar a la derecha |\n \n-The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n+El truco principal de este problema es, no obstante, que el motor del carrito no es lo suficientemente potente para escalar la montaña en una sola pasada. Por lo tanto, la única forma de lograrlo es conducir de hacia atrás y  adelante para generar impulso.\n \n-Observation space consists of just two values:\n+El espacio de observación consiste de sólo dos valores:\n \n-| Num | Observation  | Min | Max |\n-|-----|--------------|-----|-----|\n-|  0  | Car Position | -1.2| 0.6 |\n-|  1  | Car Velocity | -0.07 | 0.07 |\n+| Num | Observación           | Min   | Max  |\n+|-----|-----------------------|-------|------|\n+|  0  | Posición del carrito   | -1.2  | 0.6  |\n+|  1  | Velocidad del carrito | -0.07 | 0.07 |\n \n-Reward system for the mountain car is rather tricky:\n+El sistema de recompensas para el carrito de montaña es bastante difícil:",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430575,microsoft/ML-For-Beginners,859063464,813193111,Stevengez,53009062,2022-02-23T18:31:33+00:00,2022-02-23T18:39:15+00:00,```suggestion\r\n* La recompensa de 0 es otorgada si el agente alcanzó la bandera (posición = 0.5) en la cima de la montaña.\r\n```,true,8-Reinforcement/2-Gym/translations/assignment.es.md,NaN,"@@ -1,43 +1,44 @@\n-# Train Mountain Car\n+# Entrena el carrito de montaña\n \n-[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n+[OpenAI Gym](http://gym.openai.com) ha sido diseñado de tal forma que todos los ambientes proveen la misma API - esto es, los mismos métodos `reset`, `step` y `render`, y las mismas abstracciones de **action space** y **observation space**. Así sería posible adaptar los mismos algoritmos de aprendizaje reforzado a diferentes ambientes con mínimos cambios al código.\n \n-## A Mountain Car Environment\n+## Un ambiente de carrito de montaña\n \n-[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n+El [ambiente de carrito de montaña](https://gym.openai.com/envs/MountainCar-v0/) contiene un carrito atrapado en un valle:\n \n-<img src=""images/mountaincar.png"" width=""300""/>\n+<img src=""../images/mountaincar.png"" width=""300""/>\n \n-The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n+El objetivo es salir del valle y capturar la bandera, al hacer a cada paso una de las siguientes acciones:\n \n-| Value | Meaning |\n+| Valor | Significado |\n |---|---|\n-| 0 | Accelerate to the left |\n-| 1 | Do not accelerate |\n-| 2 | Accelerate to the right |\n+| 0 | Acelerar a la izquierda |\n+| 1 | No acelerar |\n+| 2 | Acelerar a la derecha |\n \n-The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n+El truco principal de este problema es, no obstante, que el motor del carrito no es lo suficientemente potente para escalar la montaña en una sola pasada. Por lo tanto, la única forma de lograrlo es conducir de hacia atrás y  adelante para generar impulso.\n \n-Observation space consists of just two values:\n+El espacio de observación consiste de sólo dos valores:\n \n-| Num | Observation  | Min | Max |\n-|-----|--------------|-----|-----|\n-|  0  | Car Position | -1.2| 0.6 |\n-|  1  | Car Velocity | -0.07 | 0.07 |\n+| Num | Observación           | Min   | Max  |\n+|-----|-----------------------|-------|------|\n+|  0  | Posición del carrito   | -1.2  | 0.6  |\n+|  1  | Velocidad del carrito | -0.07 | 0.07 |\n \n-Reward system for the mountain car is rather tricky:\n+El sistema de recompensas para el carrito de montaña es bastante difícil:\n \n- * Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.\n- * Reward of -1 is awarded if the position of the agent is less than 0.5.\n+* La recompensa de 0 es otorgada si el agente alcanzó la bandera (position = 0.5) en la cima de la montaña.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430576,microsoft/ML-For-Beginners,859063464,813196248,Stevengez,53009062,2022-02-23T18:34:40+00:00,2022-02-23T18:39:15+00:00,```suggestion\r\n> **Nota**: Es probable que ajustar los hiperparámetros sea necesario para hacer que el algoritmo converja.\r\n```,true,8-Reinforcement/2-Gym/translations/assignment.es.md,NaN,"@@ -1,43 +1,44 @@\n-# Train Mountain Car\n+# Entrena el carrito de montaña\n \n-[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n+[OpenAI Gym](http://gym.openai.com) ha sido diseñado de tal forma que todos los ambientes proveen la misma API - esto es, los mismos métodos `reset`, `step` y `render`, y las mismas abstracciones de **action space** y **observation space**. Así sería posible adaptar los mismos algoritmos de aprendizaje reforzado a diferentes ambientes con mínimos cambios al código.\n \n-## A Mountain Car Environment\n+## Un ambiente de carrito de montaña\n \n-[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n+El [ambiente de carrito de montaña](https://gym.openai.com/envs/MountainCar-v0/) contiene un carrito atrapado en un valle:\n \n-<img src=""images/mountaincar.png"" width=""300""/>\n+<img src=""../images/mountaincar.png"" width=""300""/>\n \n-The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n+El objetivo es salir del valle y capturar la bandera, al hacer a cada paso una de las siguientes acciones:\n \n-| Value | Meaning |\n+| Valor | Significado |\n |---|---|\n-| 0 | Accelerate to the left |\n-| 1 | Do not accelerate |\n-| 2 | Accelerate to the right |\n+| 0 | Acelerar a la izquierda |\n+| 1 | No acelerar |\n+| 2 | Acelerar a la derecha |\n \n-The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n+El truco principal de este problema es, no obstante, que el motor del carrito no es lo suficientemente potente para escalar la montaña en una sola pasada. Por lo tanto, la única forma de lograrlo es conducir de hacia atrás y  adelante para generar impulso.\n \n-Observation space consists of just two values:\n+El espacio de observación consiste de sólo dos valores:\n \n-| Num | Observation  | Min | Max |\n-|-----|--------------|-----|-----|\n-|  0  | Car Position | -1.2| 0.6 |\n-|  1  | Car Velocity | -0.07 | 0.07 |\n+| Num | Observación           | Min   | Max  |\n+|-----|-----------------------|-------|------|\n+|  0  | Posición del carrito   | -1.2  | 0.6  |\n+|  1  | Velocidad del carrito | -0.07 | 0.07 |\n \n-Reward system for the mountain car is rather tricky:\n+El sistema de recompensas para el carrito de montaña es bastante difícil:\n \n- * Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.\n- * Reward of -1 is awarded if the position of the agent is less than 0.5.\n+* La recompensa de 0 es otorgada si el agente alcanzó la bandera (position = 0.5) en la cima de la montaña.\n+* La recompensa de -1 es otorgada si la posición del agente es menos de 0.5.\n \n-Episode terminates if the car position is more than 0.5, or episode length is greater than 200.\n-## Instructions\n+El episodio termina si la posición del carrito es más de 0.5, o la longitud del episodio es mayor que 200.\n \n-Adapt our reinforcement learning algorithm to solve the mountain car problem. Start with existing [notebook.ipynb](notebook.ipynb) code, substitute new environment, change state discretization functions, and try to make existing algorithm to train with minimal code modifications. Optimize the result by adjusting hyperparameters.\n+## Instrucciones\n \n-> **Note**: Hyperparameters adjustment is likely to be needed to make algorithm converge. \n-## Rubric\n+Adapta nuestro algoritmo de aprendizaje reforzado para resolver el problema del carrito de montaña. Comienza con el código existente en [notebook.ipynb](../notebook.ipynb), substituye el nuevo ambiente, cambia las funciones de discretización de estado, e intenta hacer que el algoritmo existente entrene con mínimas modificaciones al código. Optimiza el resultado al ajustar los hiperparámetros.\n \n-| Criteria | Exemplary | Adequate | Needs Improvement |\n+> **Nota**: El ajuste de los hiperparámetros es probable sea necesario para hacer que el algoritmo converja.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430577,microsoft/ML-For-Beginners,859063464,813198311,Stevengez,53009062,2022-02-23T18:37:22+00:00,2022-02-23T18:40:06+00:00,"```suggestion\r\n|          | El algoritmo Q-Learning se adaptó de forma exitosa a partir del ejemplo CartPole, con mínimas modificaciones al código, el cual es capaz de resolver el problema al capturar la bandera con menos de 200 pasos. | Se adoptó un nuevo algoritmo Q-Learning de internet, pero está bien documentado; o se adoptó el algoritmo existente, pero no cumple los resultados deseados | El estudiante no fue capaz de adoptar ningún algoritmo de forma exitosa, pero ha hecho pasos substanciales hacia la solución (implementó la discretización de estado, la estructura de datos de Q-Table, etc.) |\r\n```",true,8-Reinforcement/2-Gym/translations/assignment.es.md,NaN,"@@ -1,43 +1,44 @@\n-# Train Mountain Car\n+# Entrena el carrito de montaña\n \n-[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n+[OpenAI Gym](http://gym.openai.com) ha sido diseñado de tal forma que todos los ambientes proveen la misma API - esto es, los mismos métodos `reset`, `step` y `render`, y las mismas abstracciones de **action space** y **observation space**. Así sería posible adaptar los mismos algoritmos de aprendizaje reforzado a diferentes ambientes con mínimos cambios al código.\n \n-## A Mountain Car Environment\n+## Un ambiente de carrito de montaña\n \n-[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n+El [ambiente de carrito de montaña](https://gym.openai.com/envs/MountainCar-v0/) contiene un carrito atrapado en un valle:\n \n-<img src=""images/mountaincar.png"" width=""300""/>\n+<img src=""../images/mountaincar.png"" width=""300""/>\n \n-The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n+El objetivo es salir del valle y capturar la bandera, al hacer a cada paso una de las siguientes acciones:\n \n-| Value | Meaning |\n+| Valor | Significado |\n |---|---|\n-| 0 | Accelerate to the left |\n-| 1 | Do not accelerate |\n-| 2 | Accelerate to the right |\n+| 0 | Acelerar a la izquierda |\n+| 1 | No acelerar |\n+| 2 | Acelerar a la derecha |\n \n-The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n+El truco principal de este problema es, no obstante, que el motor del carrito no es lo suficientemente potente para escalar la montaña en una sola pasada. Por lo tanto, la única forma de lograrlo es conducir de hacia atrás y  adelante para generar impulso.\n \n-Observation space consists of just two values:\n+El espacio de observación consiste de sólo dos valores:\n \n-| Num | Observation  | Min | Max |\n-|-----|--------------|-----|-----|\n-|  0  | Car Position | -1.2| 0.6 |\n-|  1  | Car Velocity | -0.07 | 0.07 |\n+| Num | Observación           | Min   | Max  |\n+|-----|-----------------------|-------|------|\n+|  0  | Posición del carrito   | -1.2  | 0.6  |\n+|  1  | Velocidad del carrito | -0.07 | 0.07 |\n \n-Reward system for the mountain car is rather tricky:\n+El sistema de recompensas para el carrito de montaña es bastante difícil:\n \n- * Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.\n- * Reward of -1 is awarded if the position of the agent is less than 0.5.\n+* La recompensa de 0 es otorgada si el agente alcanzó la bandera (position = 0.5) en la cima de la montaña.\n+* La recompensa de -1 es otorgada si la posición del agente es menos de 0.5.\n \n-Episode terminates if the car position is more than 0.5, or episode length is greater than 200.\n-## Instructions\n+El episodio termina si la posición del carrito es más de 0.5, o la longitud del episodio es mayor que 200.\n \n-Adapt our reinforcement learning algorithm to solve the mountain car problem. Start with existing [notebook.ipynb](notebook.ipynb) code, substitute new environment, change state discretization functions, and try to make existing algorithm to train with minimal code modifications. Optimize the result by adjusting hyperparameters.\n+## Instrucciones\n \n-> **Note**: Hyperparameters adjustment is likely to be needed to make algorithm converge. \n-## Rubric\n+Adapta nuestro algoritmo de aprendizaje reforzado para resolver el problema del carrito de montaña. Comienza con el código existente en [notebook.ipynb](../notebook.ipynb), substituye el nuevo ambiente, cambia las funciones de discretización de estado, e intenta hacer que el algoritmo existente entrene con mínimas modificaciones al código. Optimiza el resultado al ajustar los hiperparámetros.\n \n-| Criteria | Exemplary | Adequate | Needs Improvement |\n+> **Nota**: El ajuste de los hiperparámetros es probable sea necesario para hacer que el algoritmo converja.\n+## Rúbrica\n+\n+| Criterio | Ejemplar | Adecuado | Necesita mejorar |\n | -------- | --------- | -------- | ----------------- |\n-|          | Q-Learning algorithm is successfully adapted from CartPole example, with minimal code modifications, which is able to solve the problem of capturing the flag under 200 steps. | A new Q-Learning algorithm has been adopted from the Internet, but is well-documented; or existing algorithm adopted, but does not reach desired results | Student was not able to successfully adopt any algorithm, but has mede substantial steps towards solution (implemented state discretization, Q-Table data structure, etc.) |\n+|          | El algoritmo Q-Learning se adaptó de forma exitosa a partir del ejemplo CartPole, con mínimas modificaciones al código, el cual es capaz de resolver el problema al capturar la bandera con menos de 200 pasos. | Se adoptó un nuevo algoritmo Q-Learning de internet, pero está bien documentado; o se adoptó el algoritmo existente, pero no cumple los resultados deseados | El estudiante no fue capaz de adoptar algún algoritmo de forma exitosa, pero ha hecho pasos substanciales hacia la solución (implementó la discretización de estado, la estructura de datos de Q-Table, etc.) |",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430578,microsoft/ML-For-Beginners,859045385,813223217,Stevengez,53009062,2022-02-23T19:01:22+00:00,2022-02-23T19:28:46+00:00,```suggestion\r\n3. Pedro puede deshacerse de la fatiga al descansar bajo un árbol o en el pasto (ej. caminar en una ubicación del tablero con un árbol o pasto - campo verde)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430579,microsoft/ML-For-Beginners,859045385,813228651,Stevengez,53009062,2022-02-23T19:07:49+00:00,2022-02-23T19:28:46+00:00,"```suggestion\r\n> **Nota**: En tu nuevo mundo, el estado es más complejo, y además a la posición humana también incluye la fatiga y los niveles de energía. Puedes optar por representar el estado como una tupla (tablero, energía, fatiga), o definir una clase para el estado (también puedes querer derivarlo de `Board`), o incluso modifica la clase original `Board` dentro de [rlboard.py](../rlboard.py).\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843057a,microsoft/ML-For-Beginners,859045385,813244298,Stevengez,53009062,2022-02-23T19:28:12+00:00,2022-02-23T19:28:46+00:00,"```suggestion\r\n|          | Se presentó un notebook con la definición de las nuevas reglas del mundo. El algoritmo Q-Learning y algunas explicaciones textuales. Q-Learning es capaz de mejorar significativamente los resultados comparado con la caminata aleatoria. | Se presentó un notebook, Q-Learning se implementó y mejoró los resultados comparado con la caminata aleatoria, pero no de forma significativa; o el notebook está pobremente documentado y el código no está bien estructurado | Se hicieron algunos intentos para redefinir las reglas del mundo, pero el algoritmo de Q-Learning no funciona, o la función reward no está totalmente definida |\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843057b,microsoft/ML-For-Beginners,859045385,813223217,Stevengez,53009062,2022-02-23T19:01:22+00:00,2022-02-23T19:28:46+00:00,```suggestion\r\n3. Pedro puede deshacerse de la fatiga al descansar bajo un árbol o en el pasto (ej. caminar en una ubicación del tablero con un árbol o pasto - campo verde)\r\n```,true,8-Reinforcement/1-QLearning/translations/assignment.es.md,NaN,"@@ -1,25 +1,27 @@\n-# A More Realistic World\n+# Un mundo más realista\n \n-In our situation, Peter was able to move around almost without getting tired or hungry. In a more realistic world, we has to sit down and rest from time to time, and also to feed himself. Let's make our world more realistic, by implementing the following rules:\n+En nuestro caso, Pedro (Peter) fue capaz de moverse y casi sin cansarse o estar hambriento. En un mundo más realista, él tiene que sentarse y descansar de vez en cuando, así como también alimentarse. Hagamos nuestro mundo más realista, al implementar las siguientes reglas:\n \n-1. By moving from one place to another, Peter loses **energy** and gains some **fatigue**.\n-2. Peter can gain more energy by eating apples.\n-3. Peter can get rid of fatigue by resting under the tree or on the grass (i.e. walking into a board location with a tree or grass - green field)\n-4. Peter needs to find and kill the wolf\n-5. In order to kill the wolf, Peter needs to have certain levels of energy and fatigue, otherwise he loses the battle.\n-## Instructions\n+1. Al moverse de un lugar a otro, Pedro pierde **energía** y obtiene algo de **fatiga**.\n+2. Pedro puede adquirir más energía al comer manzanas.\n+3. Pedro puede deshacerse de la fatiga al descansar bajo un árbol o en el pasto (esto es, entrar en una ubicación del tablero con un árbol o pasto - campo verde)",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843057c,microsoft/ML-For-Beginners,859045385,813228651,Stevengez,53009062,2022-02-23T19:07:49+00:00,2022-02-23T19:28:46+00:00,"```suggestion\r\n> **Nota**: En tu nuevo mundo, el estado es más complejo, y además a la posición humana también incluye la fatiga y los niveles de energía. Puedes optar por representar el estado como una tupla (tablero, energía, fatiga), o definir una clase para el estado (también puedes querer derivarlo de `Board`), o incluso modifica la clase original `Board` dentro de [rlboard.py](../rlboard.py).\r\n```",true,8-Reinforcement/1-QLearning/translations/assignment.es.md,NaN,"@@ -1,25 +1,27 @@\n-# A More Realistic World\n+# Un mundo más realista\n \n-In our situation, Peter was able to move around almost without getting tired or hungry. In a more realistic world, we has to sit down and rest from time to time, and also to feed himself. Let's make our world more realistic, by implementing the following rules:\n+En nuestro caso, Pedro (Peter) fue capaz de moverse y casi sin cansarse o estar hambriento. En un mundo más realista, él tiene que sentarse y descansar de vez en cuando, así como también alimentarse. Hagamos nuestro mundo más realista, al implementar las siguientes reglas:\n \n-1. By moving from one place to another, Peter loses **energy** and gains some **fatigue**.\n-2. Peter can gain more energy by eating apples.\n-3. Peter can get rid of fatigue by resting under the tree or on the grass (i.e. walking into a board location with a tree or grass - green field)\n-4. Peter needs to find and kill the wolf\n-5. In order to kill the wolf, Peter needs to have certain levels of energy and fatigue, otherwise he loses the battle.\n-## Instructions\n+1. Al moverse de un lugar a otro, Pedro pierde **energía** y obtiene algo de **fatiga**.\n+2. Pedro puede adquirir más energía al comer manzanas.\n+3. Pedro puede deshacerse de la fatiga al descansar bajo un árbol o en el pasto (esto es, entrar en una ubicación del tablero con un árbol o pasto - campo verde)\n+4. Pedro necesita encontrar y matar al lobo\n+5. Con el fin de matar al lobo, Pedro necesita tener ciertos niveles de energía y fatiga, de lo contrario pierde la batalla.\n \n-Use the original [notebook.ipynb](notebook.ipynb) notebook as a starting point for your solution.\n+## Instrucciones\n \n-Modify the reward function above according to the rules of the game, run the reinforcement learning algorithm to learn the best strategy for winning the game, and compare the results of random walk with your algorithm in terms of number of games won and lost.\n+Usa el notebook original [notebook.ipynb](../notebook.ipynb) como punto de partida para tu solución.\n \n-> **Note**: In your new world, the state is more complex, and in addition to human position also includes fatigue and energy levels. You may chose to represent the state as a tuple (Board,energy,fatigue), or define a class for the state (you may also want to derive it from `Board`), or even modify the original `Board` class inside [rlboard.py](rlboard.py).\n+Modifica la función reward de arriba de acuerdo a las reglas del juego, ejecuta el algoritmo de aprendizaje reforzado para aprender la mejor estrategia para ganar el juego, y compara los resultados de caminata aleatoria con tu algoritmo en términos de el número de juegos ganados y perdidos.\n \n-In your solution, please keep the code responsible for random walk strategy, and compare the results of your algorithm with random walk at the end.\n+> **Nota**: En tu nuevo mundo, el estado es más complejo, y además a la posición humana también incluye la fatiga y los niveles de energía. Puedes optar para representar el estado como una tupla (tablero, energía, fatiga), o define una clase para el estado (también puedes querer derivarlo de `Board`), o incluso modifica la clase original `Board` dentro de [rlboard.py](../rlboard.py).",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843057d,microsoft/ML-For-Beginners,859045385,813244298,Stevengez,53009062,2022-02-23T19:28:12+00:00,2022-02-23T19:28:46+00:00,"```suggestion\r\n|          | Se presentó un notebook con la definición de las nuevas reglas del mundo. El algoritmo Q-Learning y algunas explicaciones textuales. Q-Learning es capaz de mejorar significativamente los resultados comparado con la caminata aleatoria. | Se presentó un notebook, Q-Learning se implementó y mejoró los resultados comparado con la caminata aleatoria, pero no de forma significativa; o el notebook está pobremente documentado y el código no está bien estructurado | Se hicieron algunos intentos para redefinir las reglas del mundo, pero el algoritmo de Q-Learning no funciona, o la función reward no está totalmente definida |\r\n```",true,8-Reinforcement/1-QLearning/translations/assignment.es.md,NaN,"@@ -1,25 +1,27 @@\n-# A More Realistic World\n+# Un mundo más realista\n \n-In our situation, Peter was able to move around almost without getting tired or hungry. In a more realistic world, we has to sit down and rest from time to time, and also to feed himself. Let's make our world more realistic, by implementing the following rules:\n+En nuestro caso, Pedro (Peter) fue capaz de moverse y casi sin cansarse o estar hambriento. En un mundo más realista, él tiene que sentarse y descansar de vez en cuando, así como también alimentarse. Hagamos nuestro mundo más realista, al implementar las siguientes reglas:\n \n-1. By moving from one place to another, Peter loses **energy** and gains some **fatigue**.\n-2. Peter can gain more energy by eating apples.\n-3. Peter can get rid of fatigue by resting under the tree or on the grass (i.e. walking into a board location with a tree or grass - green field)\n-4. Peter needs to find and kill the wolf\n-5. In order to kill the wolf, Peter needs to have certain levels of energy and fatigue, otherwise he loses the battle.\n-## Instructions\n+1. Al moverse de un lugar a otro, Pedro pierde **energía** y obtiene algo de **fatiga**.\n+2. Pedro puede adquirir más energía al comer manzanas.\n+3. Pedro puede deshacerse de la fatiga al descansar bajo un árbol o en el pasto (esto es, entrar en una ubicación del tablero con un árbol o pasto - campo verde)\n+4. Pedro necesita encontrar y matar al lobo\n+5. Con el fin de matar al lobo, Pedro necesita tener ciertos niveles de energía y fatiga, de lo contrario pierde la batalla.\n \n-Use the original [notebook.ipynb](notebook.ipynb) notebook as a starting point for your solution.\n+## Instrucciones\n \n-Modify the reward function above according to the rules of the game, run the reinforcement learning algorithm to learn the best strategy for winning the game, and compare the results of random walk with your algorithm in terms of number of games won and lost.\n+Usa el notebook original [notebook.ipynb](../notebook.ipynb) como punto de partida para tu solución.\n \n-> **Note**: In your new world, the state is more complex, and in addition to human position also includes fatigue and energy levels. You may chose to represent the state as a tuple (Board,energy,fatigue), or define a class for the state (you may also want to derive it from `Board`), or even modify the original `Board` class inside [rlboard.py](rlboard.py).\n+Modifica la función reward de arriba de acuerdo a las reglas del juego, ejecuta el algoritmo de aprendizaje reforzado para aprender la mejor estrategia para ganar el juego, y compara los resultados de caminata aleatoria con tu algoritmo en términos de el número de juegos ganados y perdidos.\n \n-In your solution, please keep the code responsible for random walk strategy, and compare the results of your algorithm with random walk at the end.\n+> **Nota**: En tu nuevo mundo, el estado es más complejo, y además a la posición humana también incluye la fatiga y los niveles de energía. Puedes optar para representar el estado como una tupla (tablero, energía, fatiga), o define una clase para el estado (también puedes querer derivarlo de `Board`), o incluso modifica la clase original `Board` dentro de [rlboard.py](../rlboard.py).\n \n-> **Note**: You may need to adjust hyperparameters to make it work, especially the number of epochs. Because the success of the game (fighting the wolf) is a rare event, you can expect much longer training time.\n-## Rubric\n+En tu solución, mantén el código responsable de la estrategia de caminata aleatoria, y compara los resultados de tu algoritmo con la caminata aleatoria al final.\n \n-| Criteria | Exemplary                                                                                                                                                                                             | Adequate                                                                                                                                                                                | Needs Improvement                                                                                                                          |\n+> **Nota**: Puedes necesitar ajustar los hiperparámetros para hacerlo funcionar, especialmente el número de épocas. Porque el éxito del juego (pelear contra el lobo) es un evento raro, puedes esperar un tiempo de entrenamiento mayor.\n+\n+## Rúbrica\n+\n+| Criterio | Ejemplar                                                                                                                                                                                             | Adecuado                                                                                                                                                                                | Necesita mejorar                                                                                                                          |\n | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |\n-|          | A notebook is presented with the definition of new world rules, Q-Learning algorithm and some textual explanations. Q-Learning is able to significantly improve the results comparing to random walk. | Notebook is presented, Q-Learning is implemented and improves results comparing to random walk, but not significantly; or notebook is poorly documented and code is not well-structured | Some attempt to re-define the rules of the world are made, but Q-Learning algorithm does not work, or reward function is not fully defined |\n+|          | Se presentó un notebook con la definición de las nuevas reglas del mundo. El algoritmo Q-Learning y algunas explicaciones textuales. Q-Learning es capa de mejorar significativamente los resultados comparado con la caminata aleatoria. | Se presentó un notebook, Q-Learning se implementó y mejoró los resultados comparado con la caminata aleatoria, pero no de forma significativa; o el notebook está pobremente documentado y el código no está bien estructurado | Se hicieron algunos intentos para redefinir las reglas del mundo, pero el algoritmo de Q-Learning no funciona, o la función reward no está totalmente definida |",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843057e,microsoft/ML-For-Beginners,858677883,813306379,madebygps,6733686,2022-02-23T20:50:34+00:00,2022-02-23T20:50:54+00:00,One suggestion:\r\n\r\nTambién prueba ajustar distintos hiperparámetros y valores para los pasos de tiempo.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843057f,microsoft/ML-For-Beginners,858677883,813353183,hereje,3022824,2022-02-23T21:51:13+00:00,2022-02-23T21:51:14+00:00,Done!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430580,microsoft/ML-For-Beginners,858677883,813306379,madebygps,6733686,2022-02-23T20:50:34+00:00,2022-02-23T20:50:54+00:00,One suggestion:\r\n\r\nTambién prueba ajustar distintos hiperparámetros y valores para los pasos de tiempo.,true,7-TimeSeries/3-SVR/translations/assignment.es.md,NaN,"@@ -0,0 +1,13 @@\n+# Un nuevo modelo SVR\n+\n+## Instrucciones [^1]\n+\n+Ahora que has construido un modelo SVR, construye uno nuevo con datos frescos (prueba uno de [estos conjuntos de datos de Duke](http://www2.stat.duke.edu/~mw/ts_data_sets.html)). Anota tu trabajo en un notebook. Visualiza los datos y tu modelo, y prueba su precisión usando MAPE y gráficos apropiados. También prueba ajustar distintos hiperparámetros y también usa distintos valores para los pasos de tiempo.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430581,microsoft/ML-For-Beginners,858677883,813353183,hereje,3022824,2022-02-23T21:51:13+00:00,2022-02-23T21:51:14+00:00,Done!,true,7-TimeSeries/3-SVR/translations/assignment.es.md,NaN,"@@ -0,0 +1,13 @@\n+# Un nuevo modelo SVR\n+\n+## Instrucciones [^1]\n+\n+Ahora que has construido un modelo SVR, construye uno nuevo con datos frescos (prueba uno de [estos conjuntos de datos de Duke](http://www2.stat.duke.edu/~mw/ts_data_sets.html)). Anota tu trabajo en un notebook. Visualiza los datos y tu modelo, y prueba su precisión usando MAPE y gráficos apropiados. También prueba ajustar distintos hiperparámetros y también usa distintos valores para los pasos de tiempo.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430582,microsoft/ML-For-Beginners,858671406,813268691,Stevengez,53009062,2022-02-23T19:59:59+00:00,2022-02-23T20:09:32+00:00,```suggestion\r\n# Prueba un conjunto de datos diferente\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430583,microsoft/ML-For-Beginners,858671406,813274004,Stevengez,53009062,2022-02-23T20:07:05+00:00,2022-02-23T20:09:32+00:00,"```suggestion\r\nHas comenzado a aprender acerca de la predicción de series de tiempo al mirar el tipo de datos que requiere este modelado especial. Ya has visualizado algunos datos referentes a la energía. Ahora, busca otros datos que te beneficiarían de la predicción de series de tiempo. Encuentra tres ejemplos (prueba [Kaggle](https://kaggle.com) y [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/catalog/?WT.mc_id=academic-15963-cxa)) y crea un notebook para visualizarlos. Anota cualquier característica especial que tengan (estacionalidad, cambios abruptos, u otras tendencias) en el notebook.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430584,microsoft/ML-For-Beginners,858671406,813274957,Stevengez,53009062,2022-02-23T20:08:24+00:00,2022-02-23T20:09:33+00:00,```suggestion\r\n|          | Se graficaron y explicaron tres conjuntos de datos en un notebook | Se graficaron y explicaron dos conjuntos de datos en un notebook | Se graficaron y explicaron pocos conjuntos de datos en un notebook o los datos presentados fueron insuficientes |\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430585,microsoft/ML-For-Beginners,858671406,813268691,Stevengez,53009062,2022-02-23T19:59:59+00:00,2022-02-23T20:09:32+00:00,```suggestion\r\n# Prueba un conjunto de datos diferente\r\n```,true,6-NLP/5-Hotel-Reviews-2/translations/assignment.es.md,NaN,"@@ -1,11 +1,11 @@\n-# Try a different dataset\n+# Prueba un conjunto de dato diferente",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430586,microsoft/ML-For-Beginners,858671406,813274004,Stevengez,53009062,2022-02-23T20:07:05+00:00,2022-02-23T20:09:32+00:00,"```suggestion\r\nHas comenzado a aprender acerca de la predicción de series de tiempo al mirar el tipo de datos que requiere este modelado especial. Ya has visualizado algunos datos referentes a la energía. Ahora, busca otros datos que te beneficiarían de la predicción de series de tiempo. Encuentra tres ejemplos (prueba [Kaggle](https://kaggle.com) y [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/catalog/?WT.mc_id=academic-15963-cxa)) y crea un notebook para visualizarlos. Anota cualquier característica especial que tengan (estacionalidad, cambios abruptos, u otras tendencias) en el notebook.\r\n```",true,7-TimeSeries/1-Introduction/translations/assignment.es.md,NaN,"@@ -1,11 +1,11 @@\n-# Visualize some more Time Series\n+# Visualiza algunas series de tiempo más\n \n-## Instructions\n+## Instrucciones\n \n-You've begun to learn about Time Series Forecasting by looking at the type of data that requires this special modeling. You've visualized some data around energy. Now, look around for some other data that would benefit from Time Series Forecasting. Find three examples (try [Kaggle](https://kaggle.com) and [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/catalog/?WT.mc_id=academic-15963-cxa)) and create a notebook to visualize them. Notate any special characteristics they have (seasonality, abrupt changes, or other trends) in the notebook.\n+Has comenzado a aprender acerca de la predicción de series de tiempo al mirar el tipo de datos que requiere este modelado especial. Ya has visualizado algunos datos referentes a la energía. Ahora, mira alrededor para algunos otros datos que te beneficiarían de la predicción de series de tiempo. Encuentra tres ejemplos (prueba [Kaggle](https://kaggle.com) y [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/catalog/?WT.mc_id=academic-15963-cxa)) y crea un notebook para visualizarlos. Anota cualquier característica especial que tengan (estacionalidad, cambios abruptos, u otras tendencias) en el notebook.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430587,microsoft/ML-For-Beginners,858671406,813274957,Stevengez,53009062,2022-02-23T20:08:24+00:00,2022-02-23T20:09:33+00:00,```suggestion\r\n|          | Se graficaron y explicaron tres conjuntos de datos en un notebook | Se graficaron y explicaron dos conjuntos de datos en un notebook | Se graficaron y explicaron pocos conjuntos de datos en un notebook o los datos presentados fueron insuficientes |\r\n```,true,7-TimeSeries/1-Introduction/translations/assignment.es.md,NaN,"@@ -1,11 +1,11 @@\n-# Visualize some more Time Series\n+# Visualiza algunas series de tiempo más\n \n-## Instructions\n+## Instrucciones\n \n-You've begun to learn about Time Series Forecasting by looking at the type of data that requires this special modeling. You've visualized some data around energy. Now, look around for some other data that would benefit from Time Series Forecasting. Find three examples (try [Kaggle](https://kaggle.com) and [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/catalog/?WT.mc_id=academic-15963-cxa)) and create a notebook to visualize them. Notate any special characteristics they have (seasonality, abrupt changes, or other trends) in the notebook.\n+Has comenzado a aprender acerca de la predicción de series de tiempo al mirar el tipo de datos que requiere este modelado especial. Ya has visualizado algunos datos referentes a la energía. Ahora, mira alrededor para algunos otros datos que te beneficiarían de la predicción de series de tiempo. Encuentra tres ejemplos (prueba [Kaggle](https://kaggle.com) y [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/catalog/?WT.mc_id=academic-15963-cxa)) y crea un notebook para visualizarlos. Anota cualquier característica especial que tengan (estacionalidad, cambios abruptos, u otras tendencias) en el notebook.\n \n-## Rubric\n+## Rúbrica\n \n-| Criteria | Exemplary                                              | Adequate                                             | Needs Improvement                                                                         |\n+| Criterio | Ejemplar                                              | Adecuado                                             | Necesita mejorar                                                                         |\n | -------- | ------------------------------------------------------ | ---------------------------------------------------- | ----------------------------------------------------------------------------------------- |\n-|          | Three datasets are plotted and explained in a notebook | Two datasets are plotted and explained in a notebook | Few datasets are plotted or explained in a notebook or the data presented is insufficient |\n+|          | Se graficaron y explicaron tres conjuntos de datos en un notebook | Se graficaron y explicaron tres conjuntos de datos en un notebook | Se graficaron y explicaron pocos conjuntos de datos en un notebook o los datos presentados fueron insuficientes |",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430588,microsoft/ML-For-Beginners,858668702,813276879,Stevengez,53009062,2022-02-23T20:10:56+00:00,2022-02-23T20:12:43+00:00,```suggestion\r\n# Prueba un conjunto de datos diferente\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430589,microsoft/ML-For-Beginners,858668702,813276879,Stevengez,53009062,2022-02-23T20:10:56+00:00,2022-02-23T20:12:43+00:00,```suggestion\r\n# Prueba un conjunto de datos diferente\r\n```,true,6-NLP/5-Hotel-Reviews-2/translations/assignment.es.md,NaN,"@@ -1,11 +1,11 @@\n-# Try a different dataset\n+# Prueba un conjunto de dato diferente",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843058a,microsoft/ML-For-Beginners,858667229,813282046,Stevengez,53009062,2022-02-23T20:18:12+00:00,2022-02-23T20:18:41+00:00,"```suggestion\r\nNLTK es una biblioteca bien conocida para su uso en lingüística computacional y NLP. Toma esta oportunidad para leer el '[libro NLTK](https://www.nltk.org/book/)' y prueba sus ejercicios. En esta asignación sin ponderación, conocerás esta biblioteca más a fondo.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843058b,microsoft/ML-For-Beginners,858667229,813346919,hereje,3022824,2022-02-23T21:44:34+00:00,2022-02-23T21:44:34+00:00,Thanks!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843058c,microsoft/ML-For-Beginners,858667229,813282046,Stevengez,53009062,2022-02-23T20:18:12+00:00,2022-02-23T20:18:41+00:00,"```suggestion\r\nNLTK es una biblioteca bien conocida para su uso en lingüística computacional y NLP. Toma esta oportunidad para leer el '[libro NLTK](https://www.nltk.org/book/)' y prueba sus ejercicios. En esta asignación sin ponderación, conocerás esta biblioteca más a fondo.\r\n```",true,6-NLP/4-Hotel-Reviews-1/translations/assignment.es.md,NaN,"@@ -1,6 +1,5 @@\n # NLTK\n \n-## Instructions\n-\n-NLTK is a well-known library for use in computational linguistics and NLP. Take this opportunity to read through the '[NLTK book](https://www.nltk.org/book/)' and try out its exercises. In this ungraded assignment, you will get to know this library more deeply.\n+## Instrucciones\n \n+NLTK es una biblioteca bien conocida para su uso en lingüística computacional y NLP. Toma esta oportunidad para leer el '[libro NLTK](https://www.nltk.org/book/)' y prueba sus ejercicios. En esta asignación sin calificar, conocerás más profundamente esta biblioteca más a fondo.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843058d,microsoft/ML-For-Beginners,858667229,813346919,hereje,3022824,2022-02-23T21:44:34+00:00,2022-02-23T21:44:34+00:00,Thanks!,true,6-NLP/4-Hotel-Reviews-1/translations/assignment.es.md,NaN,"@@ -1,6 +1,5 @@\n # NLTK\n \n-## Instructions\n-\n-NLTK is a well-known library for use in computational linguistics and NLP. Take this opportunity to read through the '[NLTK book](https://www.nltk.org/book/)' and try out its exercises. In this ungraded assignment, you will get to know this library more deeply.\n+## Instrucciones\n \n+NLTK es una biblioteca bien conocida para su uso en lingüística computacional y NLP. Toma esta oportunidad para leer el '[libro NLTK](https://www.nltk.org/book/)' y prueba sus ejercicios. En esta asignación sin calificar, conocerás más profundamente esta biblioteca más a fondo.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843058e,microsoft/ML-For-Beginners,858665093,813307390,madebygps,6733686,2022-02-23T20:52:01+00:00,2022-02-23T20:52:01+00:00,Typo in AZure,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec7843058f,microsoft/ML-For-Beginners,858665093,813307390,madebygps,6733686,2022-02-23T20:52:01+00:00,2022-02-23T20:52:01+00:00,Typo in AZure,true,6-NLP/3-Translation-Sentiment/translations/assignment.es.md,NaN,"@@ -0,0 +1,11 @@\n+# Licencia poética\n+\n+## Instrucciones\n+\n+En [este notebook](https://www.kaggle.com/jenlooper/emily-dickinson-word-frequency) puedes encontrar más de 500 poemas de Emily Dickinson previamente analizados por sentimiento usando análisis de texto de AZure. Usando este conjunto de datos, analízalo usando las técnicas descritas en la lección. ¿El sentimiento sugerido de un poema coincide con la decisión más sofisticada del servicio de Azure? ¿En tu opinión por qué o por qué no? ¿Algo te sorprende?",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430590,microsoft/ML-For-Beginners,856376870,813293927,Stevengez,53009062,2022-02-23T20:34:46+00:00,2022-02-23T20:34:48+00:00,"```suggestion\r\nEn las lecciones anteriores, programaste un bot básico con el cual chatear. Este bot da respuestas aleatorias hasta que le digas 'bye'. ¿Puedes hacer las respuestas menos aleatorias, y activar respuestas si dices cosas específicas, como 'why' o 'how'? Piensa un poco en cómo el aprendizaje automático puede hacer este tipo de trabajo menos manual a medida que extiendes tu bot. Puedes usar las librerías NLTK o TextBlob para hacer tus tareas más fáciles.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430591,microsoft/ML-For-Beginners,856376870,813293927,Stevengez,53009062,2022-02-23T20:34:46+00:00,2022-02-23T20:34:48+00:00,"```suggestion\r\nEn las lecciones anteriores, programaste un bot básico con el cual chatear. Este bot da respuestas aleatorias hasta que le digas 'bye'. ¿Puedes hacer las respuestas menos aleatorias, y activar respuestas si dices cosas específicas, como 'why' o 'how'? Piensa un poco en cómo el aprendizaje automático puede hacer este tipo de trabajo menos manual a medida que extiendes tu bot. Puedes usar las librerías NLTK o TextBlob para hacer tus tareas más fáciles.\r\n```",true,6-NLP/2-Tasks/translations/assignment.es.md,NaN,"@@ -1,11 +1,11 @@\n-# Make a Bot talk back\n+# Hacer que un Bot responda\n \n-## Instructions\n+## Instrucciones\n \n-In the past few lessons, you programmed a basic bot with whom to chat. This bot gives random answers until you say 'bye'. Can you make the answers a little less random, and trigger answers if you say specific things, like 'why' or 'how'? Think a bit how machine learning might make this type of work less manual as you extend your bot. You can use NLTK or TextBlob libraries to make your tasks easier.\n+En las lecciones anteriores, programaste un bot básico con el cual chatear. Este bot da respuestas aleatorias hasta que le digas 'bye'. ¿Puedes hacer las respuestas menos aleatorias, y enviar respuestas si dices cosas específicas, como 'why' o 'how'? Piensa un poco en cómo el aprendizaje automático puede realizar este tipo de trabajo menos manual a medida que extiendes tu bot. Puedes usar las librerías NLTK o TextBlob para hacer más fáciles tus actividades.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430592,microsoft/ML-For-Beginners,856368148,813302765,Stevengez,53009062,2022-02-23T20:45:30+00:00,2022-02-23T20:45:30+00:00,"```suggestion\r\nEn estas dos lecciones, se te presentará la predicción de series de tiempo, un área algo menos conocida del aprendizaje automático que sin embargo es extremadamente valiosa para la industria y aplicaciones empresariales, entre otros campos. Mientras las redes neuronales pueden ser usadas para mejorar la utilidad de estos modelos, las estudiaremos en el contexto del aprendizaje automático clásico como modelos que ayudan a predecir el desempeño futuro basado en el pasado.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430593,microsoft/ML-For-Beginners,856368148,813302765,Stevengez,53009062,2022-02-23T20:45:30+00:00,2022-02-23T20:45:30+00:00,"```suggestion\r\nEn estas dos lecciones, se te presentará la predicción de series de tiempo, un área algo menos conocida del aprendizaje automático que sin embargo es extremadamente valiosa para la industria y aplicaciones empresariales, entre otros campos. Mientras las redes neuronales pueden ser usadas para mejorar la utilidad de estos modelos, las estudiaremos en el contexto del aprendizaje automático clásico como modelos que ayudan a predecir el desempeño futuro basado en el pasado.\r\n```",true,7-TimeSeries/translations/README.es.md,NaN,"@@ -1,23 +1,23 @@\n-# Introduction to time series forecasting\n+# Introducción a la predicción de series de tiempo\n \n-What is time series forecasting? It's about predicting future events by analyzing trends of the past.\n+¿Qué es la predicción de series de tiempo? Se trata de predecir eventos futuros al analizar las tendencias del pasado.\n \n-## Regional topic: worldwide electricity usage ✨\n+## Tema regional: Uso de la electricidad en el mundo ✨\n \n-In these two lessons, you will be introduced to time series forecasting, a somewhat lesser known area of machine learning that is nevertheless extremely valuable for industry and business applications, among other fields. While neural networks can be used to enhance the utility of these models, we will study them in the context of classical machine learning as models help predict future performance based on the past.\n+En estas dos lecciones, se te presentará la predicción de series de tiempo, un área algo menos conocida del aprendizaje automático que sin embargo es extremadamente valiosa para la industria y aplicaciones empresariales, entre otros campos. Mientras las redes neuronales pueden ser usadas para mejorar la utilidad de estos modelos, las estudiaremos en el contexto del aprendizaje automático clásico como modelos que ayudan a predecir el desempeño a futuro basado en el pasado.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430594,microsoft/ML-For-Beginners,856195037,813310313,madebygps,6733686,2022-02-23T20:55:53+00:00,2022-02-23T20:56:06+00:00,"Suggestion:\r\n\r\nremove the así from para así comprender\r\n\r\nEn esta lección, has trabajado con algunas técnicas de visualización para comprender cómo graficar tus datos en preparación para agruparlos.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430595,microsoft/ML-For-Beginners,856195037,813356440,hereje,3022824,2022-02-23T21:54:28+00:00,2022-02-23T21:54:28+00:00,Thanks!,false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430596,microsoft/ML-For-Beginners,856195037,813310313,madebygps,6733686,2022-02-23T20:55:53+00:00,2022-02-23T20:56:06+00:00,"Suggestion:\r\n\r\nremove the así from para así comprender\r\n\r\nEn esta lección, has trabajado con algunas técnicas de visualización para comprender cómo graficar tus datos en preparación para agruparlos.",true,5-Clustering/1-Visualize/translations/assignment.es.md,NaN,"@@ -0,0 +1,11 @@\n+# Investiga otras visualizaciones para agruparlas\n+\n+## Instrucciones\n+\n+En esta lección, has trabajado con algunas técnicas de visualización para así comprender cómo graficar tus datos en preparación para agruparlos. Los gráficos de dispersión son particularmente útiles para encontrar grupos de objetos. Investiga distintas formas y librerías para crear gráficos de dispersión y documenta tu trabajo en un notebook. Puedes usar los datos de esta lección, otras lecciones, o datos que tú mismo proporciones (por favor, dá crédito a su fuente en tu notebook). Grafica algunos datos usando gráficos de dispersión y explica lo que descubras.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430597,microsoft/ML-For-Beginners,856195037,813356440,hereje,3022824,2022-02-23T21:54:28+00:00,2022-02-23T21:54:28+00:00,Thanks!,true,5-Clustering/1-Visualize/translations/assignment.es.md,NaN,"@@ -0,0 +1,11 @@\n+# Investiga otras visualizaciones para agruparlas\n+\n+## Instrucciones\n+\n+En esta lección, has trabajado con algunas técnicas de visualización para así comprender cómo graficar tus datos en preparación para agruparlos. Los gráficos de dispersión son particularmente útiles para encontrar grupos de objetos. Investiga distintas formas y librerías para crear gráficos de dispersión y documenta tu trabajo en un notebook. Puedes usar los datos de esta lección, otras lecciones, o datos que tú mismo proporciones (por favor, dá crédito a su fuente en tu notebook). Grafica algunos datos usando gráficos de dispersión y explica lo que descubras.",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430598,microsoft/ML-For-Beginners,855970283,813312624,Stevengez,53009062,2022-02-23T20:59:07+00:00,2022-02-23T20:59:08+00:00,"```suggestion\r\n|          | Se presentó tanto una aplicación web como un notebook, ambos bien documentados y ejecutándose | Uno de esos dos falta o contiene errores | Ambos faltan o están defectuosos |\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430599,microsoft/ML-For-Beginners,855970283,813312624,Stevengez,53009062,2022-02-23T20:59:07+00:00,2022-02-23T20:59:08+00:00,"```suggestion\r\n|          | Se presentó tanto una aplicación web como un notebook, ambos bien documentados y ejecutándose | Uno de esos dos falta o contiene errores | Ambos faltan o están defectuosos |\r\n```",true,4-Classification/4-Applied/translations/assignment.es.md,NaN,"@@ -1,11 +1,11 @@\n-# Build a recommender\n+# Construye un recomendador\n \n-## Instructions\n+## Instrucciones\n \n-Given your exercises in this lesson, you now know how to build JavaScript-based web app using Onnx Runtime and a converted Onnx model. Experiment with building a new recommender using data from these lessons or sourced elsewhere (give credit, please). You might create a pet recommender given various personality attributes, or a music genre recommender based on a person's mood. Be creative!\n+Dados tus ejercicios en esta lección, ahora sabes cómo construir aplicaciones web basadas en JavaScript usando el Runtime Onnx y un modelo Onnx convertido. Experimenta con la construcción de un nuevo recomendador usando los datos de estas lecciones u obtenidos de otro lugar (por favor, da los créditos correspondientes). Debes crear un recomendador de mascotas dados diversos atributos de personalidad, o un recomendador de géneros musicales basados en el estado de ánimo de una persona. ¡Sé creativo!\n \n-## Rubric\n+## Rúbrica\n \n-| Criteria | Exemplary                                                              | Adequate                              | Needs Improvement                 |\n+| Criterio | Ejemplar                                                              | Adecuado                              | Necesita mejorar                 |\n | -------- | ---------------------------------------------------------------------- | ------------------------------------- | --------------------------------- |\n-|          | A web app and notebook are presented, both well documented and running | One of those two is missing or flawed | Both are either missing or flawed |\n+|          | Se presentó tanto una aplicación web como un notebook, ambos bien documentados y ejecutándose | Uno de esos dos falta o contiene errores | A web app and notebook are presented, both well documented and running | One of those two is missing or flawed | Ambos faltan o están defectuosos |",0,0,0,0,0,0,0,microsoft_ML-For-Beginners
6824cd900ce5dcec78430c88,pandas-dev/pandas,2512205764,2085085493,mroeschke,10647082,2025-05-12T16:54:19+00:00,2025-05-12T16:54:19+00:00,1. Can you combine these two installs?\r\n2. Can you include `--no-install-recommends`?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c89,pandas-dev/pandas,2512205764,2085129184,huisman,23581164,2025-05-12T17:22:57+00:00,2025-05-12T17:22:57+00:00,"Yes, done. That saves another ~0.16 Gb.\r\n\r\nDo note that the --no-install-recommends could impact  'downstream' images based on this image, if they depend on having a recommended library installed. ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c8a,pandas-dev/pandas,2512205764,2085085493,mroeschke,10647082,2025-05-12T16:54:19+00:00,2025-05-12T16:54:19+00:00,1. Can you combine these two installs?\r\n2. Can you include `--no-install-recommends`?,true,Dockerfile,NaN,"@@ -1,16 +1,22 @@\n FROM python:3.10.8\n WORKDIR /home/pandas\n \n-RUN apt-get update && apt-get -y upgrade\n-RUN apt-get install -y build-essential bash-completion\n+RUN apt-get update && \\n+    apt-get -y upgrade && \\n+    rm -rf /var/lib/apt/lists/*\n \n-# hdf5 needed for pytables installation\n-# libgles2-mesa needed for pytest-qt\n-RUN apt-get install -y libhdf5-dev libgles2-mesa-dev\n+RUN apt-get update && apt-get install -y \",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c8b,pandas-dev/pandas,2512205764,2085129184,huisman,23581164,2025-05-12T17:22:57+00:00,2025-05-12T17:22:57+00:00,"Yes, done. That saves another ~0.16 Gb.\r\n\r\nDo note that the --no-install-recommends could impact  'downstream' images based on this image, if they depend on having a recommended library installed. ",true,Dockerfile,NaN,"@@ -1,16 +1,22 @@\n FROM python:3.10.8\n WORKDIR /home/pandas\n \n-RUN apt-get update && apt-get -y upgrade\n-RUN apt-get install -y build-essential bash-completion\n+RUN apt-get update && \\n+    apt-get -y upgrade && \\n+    rm -rf /var/lib/apt/lists/*\n \n-# hdf5 needed for pytables installation\n-# libgles2-mesa needed for pytest-qt\n-RUN apt-get install -y libhdf5-dev libgles2-mesa-dev\n+RUN apt-get update && apt-get install -y \",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c8c,pandas-dev/pandas,2511107712,2085347364,mroeschke,10647082,2025-05-12T19:53:07+00:00,2025-05-12T19:53:07+00:00,Can you add a comment somewhere here about why we're pinning Cython? The we can take this out of draft and merge.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c8d,pandas-dev/pandas,2511107712,2085347364,mroeschke,10647082,2025-05-12T19:53:07+00:00,2025-05-12T19:53:07+00:00,Can you add a comment somewhere here about why we're pinning Cython? The we can take this out of draft and merge.,true,.github/workflows/unit-tests.yml,5,"@@ -246,7 +246,7 @@ jobs:\n           . ~/virtualenvs/pandas-dev/bin/activate\n           python -m pip install --no-cache-dir -U pip wheel setuptools meson[ninja]==1.2.1 meson-python==0.13.1\n           python -m pip install numpy -Csetup-args=""-Dallow-noblas=true""\n-          python -m pip install --no-cache-dir versioneer[toml] cython python-dateutil pytest>=7.3.2 pytest-xdist>=3.4.0 hypothesis>=6.84.0\n+          python -m pip install --no-cache-dir versioneer[toml] cython==3.0.10 python-dateutil pytest>=7.3.2 pytest-xdist>=3.4.0 hypothesis>=6.84.0",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c8e,pandas-dev/pandas,2510727220,2083020363,nikaltipar,45468465,2025-05-10T08:15:16+00:00,2025-05-10T08:42:02+00:00,I would recommend you combine this into a common error to reduce repetition (bonus points for combining it with the pre-existing error just a few lines below),false,NaN,NaN,NaN,1,1,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c8f,pandas-dev/pandas,2510727220,2083028429,nikaltipar,45468465,2025-05-10T08:26:58+00:00,2025-05-10T08:42:02+00:00,Doesn't just `if not left_collisions.empty:` work? Same for a similar check below.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c90,pandas-dev/pandas,2510727220,2083031752,nikaltipar,45468465,2025-05-10T08:36:02+00:00,2025-05-10T08:42:02+00:00,"For new readers of this code, the comment might not be descriptive enough. While your code is supposed to find suffixes that are caused by duplicated would-be created columns across dataframes, there is an extra section that does a duplicate checking just below your new code (but would-be created columns due to duplicity within the same dataframe).",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c91,pandas-dev/pandas,2510727220,2083020363,nikaltipar,45468465,2025-05-10T08:15:16+00:00,2025-05-10T08:42:02+00:00,I would recommend you combine this into a common error to reduce repetition (bonus points for combining it with the pre-existing error just a few lines below),true,pandas/core/reshape/merge.py,NaN,"@@ -3058,6 +3058,20 @@ def renamer(x, suffix: str | None):\n     llabels = left._transform_index(lrenamer)\n     rlabels = right._transform_index(rrenamer)\n \n+    # Check for duplicates created by suffixes\n+    left_collisions = llabels.intersection(right.difference(to_rename))\n+    right_collisions = rlabels.intersection(left.difference(to_rename))\n+    if len(left_collisions) > 0:\n+        raise MergeError(\n+            ""Passing 'suffixes' which cause duplicate columns ""\n+            f""{set(left_collisions)} is not allowed""\n+        )\n+    if len(right_collisions) > 0:\n+        raise MergeError(\n+            ""Passing 'suffixes' which cause duplicate columns ""\n+            f""{set(right_collisions)} is not allowed""\n+        )",1,1,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c92,pandas-dev/pandas,2510727220,2083028429,nikaltipar,45468465,2025-05-10T08:26:58+00:00,2025-05-10T08:42:02+00:00,Doesn't just `if not left_collisions.empty:` work? Same for a similar check below.,true,pandas/core/reshape/merge.py,NaN,"@@ -3058,6 +3058,20 @@ def renamer(x, suffix: str | None):\n     llabels = left._transform_index(lrenamer)\n     rlabels = right._transform_index(rrenamer)\n \n+    # Check for duplicates created by suffixes\n+    left_collisions = llabels.intersection(right.difference(to_rename))\n+    right_collisions = rlabels.intersection(left.difference(to_rename))\n+    if len(left_collisions) > 0:",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c93,pandas-dev/pandas,2510727220,2083031752,nikaltipar,45468465,2025-05-10T08:36:02+00:00,2025-05-10T08:42:02+00:00,"For new readers of this code, the comment might not be descriptive enough. While your code is supposed to find suffixes that are caused by duplicated would-be created columns across dataframes, there is an extra section that does a duplicate checking just below your new code (but would-be created columns due to duplicity within the same dataframe).",true,pandas/core/reshape/merge.py,NaN,"@@ -3058,6 +3058,20 @@ def renamer(x, suffix: str | None):\n     llabels = left._transform_index(lrenamer)\n     rlabels = right._transform_index(rrenamer)\n \n+    # Check for duplicates created by suffixes",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c94,pandas-dev/pandas,2496720626,2072371842,datapythonista,10058240,2025-05-03T10:54:59+00:00,2025-05-03T11:10:22+00:00,```suggestion\r\n    source : type or str\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c95,pandas-dev/pandas,2496720626,2072371862,datapythonista,10058240,2025-05-03T10:55:08+00:00,2025-05-03T11:10:22+00:00,Same as above,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c96,pandas-dev/pandas,2496720626,2072373898,datapythonista,10058240,2025-05-03T11:08:34+00:00,2025-05-03T11:10:22+00:00,```suggestion\r\n        The second dtype to compare.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c97,pandas-dev/pandas,2496720626,2072374007,datapythonista,10058240,2025-05-03T11:09:08+00:00,2025-05-03T11:10:22+00:00,```suggestion\r\n    api.types.is_categorical_dtype : Check whether the provided array or dtype\r\n```\r\n\r\nAlso for the others.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c98,pandas-dev/pandas,2496720626,2072422205,chilin0525,41913261,2025-05-03T16:12:12+00:00,2025-05-03T16:12:12+00:00,Resolved in [7ca7023](https://github.com/pandas-dev/pandas/pull/61394/commits/7ca7023d1803926cd3797af06522cb86f5004e4a).,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c99,pandas-dev/pandas,2496720626,2072371842,datapythonista,10058240,2025-05-03T10:54:59+00:00,2025-05-03T11:10:22+00:00,```suggestion\r\n    source : type or str\r\n```,true,pandas/core/dtypes/common.py,NaN,"@@ -655,24 +655,38 @@ def is_dtype_equal(source, target) -> bool:\n \n     Parameters\n     ----------\n-    source : The first dtype to compare\n-    target : The second dtype to compare\n+    source : dtype",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c9a,pandas-dev/pandas,2496720626,2072371862,datapythonista,10058240,2025-05-03T10:55:08+00:00,2025-05-03T11:10:22+00:00,Same as above,true,pandas/core/dtypes/common.py,NaN,"@@ -655,24 +655,38 @@ def is_dtype_equal(source, target) -> bool:\n \n     Parameters\n     ----------\n-    source : The first dtype to compare\n-    target : The second dtype to compare\n+    source : dtype\n+        The first dtype to compare\n+    target : dtype",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c9b,pandas-dev/pandas,2496720626,2072373898,datapythonista,10058240,2025-05-03T11:08:34+00:00,2025-05-03T11:10:22+00:00,```suggestion\r\n        The second dtype to compare.\r\n```,true,pandas/core/dtypes/common.py,NaN,"@@ -655,24 +655,38 @@ def is_dtype_equal(source, target) -> bool:\n \n     Parameters\n     ----------\n-    source : The first dtype to compare\n-    target : The second dtype to compare\n+    source : dtype\n+        The first dtype to compare\n+    target : dtype\n+        The second dtype to compare",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c9c,pandas-dev/pandas,2496720626,2072374007,datapythonista,10058240,2025-05-03T11:09:08+00:00,2025-05-03T11:10:22+00:00,```suggestion\r\n    api.types.is_categorical_dtype : Check whether the provided array or dtype\r\n```\r\n\r\nAlso for the others.,true,pandas/core/dtypes/common.py,NaN,"@@ -655,24 +655,38 @@ def is_dtype_equal(source, target) -> bool:\n \n     Parameters\n     ----------\n-    source : The first dtype to compare\n-    target : The second dtype to compare\n+    source : dtype\n+        The first dtype to compare\n+    target : dtype\n+        The second dtype to compare\n \n     Returns\n     -------\n     boolean\n         Whether or not the two dtypes are equal.\n \n+    See Also\n+    --------\n+    pandas.api.types.is_categorical_dtype : Check whether the provided array or dtype",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c9d,pandas-dev/pandas,2496720626,2072422205,chilin0525,41913261,2025-05-03T16:12:12+00:00,2025-05-03T16:12:12+00:00,Resolved in [7ca7023](https://github.com/pandas-dev/pandas/pull/61394/commits/7ca7023d1803926cd3797af06522cb86f5004e4a).,true,pandas/core/dtypes/common.py,NaN,"@@ -655,24 +655,38 @@ def is_dtype_equal(source, target) -> bool:\n \n     Parameters\n     ----------\n-    source : The first dtype to compare\n-    target : The second dtype to compare\n+    source : dtype\n+        The first dtype to compare\n+    target : dtype",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c9e,pandas-dev/pandas,2496447070,2073825640,mroeschke,10647082,2025-05-05T17:04:38+00:00,2025-05-05T17:04:38+00:00,"```suggestion\r\n                    if not isinstance(self.subplots, bool):\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430c9f,pandas-dev/pandas,2496447070,2073826400,mroeschke,10647082,2025-05-05T17:05:16+00:00,2025-05-05T17:05:17+00:00,```suggestion\r\n- Bug in :meth:`DataFrame.plot` where ``title`` would require extra tittles when plotting more than one column per subplot. (:issue:`61019`)\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca0,pandas-dev/pandas,2496447070,2073830586,mroeschke,10647082,2025-05-05T17:07:32+00:00,2025-05-05T17:07:32+00:00,"```suggestion\r\n                                f""The number of titles ({len(self.title)}) must equal the number ""\r\n                                f""of subplots ({len(self.subplots)}).""\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca1,pandas-dev/pandas,2496447070,2076292195,mroeschke,10647082,2025-05-06T21:11:50+00:00,2025-05-06T21:11:50+00:00,```suggestion\r\n```\r\n\r\n(This is redundant with the message below),false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca2,pandas-dev/pandas,2496447070,2073825640,mroeschke,10647082,2025-05-05T17:04:38+00:00,2025-05-05T17:04:38+00:00,"```suggestion\r\n                    if not isinstance(self.subplots, bool):\r\n```",true,pandas/plotting/_matplotlib/core.py,NaN,"@@ -802,7 +802,16 @@ def _adorn_subplots(self, fig: Figure) -> None:\n         if self.title:\n             if self.subplots:\n                 if is_list_like(self.title):\n-                    if len(self.title) != self.nseries:\n+                    if type(self.subplots) != bool:",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca3,pandas-dev/pandas,2496447070,2073826400,mroeschke,10647082,2025-05-05T17:05:16+00:00,2025-05-05T17:05:17+00:00,```suggestion\r\n- Bug in :meth:`DataFrame.plot` where ``title`` would require extra tittles when plotting more than one column per subplot. (:issue:`61019`)\r\n```,true,doc/source/whatsnew/v3.0.0.rst,NaN,"@@ -800,6 +800,7 @@ Plotting\n - Bug in :meth:`DataFrame.plot.bar` with ``stacked=True`` where labels on stacked bars with zero-height segments were incorrectly positioned at the base instead of the label position of the previous segment (:issue:`59429`)\n - Bug in :meth:`DataFrame.plot.line` raising ``ValueError`` when set both color and a ``dict`` style (:issue:`59461`)\n - Bug in :meth:`DataFrame.plot` that causes a shift to the right when the frequency multiplier is greater than one. (:issue:`57587`)\n+- Bug in :meth:`DataFrame.plot` where ``title`` would require more titles than needed when graphing more than one column per subplot. (:issue:`61019`)",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca4,pandas-dev/pandas,2496447070,2073830586,mroeschke,10647082,2025-05-05T17:07:32+00:00,2025-05-05T17:07:32+00:00,"```suggestion\r\n                                f""The number of titles ({len(self.title)}) must equal the number ""\r\n                                f""of subplots ({len(self.subplots)}).""\r\n```",true,pandas/plotting/_matplotlib/core.py,NaN,"@@ -802,7 +802,16 @@ def _adorn_subplots(self, fig: Figure) -> None:\n         if self.title:\n             if self.subplots:\n                 if is_list_like(self.title):\n-                    if len(self.title) != self.nseries:\n+                    if type(self.subplots) != bool:\n+                        if len(self.subplots) != len(self.title):\n+                            raise ValueError(\n+                                ""The length of `title` must equal the number ""\n+                                ""of subplots if `title` of type `list` ""\n+                                ""and subplots is iterable.\n""\n+                                f""length of title = {len(self.title)}\n""\n+                                f""number of subplots = {len(self.subplots)}""",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca5,pandas-dev/pandas,2496447070,2076292195,mroeschke,10647082,2025-05-06T21:11:50+00:00,2025-05-06T21:11:50+00:00,```suggestion\r\n```\r\n\r\n(This is redundant with the message below),true,pandas/plotting/_matplotlib/core.py,NaN,"@@ -802,7 +802,16 @@ def _adorn_subplots(self, fig: Figure) -> None:\n         if self.title:\n             if self.subplots:\n                 if is_list_like(self.title):\n-                    if len(self.title) != self.nseries:\n+                    if not isinstance(self.subplots, bool):\n+                        if len(self.subplots) != len(self.title):\n+                            raise ValueError(\n+                                ""The length of `title` must equal the number ""\n+                                ""of subplots if `title` of type `list` ""\n+                                ""and subplots is iterable.\n""",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca6,pandas-dev/pandas,2492884558,2072660141,WillAyd,609873,2025-05-04T16:44:08+00:00,2025-05-04T16:44:08+00:00,Cool seems useful. Is there any discussion to add this natively to Meson instead of custom-coding this ourselves? ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca7,pandas-dev/pandas,2492884558,2072660141,WillAyd,609873,2025-05-04T16:44:08+00:00,2025-05-04T16:44:08+00:00,Cool seems useful. Is there any discussion to add this natively to Meson instead of custom-coding this ourselves? ,true,meson.build,5,"@@ -47,6 +47,24 @@ endif\n cy = meson.get_compiler('cython')\n if cy.version().version_compare('>=3.1.0')\n     add_project_arguments('-Xfreethreading_compatible=true', language: 'cython')\n+\n+    # Use shared utility code to reduce wheel sizes",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca8,pandas-dev/pandas,2492583257,2080044141,mroeschke,10647082,2025-05-08T16:13:45+00:00,2025-05-08T16:13:45+00:00,Do you know what version of pyiceberg stopped pinning fsspec?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ca9,pandas-dev/pandas,2492583257,2080053775,mroeschke,10647082,2025-05-08T16:20:26+00:00,2025-05-08T16:20:26+00:00,Does it make sense for this to be a pytest fixture? It could be parametrized over different catalog names by default and could use the `tmpdir` fixture to do the temporary directory stuff automatically,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430caa,pandas-dev/pandas,2492583257,2080056110,mroeschke,10647082,2025-05-08T16:22:06+00:00,2025-05-08T16:22:06+00:00,Might be good to flag as `experimental` so if we revisit and implement the IO plugin model we can pivot this to that model a little quicker.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cab,pandas-dev/pandas,2492583257,2080081392,datapythonista,10058240,2025-05-08T16:38:05+00:00,2025-05-08T16:38:05+00:00,"0.7 is the last to have upper version for all the cloud fs packages, 0.8 removed them. But 0.8 isn't compatible with our minimum supported sqlalchemy",false,NaN,NaN,NaN,1,1,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cac,pandas-dev/pandas,2492583257,2080086790,datapythonista,10058240,2025-05-08T16:41:33+00:00,2025-05-08T16:41:33+00:00,"That was my first implementation, but I couldn't make a fixture remove the files when the test finish. That's why I implemented it as a context manager.\r\n\r\nI'll check again, as I didn't try with the tmpdir fixture, but it may not be easy.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cad,pandas-dev/pandas,2492583257,2080087383,datapythonista,10058240,2025-05-08T16:41:58+00:00,2025-05-08T16:41:59+00:00,"Makes sense, thanks for all the feedback.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cae,pandas-dev/pandas,2492583257,2085098409,mroeschke,10647082,2025-05-12T17:01:46+00:00,2025-05-12T17:01:47+00:00,"Can this be written in the `tmp_path` too? Otherwise, I imagine after running the tests this will be leftover",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430caf,pandas-dev/pandas,2492583257,2085098714,mroeschke,10647082,2025-05-12T17:02:03+00:00,2025-05-12T17:02:04+00:00,```suggestion\r\npytestmark = pytest.mark.single_cpu\r\n```\r\n\r\nNit: Since there only 1 entry,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb0,pandas-dev/pandas,2492583257,2085109279,datapythonista,10058240,2025-05-12T17:09:51+00:00,2025-05-12T17:09:51+00:00,"This file is being removed in line 64: https://github.com/pandas-dev/pandas/pull/61383/files#diff-dc72941f90793049e0faeefb11e9e743bd46697b5d8b44be927c8c3961609914R64\r\n\r\nTo be able to write it in `tmp_path` I need to set up an environment variable for pyiceberg to find it. I thought this was easier, but I can do that if you have a strong preference for it.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb1,pandas-dev/pandas,2492583257,2085118329,mroeschke,10647082,2025-05-12T17:15:37+00:00,2025-05-12T17:15:38+00:00,Ah sorry I missed this. What you have now is fine,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb2,pandas-dev/pandas,2492583257,2080044141,mroeschke,10647082,2025-05-08T16:13:45+00:00,2025-05-08T16:13:45+00:00,Do you know what version of pyiceberg stopped pinning fsspec?,true,ci/deps/actions-310-minimum_versions.yaml,5,"@@ -29,10 +29,10 @@ dependencies:\n   - blosc=1.21.3\n   - bottleneck=1.3.6\n   - fastparquet=2024.2.0\n-  - fsspec=2024.2.0\n+  - fsspec=2023.12.2",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb3,pandas-dev/pandas,2492583257,2080053775,mroeschke,10647082,2025-05-08T16:20:26+00:00,2025-05-08T16:20:26+00:00,Does it make sense for this to be a pytest fixture? It could be parametrized over different catalog names by default and could use the `tmpdir` fixture to do the temporary directory stuff automatically,true,pandas/tests/io/test_iceberg.py,NaN,"@@ -0,0 +1,148 @@\n+""""""\n+Tests for the Apache Iceberg format.\n+\n+Tests in this file use a simple Iceberg catalog based on SQLite, with the same\n+data used for Parquet tests (``pandas/tests/io/data/parquet/simple.parquet``).\n+""""""\n+\n+from contextlib import contextmanager\n+import importlib\n+import pathlib\n+import tempfile\n+\n+import pytest\n+\n+import pandas as pd\n+import pandas._testing as tm\n+\n+from pandas.io.iceberg import read_iceberg\n+\n+pytestmark = [pytest.mark.single_cpu]\n+\n+pyiceberg = pytest.importorskip(""pyiceberg"")\n+pyiceberg_catalog = pytest.importorskip(""pyiceberg.catalog"")\n+pq = pytest.importorskip(""pyarrow.parquet"")\n+\n+\n+@contextmanager\n+def create_catalog(catalog_name_in_pyiceberg_config=None):",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb4,pandas-dev/pandas,2492583257,2080056110,mroeschke,10647082,2025-05-08T16:22:06+00:00,2025-05-08T16:22:06+00:00,Might be good to flag as `experimental` so if we revisit and implement the IO plugin model we can pivot this to that model a little quicker.,true,pandas/io/iceberg.py,22,"@@ -0,0 +1,89 @@\n+from typing import (\n+    Any,\n+)\n+\n+from pandas.compat._optional import import_optional_dependency\n+\n+from pandas import DataFrame\n+\n+\n+def read_iceberg(\n+    table_identifier: str,\n+    catalog_name: str | None = None,\n+    catalog_properties: dict[str, Any] | None = None,\n+    row_filter: str | None = None,\n+    selected_fields: tuple[str] | None = None,\n+    case_sensitive: bool = True,\n+    snapshot_id: int | None = None,\n+    limit: int | None = None,\n+    scan_properties: dict[str, Any] | None = None,\n+) -> DataFrame:\n+    """"""\n+    Read an Apache Iceberg table into a pandas DataFrame.",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb5,pandas-dev/pandas,2492583257,2080081392,datapythonista,10058240,2025-05-08T16:38:05+00:00,2025-05-08T16:38:05+00:00,"0.7 is the last to have upper version for all the cloud fs packages, 0.8 removed them. But 0.8 isn't compatible with our minimum supported sqlalchemy",true,ci/deps/actions-310-minimum_versions.yaml,5,"@@ -29,10 +29,10 @@ dependencies:\n   - blosc=1.21.3\n   - bottleneck=1.3.6\n   - fastparquet=2024.2.0\n-  - fsspec=2024.2.0\n+  - fsspec=2023.12.2",1,1,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb6,pandas-dev/pandas,2492583257,2080086790,datapythonista,10058240,2025-05-08T16:41:33+00:00,2025-05-08T16:41:33+00:00,"That was my first implementation, but I couldn't make a fixture remove the files when the test finish. That's why I implemented it as a context manager.\r\n\r\nI'll check again, as I didn't try with the tmpdir fixture, but it may not be easy.",true,pandas/tests/io/test_iceberg.py,NaN,"@@ -0,0 +1,148 @@\n+""""""\n+Tests for the Apache Iceberg format.\n+\n+Tests in this file use a simple Iceberg catalog based on SQLite, with the same\n+data used for Parquet tests (``pandas/tests/io/data/parquet/simple.parquet``).\n+""""""\n+\n+from contextlib import contextmanager\n+import importlib\n+import pathlib\n+import tempfile\n+\n+import pytest\n+\n+import pandas as pd\n+import pandas._testing as tm\n+\n+from pandas.io.iceberg import read_iceberg\n+\n+pytestmark = [pytest.mark.single_cpu]\n+\n+pyiceberg = pytest.importorskip(""pyiceberg"")\n+pyiceberg_catalog = pytest.importorskip(""pyiceberg.catalog"")\n+pq = pytest.importorskip(""pyarrow.parquet"")\n+\n+\n+@contextmanager\n+def create_catalog(catalog_name_in_pyiceberg_config=None):",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb7,pandas-dev/pandas,2492583257,2080087383,datapythonista,10058240,2025-05-08T16:41:58+00:00,2025-05-08T16:41:59+00:00,"Makes sense, thanks for all the feedback.",true,pandas/io/iceberg.py,22,"@@ -0,0 +1,89 @@\n+from typing import (\n+    Any,\n+)\n+\n+from pandas.compat._optional import import_optional_dependency\n+\n+from pandas import DataFrame\n+\n+\n+def read_iceberg(\n+    table_identifier: str,\n+    catalog_name: str | None = None,\n+    catalog_properties: dict[str, Any] | None = None,\n+    row_filter: str | None = None,\n+    selected_fields: tuple[str] | None = None,\n+    case_sensitive: bool = True,\n+    snapshot_id: int | None = None,\n+    limit: int | None = None,\n+    scan_properties: dict[str, Any] | None = None,\n+) -> DataFrame:\n+    """"""\n+    Read an Apache Iceberg table into a pandas DataFrame.",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb8,pandas-dev/pandas,2492583257,2085098409,mroeschke,10647082,2025-05-12T17:01:46+00:00,2025-05-12T17:01:47+00:00,"Can this be written in the `tmp_path` too? Otherwise, I imagine after running the tests this will be leftover",true,pandas/tests/io/test_iceberg.py,50,"@@ -0,0 +1,143 @@\n+""""""\n+Tests for the Apache Iceberg format.\n+\n+Tests in this file use a simple Iceberg catalog based on SQLite, with the same\n+data used for Parquet tests (``pandas/tests/io/data/parquet/simple.parquet``).\n+""""""\n+\n+import collections\n+import importlib\n+import pathlib\n+\n+import pytest\n+\n+import pandas as pd\n+import pandas._testing as tm\n+\n+from pandas.io.iceberg import read_iceberg\n+\n+pytestmark = [pytest.mark.single_cpu]\n+\n+pyiceberg = pytest.importorskip(""pyiceberg"")\n+pyiceberg_catalog = pytest.importorskip(""pyiceberg.catalog"")\n+pq = pytest.importorskip(""pyarrow.parquet"")\n+\n+Catalog = collections.namedtuple(""Catalog"", [""name"", ""uri""])\n+\n+\n+@pytest.fixture\n+def catalog(request, tmp_path):\n+    # the catalog stores the full path of data files, so the catalog needs to be\n+    # created dynamically, and not saved in pandas/tests/io/data as other formats\n+    uri = f""sqlite:///{tmp_path}/catalog.sqlite""\n+    warehouse = f""file://{tmp_path}""\n+    catalog_name = request.param if hasattr(request, ""param"") else None\n+    catalog = pyiceberg_catalog.load_catalog(\n+        catalog_name or ""default"",\n+        type=""sql"",\n+        uri=uri,\n+        warehouse=warehouse,\n+    )\n+    catalog.create_namespace(""ns"")\n+\n+    df = pq.read_table(\n+        pathlib.Path(__file__).parent / ""data"" / ""parquet"" / ""simple.parquet""\n+    )\n+    table = catalog.create_table(""ns.my_table"", schema=df.schema)\n+    table.append(df)\n+\n+    if catalog_name is not None:\n+        config_path = pathlib.Path.home() / "".pyiceberg.yaml""",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cb9,pandas-dev/pandas,2492583257,2085098714,mroeschke,10647082,2025-05-12T17:02:03+00:00,2025-05-12T17:02:04+00:00,```suggestion\r\npytestmark = pytest.mark.single_cpu\r\n```\r\n\r\nNit: Since there only 1 entry,true,pandas/tests/io/test_iceberg.py,NaN,"@@ -0,0 +1,143 @@\n+""""""\n+Tests for the Apache Iceberg format.\n+\n+Tests in this file use a simple Iceberg catalog based on SQLite, with the same\n+data used for Parquet tests (``pandas/tests/io/data/parquet/simple.parquet``).\n+""""""\n+\n+import collections\n+import importlib\n+import pathlib\n+\n+import pytest\n+\n+import pandas as pd\n+import pandas._testing as tm\n+\n+from pandas.io.iceberg import read_iceberg\n+\n+pytestmark = [pytest.mark.single_cpu]",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cba,pandas-dev/pandas,2492583257,2085109279,datapythonista,10058240,2025-05-12T17:09:51+00:00,2025-05-12T17:09:51+00:00,"This file is being removed in line 64: https://github.com/pandas-dev/pandas/pull/61383/files#diff-dc72941f90793049e0faeefb11e9e743bd46697b5d8b44be927c8c3961609914R64\r\n\r\nTo be able to write it in `tmp_path` I need to set up an environment variable for pyiceberg to find it. I thought this was easier, but I can do that if you have a strong preference for it.",true,pandas/tests/io/test_iceberg.py,50,"@@ -0,0 +1,143 @@\n+""""""\n+Tests for the Apache Iceberg format.\n+\n+Tests in this file use a simple Iceberg catalog based on SQLite, with the same\n+data used for Parquet tests (``pandas/tests/io/data/parquet/simple.parquet``).\n+""""""\n+\n+import collections\n+import importlib\n+import pathlib\n+\n+import pytest\n+\n+import pandas as pd\n+import pandas._testing as tm\n+\n+from pandas.io.iceberg import read_iceberg\n+\n+pytestmark = [pytest.mark.single_cpu]\n+\n+pyiceberg = pytest.importorskip(""pyiceberg"")\n+pyiceberg_catalog = pytest.importorskip(""pyiceberg.catalog"")\n+pq = pytest.importorskip(""pyarrow.parquet"")\n+\n+Catalog = collections.namedtuple(""Catalog"", [""name"", ""uri""])\n+\n+\n+@pytest.fixture\n+def catalog(request, tmp_path):\n+    # the catalog stores the full path of data files, so the catalog needs to be\n+    # created dynamically, and not saved in pandas/tests/io/data as other formats\n+    uri = f""sqlite:///{tmp_path}/catalog.sqlite""\n+    warehouse = f""file://{tmp_path}""\n+    catalog_name = request.param if hasattr(request, ""param"") else None\n+    catalog = pyiceberg_catalog.load_catalog(\n+        catalog_name or ""default"",\n+        type=""sql"",\n+        uri=uri,\n+        warehouse=warehouse,\n+    )\n+    catalog.create_namespace(""ns"")\n+\n+    df = pq.read_table(\n+        pathlib.Path(__file__).parent / ""data"" / ""parquet"" / ""simple.parquet""\n+    )\n+    table = catalog.create_table(""ns.my_table"", schema=df.schema)\n+    table.append(df)\n+\n+    if catalog_name is not None:\n+        config_path = pathlib.Path.home() / "".pyiceberg.yaml""",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cbb,pandas-dev/pandas,2492583257,2085118329,mroeschke,10647082,2025-05-12T17:15:37+00:00,2025-05-12T17:15:38+00:00,Ah sorry I missed this. What you have now is fine,true,pandas/tests/io/test_iceberg.py,50,"@@ -0,0 +1,143 @@\n+""""""\n+Tests for the Apache Iceberg format.\n+\n+Tests in this file use a simple Iceberg catalog based on SQLite, with the same\n+data used for Parquet tests (``pandas/tests/io/data/parquet/simple.parquet``).\n+""""""\n+\n+import collections\n+import importlib\n+import pathlib\n+\n+import pytest\n+\n+import pandas as pd\n+import pandas._testing as tm\n+\n+from pandas.io.iceberg import read_iceberg\n+\n+pytestmark = [pytest.mark.single_cpu]\n+\n+pyiceberg = pytest.importorskip(""pyiceberg"")\n+pyiceberg_catalog = pytest.importorskip(""pyiceberg.catalog"")\n+pq = pytest.importorskip(""pyarrow.parquet"")\n+\n+Catalog = collections.namedtuple(""Catalog"", [""name"", ""uri""])\n+\n+\n+@pytest.fixture\n+def catalog(request, tmp_path):\n+    # the catalog stores the full path of data files, so the catalog needs to be\n+    # created dynamically, and not saved in pandas/tests/io/data as other formats\n+    uri = f""sqlite:///{tmp_path}/catalog.sqlite""\n+    warehouse = f""file://{tmp_path}""\n+    catalog_name = request.param if hasattr(request, ""param"") else None\n+    catalog = pyiceberg_catalog.load_catalog(\n+        catalog_name or ""default"",\n+        type=""sql"",\n+        uri=uri,\n+        warehouse=warehouse,\n+    )\n+    catalog.create_namespace(""ns"")\n+\n+    df = pq.read_table(\n+        pathlib.Path(__file__).parent / ""data"" / ""parquet"" / ""simple.parquet""\n+    )\n+    table = catalog.create_table(""ns.my_table"", schema=df.schema)\n+    table.append(df)\n+\n+    if catalog_name is not None:\n+        config_path = pathlib.Path.home() / "".pyiceberg.yaml""",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cbc,pandas-dev/pandas,2484334365,2062723059,mroeschke,10647082,2025-04-27T21:11:12+00:00,2025-04-27T21:11:12+00:00,"```suggestion\r\n        is the separator/delimiter to use between the two. Alternatively,\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cbd,pandas-dev/pandas,2484334365,2062723059,mroeschke,10647082,2025-04-27T21:11:12+00:00,2025-04-27T21:11:12+00:00,"```suggestion\r\n        is the separator/delimiter to use between the two. Alternatively,\r\n```",true,pandas/core/reshape/encoding.py,NaN,"@@ -60,13 +60,15 @@ def get_dummies(\n     data : array-like, Series, or DataFrame\n         Data of which to get dummy indicators.\n     prefix : str, list of str, or dict of str, default None\n-        String to append DataFrame column names.\n+        A string to be prepended to DataFrame column names.\n         Pass a list with length equal to the number of columns\n         when calling get_dummies on a DataFrame. Alternatively, `prefix`\n         can be a dictionary mapping column names to prefixes.\n-    prefix_sep : str, default '_'\n-        If appending prefix, separator/delimiter to use. Or pass a\n-        list or dictionary as with `prefix`.\n+    prefix_sep : str, list of str, or dict of str, default '_'\n+        Should you choose to prepend DataFrame column names with a prefix, this\n+        is the separator/delimiter to use between the two. Alternatively, ",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cbe,pandas-dev/pandas,2484247240,2062660932,tehunter,7980666,2025-04-27T15:34:59+00:00,2025-04-27T15:35:04+00:00,"When both arguments are False, should NaN come after non-observed groups? That seems more intuitive to me, especially for an ordered categorical",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cbf,pandas-dev/pandas,2484247240,2062662853,rhshadrach,45562402,2025-04-27T15:44:41+00:00,2025-04-27T15:44:41+00:00,No - if you do an operation like sum the order here matches the order in that result.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc0,pandas-dev/pandas,2484247240,2063496521,tehunter,7980666,2025-04-28T11:48:36+00:00,2025-04-28T11:48:36+00:00,"This is what I'm getting on both main and 2.2.3.\r\n\r\n```\r\n>>> df = DataFrame(\r\n...         {""cat"": Categorical([""a"", np.nan, ""a""], categories=list(""adb""))},\r\n...         index=list(""xyz""),\r\n...     )\r\n>>> df[""val""] = [1, 2, 3]\r\n>>> g = df.groupby(""cat"", observed=False, dropna=False)\r\n>>> g.sum()\r\n     val\r\ncat\r\na      4\r\nd      0\r\nb      0\r\nNaN    2\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc1,pandas-dev/pandas,2484247240,2063511077,tehunter,7980666,2025-04-28T11:58:55+00:00,2025-04-28T11:58:55+00:00,"Ah, `tm.assert_dict_equal` appears to be order-invariant, so it doesn't matter for the test.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc2,pandas-dev/pandas,2484247240,2064528596,rhshadrach,45562402,2025-04-28T20:30:42+00:00,2025-04-28T20:30:54+00:00,"Ah, I see now. I was correct in that the order was the same, but I failed to notice that the test added the groups in the incorrect order. I do wonder if `assert_dict_equal` should default to checking the order (perhaps with an argument to ignore order).",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc3,pandas-dev/pandas,2484247240,2062660932,tehunter,7980666,2025-04-27T15:34:59+00:00,2025-04-27T15:35:04+00:00,"When both arguments are False, should NaN come after non-observed groups? That seems more intuitive to me, especially for an ordered categorical",true,pandas/tests/groupby/test_categorical.py,15,"@@ -506,6 +506,23 @@ def test_observed_groups(observed):\n     tm.assert_dict_equal(result, expected)\n \n \n+def test_groups_na_category(dropna, observed):\n+    # https://github.com/pandas-dev/pandas/issues/61356\n+    df = DataFrame(\n+        {""cat"": Categorical([""a"", np.nan, ""a""], categories=list(""adb""))},\n+        index=list(""xyz""),\n+    )\n+    g = df.groupby(""cat"", observed=observed, dropna=dropna)\n+\n+    result = g.groups\n+    expected = {""a"": Index([""x"", ""z""])}\n+    if not dropna:\n+        expected |= {np.nan: Index([""y""])}",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc4,pandas-dev/pandas,2484247240,2062662853,rhshadrach,45562402,2025-04-27T15:44:41+00:00,2025-04-27T15:44:41+00:00,No - if you do an operation like sum the order here matches the order in that result.,true,pandas/tests/groupby/test_categorical.py,15,"@@ -506,6 +506,23 @@ def test_observed_groups(observed):\n     tm.assert_dict_equal(result, expected)\n \n \n+def test_groups_na_category(dropna, observed):\n+    # https://github.com/pandas-dev/pandas/issues/61356\n+    df = DataFrame(\n+        {""cat"": Categorical([""a"", np.nan, ""a""], categories=list(""adb""))},\n+        index=list(""xyz""),\n+    )\n+    g = df.groupby(""cat"", observed=observed, dropna=dropna)\n+\n+    result = g.groups\n+    expected = {""a"": Index([""x"", ""z""])}\n+    if not dropna:\n+        expected |= {np.nan: Index([""y""])}",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc5,pandas-dev/pandas,2484247240,2063496521,tehunter,7980666,2025-04-28T11:48:36+00:00,2025-04-28T11:48:36+00:00,"This is what I'm getting on both main and 2.2.3.\r\n\r\n```\r\n>>> df = DataFrame(\r\n...         {""cat"": Categorical([""a"", np.nan, ""a""], categories=list(""adb""))},\r\n...         index=list(""xyz""),\r\n...     )\r\n>>> df[""val""] = [1, 2, 3]\r\n>>> g = df.groupby(""cat"", observed=False, dropna=False)\r\n>>> g.sum()\r\n     val\r\ncat\r\na      4\r\nd      0\r\nb      0\r\nNaN    2\r\n```",true,pandas/tests/groupby/test_categorical.py,15,"@@ -506,6 +506,23 @@ def test_observed_groups(observed):\n     tm.assert_dict_equal(result, expected)\n \n \n+def test_groups_na_category(dropna, observed):\n+    # https://github.com/pandas-dev/pandas/issues/61356\n+    df = DataFrame(\n+        {""cat"": Categorical([""a"", np.nan, ""a""], categories=list(""adb""))},\n+        index=list(""xyz""),\n+    )\n+    g = df.groupby(""cat"", observed=observed, dropna=dropna)\n+\n+    result = g.groups\n+    expected = {""a"": Index([""x"", ""z""])}\n+    if not dropna:\n+        expected |= {np.nan: Index([""y""])}",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc6,pandas-dev/pandas,2484247240,2063511077,tehunter,7980666,2025-04-28T11:58:55+00:00,2025-04-28T11:58:55+00:00,"Ah, `tm.assert_dict_equal` appears to be order-invariant, so it doesn't matter for the test.",true,pandas/tests/groupby/test_categorical.py,15,"@@ -506,6 +506,23 @@ def test_observed_groups(observed):\n     tm.assert_dict_equal(result, expected)\n \n \n+def test_groups_na_category(dropna, observed):\n+    # https://github.com/pandas-dev/pandas/issues/61356\n+    df = DataFrame(\n+        {""cat"": Categorical([""a"", np.nan, ""a""], categories=list(""adb""))},\n+        index=list(""xyz""),\n+    )\n+    g = df.groupby(""cat"", observed=observed, dropna=dropna)\n+\n+    result = g.groups\n+    expected = {""a"": Index([""x"", ""z""])}\n+    if not dropna:\n+        expected |= {np.nan: Index([""y""])}",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc7,pandas-dev/pandas,2484247240,2064528596,rhshadrach,45562402,2025-04-28T20:30:42+00:00,2025-04-28T20:30:54+00:00,"Ah, I see now. I was correct in that the order was the same, but I failed to notice that the test added the groups in the incorrect order. I do wonder if `assert_dict_equal` should default to checking the order (perhaps with an argument to ignore order).",true,pandas/tests/groupby/test_categorical.py,15,"@@ -506,6 +506,23 @@ def test_observed_groups(observed):\n     tm.assert_dict_equal(result, expected)\n \n \n+def test_groups_na_category(dropna, observed):\n+    # https://github.com/pandas-dev/pandas/issues/61356\n+    df = DataFrame(\n+        {""cat"": Categorical([""a"", np.nan, ""a""], categories=list(""adb""))},\n+        index=list(""xyz""),\n+    )\n+    g = df.groupby(""cat"", observed=observed, dropna=dropna)\n+\n+    result = g.groups\n+    expected = {""a"": Index([""x"", ""z""])}\n+    if not dropna:\n+        expected |= {np.nan: Index([""y""])}",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc8,pandas-dev/pandas,2482707214,2064093797,mroeschke,10647082,2025-04-28T16:53:32+00:00,2025-04-28T16:53:32+00:00,"Can you replace other instances of `table[""schema""][""fields""]` below with `fields`?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cc9,pandas-dev/pandas,2482707214,2064094707,mroeschke,10647082,2025-04-28T16:54:14+00:00,2025-04-28T16:54:14+00:00,"```suggestion\r\n    if not all(isinstance(field[""name""], str) for field in fields):\r\n```\r\n\r\nNIt: IMO this reads a little clearer with me",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cca,pandas-dev/pandas,2482707214,2064093797,mroeschke,10647082,2025-04-28T16:53:32+00:00,2025-04-28T16:53:32+00:00,"Can you replace other instances of `table[""schema""][""fields""]` below with `fields`?",true,pandas/io/json/_table_schema.py,4,"@@ -372,6 +372,11 @@ def parse_table_schema(json, precise_float: bool) -> DataFrame:\n     pandas.read_json\n     """"""\n     table = ujson_loads(json, precise_float=precise_float)\n+    fields = table[""schema""][""fields""]",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ccb,pandas-dev/pandas,2482707214,2064094707,mroeschke,10647082,2025-04-28T16:54:14+00:00,2025-04-28T16:54:14+00:00,"```suggestion\r\n    if not all(isinstance(field[""name""], str) for field in fields):\r\n```\r\n\r\nNIt: IMO this reads a little clearer with me",true,pandas/io/json/_table_schema.py,6,"@@ -372,6 +372,11 @@ def parse_table_schema(json, precise_float: bool) -> DataFrame:\n     pandas.read_json\n     """"""\n     table = ujson_loads(json, precise_float=precise_float)\n+    fields = table[""schema""][""fields""]\n+\n+    if any(not isinstance(field[""name""], str) for field in fields):",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ccc,pandas-dev/pandas,2480421689,2060726723,rhshadrach,45562402,2025-04-25T18:46:16+00:00,2025-04-25T18:46:23+00:00,"Doesn't rolling accept many more arguments, e.g. `window`?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ccd,pandas-dev/pandas,2480421689,2061505943,rhshadrach,45562402,2025-04-26T17:39:15+00:00,2025-04-26T17:39:15+00:00,I opened #61361 just to be sure this doesn't get forgotten.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cce,pandas-dev/pandas,2480421689,2062601844,rhshadrach,45562402,2025-04-27T11:30:09+00:00,2025-04-27T11:30:09+00:00,"This was incorrect, the signature is fine here.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ccf,pandas-dev/pandas,2480421689,2060726723,rhshadrach,45562402,2025-04-25T18:46:16+00:00,2025-04-25T18:46:23+00:00,"Doesn't rolling accept many more arguments, e.g. `window`?",true,pandas/core/groupby/groupby.py,14,"@@ -3809,19 +3809,26 @@ def rolling(\n         )\n \n     @final\n-    def expanding(self, *args, **kwargs) -> ExpandingGroupby:\n+    def expanding(\n+        self,\n+        min_periods: int = 1,\n+        method: str = ""single"",\n+    ) -> ExpandingGroupby:\n         """"""\n         Return an expanding grouper, providing expanding functionality per group.\n \n-        Arguments are the same as `:meth:DataFrame.rolling` except that ``step`` cannot\n-        be specified.",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd0,pandas-dev/pandas,2480421689,2061505943,rhshadrach,45562402,2025-04-26T17:39:15+00:00,2025-04-26T17:39:15+00:00,I opened #61361 just to be sure this doesn't get forgotten.,true,pandas/core/groupby/groupby.py,14,"@@ -3809,19 +3809,26 @@ def rolling(\n         )\n \n     @final\n-    def expanding(self, *args, **kwargs) -> ExpandingGroupby:\n+    def expanding(\n+        self,\n+        min_periods: int = 1,\n+        method: str = ""single"",\n+    ) -> ExpandingGroupby:\n         """"""\n         Return an expanding grouper, providing expanding functionality per group.\n \n-        Arguments are the same as `:meth:DataFrame.rolling` except that ``step`` cannot\n-        be specified.",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd1,pandas-dev/pandas,2480421689,2062601844,rhshadrach,45562402,2025-04-27T11:30:09+00:00,2025-04-27T11:30:09+00:00,"This was incorrect, the signature is fine here.",true,pandas/core/groupby/groupby.py,14,"@@ -3809,19 +3809,26 @@ def rolling(\n         )\n \n     @final\n-    def expanding(self, *args, **kwargs) -> ExpandingGroupby:\n+    def expanding(\n+        self,\n+        min_periods: int = 1,\n+        method: str = ""single"",\n+    ) -> ExpandingGroupby:\n         """"""\n         Return an expanding grouper, providing expanding functionality per group.\n \n-        Arguments are the same as `:meth:DataFrame.rolling` except that ``step`` cannot\n-        be specified.",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd2,pandas-dev/pandas,2480172088,2059227775,tehunter,7980666,2025-04-24T21:02:31+00:00,2025-04-24T21:04:50+00:00,Is this a standard approach for a warning message that could be hit from two lines of code?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd3,pandas-dev/pandas,2480172088,2059229975,tehunter,7980666,2025-04-24T21:04:31+00:00,2025-04-24T21:04:50+00:00,"I know the implementation is trivial, but this is redundant with `Grouper`. I'm not sure we can get around it while still being a class property, but should the default value be referenced as a constant defined just once?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd4,pandas-dev/pandas,2480172088,2059227775,tehunter,7980666,2025-04-24T21:02:31+00:00,2025-04-24T21:04:50+00:00,Is this a standard approach for a warning message that could be hit from two lines of code?,true,pandas/core/groupby/grouper.py,30,"@@ -55,6 +61,13 @@\n     from pandas.core.generic import NDFrame\n \n \n+_NULL_KEY_MESSAGE = (\n+    ""`dropna` is not specified but grouper encountered null group keys. These keys ""\n+    ""will be dropped from the result by default. To keep null keys, set `dropna=True`, ""\n+    ""or to hide this warning and drop null keys, set `dropna=False`.""\n+)",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd5,pandas-dev/pandas,2480172088,2059229975,tehunter,7980666,2025-04-24T21:04:31+00:00,2025-04-24T21:04:50+00:00,"I know the implementation is trivial, but this is redundant with `Grouper`. I'm not sure we can get around it while still being a class property, but should the default value be referenced as a constant defined just once?",true,pandas/core/groupby/groupby.py,8,"@@ -486,6 +486,12 @@ def __repr__(self) -> str:\n         # TODO: Better repr for GroupBy object\n         return object.__repr__(self)\n \n+    @property\n+    def dropna(self) -> bool:\n+        if self._dropna is lib.no_default:\n+            return True\n+        return self._dropna",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd6,pandas-dev/pandas,2479520868,2080030446,mroeschke,10647082,2025-05-08T16:05:47+00:00,2025-05-08T16:05:48+00:00,"Can you just do `if obj.inferred_type == ""mixed""`? And everywhere below where you need to xfail this new index",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd7,pandas-dev/pandas,2479520868,2080032468,mroeschke,10647082,2025-05-08T16:06:47+00:00,2025-05-08T16:06:48+00:00,Can you use the `xfail` pattern you used above?,false,NaN,NaN,NaN,1,1,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd8,pandas-dev/pandas,2479520868,2080030446,mroeschke,10647082,2025-05-08T16:05:47+00:00,2025-05-08T16:05:48+00:00,"Can you just do `if obj.inferred_type == ""mixed""`? And everywhere below where you need to xfail this new index",true,pandas/tests/base/test_misc.py,NaN,"@@ -147,6 +147,11 @@ def test_searchsorted(request, index_or_series_obj):\n     # See gh-12238\n     obj = index_or_series_obj\n \n+    if any(isinstance(x, str) for x in obj) and any(isinstance(x, int) for x in obj):",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cd9,pandas-dev/pandas,2479520868,2080032468,mroeschke,10647082,2025-05-08T16:06:47+00:00,2025-05-08T16:06:48+00:00,Can you use the `xfail` pattern you used above?,true,pandas/tests/indexes/test_setops.py,NaN,"@@ -248,12 +235,21 @@ def test_intersection_base(self, index):\n \n     @pytest.mark.filterwarnings(r""ignore:PeriodDtype\[B\] is deprecated:FutureWarning"")\n     def test_union_base(self, index):\n+        if index.inferred_type in [""mixed"", ""mixed-integer""]:\n+            pytest.skip(""Mixed-type Index not orderable; union fails"")",1,1,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cda,pandas-dev/pandas,2477066389,2059203229,rhshadrach,45562402,2025-04-24T20:43:36+00:00,2025-04-24T20:49:30+00:00,pandas documentation is quite consistent with using `NA` instead of `null`. Can you use `NA` throughout.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cdb,pandas-dev/pandas,2477066389,2059204121,rhshadrach,45562402,2025-04-24T20:44:13+00:00,2025-04-24T20:49:30+00:00,"In addition, this line is incorrect as you can pass `skipna=False`.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cdc,pandas-dev/pandas,2477066389,2059206074,rhshadrach,45562402,2025-04-24T20:45:44+00:00,2025-04-24T20:49:30+00:00,`GroupBy` is not public. Can you use `DataFrameGroupBy` instead.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cdd,pandas-dev/pandas,2477066389,2059210782,rhshadrach,45562402,2025-04-24T20:49:16+00:00,2025-04-24T20:49:30+00:00,I think this page should only include documentation on `first`. Can you remove the use of other methods.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cde,pandas-dev/pandas,2477066389,2059203229,rhshadrach,45562402,2025-04-24T20:43:36+00:00,2025-04-24T20:49:30+00:00,pandas documentation is quite consistent with using `NA` instead of `null`. Can you use `NA` throughout.,true,pandas/core/groupby/groupby.py,5,"@@ -3232,9 +3232,12 @@ def first(\n         self, numeric_only: bool = False, min_count: int = -1, skipna: bool = True\n     ) -> NDFrameT:\n         """"""\n-        Compute the first entry of each column within each group.\n+        Compute the first non-null entry of each column within each group.",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cdf,pandas-dev/pandas,2477066389,2059204121,rhshadrach,45562402,2025-04-24T20:44:13+00:00,2025-04-24T20:49:30+00:00,"In addition, this line is incorrect as you can pass `skipna=False`.",true,pandas/core/groupby/groupby.py,5,"@@ -3232,9 +3232,12 @@ def first(\n         self, numeric_only: bool = False, min_count: int = -1, skipna: bool = True\n     ) -> NDFrameT:\n         """"""\n-        Compute the first entry of each column within each group.\n+        Compute the first non-null entry of each column within each group.",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce0,pandas-dev/pandas,2477066389,2059206074,rhshadrach,45562402,2025-04-24T20:45:44+00:00,2025-04-24T20:49:30+00:00,`GroupBy` is not public. Can you use `DataFrameGroupBy` instead.,true,pandas/core/groupby/groupby.py,31,"@@ -3251,15 +3254,15 @@ def first(\n         Returns\n         -------\n         Series or DataFrame\n-            First values within each group.\n+            First non-null values within each group, selected independently per column.\n \n         See Also\n         --------\n-        DataFrame.groupby : Apply a function groupby to each row or column of a\n-            DataFrame.\n-        core.groupby.DataFrameGroupBy.last : Compute the last non-null entry\n-            of each column.\n-        core.groupby.DataFrameGroupBy.nth : Take the nth row from each group.\n+        DataFrame.groupby : Group DataFrame using a mapper or by a Series of columns.\n+        Series.groupby : Group Series using a mapper or by a Series of values.\n+        GroupBy.nth : Take the nth row from each group.",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce1,pandas-dev/pandas,2477066389,2059210782,rhshadrach,45562402,2025-04-24T20:49:16+00:00,2025-04-24T20:49:30+00:00,I think this page should only include documentation on `first`. Can you remove the use of other methods.,true,pandas/core/groupby/groupby.py,53,"@@ -3272,23 +3275,38 @@ def first(\n         ...     )\n         ... )\n         >>> df[""D""] = pd.to_datetime(df[""D""])\n+\n         >>> df.groupby(""A"").first()\n-             B  C          D\n+            B  C          D\n         A\n         1  5.0  1 2000-03-11\n         3  6.0  3 2000-03-13\n+\n+        >>> df.groupby(""A"").nth(0)\n+            B  C          D\n+        A\n+        1  NaN  1 2000-03-11\n+        3  6.0  3 2000-03-13",0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce2,pandas-dev/pandas,2474815739,2056434703,mroeschke,10647082,2025-04-23T16:25:49+00:00,2025-04-23T16:26:04+00:00,```suggestion\r\n        if self.subplots and self.stacked:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce3,pandas-dev/pandas,2474815739,2056456609,mroeschke,10647082,2025-04-23T16:32:12+00:00,2025-04-23T16:32:13+00:00,"```suggestion\r\n            for i, sub_plot in enumerate(self.subplots):\r\n                if len(sub_plot) <= 1:\r\n                    continue\r\n                for plot in sub_plot:\r\n                    _stacked_subplots_ind[int(plot)] = i\r\n                _stacked_subplots_offsets.append([0, 0])\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce4,pandas-dev/pandas,2474815739,2056465475,mroeschke,10647082,2025-04-23T16:34:29+00:00,2025-04-23T16:34:29+00:00,"```suggestion\r\n@pytest.fixture\r\ndef BSS_data():\r\n    return np.random.default_rng(3).integers(0, 100, 5)\r\n\r\n\r\n@pytest.fixture\r\ndef BSS_df(BSS_data) -> DataFrame:\r\n    return DataFrame(\r\n        {""A"": BSS_data, ""B"": BSS_data[::-1], ""C"": BSS_data[0], ""D"": BSS_data[-1]}\r\n    )\r\n```\r\n\r\nAlso what's the BBS abbreviation? It would be better to use a clearer name",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce5,pandas-dev/pandas,2474815739,2056466446,mroeschke,10647082,2025-04-23T16:34:45+00:00,2025-04-23T16:34:45+00:00,"```suggestion\r\n            assert (sliced_df[""y_coord""] == 0).all()\r\n```\r\n?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce6,pandas-dev/pandas,2474815739,2056467876,mroeschke,10647082,2025-04-23T16:35:12+00:00,2025-04-23T16:35:12+00:00,Could you remove the test class? We're moving away from using this style,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce7,pandas-dev/pandas,2474815739,2060837534,eicchen,63069720,2025-04-25T20:19:50+00:00,2025-04-25T20:34:25+00:00,"It was for Bar Stacked Subplots, I was worried it would be too long to use in every variable, I can change it to ""BarStackedSub"", although it will cause the formatting of the code to look weird due to the line word limit",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce8,pandas-dev/pandas,2474815739,2060842211,eicchen,63069720,2025-04-25T20:24:21+00:00,2025-04-25T20:25:25+00:00,"There were some issues where the offset from stacking would be unintentionally carried over to other subplots causing the bars to not start at 0. This is now fixed with the new implementation, so this shouldn't be an issue. This is the check for that, I can remove it if it's deemed unnecessary though.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ce9,pandas-dev/pandas,2474815739,2060849685,eicchen,63069720,2025-04-25T20:32:27+00:00,2025-04-25T20:32:40+00:00,"So to clarify, just leave the functions as is? I'm not quite well versed on the pros and cons of class vs function-based testing frameworks, but would like to know more. Was just using the class as a grouping feature from pytest here.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430cea,pandas-dev/pandas,2474815739,2060857095,eicchen,63069720,2025-04-25T20:38:27+00:00,2025-04-25T20:38:28+00:00,"Nvm, misunderstood the comment",false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd900ce5dcec78430ceb,pandas-dev/pandas,2474815739,2060913768,mroeschke,10647082,2025-04-25T21:40:40+00:00,2025-04-25T21:40:40+00:00,```suggestion\r\n        if self.subplots and self.stacked:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,pandas-dev_pandas
6824cd910ce5dcec78431390,ray-project/ray,2514984359,2085586844,edoakes,9871461,2025-05-12T21:47:29+00:00,2025-05-12T21:47:30+00:00,bugfix mentioned in PR description,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431391,ray-project/ray,2514984359,2085910377,jjyao,898023,2025-05-13T04:46:39+00:00,2025-05-13T04:46:39+00:00,Can we use both `start_cluster` and `start_cluster_shared` in the same test file?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431392,ray-project/ray,2514984359,2085586844,edoakes,9871461,2025-05-12T21:47:29+00:00,2025-05-12T21:47:30+00:00,bugfix mentioned in PR description,true,python/ray/util/client/worker.py,5,"@@ -654,12 +654,13 @@ def call_retain(self, id: bytes) -> None:\n     def close(self):\n         self._in_shutdown = True\n         self.closed = True\n+        if self.channel:\n+            self.channel.close()",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431393,ray-project/ray,2514984359,2085910377,jjyao,898023,2025-05-13T04:46:39+00:00,2025-05-13T04:46:39+00:00,Can we use both `start_cluster` and `start_cluster_shared` in the same test file?,true,python/ray/tests/conftest.py,94,"@@ -855,7 +870,20 @@ def start_cluster(ray_start_cluster_enabled, request):\n     else:\n         address = cluster.address\n \n-    yield cluster, address\n+    return cluster, address\n+\n+\n+# Used to test both Ray Client and non-Ray Client codepaths.\n+# Usage: In your test, call `ray.init(address)`.\n+@pytest.fixture(scope=""function"", params=[""ray_client"", ""no_ray_client""])\n+def start_cluster(ray_start_cluster_enabled, request):\n+    yield _start_cluster(ray_start_cluster_enabled, request)\n+\n+\n+# Same as `start_cluster` but module-scoped.\n+@pytest.fixture(scope=""module"", params=[""ray_client"", ""no_ray_client""])\n+def start_cluster_shared(ray_start_cluster_enabled_shared, request):",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431394,ray-project/ray,2512520056,2085036410,jjyao,898023,2025-05-12T16:22:19+00:00,2025-05-12T16:22:19+00:00,Another follow-up is renaming ActorCounter to probably just SequenceNumber.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431395,ray-project/ray,2512520056,2085036410,jjyao,898023,2025-05-12T16:22:19+00:00,2025-05-12T16:22:19+00:00,Another follow-up is renaming ActorCounter to probably just SequenceNumber.,true,src/ray/core_worker/transport/actor_task_submitter.cc,4,"@@ -579,7 +579,7 @@ void ActorTaskSubmitter::PushActorTask(ClientQueue &queue,\n   request->mutable_task_spec()->CopyFrom(task_spec.GetMessage());\n \n   request->set_intended_worker_id(queue.worker_id);\n-  request->set_sequence_number(queue.actor_submit_queue->GetSequenceNumber(task_spec));",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431396,ray-project/ray,2510779588,2085287449,justinvyu,3887863,2025-05-12T19:07:15+00:00,2025-05-12T19:12:43+00:00,"Can you add a reference tag here (ex: `.. _forking-ray-processes-antipattern:`)?\r\n\r\nhttps://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/fork-new-processes.rst#L1\r\n\r\nThen, just point to this in the note, and no need to go into so much detail about the specific case I ran into. \r\n\r\n```suggestion\r\n.. note::\r\n    When using PyTorch DataLoader multiprocessing with Ray Train, you should set the process start method to be `forkserver` or `spawn` as\r\n    shown in the example above. :ref:`Forking Ray Actors and Tasks is an anti-pattern <forking-ray-processes-antipattern>` that\r\n    can lead to unexpected issues such as deadlocks.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431397,ray-project/ray,2510779588,2085290425,justinvyu,3887863,2025-05-12T19:09:40+00:00,2025-05-12T19:12:43+00:00,"I think this is the more common way to get the context:\r\n\r\n```suggestion\r\n                train_loader = DataLoader(train_data, batch_size=128, shuffle=True, multiprocessing_context=multiprocessing.get_context(""forkserver""))\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431398,ray-project/ray,2510779588,2085291811,justinvyu,3887863,2025-05-12T19:10:44+00:00,2025-05-12T19:12:43+00:00,Can you also move this quickstart code to a tested doctest file?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec78431399,ray-project/ray,2510779588,2085287449,justinvyu,3887863,2025-05-12T19:07:15+00:00,2025-05-12T19:12:43+00:00,"Can you add a reference tag here (ex: `.. _forking-ray-processes-antipattern:`)?\r\n\r\nhttps://github.com/ray-project/ray/blob/master/doc/source/ray-core/patterns/fork-new-processes.rst#L1\r\n\r\nThen, just point to this in the note, and no need to go into so much detail about the specific case I ran into. \r\n\r\n```suggestion\r\n.. note::\r\n    When using PyTorch DataLoader multiprocessing with Ray Train, you should set the process start method to be `forkserver` or `spawn` as\r\n    shown in the example above. :ref:`Forking Ray Actors and Tasks is an anti-pattern <forking-ray-processes-antipattern>` that\r\n    can lead to unexpected issues such as deadlocks.\r\n```",true,doc/source/train/getting-started-pytorch.rst,NaN,"@@ -273,6 +275,14 @@ See :ref:`data-ingest-torch`.\n \n     See :ref:`data-ingest-torch` for more details.\n \n+.. note::\n+    When using PyTorch DataLoader multiprocessing with Ray Train, you should set the\n+    ``multiprocessing_context`` to ``multiprocessing.context.ForkServerContext()`` as\n+    shown in the example above. This is because Ray Train configures the global logger with\n+    Ray-specific context like ``get_actor_id()``, which, when inherited by Torch Dataloader\n+    subprocesses using ``fork``, can cause hangs during logging since these subprocesses\n+    aren't Ray actors.",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec7843139a,ray-project/ray,2510779588,2085290425,justinvyu,3887863,2025-05-12T19:09:40+00:00,2025-05-12T19:12:43+00:00,"I think this is the more common way to get the context:\r\n\r\n```suggestion\r\n                train_loader = DataLoader(train_data, batch_size=128, shuffle=True, multiprocessing_context=multiprocessing.get_context(""forkserver""))\r\n```",true,doc/source/train/getting-started-pytorch.rst,NaN,"@@ -124,7 +126,7 @@ Compare a PyTorch training script with and without Ray Train.\n                 transform = Compose([ToTensor(), Normalize((0.28604,), (0.32025,))])\n                 data_dir = os.path.join(tempfile.gettempdir(), ""data"")\n                 train_data = FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n-                train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n+                train_loader = DataLoader(train_data, batch_size=128, shuffle=True, multiprocessing_context=multiprocessing.context.ForkServerContext())",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec7843139b,ray-project/ray,2510779588,2085291811,justinvyu,3887863,2025-05-12T19:10:44+00:00,2025-05-12T19:12:43+00:00,Can you also move this quickstart code to a tested doctest file?,true,doc/source/train/getting-started-pytorch.rst,NaN,"@@ -124,7 +126,7 @@ Compare a PyTorch training script with and without Ray Train.\n                 transform = Compose([ToTensor(), Normalize((0.28604,), (0.32025,))])\n                 data_dir = os.path.join(tempfile.gettempdir(), ""data"")\n                 train_data = FashionMNIST(root=data_dir, train=True, download=True, transform=transform)\n-                train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n+                train_loader = DataLoader(train_data, batch_size=128, shuffle=True, multiprocessing_context=multiprocessing.context.ForkServerContext())",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec7843139c,ray-project/ray,2510685905,2085188933,edoakes,9871461,2025-05-12T18:05:06+00:00,2025-05-12T18:06:30+00:00,let's inline these message types in the same file... it's excessively verbose as-is,false,NaN,NaN,NaN,1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec7843139d,ray-project/ray,2510685905,2085190467,edoakes,9871461,2025-05-12T18:06:14+00:00,2025-05-12T18:06:30+00:00,"what is the existing `timestamp` type? and this is not a public API change, right?\r\n\r\nplease also update the comment (not in ns anymore, but in protobuf-native timestamp format)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec7843139e,ray-project/ray,2510685905,2085626802,MengjinYan,8065093,2025-05-12T22:32:50+00:00,2025-05-12T22:32:51+00:00,This is not an API that is used publicly. \r\n\r\nThe exiting `timestamp` is a wrong type and I corrected it in this PR after the build file is updated. \r\n\r\nWill make change to update the comment.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec7843139f,ray-project/ray,2510685905,2085628541,kevin85421,20109646,2025-05-12T22:35:14+00:00,2025-05-12T22:44:03+00:00,https://youtu.be/sImWl7JyK_Q?si=TAPfwzw1_opdjgRf\r\n\r\nThe gRPC maintainer's talk mentions that setting a deadline or timeout is considered a best practice. It's not necessary to update it in this PR.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a0,ray-project/ray,2510685905,2085628898,kevin85421,20109646,2025-05-12T22:35:49+00:00,2025-05-12T22:44:03+00:00,"nit\r\n```suggestion\r\n    RAY_LOG(INFO) << ""Initiating the event aggregator client with address: "" << address\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a1,ray-project/ray,2510685905,2085629498,kevin85421,20109646,2025-05-12T22:36:42+00:00,2025-05-12T22:44:03+00:00,"```suggestion\r\n                  << "" port: "" << port;\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a2,ray-project/ray,2510685905,2085632774,kevin85421,20109646,2025-05-12T22:40:52+00:00,2025-05-12T22:44:03+00:00,remove this?,false,NaN,NaN,NaN,1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a3,ray-project/ray,2510685905,2085695642,MengjinYan,8065093,2025-05-12T23:47:32+00:00,2025-05-12T23:47:33+00:00,Thanks for the reference! Will add a comment for the improvement in a future PR. ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a4,ray-project/ray,2510685905,2085188933,edoakes,9871461,2025-05-12T18:05:06+00:00,2025-05-12T18:06:30+00:00,let's inline these message types in the same file... it's excessively verbose as-is,true,src/ray/protobuf/events/aggregator_service/event_aggregator_service.proto,57,"@@ -0,0 +1,26 @@\n+// Copyright 2025 The Ray Authors.\n+//\n+// Licensed under the Apache License, Version 2.0 (the ""License"");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//  http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an ""AS IS"" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+syntax = ""proto3"";\n+\n+import ""src/ray/protobuf/events/aggregator_service/add_event_request.proto"";\n+import ""src/ray/protobuf/events/aggregator_service/add_event_reply.proto"";\n+\n+package ray.rpc.events;\n+\n+// Service for adding events to the event aggregator.\n+service EventAggregatorService {\n+  // Add events to the event aggregator.\n+  rpc AddEvents(AddEventRequest) returns (AddEventReply);",1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a5,ray-project/ray,2510685905,2085190467,edoakes,9871461,2025-05-12T18:06:14+00:00,2025-05-12T18:06:30+00:00,"what is the existing `timestamp` type? and this is not a public API change, right?\r\n\r\nplease also update the comment (not in ns anymore, but in protobuf-native timestamp format)",true,src/ray/protobuf/events/base.proto,20,"@@ -62,7 +68,7 @@ message RayEvent {\n 	// nested message.\n 	EventType event_type = 3;\n 	// Epoch timestamp in ns, captured when the event is created\n-	timestamp timestamp_ns = 4;\n+	google.protobuf.Timestamp timestamp = 4;",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a6,ray-project/ray,2510685905,2085626802,MengjinYan,8065093,2025-05-12T22:32:50+00:00,2025-05-12T22:32:51+00:00,This is not an API that is used publicly. \r\n\r\nThe exiting `timestamp` is a wrong type and I corrected it in this PR after the build file is updated. \r\n\r\nWill make change to update the comment.,true,src/ray/protobuf/events/base.proto,20,"@@ -62,7 +68,7 @@ message RayEvent {\n 	// nested message.\n 	EventType event_type = 3;\n 	// Epoch timestamp in ns, captured when the event is created\n-	timestamp timestamp_ns = 4;\n+	google.protobuf.Timestamp timestamp = 4;",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a7,ray-project/ray,2510685905,2085628541,kevin85421,20109646,2025-05-12T22:35:14+00:00,2025-05-12T22:44:03+00:00,https://youtu.be/sImWl7JyK_Q?si=TAPfwzw1_opdjgRf\r\n\r\nThe gRPC maintainer's talk mentions that setting a deadline or timeout is considered a best practice. It's not necessary to update it in this PR.,true,src/ray/rpc/event_aggregator_client.h,68,"@@ -0,0 +1,76 @@\n+// Copyright 2025 The Ray Authors.\n+//\n+// Licensed under the Apache License, Version 2.0 (the ""License"");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//  http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an ""AS IS"" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+#pragma once\n+\n+#include <grpcpp/grpcpp.h>\n+\n+#include <memory>\n+#include <string>\n+\n+#include ""ray/rpc/grpc_client.h""\n+#include ""ray/util/logging.h""\n+#include ""src/ray/protobuf/event_aggregator_service.grpc.pb.h""\n+#include ""src/ray/protobuf/event_aggregator_service.pb.h""\n+\n+namespace ray {\n+namespace rpc {\n+\n+/// Client used for communicating with an event aggregator server in the dashboard\n+/// agent.\n+class EventAggregatorClient {\n+ public:\n+  virtual ~EventAggregatorClient() = default;\n+\n+  /// Report event to event aggregator.\n+  ///\n+  /// \param[in] request The request message.\n+  /// \param[in] callback The callback function that handles reply.\n+  virtual void AddEvents(const rpc::AddEventRequest &request,\n+                         const ClientCallback<rpc::AddEventReply> &callback) = 0;\n+};\n+\n+class EventAggregatorClientImpl : public EventAggregatorClient {\n+ public:\n+  /// Constructor.\n+  ///\n+  /// \param[in] address Address of the event aggregator server.\n+  /// \param[in] port Port of the event aggregator server.\n+  /// \param[in] client_call_manager The `ClientCallManager` used for managing requests.\n+  EventAggregatorClientImpl(const std::string &address,\n+                            const int port,\n+                            ClientCallManager &client_call_manager) {\n+    RAY_LOG(INFO) << ""Initiating the event aggregator client with address:"" << address\n+                  << "" port:"" << port;\n+    grpc_client_ = std::make_unique<GrpcClient<EventAggregatorService>>(\n+        address, port, client_call_manager);\n+  };\n+\n+  void AddEvents(const rpc::AddEventRequest &request,\n+                 const ClientCallback<rpc::AddEventReply> &callback) override {\n+    grpc_client_->CallMethod<rpc::AddEventRequest, rpc::AddEventReply>(\n+        &EventAggregatorService::Stub::PrepareAsyncReceiveEvents,\n+        request,\n+        callback,\n+        ""EventAggregatorService.grpc_client.AddEvents"",\n+        /*timeout_ms*/ -1);",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a8,ray-project/ray,2510685905,2085628898,kevin85421,20109646,2025-05-12T22:35:49+00:00,2025-05-12T22:44:03+00:00,"nit\r\n```suggestion\r\n    RAY_LOG(INFO) << ""Initiating the event aggregator client with address: "" << address\r\n```",true,src/ray/rpc/event_aggregator_client.h,NaN,"@@ -0,0 +1,76 @@\n+// Copyright 2025 The Ray Authors.\n+//\n+// Licensed under the Apache License, Version 2.0 (the ""License"");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//  http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an ""AS IS"" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+#pragma once\n+\n+#include <grpcpp/grpcpp.h>\n+\n+#include <memory>\n+#include <string>\n+\n+#include ""ray/rpc/grpc_client.h""\n+#include ""ray/util/logging.h""\n+#include ""src/ray/protobuf/event_aggregator_service.grpc.pb.h""\n+#include ""src/ray/protobuf/event_aggregator_service.pb.h""\n+\n+namespace ray {\n+namespace rpc {\n+\n+/// Client used for communicating with an event aggregator server in the dashboard\n+/// agent.\n+class EventAggregatorClient {\n+ public:\n+  virtual ~EventAggregatorClient() = default;\n+\n+  /// Report event to event aggregator.\n+  ///\n+  /// \param[in] request The request message.\n+  /// \param[in] callback The callback function that handles reply.\n+  virtual void AddEvents(const rpc::AddEventRequest &request,\n+                         const ClientCallback<rpc::AddEventReply> &callback) = 0;\n+};\n+\n+class EventAggregatorClientImpl : public EventAggregatorClient {\n+ public:\n+  /// Constructor.\n+  ///\n+  /// \param[in] address Address of the event aggregator server.\n+  /// \param[in] port Port of the event aggregator server.\n+  /// \param[in] client_call_manager The `ClientCallManager` used for managing requests.\n+  EventAggregatorClientImpl(const std::string &address,\n+                            const int port,\n+                            ClientCallManager &client_call_manager) {\n+    RAY_LOG(INFO) << ""Initiating the event aggregator client with address:"" << address",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313a9,ray-project/ray,2510685905,2085629498,kevin85421,20109646,2025-05-12T22:36:42+00:00,2025-05-12T22:44:03+00:00,"```suggestion\r\n                  << "" port: "" << port;\r\n```",true,src/ray/rpc/event_aggregator_client.h,NaN,"@@ -0,0 +1,76 @@\n+// Copyright 2025 The Ray Authors.\n+//\n+// Licensed under the Apache License, Version 2.0 (the ""License"");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//  http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an ""AS IS"" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+#pragma once\n+\n+#include <grpcpp/grpcpp.h>\n+\n+#include <memory>\n+#include <string>\n+\n+#include ""ray/rpc/grpc_client.h""\n+#include ""ray/util/logging.h""\n+#include ""src/ray/protobuf/event_aggregator_service.grpc.pb.h""\n+#include ""src/ray/protobuf/event_aggregator_service.pb.h""\n+\n+namespace ray {\n+namespace rpc {\n+\n+/// Client used for communicating with an event aggregator server in the dashboard\n+/// agent.\n+class EventAggregatorClient {\n+ public:\n+  virtual ~EventAggregatorClient() = default;\n+\n+  /// Report event to event aggregator.\n+  ///\n+  /// \param[in] request The request message.\n+  /// \param[in] callback The callback function that handles reply.\n+  virtual void AddEvents(const rpc::AddEventRequest &request,\n+                         const ClientCallback<rpc::AddEventReply> &callback) = 0;\n+};\n+\n+class EventAggregatorClientImpl : public EventAggregatorClient {\n+ public:\n+  /// Constructor.\n+  ///\n+  /// \param[in] address Address of the event aggregator server.\n+  /// \param[in] port Port of the event aggregator server.\n+  /// \param[in] client_call_manager The `ClientCallManager` used for managing requests.\n+  EventAggregatorClientImpl(const std::string &address,\n+                            const int port,\n+                            ClientCallManager &client_call_manager) {\n+    RAY_LOG(INFO) << ""Initiating the event aggregator client with address:"" << address\n+                  << "" port:"" << port;",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313aa,ray-project/ray,2510685905,2085632774,kevin85421,20109646,2025-05-12T22:40:52+00:00,2025-05-12T22:44:03+00:00,remove this?,true,BUILD.bazel,NaN,"@@ -184,13 +184,23 @@ ray_cc_library(\n     hdrs = [""src/ray/rpc/metrics_agent_client.h""],\n     deps = [\n         "":grpc_client"",\n-        ""//src/ray/common:status"",\n         ""//src/ray/protobuf:reporter_cc_proto"",\n         ""//src/ray/util:logging"",\n         ""@com_github_grpc_grpc//:grpc++"",\n     ],\n )\n \n+ray_cc_library(\n+    name = ""event_aggregator_client"",\n+    hdrs = [""src/ray/rpc/event_aggregator_client.h""],\n+    deps = [\n+        "":grpc_client"",\n+        ""//src/ray/common:status"",",1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ab,ray-project/ray,2510685905,2085695642,MengjinYan,8065093,2025-05-12T23:47:32+00:00,2025-05-12T23:47:33+00:00,Thanks for the reference! Will add a comment for the improvement in a future PR. ,true,src/ray/rpc/event_aggregator_client.h,68,"@@ -0,0 +1,76 @@\n+// Copyright 2025 The Ray Authors.\n+//\n+// Licensed under the Apache License, Version 2.0 (the ""License"");\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//  http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an ""AS IS"" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+#pragma once\n+\n+#include <grpcpp/grpcpp.h>\n+\n+#include <memory>\n+#include <string>\n+\n+#include ""ray/rpc/grpc_client.h""\n+#include ""ray/util/logging.h""\n+#include ""src/ray/protobuf/event_aggregator_service.grpc.pb.h""\n+#include ""src/ray/protobuf/event_aggregator_service.pb.h""\n+\n+namespace ray {\n+namespace rpc {\n+\n+/// Client used for communicating with an event aggregator server in the dashboard\n+/// agent.\n+class EventAggregatorClient {\n+ public:\n+  virtual ~EventAggregatorClient() = default;\n+\n+  /// Report event to event aggregator.\n+  ///\n+  /// \param[in] request The request message.\n+  /// \param[in] callback The callback function that handles reply.\n+  virtual void AddEvents(const rpc::AddEventRequest &request,\n+                         const ClientCallback<rpc::AddEventReply> &callback) = 0;\n+};\n+\n+class EventAggregatorClientImpl : public EventAggregatorClient {\n+ public:\n+  /// Constructor.\n+  ///\n+  /// \param[in] address Address of the event aggregator server.\n+  /// \param[in] port Port of the event aggregator server.\n+  /// \param[in] client_call_manager The `ClientCallManager` used for managing requests.\n+  EventAggregatorClientImpl(const std::string &address,\n+                            const int port,\n+                            ClientCallManager &client_call_manager) {\n+    RAY_LOG(INFO) << ""Initiating the event aggregator client with address:"" << address\n+                  << "" port:"" << port;\n+    grpc_client_ = std::make_unique<GrpcClient<EventAggregatorService>>(\n+        address, port, client_call_manager);\n+  };\n+\n+  void AddEvents(const rpc::AddEventRequest &request,\n+                 const ClientCallback<rpc::AddEventReply> &callback) override {\n+    grpc_client_->CallMethod<rpc::AddEventRequest, rpc::AddEventReply>(\n+        &EventAggregatorService::Stub::PrepareAsyncReceiveEvents,\n+        request,\n+        callback,\n+        ""EventAggregatorService.grpc_client.AddEvents"",\n+        /*timeout_ms*/ -1);",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ac,ray-project/ray,2510675686,2082583523,raulchen,2883335,2025-05-09T22:46:03+00:00,2025-05-09T22:46:13+00:00,let's use the whole `train/epoch` time,false,NaN,NaN,NaN,1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ad,ray-project/ray,2510675686,2082583523,raulchen,2883335,2025-05-09T22:46:03+00:00,2025-05-09T22:46:13+00:00,let's use the whole `train/epoch` time,true,release/train_tests/benchmark/runner.py,20,"@@ -304,8 +304,7 @@ def get_metrics(self) -> Dict[str, float]:\n         num_workers = ray.train.get_context().get_world_size()\n         train_time = (\n             self._metrics[""train/step""].get()\n-            # Exclude the time it takes to get the first batch.\n-            # + self._metrics[""train/iter_first_batch""].get()\n+            + self._metrics[""train/iter_first_batch""].get()\n             + self._metrics[""train/iter_batch""].get()",1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ae,ray-project/ray,2510525505,2082484063,matthewdeng,3967392,2025-05-09T20:55:17+00:00,2025-05-09T20:55:23+00:00,It looks like this is being (incorrectly) referenced in this link [here](https://github.com/ray-project/ray/blame/86c0958a5f051780e1f4cf08ad37bde942040774/doc/source/train/huggingface-accelerate.rst#L209). Can you update it to point to the persistent storage guide? ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313af,ray-project/ray,2510525505,2082602707,TimothySeah,5440944,2025-05-09T23:06:14+00:00,2025-05-09T23:06:14+00:00,Done and verified in `file:///Users/tseah/ray/doc/_build/html/index.html`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b0,ray-project/ray,2510525505,2082484063,matthewdeng,3967392,2025-05-09T20:55:17+00:00,2025-05-09T20:55:23+00:00,It looks like this is being (incorrectly) referenced in this link [here](https://github.com/ray-project/ray/blame/86c0958a5f051780e1f4cf08ad37bde942040774/doc/source/train/huggingface-accelerate.rst#L209). Can you update it to point to the persistent storage guide? ,true,doc/source/train/user-guides/configuration-overview.rst,6,"@@ -1,32 +0,0 @@\n-:orphan:\n-\n-Configuration Overview\n-======================\n-\n-.. _train-run-config:",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b1,ray-project/ray,2510525505,2082602707,TimothySeah,5440944,2025-05-09T23:06:14+00:00,2025-05-09T23:06:14+00:00,Done and verified in `file:///Users/tseah/ray/doc/_build/html/index.html`,true,doc/source/train/user-guides/configuration-overview.rst,6,"@@ -1,32 +0,0 @@\n-:orphan:\n-\n-Configuration Overview\n-======================\n-\n-.. _train-run-config:",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b2,ray-project/ray,2510369657,2082363099,alexeykudinkin,428277,2025-05-09T19:08:41+00:00,2025-05-09T19:08:46+00:00,Please capitalize,false,NaN,NaN,NaN,1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b3,ray-project/ray,2510369657,2082363099,alexeykudinkin,428277,2025-05-09T19:08:41+00:00,2025-05-09T19:08:46+00:00,Please capitalize,true,python/ray/data/_internal/datasource/torch_datasource.py,NaN,"@@ -27,7 +27,9 @@ def get_read_tasks(self, parallelism):\n         assert parallelism == 1\n \n         meta = BlockMetadata(\n-            num_rows=len(self._dataset),\n+            # note: avoid len(self._dataset) because it will trigger",1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b4,ray-project/ray,2510356649,2085407073,lanbochen-anyscale,103082133,2025-05-12T20:30:56+00:00,2025-05-12T20:30:57+00:00,```suggestion\r\nflags:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b5,ray-project/ray,2510356649,2085407531,lanbochen-anyscale,103082133,2025-05-12T20:31:05+00:00,2025-05-12T20:31:06+00:00,```suggestion\r\nflags:\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b6,ray-project/ray,2510356649,2085407073,lanbochen-anyscale,103082133,2025-05-12T20:30:56+00:00,2025-05-12T20:30:57+00:00,```suggestion\r\nflags:\r\n```,true,release/train_tests/benchmark/compute_configs/compute_gpu_1x1_aws.yaml,NaN,"@@ -1,6 +1,9 @@\n cloud_id: {{env[""ANYSCALE_CLOUD_ID""]}}\n region: us-west-2\n \n+advanced_configurations_json:",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b7,ray-project/ray,2510356649,2085407531,lanbochen-anyscale,103082133,2025-05-12T20:31:05+00:00,2025-05-12T20:31:06+00:00,```suggestion\r\nflags:\r\n```,true,release/train_tests/benchmark/compute_configs/compute_gpu_1x1_multi_gpus_aws.yaml,NaN,"@@ -1,6 +1,9 @@\n cloud_id: {{env[""ANYSCALE_CLOUD_ID""]}}\n region: us-west-2\n \n+advanced_configurations_json:",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b8,ray-project/ray,2510271601,2085095056,jjyao,898023,2025-05-12T16:59:25+00:00,2025-05-12T16:59:26+00:00,"Does this work? For files inside `psutil`, if they import each other, will they import the public one instead of the `thirdparty_files` one?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313b9,ray-project/ray,2510271601,2085113943,Bye-legumes,121425509,2025-05-12T17:13:07+00:00,2025-05-12T17:13:07+00:00,"we are testing and it should work, let me check. I just remove `psutil` as there are too many places that use this. If this work we can check if we should refactor all or some libs that into internal, i.e. numpy, traceback,, time. Or we just modify the libs that tends to easily have issues? What do you think?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ba,ray-project/ray,2510271601,2085147341,jjyao,898023,2025-05-12T17:35:45+00:00,2025-05-12T17:35:46+00:00,"Yea, I'm not sure if that works. The way I'm aware is checking in the source code directly and change the import path of all the files. Something like https://github.com/pradyunsg/vendoring does.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313bb,ray-project/ray,2510271601,2085095056,jjyao,898023,2025-05-12T16:59:25+00:00,2025-05-12T16:59:26+00:00,"Does this work? For files inside `psutil`, if they import each other, will they import the public one instead of the `thirdparty_files` one?",true,python/ray/__init__.py,NaN,"@@ -50,8 +50,26 @@ def _configure_system():\n     thirdparty_files = os.path.join(\n         os.path.abspath(os.path.dirname(__file__)), ""thirdparty_files""\n     )\n+\n+    # Temporarily add thirdparty_files to sys.path to import the libraries,\n+    # but ensure we restore path and properly isolate the imports\n+    original_sys_path = sys.path.copy()\n     sys.path.insert(0, thirdparty_files)\n \n+    try:\n+        # Import for Ray internal usage\n+        import psutil\n+        import setproctitle\n+        import colorama\n+\n+        # Save the modules in Ray's private namespace\n+        sys.modules[""ray._private.psutil""] = psutil\n+        sys.modules[""ray._private.setproctitle""] = setproctitle\n+        sys.modules[""ray._private.colorama""] = colorama",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313bc,ray-project/ray,2510271601,2085113943,Bye-legumes,121425509,2025-05-12T17:13:07+00:00,2025-05-12T17:13:07+00:00,"we are testing and it should work, let me check. I just remove `psutil` as there are too many places that use this. If this work we can check if we should refactor all or some libs that into internal, i.e. numpy, traceback,, time. Or we just modify the libs that tends to easily have issues? What do you think?",true,python/ray/__init__.py,NaN,"@@ -50,8 +50,26 @@ def _configure_system():\n     thirdparty_files = os.path.join(\n         os.path.abspath(os.path.dirname(__file__)), ""thirdparty_files""\n     )\n+\n+    # Temporarily add thirdparty_files to sys.path to import the libraries,\n+    # but ensure we restore path and properly isolate the imports\n+    original_sys_path = sys.path.copy()\n     sys.path.insert(0, thirdparty_files)\n \n+    try:\n+        # Import for Ray internal usage\n+        import psutil\n+        import setproctitle\n+        import colorama\n+\n+        # Save the modules in Ray's private namespace\n+        sys.modules[""ray._private.psutil""] = psutil\n+        sys.modules[""ray._private.setproctitle""] = setproctitle\n+        sys.modules[""ray._private.colorama""] = colorama",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313bd,ray-project/ray,2510271601,2085147341,jjyao,898023,2025-05-12T17:35:45+00:00,2025-05-12T17:35:46+00:00,"Yea, I'm not sure if that works. The way I'm aware is checking in the source code directly and change the import path of all the files. Something like https://github.com/pradyunsg/vendoring does.",true,python/ray/__init__.py,NaN,"@@ -50,8 +50,26 @@ def _configure_system():\n     thirdparty_files = os.path.join(\n         os.path.abspath(os.path.dirname(__file__)), ""thirdparty_files""\n     )\n+\n+    # Temporarily add thirdparty_files to sys.path to import the libraries,\n+    # but ensure we restore path and properly isolate the imports\n+    original_sys_path = sys.path.copy()\n     sys.path.insert(0, thirdparty_files)\n \n+    try:\n+        # Import for Ray internal usage\n+        import psutil\n+        import setproctitle\n+        import colorama\n+\n+        # Save the modules in Ray's private namespace\n+        sys.modules[""ray._private.psutil""] = psutil\n+        sys.modules[""ray._private.setproctitle""] = setproctitle\n+        sys.modules[""ray._private.colorama""] = colorama",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313be,ray-project/ray,2508655629,2082092091,abrarsheikh,5113943,2025-05-09T16:44:30+00:00,2025-05-09T19:09:58+00:00,I am yet to figure out a way to get the docs_path attached to the application state.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313bf,ray-project/ray,2508655629,2082671571,abrarsheikh,5113943,2025-05-10T00:05:51+00:00,2025-05-10T00:05:52+00:00,"simply refactored into a function, no change in logic",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c0,ray-project/ray,2508655629,2082682185,abrarsheikh,5113943,2025-05-10T00:19:14+00:00,2025-05-10T00:19:14+00:00,"A problem here that is not critical but annoying, if there are 100 replicas, then we would be making 100 controller calls\r\n",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c1,ray-project/ray,2508655629,2082092091,abrarsheikh,5113943,2025-05-09T16:44:30+00:00,2025-05-09T19:09:58+00:00,I am yet to figure out a way to get the docs_path attached to the application state.,true,python/ray/serve/api.py,NaN,"@@ -236,8 +289,8 @@ async def __del__(self):\n                         cls.__del__(self)\n \n         ASGIIngressWrapper.__name__ = cls.__name__\n-        if hasattr(frozen_app, ""docs_url""):\n-            ASGIIngressWrapper.__fastapi_docs_path__ = frozen_app.docs_url\n+        if hasattr(frozen_app_or_func, ""docs_url""):\n+            ASGIIngressWrapper.__fastapi_docs_path__ = frozen_app_or_func.docs_url",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c2,ray-project/ray,2508655629,2082671571,abrarsheikh,5113943,2025-05-10T00:05:51+00:00,2025-05-10T00:05:52+00:00,"simply refactored into a function, no change in logic",true,python/ray/serve/_private/replica.py,65,"@@ -1390,18 +1419,7 @@ async def initialize_callable(self) -> Optional[ASGIApp]:\n             )\n \n             if isinstance(self._callable, ASGIAppReplicaWrapper):\n-                app: Starlette = self._callable.app\n-                # The reason we need to do this is because BackPressureError is a serve internal exception\n-                # and FastAPI doesn't know how to handle it, so it treats it as a 500 error.\n-                # With same reasoning, we are not handling TimeoutError because it's a generic exception\n-                # the FastAPI knows how to handle. See https://www.starlette.io/exceptions/\n-                def handle_exception(_: Request, exc: Exception):\n-                    return self.handle_exception(exc)\n-\n-                for exc in self.service_unavailable_exceptions:\n-                    app.add_exception_handler(exc, handle_exception)\n-\n-                await self._callable._run_asgi_lifespan_startup()",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c3,ray-project/ray,2508655629,2082682185,abrarsheikh,5113943,2025-05-10T00:19:14+00:00,2025-05-10T00:19:14+00:00,"A problem here that is not critical but annoying, if there are 100 replicas, then we would be making 100 controller calls\r\n",true,python/ray/serve/_private/replica.py,22,"@@ -690,6 +694,14 @@ async def initialize(self, deployment_config: DeploymentConfig):\n                     self._user_callable_asgi_app = await asyncio.wrap_future(\n                         self._user_callable_wrapper.initialize_callable()\n                     )\n+                    if self._user_callable_asgi_app:\n+                        docs_path = self._user_callable_wrapper._callable.docs_path\n+                        # Since it's possible for user to pass a builder function that returns a FastAPI app\n+                        # we will not have a docs path on the ASGI app object untill initialize_callable is called\n+                        # so we set the docs path on the controller in this case\n+                        await self._controller_handle.set_docs_path.remote(\n+                            self._deployment_id.app_name, docs_path\n+                        )",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c4,ray-project/ray,2508331290,2082034513,omatthew98,22609193,2025-05-09T16:28:32+00:00,2025-05-09T16:48:37+00:00,Not sure if we need to add it to this or just `python/ray/dashboard/modules/metrics/dashboards/data_dashboard_panels.py`? There are a lot of metrics that we only show on the grafana dashboard. If others have strong opinions happy to defer to them.,false,NaN,NaN,NaN,1,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c5,ray-project/ray,2508331290,2082035761,omatthew98,22609193,2025-05-09T16:29:30+00:00,2025-05-09T16:48:37+00:00,"If this is just the mean time, let's include `mean` in the name of the metric.",false,NaN,NaN,NaN,1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c6,ray-project/ray,2508331290,2082086883,omatthew98,22609193,2025-05-09T16:42:38+00:00,2025-05-09T16:48:37+00:00,"Nit: ""Mean time spent...""",false,NaN,NaN,NaN,1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c7,ray-project/ray,2508331290,2085621315,iamjustinhsu,140442892,2025-05-12T22:27:12+00:00,2025-05-12T22:27:12+00:00,someone double check what the buckets should be,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c8,ray-project/ray,2508331290,2085621595,iamjustinhsu,140442892,2025-05-12T22:27:35+00:00,2025-05-12T22:27:36+00:00,someone double check the query,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313c9,ray-project/ray,2508331290,2085641742,raulchen,2883335,2025-05-12T22:53:05+00:00,2025-05-12T22:55:47+00:00,"I don't think we need so many percentiles. \r\nThese are probably good enough: min, P50, P90, P99, max",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ca,ray-project/ray,2508331290,2085642740,raulchen,2883335,2025-05-12T22:54:33+00:00,2025-05-12T22:55:47+00:00,It looks weird to have a method that does completely different things in each subclass. \r\nCan we just check the type on the stats.py side?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313cb,ray-project/ray,2508331290,2085643596,raulchen,2883335,2025-05-12T22:55:43+00:00,2025-05-12T22:55:47+00:00,not familiar with this. but let's manually run a job and check the charts. ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313cc,ray-project/ray,2508331290,2085651428,alexeykudinkin,428277,2025-05-12T23:07:03+00:00,2025-05-12T23:08:35+00:00,"+1 to Hao comment\r\n\r\nP5, P50, P75, P99",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313cd,ray-project/ray,2508331290,2085652323,alexeykudinkin,428277,2025-05-12T23:08:31+00:00,2025-05-12T23:08:35+00:00,"+1 \r\n\r\nLet's make sure we call appropriate methods for every metric type (set for Gauge, inc for Counter, etc)",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ce,ray-project/ray,2508331290,2082034513,omatthew98,22609193,2025-05-09T16:28:32+00:00,2025-05-09T16:48:37+00:00,Not sure if we need to add it to this or just `python/ray/dashboard/modules/metrics/dashboards/data_dashboard_panels.py`? There are a lot of metrics that we only show on the grafana dashboard. If others have strong opinions happy to defer to them.,true,python/ray/dashboard/client/src/pages/metrics/Metrics.tsx,4,"@@ -304,6 +304,10 @@ const DATA_METRICS_CONFIG: MetricsSectionConfig[] = [\n         title: ""Task Submission Backpressure Time"",\n         pathParams: ""orgId=1&theme=light&panelId=37"",\n       },\n+      {",1,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313cf,ray-project/ray,2508331290,2082035761,omatthew98,22609193,2025-05-09T16:29:30+00:00,2025-05-09T16:48:37+00:00,"If this is just the mean time, let's include `mean` in the name of the metric.",true,python/ray/data/_internal/execution/interfaces/op_runtime_metrics.py,92,"@@ -338,6 +338,11 @@ class OpRuntimeMetrics(metaclass=OpRuntimesMetricsMeta):\n         description=""Time spent in task submission backpressure."",\n         metrics_group=MetricsGroup.TASKS,\n     )\n+    task_completion_time: float = metric_field(",1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d0,ray-project/ray,2508331290,2082086883,omatthew98,22609193,2025-05-09T16:42:38+00:00,2025-05-09T16:48:37+00:00,"Nit: ""Mean time spent...""",true,python/ray/data/_internal/execution/interfaces/op_runtime_metrics.py,NaN,"@@ -338,6 +338,11 @@ class OpRuntimeMetrics(metaclass=OpRuntimesMetricsMeta):\n         description=""Time spent in task submission backpressure."",\n         metrics_group=MetricsGroup.TASKS,\n     )\n+    task_completion_time: float = metric_field(\n+        default=0,\n+        description=""Time spent running all tasks to completion."",",1,1,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d1,ray-project/ray,2508331290,2085621315,iamjustinhsu,140442892,2025-05-12T22:27:12+00:00,2025-05-12T22:27:12+00:00,someone double check what the buckets should be,true,python/ray/data/_internal/execution/interfaces/op_runtime_metrics.py,80,"@@ -338,6 +358,25 @@ class OpRuntimeMetrics(metaclass=OpRuntimesMetricsMeta):\n         description=""Time spent in task submission backpressure."",\n         metrics_group=MetricsGroup.TASKS,\n     )\n+    histogram_buckets_s = [",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d2,ray-project/ray,2508331290,2085621595,iamjustinhsu,140442892,2025-05-12T22:27:35+00:00,2025-05-12T22:27:36+00:00,someone double check the query,true,python/ray/dashboard/modules/metrics/dashboards/data_dashboard_panels.py,11,"@@ -431,6 +431,62 @@\n         fill=0,\n         stack=True,\n     ),\n+    Panel(\n+        id=38,\n+        title=""(p50) Task Completion Time"",\n+        description=""Time spent running tasks to completion."",\n+        unit=""ms"",\n+        targets=[\n+            Target(\n+                expr=""histogram_quantile(0.50, sum by (dataset, operator, le) (ray_data_task_completion_time_bucket{{{global_filters}}}))"",",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d3,ray-project/ray,2508331290,2085641742,raulchen,2883335,2025-05-12T22:53:05+00:00,2025-05-12T22:55:47+00:00,"I don't think we need so many percentiles. \r\nThese are probably good enough: min, P50, P90, P99, max",true,python/ray/data/_internal/execution/interfaces/op_runtime_metrics.py,80,"@@ -338,6 +358,25 @@ class OpRuntimeMetrics(metaclass=OpRuntimesMetricsMeta):\n         description=""Time spent in task submission backpressure."",\n         metrics_group=MetricsGroup.TASKS,\n     )\n+    histogram_buckets_s = [",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d4,ray-project/ray,2508331290,2085642740,raulchen,2883335,2025-05-12T22:54:33+00:00,2025-05-12T22:55:47+00:00,It looks weird to have a method that does completely different things in each subclass. \r\nCan we just check the type on the stats.py side?,true,python/ray/util/metrics.py,4,"@@ -121,6 +121,13 @@ def _record(\n         self._validate_tags(final_tags)\n         self._metric.record(value, tags=final_tags)\n \n+    def inc_set_or_observe(",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d5,ray-project/ray,2508331290,2085643596,raulchen,2883335,2025-05-12T22:55:43+00:00,2025-05-12T22:55:47+00:00,not familiar with this. but let's manually run a job and check the charts. ,true,python/ray/dashboard/modules/metrics/dashboards/data_dashboard_panels.py,11,"@@ -431,6 +431,62 @@\n         fill=0,\n         stack=True,\n     ),\n+    Panel(\n+        id=38,\n+        title=""(p50) Task Completion Time"",\n+        description=""Time spent running tasks to completion."",\n+        unit=""ms"",\n+        targets=[\n+            Target(\n+                expr=""histogram_quantile(0.50, sum by (dataset, operator, le) (ray_data_task_completion_time_bucket{{{global_filters}}}))"",",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d6,ray-project/ray,2508331290,2085651428,alexeykudinkin,428277,2025-05-12T23:07:03+00:00,2025-05-12T23:08:35+00:00,"+1 to Hao comment\r\n\r\nP5, P50, P75, P99",true,python/ray/data/_internal/execution/interfaces/op_runtime_metrics.py,80,"@@ -338,6 +358,25 @@ class OpRuntimeMetrics(metaclass=OpRuntimesMetricsMeta):\n         description=""Time spent in task submission backpressure."",\n         metrics_group=MetricsGroup.TASKS,\n     )\n+    histogram_buckets_s = [",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d7,ray-project/ray,2508331290,2085652323,alexeykudinkin,428277,2025-05-12T23:08:31+00:00,2025-05-12T23:08:35+00:00,"+1 \r\n\r\nLet's make sure we call appropriate methods for every metric type (set for Gauge, inc for Counter, etc)",true,python/ray/util/metrics.py,4,"@@ -121,6 +121,13 @@ def _record(\n         self._validate_tags(final_tags)\n         self._metric.record(value, tags=final_tags)\n \n+    def inc_set_or_observe(",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d8,ray-project/ray,2508198937,2080674770,raulchen,2883335,2025-05-09T00:35:37+00:00,2025-05-09T00:35:38+00:00,this function will be called in a different process. \r\nIf we simply capture the `wrapper`. `init_fn` and `fn` will capture 2 different instances of the wrappers. \r\nWe need to pass the wrapper class to the map task. And `init_fn/fn` need to be defined in the wrapper and access the context with `self.map_actor_context`. ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313d9,ray-project/ray,2508198937,2080674770,raulchen,2883335,2025-05-09T00:35:37+00:00,2025-05-09T00:35:38+00:00,this function will be called in a different process. \r\nIf we simply capture the `wrapper`. `init_fn` and `fn` will capture 2 different instances of the wrappers. \r\nWe need to pass the wrapper class to the map task. And `init_fn/fn` need to be defined in the wrapper and access the context with `self.map_actor_context`. ,true,python/ray/data/_internal/planner/plan_udf_map_op.py,NaN,"@@ -266,8 +287,8 @@ def _parse_op_fn(op: AbstractUDFMap):\n             op_fn = make_callable_class_concurrent(op_fn)\n \n         def init_fn():\n-            if ray.data._map_actor_context is None:\n-                ray.data._map_actor_context = _MapActorContext(\n+            if wrapper.map_actor_context is None:\n+                wrapper.map_actor_context = _MapActorContext(",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313da,ray-project/ray,2508049657,2080627660,MengjinYan,8065093,2025-05-08T23:29:34+00:00,2025-05-08T23:30:41+00:00,"Just curious, if we already make it a enum class and using it with scope, wondering do we still need to update the name of the items in the enum? ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313db,ray-project/ray,2508049657,2080631268,dayshah,85444498,2025-05-08T23:33:37+00:00,2025-05-08T23:33:37+00:00,"ya we do, i tried without the windows build fails",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313dc,ray-project/ray,2508049657,2080633080,dayshah,85444498,2025-05-08T23:34:39+00:00,2025-05-08T23:34:39+00:00,updated description a bit,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313dd,ray-project/ray,2508049657,2080639157,israbbani,7349323,2025-05-08T23:40:37+00:00,2025-05-08T23:40:37+00:00,Am I reading this right? `LabelSelector::IN` is broken in windows but `LabelSelectors::LABEL_IN` is not. The theory is that there's macro substitution happening with the keyword IN? ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313de,ray-project/ray,2508049657,2080642182,dayshah,85444498,2025-05-08T23:44:15+00:00,2025-05-08T23:44:15+00:00,correct https://learn.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/compiler-error-c2059?view=msvc-170 this is the specific error I saw. And it compiled as LABEL_IN but not as IN even after the change to enum class 😃 ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313df,ray-project/ray,2508049657,2080656501,israbbani,7349323,2025-05-09T00:05:33+00:00,2025-05-09T00:05:33+00:00,🤯 ,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e0,ray-project/ray,2508049657,2080627660,MengjinYan,8065093,2025-05-08T23:29:34+00:00,2025-05-08T23:30:41+00:00,"Just curious, if we already make it a enum class and using it with scope, wondering do we still need to update the name of the items in the enum? ",true,src/ray/common/scheduling/label_selector.h,8,"@@ -22,11 +22,11 @@\n \n namespace ray {\n \n-enum LabelSelectorOperator {\n+enum class LabelSelectorOperator {\n   // This is to support equality or in semantics.\n-  IN = 0,\n+  LABEL_IN = 0,",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e1,ray-project/ray,2508049657,2080631268,dayshah,85444498,2025-05-08T23:33:37+00:00,2025-05-08T23:33:37+00:00,"ya we do, i tried without the windows build fails",true,src/ray/common/scheduling/label_selector.h,8,"@@ -22,11 +22,11 @@\n \n namespace ray {\n \n-enum LabelSelectorOperator {\n+enum class LabelSelectorOperator {\n   // This is to support equality or in semantics.\n-  IN = 0,\n+  LABEL_IN = 0,",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e2,ray-project/ray,2508049657,2080633080,dayshah,85444498,2025-05-08T23:34:39+00:00,2025-05-08T23:34:39+00:00,updated description a bit,true,src/ray/common/scheduling/label_selector.h,8,"@@ -22,11 +22,11 @@\n \n namespace ray {\n \n-enum LabelSelectorOperator {\n+enum class LabelSelectorOperator {\n   // This is to support equality or in semantics.\n-  IN = 0,\n+  LABEL_IN = 0,",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e3,ray-project/ray,2508049657,2080639157,israbbani,7349323,2025-05-08T23:40:37+00:00,2025-05-08T23:40:37+00:00,Am I reading this right? `LabelSelector::IN` is broken in windows but `LabelSelectors::LABEL_IN` is not. The theory is that there's macro substitution happening with the keyword IN? ,true,src/ray/common/scheduling/label_selector.h,8,"@@ -22,11 +22,11 @@\n \n namespace ray {\n \n-enum LabelSelectorOperator {\n+enum class LabelSelectorOperator {\n   // This is to support equality or in semantics.\n-  IN = 0,\n+  LABEL_IN = 0,",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e4,ray-project/ray,2508049657,2080642182,dayshah,85444498,2025-05-08T23:44:15+00:00,2025-05-08T23:44:15+00:00,correct https://learn.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/compiler-error-c2059?view=msvc-170 this is the specific error I saw. And it compiled as LABEL_IN but not as IN even after the change to enum class 😃 ,true,src/ray/common/scheduling/label_selector.h,8,"@@ -22,11 +22,11 @@\n \n namespace ray {\n \n-enum LabelSelectorOperator {\n+enum class LabelSelectorOperator {\n   // This is to support equality or in semantics.\n-  IN = 0,\n+  LABEL_IN = 0,",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e5,ray-project/ray,2508049657,2080656501,israbbani,7349323,2025-05-09T00:05:33+00:00,2025-05-09T00:05:33+00:00,🤯 ,true,src/ray/common/scheduling/label_selector.h,8,"@@ -22,11 +22,11 @@\n \n namespace ray {\n \n-enum LabelSelectorOperator {\n+enum class LabelSelectorOperator {\n   // This is to support equality or in semantics.\n-  IN = 0,\n+  LABEL_IN = 0,",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e6,ray-project/ray,2507948543,2080381134,pcmoritz,113316,2025-05-08T19:56:51+00:00,2025-05-08T19:56:52+00:00,I think we need to add this back like in the other endpoints (it is probably thrown by one of the wrappers?),false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e7,ray-project/ray,2507948543,2080402319,thomasdesr,681004,2025-05-08T20:14:43+00:00,2025-05-08T20:14:43+00:00,"Whoops, fixing. Thanks!",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e8,ray-project/ray,2507948543,2080381134,pcmoritz,113316,2025-05-08T19:56:51+00:00,2025-05-08T19:56:52+00:00,I think we need to add this back like in the other endpoints (it is probably thrown by one of the wrappers?),true,python/ray/util/state/state_cli.py,11,"@@ -1103,10 +1103,8 @@ def log_actor(\n         ```\n \n     Raises:\n-        :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313e9,ray-project/ray,2507948543,2080402319,thomasdesr,681004,2025-05-08T20:14:43+00:00,2025-05-08T20:14:43+00:00,"Whoops, fixing. Thanks!",true,python/ray/util/state/state_cli.py,11,"@@ -1103,10 +1103,8 @@ def log_actor(\n         ```\n \n     Raises:\n-        :class:`RayStateApiException <ray.util.state.exception.RayStateApiException>`",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ea,ray-project/ray,2507803464,2080255983,jjyao,898023,2025-05-08T18:31:24+00:00,2025-05-08T18:31:24+00:00,Do we plan to add `pydoclint` to pre-commit?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313eb,ray-project/ray,2507803464,2080259840,thomasdesr,681004,2025-05-08T18:32:49+00:00,2025-05-08T18:32:49+00:00,"Yep, just doing a fast-pass on a lot of the big errors before we do so the set of ""exceptions"" is small enough to be vaguely sensible",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ec,ray-project/ray,2507803464,2080255983,jjyao,898023,2025-05-08T18:31:24+00:00,2025-05-08T18:31:24+00:00,Do we plan to add `pydoclint` to pre-commit?,true,python/ray/_private/external_storage.py,2,"@@ -187,15 +187,14 @@ def _size_check(self, address_len, metadata_len, buffer_len, obtained_data_size)\n         """"""Check whether or not the obtained_data_size is as expected.\n ",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ed,ray-project/ray,2507803464,2080259840,thomasdesr,681004,2025-05-08T18:32:49+00:00,2025-05-08T18:32:49+00:00,"Yep, just doing a fast-pass on a lot of the big errors before we do so the set of ""exceptions"" is small enough to be vaguely sensible",true,python/ray/_private/external_storage.py,2,"@@ -187,15 +187,14 @@ def _size_check(self, address_len, metadata_len, buffer_len, obtained_data_size)\n         """"""Check whether or not the obtained_data_size is as expected.\n ",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ee,ray-project/ray,2506968451,2079693060,aslonnie,95255098,2025-05-08T13:12:56+00:00,2025-05-08T13:12:58+00:00,what is causing these kind of moves?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313ef,ray-project/ray,2506968451,2079734312,MortalHappiness,47914085,2025-05-08T13:35:15+00:00,2025-05-08T13:36:01+00:00,"Because we set `known-local-folder = [""ray""]`\r\n\r\nhttps://pycqa.github.io/isort/docs/configuration/options.html#known-local-folder\r\n\r\nhttps://docs.astral.sh/ruff/settings/#lint_isort_known-local-folder\r\n\r\nSo `ray` will be grouped with those `.` imports.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313f0,ray-project/ray,2506968451,2079693060,aslonnie,95255098,2025-05-08T13:12:56+00:00,2025-05-08T13:12:58+00:00,what is causing these kind of moves?,true,python/ray/data/_internal/execution/interfaces/execution_options.py,4,"@@ -1,11 +1,10 @@\n import os\n from typing import Dict, List, Optional, Union\n \n+from .common import NodeIdStr",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313f1,ray-project/ray,2506968451,2079734312,MortalHappiness,47914085,2025-05-08T13:35:15+00:00,2025-05-08T13:36:01+00:00,"Because we set `known-local-folder = [""ray""]`\r\n\r\nhttps://pycqa.github.io/isort/docs/configuration/options.html#known-local-folder\r\n\r\nhttps://docs.astral.sh/ruff/settings/#lint_isort_known-local-folder\r\n\r\nSo `ray` will be grouped with those `.` imports.",true,python/ray/data/_internal/execution/interfaces/execution_options.py,4,"@@ -1,11 +1,10 @@\n import os\n from typing import Dict, List, Optional, Union\n \n+from .common import NodeIdStr",0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313f2,ray-project/ray,2505706669,2078668767,kevin85421,20109646,2025-05-07T23:46:53+00:00,2025-05-07T23:46:53+00:00,`it->second.num_retries_left` and `it->second.num_oom_retries_left` are updated in L989 and L998.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec784313f3,ray-project/ray,2505706669,2078668927,kevin85421,20109646,2025-05-07T23:47:10+00:00,2025-05-07T23:47:10+00:00,This log prints wrong information.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,ray-project_ray
6824cd910ce5dcec7843164c,scikit-learn/scikit-learn,2515165706,2085708996,tylerjereddy,7903078,2025-05-13T00:08:00+00:00,2025-05-13T00:10:01+00:00,"Replacing `_tree` itself was *much* messier, so I tried to keep this scoped for prototyping.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843164d,scikit-learn/scikit-learn,2515165706,2085709667,tylerjereddy,7903078,2025-05-13T00:09:14+00:00,2025-05-13T00:10:01+00:00,Needing to call `query_ball_point` above and `query` here is a demonstration of API differences in `KDTree` between our libraries causing awkwardness in substituted workflows.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843164e,scikit-learn/scikit-learn,2515165706,2085710151,tylerjereddy,7903078,2025-05-13T00:09:57+00:00,2025-05-13T00:10:01+00:00,"Different handling of ragged data structures and `inf`/invalid values also seems to be present, requiring these additional shims.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843164f,scikit-learn/scikit-learn,2515165706,2085708996,tylerjereddy,7903078,2025-05-13T00:08:00+00:00,2025-05-13T00:10:01+00:00,"Replacing `_tree` itself was *much* messier, so I tried to keep this scoped for prototyping.",true,sklearn/neighbors/_base.py,15,"@@ -694,6 +695,10 @@ def _fit(self, X, y=None):\n                     ""try algorithm='ball_tree' ""\n                     ""or algorithm='brute' instead.""\n                 )\n+            self._sp_tree = spKDTree(\n+                X,\n+                self.leaf_size,\n+            )",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431650,scikit-learn/scikit-learn,2515165706,2085709667,tylerjereddy,7903078,2025-05-13T00:09:14+00:00,2025-05-13T00:10:01+00:00,Needing to call `query_ball_point` above and `query` here is a demonstration of API differences in `KDTree` between our libraries causing awkwardness in substituted workflows.,true,sklearn/neighbors/_base.py,54,"@@ -1278,6 +1283,53 @@ class from an array representing our data set and ask who's\n                 delayed_query(X[s], radius, return_distance, sort_results=sort_results)\n                 for s in gen_even_slices(X.shape[0], n_jobs)\n             )\n+            if return_distance:\n+                neigh_ind, neigh_dist = tuple(zip(*chunked_results))\n+                results = np.hstack(neigh_dist), np.hstack(neigh_ind)\n+            else:\n+                results = np.hstack(chunked_results)\n+        elif self._fit_method == ""kd_tree"":\n+            if issparse(X):\n+                raise ValueError(\n+                    ""%s does not work with sparse matrices. Densify the data, ""\n+                    ""or set algorithm='brute'"" % self._fit_method\n+                )\n+\n+            n_jobs = effective_n_jobs(self.n_jobs)\n+            delayed_query = delayed(self._sp_tree.query_ball_point)\n+            chunked_results = Parallel(n_jobs, prefer=""threads"")(\n+                delayed_query(X[s], radius, return_sorted=sort_results)\n+                for s in gen_even_slices(X.shape[0], n_jobs)\n+            )\n+            nn_vals = []\n+            for sub_arr in chunked_results[0]:\n+                nn_vals.append(len(sub_arr))\n+            if return_distance:\n+                dd, ii = self._sp_tree.query(X, k=max(nn_vals), distance_upper_bound=radius)",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431651,scikit-learn/scikit-learn,2515165706,2085710151,tylerjereddy,7903078,2025-05-13T00:09:57+00:00,2025-05-13T00:10:01+00:00,"Different handling of ragged data structures and `inf`/invalid values also seems to be present, requiring these additional shims.",true,sklearn/neighbors/_base.py,63,"@@ -1278,6 +1283,53 @@ class from an array representing our data set and ask who's\n                 delayed_query(X[s], radius, return_distance, sort_results=sort_results)\n                 for s in gen_even_slices(X.shape[0], n_jobs)\n             )\n+            if return_distance:\n+                neigh_ind, neigh_dist = tuple(zip(*chunked_results))\n+                results = np.hstack(neigh_dist), np.hstack(neigh_ind)\n+            else:\n+                results = np.hstack(chunked_results)\n+        elif self._fit_method == ""kd_tree"":\n+            if issparse(X):\n+                raise ValueError(\n+                    ""%s does not work with sparse matrices. Densify the data, ""\n+                    ""or set algorithm='brute'"" % self._fit_method\n+                )\n+\n+            n_jobs = effective_n_jobs(self.n_jobs)\n+            delayed_query = delayed(self._sp_tree.query_ball_point)\n+            chunked_results = Parallel(n_jobs, prefer=""threads"")(\n+                delayed_query(X[s], radius, return_sorted=sort_results)\n+                for s in gen_even_slices(X.shape[0], n_jobs)\n+            )\n+            nn_vals = []\n+            for sub_arr in chunked_results[0]:\n+                nn_vals.append(len(sub_arr))\n+            if return_distance:\n+                dd, ii = self._sp_tree.query(X, k=max(nn_vals), distance_upper_bound=radius)\n+                dd_new = []\n+                ii_new = []\n+                for i in range(len(dd)):\n+                    finite_indices = ii[i][np.isfinite(dd[i])]\n+                    finite_dists = dd[i][np.isfinite(dd[i])]\n+                    if sort_results:\n+                        sort_inds = np.argsort(finite_indices)\n+                        sorted_inds = finite_indices[sort_inds]\n+                        sorted_dists = finite_dists[sort_inds]",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431652,scikit-learn/scikit-learn,2510721961,2083297871,tylerjereddy,7903078,2025-05-10T22:03:13+00:00,2025-05-10T22:03:13+00:00,"Here and elsewhere in this diff, we can do even better (see below). It seems that the two APIs have effectively developed via convergent evolution to offer similar options with different names.\r\n\r\n```diff\r\n     kd = KDTree(y)\r\n-    ny = kd.query_ball_point(y, radius, p=np.inf)\r\n-    ny = [len(sub_list) for sub_list in ny]\r\n+    ny = kd.query_ball_point(y, radius, p=np.inf, return_length=True)\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431653,scikit-learn/scikit-learn,2510721961,2083297871,tylerjereddy,7903078,2025-05-10T22:03:13+00:00,2025-05-10T22:03:13+00:00,"Here and elsewhere in this diff, we can do even better (see below). It seems that the two APIs have effectively developed via convergent evolution to offer similar options with different names.\r\n\r\n```diff\r\n     kd = KDTree(y)\r\n-    ny = kd.query_ball_point(y, radius, p=np.inf)\r\n-    ny = [len(sub_list) for sub_list in ny]\r\n+    ny = kd.query_ball_point(y, radius, p=np.inf, return_length=True)\r\n```",true,sklearn/feature_selection/_mutual_info.py,NaN,"@@ -62,12 +63,14 @@ def _compute_mi_cc(x, y, n_neighbors):\n \n     # KDTree is explicitly fit to allow for the querying of number of\n     # neighbors within a specified radius\n-    kd = KDTree(x, metric=""chebyshev"")\n-    nx = kd.query_radius(x, radius, count_only=True, return_distance=False)\n+    kd = KDTree(x)\n+    nx = kd.query_ball_point(x, radius, p=np.inf)\n+    nx = [len(sub_list) for sub_list in nx]\n     nx = np.array(nx) - 1.0\n \n-    kd = KDTree(y, metric=""chebyshev"")\n-    ny = kd.query_radius(y, radius, count_only=True, return_distance=False)\n+    kd = KDTree(y)\n+    ny = kd.query_ball_point(y, radius, p=np.inf)\n+    ny = [len(sub_list) for sub_list in ny]",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431654,scikit-learn/scikit-learn,2510006934,2083712232,lucyleeow,23182829,2025-05-12T02:08:25+00:00,2025-05-12T02:08:25+00:00,"```suggestion\r\n      numpy 2.2.5 is the most recent version, its upper bound should be set to <2.3.0.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431655,scikit-learn/scikit-learn,2510006934,2083712232,lucyleeow,23182829,2025-05-12T02:08:25+00:00,2025-05-12T02:08:25+00:00,"```suggestion\r\n      numpy 2.2.5 is the most recent version, its upper bound should be set to <2.3.0.\r\n```",true,doc/developers/maintainer.rst.template,NaN,"@@ -162,6 +165,17 @@ Reference Steps\n     - In the release branch, change the version number `__version__` in\n       `sklearn/__init__.py` to `{{ version_full }}`.\n \n+    {% if key == ""rc"" %}\n+    - Still in the release branch, set or update the upper bound on the build\n+      dependencies in the `[build-system]` section of `pyproject.toml`. The goal is to\n+      prevent future backward incompatible releases of the dependencies to break the\n+      build in the maintenance branch.\n+      \n+      The upper bounds should match the latest already-released minor versions of the\n+      dependencies and should allow future micro (bug-fix) versions. For instance, if\n+      numpy 2.2.5 is the most rencent version, its upper bound should be set <2.3.0.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431656,scikit-learn/scikit-learn,2509425856,2081523092,betatim,1448859,2025-05-09T12:00:00+00:00,2025-05-09T12:00:01+00:00,Setting this means that the C extensions that come with scikit-learn are marked as working with free threaded Python right? One question I've had about marking extensions as compatible: how do we know? With the GIL there are a lot of things you could do that without the GIL enabled are bugs because they lead to race conditions. How the heck would we find all of them?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431657,scikit-learn/scikit-learn,2509425856,2081593593,ogrisel,89061,2025-05-09T12:43:58+00:00,2025-05-09T21:10:48+00:00,"> How the heck would we find all of them?\r\n\r\nWe do not need to find them all to declare that as a project we would consider such race conditions as bugs that we are willing to get fixed.\r\n\r\nBut I agree, it's better to try to catch as many as possible ahead of time. In the past (e.g. #30007) we started to take a look at pytest extensions that should help us catch those bugs before making a release. We need to give those tools a second look to see which one is the easiest to adopt for scikit-learn, either for manual checks of a particular submodule or automated check on the CI for the full code base.",false,NaN,NaN,NaN,1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431658,scikit-learn/scikit-learn,2509425856,2081624279,betatim,1448859,2025-05-09T13:03:31+00:00,2025-05-09T13:03:31+00:00,"True, we don't have to find all of them ahead of time. We could attach a note to free-threaded usage mentioning that it is not yet ready for production use?\r\n\r\nI'll take a look at #30007. ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431659,scikit-learn/scikit-learn,2509425856,2081817713,ogrisel,89061,2025-05-09T14:34:24+00:00,2025-05-09T14:34:24+00:00,> We could attach a note to free-threaded usage mentioning that it is not yet ready for production use?\r\n\r\nIt would be a good idea to include such a note in the release notes for 1.7. cc @jeremiedbb.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843165a,scikit-learn/scikit-learn,2509425856,2081944978,thomasjpfan,5402633,2025-05-09T15:33:46+00:00,2025-05-09T15:36:10+00:00,"Reading the docs for `freethreading_compatible=True`: https://docs.cython.org/en/latest/src/userguide/freethreading.html#status it states:\r\n\r\n> When you specify this directive, importing the module will not cause the interpreter to re-enable the GIL. The directive itself does not do anything to ensure compatibility - it is simply a way for you to indicate that you have tested your module and are confident that it works.\r\n\r\nIf we are not confident enough that it works, I do not see the value in setting the flag. It could even be a regression:\r\n\r\n- Currently, with `freethreading_compatible=False` means the GIL is reenabled. So the code works today.\r\n- If we change it to `freethreading_compatible=True` and the module is not compatible with free threading, then it could break.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843165b,scikit-learn/scikit-learn,2509425856,2082507395,ogrisel,89061,2025-05-09T21:14:42+00:00,2025-05-09T21:14:42+00:00,"But we cannot run such tests if we do not release the GIL when importing the native extension modules, right?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843165c,scikit-learn/scikit-learn,2509425856,2082512839,ogrisel,89061,2025-05-09T21:17:44+00:00,2025-05-09T21:17:44+00:00,Maybe we should finalize and merge #30041 to main and then resync that branch with `main` to actually be able to run such tests using `pytest-run-parallel`.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843165d,scikit-learn/scikit-learn,2509425856,2081523092,betatim,1448859,2025-05-09T12:00:00+00:00,2025-05-09T12:00:01+00:00,Setting this means that the C extensions that come with scikit-learn are marked as working with free threaded Python right? One question I've had about marking extensions as compatible: how do we know? With the GIL there are a lot of things you could do that without the GIL enabled are bugs because they lead to race conditions. How the heck would we find all of them?,true,sklearn/meson.build,NaN,"@@ -1,6 +1,9 @@\n fs = import('fs')\n \n cython_args = []\n+if cython.version().version_compare('>=3.1.0')\n+  cython_args += ['-Xfreethreading_compatible=True']",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843165e,scikit-learn/scikit-learn,2509425856,2081593593,ogrisel,89061,2025-05-09T12:43:58+00:00,2025-05-09T21:10:48+00:00,"> How the heck would we find all of them?\r\n\r\nWe do not need to find them all to declare that as a project we would consider such race conditions as bugs that we are willing to get fixed.\r\n\r\nBut I agree, it's better to try to catch as many as possible ahead of time. In the past (e.g. #30007) we started to take a look at pytest extensions that should help us catch those bugs before making a release. We need to give those tools a second look to see which one is the easiest to adopt for scikit-learn, either for manual checks of a particular submodule or automated check on the CI for the full code base.",true,sklearn/meson.build,NaN,"@@ -1,6 +1,9 @@\n fs = import('fs')\n \n cython_args = []\n+if cython.version().version_compare('>=3.1.0')\n+  cython_args += ['-Xfreethreading_compatible=True']",1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843165f,scikit-learn/scikit-learn,2509425856,2081624279,betatim,1448859,2025-05-09T13:03:31+00:00,2025-05-09T13:03:31+00:00,"True, we don't have to find all of them ahead of time. We could attach a note to free-threaded usage mentioning that it is not yet ready for production use?\r\n\r\nI'll take a look at #30007. ",true,sklearn/meson.build,NaN,"@@ -1,6 +1,9 @@\n fs = import('fs')\n \n cython_args = []\n+if cython.version().version_compare('>=3.1.0')\n+  cython_args += ['-Xfreethreading_compatible=True']",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431660,scikit-learn/scikit-learn,2509425856,2081817713,ogrisel,89061,2025-05-09T14:34:24+00:00,2025-05-09T14:34:24+00:00,> We could attach a note to free-threaded usage mentioning that it is not yet ready for production use?\r\n\r\nIt would be a good idea to include such a note in the release notes for 1.7. cc @jeremiedbb.,true,sklearn/meson.build,NaN,"@@ -1,6 +1,9 @@\n fs = import('fs')\n \n cython_args = []\n+if cython.version().version_compare('>=3.1.0')\n+  cython_args += ['-Xfreethreading_compatible=True']",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431661,scikit-learn/scikit-learn,2509425856,2081944978,thomasjpfan,5402633,2025-05-09T15:33:46+00:00,2025-05-09T15:36:10+00:00,"Reading the docs for `freethreading_compatible=True`: https://docs.cython.org/en/latest/src/userguide/freethreading.html#status it states:\r\n\r\n> When you specify this directive, importing the module will not cause the interpreter to re-enable the GIL. The directive itself does not do anything to ensure compatibility - it is simply a way for you to indicate that you have tested your module and are confident that it works.\r\n\r\nIf we are not confident enough that it works, I do not see the value in setting the flag. It could even be a regression:\r\n\r\n- Currently, with `freethreading_compatible=False` means the GIL is reenabled. So the code works today.\r\n- If we change it to `freethreading_compatible=True` and the module is not compatible with free threading, then it could break.",true,sklearn/meson.build,NaN,"@@ -1,6 +1,9 @@\n fs = import('fs')\n \n cython_args = []\n+if cython.version().version_compare('>=3.1.0')\n+  cython_args += ['-Xfreethreading_compatible=True']",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431662,scikit-learn/scikit-learn,2509425856,2082507395,ogrisel,89061,2025-05-09T21:14:42+00:00,2025-05-09T21:14:42+00:00,"But we cannot run such tests if we do not release the GIL when importing the native extension modules, right?",true,sklearn/meson.build,NaN,"@@ -1,6 +1,9 @@\n fs = import('fs')\n \n cython_args = []\n+if cython.version().version_compare('>=3.1.0')\n+  cython_args += ['-Xfreethreading_compatible=True']",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431663,scikit-learn/scikit-learn,2509425856,2082512839,ogrisel,89061,2025-05-09T21:17:44+00:00,2025-05-09T21:17:44+00:00,Maybe we should finalize and merge #30041 to main and then resync that branch with `main` to actually be able to run such tests using `pytest-run-parallel`.,true,sklearn/meson.build,NaN,"@@ -1,6 +1,9 @@\n fs = import('fs')\n \n cython_args = []\n+if cython.version().version_compare('>=3.1.0')\n+  cython_args += ['-Xfreethreading_compatible=True']",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431664,scikit-learn/scikit-learn,2506460829,2079215201,lucyleeow,23182829,2025-05-08T08:52:43+00:00,2025-05-08T08:52:44+00:00,"The other main question is whether we need to allow the user to set this, even when `scoring_function` does not take a `pos_label` parameter?\r\n\r\nI *think* we should have this, because `pos_label` is passed to `_threshold_scores_to_class_labels`, and a user should be able to control this.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431665,scikit-learn/scikit-learn,2506460829,2079215201,lucyleeow,23182829,2025-05-08T08:52:43+00:00,2025-05-08T08:52:44+00:00,"The other main question is whether we need to allow the user to set this, even when `scoring_function` does not take a `pos_label` parameter?\r\n\r\nI *think* we should have this, because `pos_label` is passed to `_threshold_scores_to_class_labels`, and a user should be able to control this.",true,sklearn/metrics/_decision_threshold.py,78,"@@ -0,0 +1,130 @@\n+""""""Metric per threshold curve to assess binary classification performance.\n+\n+Given threshold grid, one can undestand the behaviour of threshold-dependent\n+metrics when changing the threshold. In imbalanced scenarios or\n+cost-sensitive learning, a 0.5 threshold may not be optimal and tools like\n+this can help you visualize how the performance changes.\n+""""""\n+\n+# Authors: The scikit-learn developers\n+# SPDX-License-Identifier: BSD-3-Clause\n+\n+from numbers import Integral, Real\n+\n+from ..utils._param_validation import Interval, Options, validate_params\n+\n+\n+@validate_params(\n+    {\n+        ""scoring_function"": [callable],\n+        ""y_true"": [""array-like""],\n+        ""y_score"": [""array-like""],\n+        ""thresholds"": [\n+            Interval(Integral, 2, None, closed=""left""),\n+            ""array-like"",\n+        ],\n+        ""sign"": Options(Real, {0, 1}),\n+        ""labels"": [""array-like"", None],\n+        ""pos_label"": [Real, str, ""boolean"", None],\n+    },\n+    prefer_skip_nested_validation=True,\n+)\n+def decision_threshold_curve(\n+    scoring_function,\n+    y_true,\n+    y_score,\n+    # Should below 2 have a default value?\n+    thresholds=20,\n+    sign=1,\n+    labels=None,\n+    pos_label=None,\n+    **kwargs,\n+):\n+    """"""Compute threshold-dependent metric of interest per threshold.\n+\n+    Note: this implementation is restricted to the binary classification task.\n+\n+    Read more in the :ref:`User Guide <metric_threshold_curve>`.\n+\n+    .. versionadded:: 1.8\n+\n+    Parameters\n+    ----------\n+    scoring_function : callable\n+        The score function to use. It will be called as\n+        `score_func(y_true, y_pred, **kwargs)`.\n+        TODO: decided on `scoring_function` as term also used in forest estimators\n+\n+    y_true : array-like of shape (n_samples,)\n+        Ground truth (correct) target labels.\n+\n+    y_score : array-like of shape (n_samples,)\n+        Continuous response scores.\n+\n+    thresholds : int or array-like, default=20\n+        Specifies number of decision thresholds to compute score for. If an integer,\n+        it will be used to generate `thresholds` thresholds uniformly distributed\n+        between the minimum and maximum of `y_score`. If an array-like, it will be\n+        used as the thresholds.\n+\n+    sign : int, default=1\n+        Either 1 or -1. Score is computed as `sign * score_func(estimator, X, y)`.\n+        Thus, `sign` defines whether higher scores are better or worse.\n+\n+    labels: array-like, default=None\n+        Class labels. If `None`, inferred from `y_true`.\n+\n+    pos_label : int, float, bool or str, default=None",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431666,scikit-learn/scikit-learn,2505387311,2078431394,jeremiedbb,34657725,2025-05-07T20:19:15+00:00,2025-05-07T20:19:16+00:00,"upper bound the build dependencies in release branch. See discussions in #31183\r\n\r\nI used the same policy as scipy, i.e upper bound to the last major/minor release at the time (allow future bug-fix releases).\r\n\r\ncc/ @thomasjpfan @ogrisel @glemaitre @lesteve @betatim @adrinjalali ",false,NaN,NaN,NaN,3,3,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431667,scikit-learn/scikit-learn,2505387311,2078565214,thomasjpfan,5402633,2025-05-07T22:11:18+00:00,2025-05-07T22:11:19+00:00,"I'm okay with using the SciPy approach. (We'll need to add this policy to the release checklist)\r\n\r\nGiven this is a build dependency in `pyproject.toml`, distro packages can still choose to ignore them.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431668,scikit-learn/scikit-learn,2505387311,2079903972,jeremiedbb,34657725,2025-05-08T15:01:44+00:00,2025-05-08T15:01:44+00:00,"I fixed the cython upper bound in [9f3dd26](https://github.com/scikit-learn/scikit-learn/pull/31335/commits/9f3dd26d833e599f1372ecbad94d09c650a504a3), because I previously used a x.y.z version, not following the policy I just mentioned...\r\n\r\n>  (We'll need to add this policy to the release checklist)\r\n\r\nI'll make a PR to main when the rc is out.",false,NaN,NaN,NaN,1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431669,scikit-learn/scikit-learn,2505387311,2081167047,jeremiedbb,34657725,2025-05-09T08:14:33+00:00,2025-05-09T08:14:34+00:00,"```suggestion\r\n    ""Cython>=3.0.10,<3.2.0"",\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843166a,scikit-learn/scikit-learn,2505387311,2081299952,jeremiedbb,34657725,2025-05-09T09:28:35+00:00,2025-05-09T09:28:36+00:00,"So I had to modify the cython upper bound again because we need a pre-release for free-threading for now.\r\nThe current official releease is ``3.0.12`` so the upper bound should be ``3.1.0``.\r\nBut free-threading support is only available in ``3.1.Zrc1``, so I set the upper bound to ``3.2.0``.",false,NaN,NaN,NaN,1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843166b,scikit-learn/scikit-learn,2505387311,2081538338,ogrisel,89061,2025-05-09T12:10:11+00:00,2025-05-09T12:10:12+00:00,"Since the nightly builds are now green with 3.1.0rc1, I think it's fine.",false,NaN,NaN,NaN,1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843166c,scikit-learn/scikit-learn,2505387311,2078431394,jeremiedbb,34657725,2025-05-07T20:19:15+00:00,2025-05-07T20:19:16+00:00,"upper bound the build dependencies in release branch. See discussions in #31183\r\n\r\nI used the same policy as scipy, i.e upper bound to the last major/minor release at the time (allow future bug-fix releases).\r\n\r\ncc/ @thomasjpfan @ogrisel @glemaitre @lesteve @betatim @adrinjalali ",true,pyproject.toml,NaN,"@@ -97,10 +97,10 @@ maintenance = [""conda-lock==2.5.7""]\n build-backend = ""mesonpy""\n # Minimum requirements for the build system to execute.\n requires = [\n-    ""meson-python>=0.16.0"",\n-    ""Cython>=3.0.10"",\n-    ""numpy>=2"",\n-    ""scipy>=1.8.0"",\n+    ""meson-python>=0.16.0,<0.19.0"",\n+    ""Cython>=3.0.10,<3.0.13"",\n+    ""numpy>=2,<2.3.0"",\n+    ""scipy>=1.8.0,<1.16.0"",\n ]",3,3,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843166d,scikit-learn/scikit-learn,2505387311,2078565214,thomasjpfan,5402633,2025-05-07T22:11:18+00:00,2025-05-07T22:11:19+00:00,"I'm okay with using the SciPy approach. (We'll need to add this policy to the release checklist)\r\n\r\nGiven this is a build dependency in `pyproject.toml`, distro packages can still choose to ignore them.",true,pyproject.toml,NaN,"@@ -97,10 +97,10 @@ maintenance = [""conda-lock==2.5.7""]\n build-backend = ""mesonpy""\n # Minimum requirements for the build system to execute.\n requires = [\n-    ""meson-python>=0.16.0"",\n-    ""Cython>=3.0.10"",\n-    ""numpy>=2"",\n-    ""scipy>=1.8.0"",\n+    ""meson-python>=0.16.0,<0.19.0"",\n+    ""Cython>=3.0.10,<3.0.13"",\n+    ""numpy>=2,<2.3.0"",\n+    ""scipy>=1.8.0,<1.16.0"",\n ]",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843166e,scikit-learn/scikit-learn,2505387311,2079903972,jeremiedbb,34657725,2025-05-08T15:01:44+00:00,2025-05-08T15:01:44+00:00,"I fixed the cython upper bound in [9f3dd26](https://github.com/scikit-learn/scikit-learn/pull/31335/commits/9f3dd26d833e599f1372ecbad94d09c650a504a3), because I previously used a x.y.z version, not following the policy I just mentioned...\r\n\r\n>  (We'll need to add this policy to the release checklist)\r\n\r\nI'll make a PR to main when the rc is out.",true,pyproject.toml,NaN,"@@ -97,10 +97,10 @@ maintenance = [""conda-lock==2.5.7""]\n build-backend = ""mesonpy""\n # Minimum requirements for the build system to execute.\n requires = [\n-    ""meson-python>=0.16.0"",\n-    ""Cython>=3.0.10"",\n-    ""numpy>=2"",\n-    ""scipy>=1.8.0"",\n+    ""meson-python>=0.16.0,<0.19.0"",\n+    ""Cython>=3.0.10,<3.0.13"",\n+    ""numpy>=2,<2.3.0"",\n+    ""scipy>=1.8.0,<1.16.0"",\n ]",1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843166f,scikit-learn/scikit-learn,2505387311,2081167047,jeremiedbb,34657725,2025-05-09T08:14:33+00:00,2025-05-09T08:14:34+00:00,"```suggestion\r\n    ""Cython>=3.0.10,<3.2.0"",\r\n```",true,pyproject.toml,NaN,"@@ -97,10 +97,10 @@ maintenance = [""conda-lock==2.5.7""]\n build-backend = ""mesonpy""\n # Minimum requirements for the build system to execute.\n requires = [\n-    ""meson-python>=0.16.0"",\n-    ""Cython>=3.0.10"",\n-    ""numpy>=2"",\n-    ""scipy>=1.8.0"",\n+    ""meson-python>=0.16.0,<0.19.0"",\n+    ""Cython>=3.0.10,<3.1.0"",",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431670,scikit-learn/scikit-learn,2505387311,2081299952,jeremiedbb,34657725,2025-05-09T09:28:35+00:00,2025-05-09T09:28:36+00:00,"So I had to modify the cython upper bound again because we need a pre-release for free-threading for now.\r\nThe current official releease is ``3.0.12`` so the upper bound should be ``3.1.0``.\r\nBut free-threading support is only available in ``3.1.Zrc1``, so I set the upper bound to ``3.2.0``.",true,pyproject.toml,NaN,"@@ -97,10 +97,10 @@ maintenance = [""conda-lock==2.5.7""]\n build-backend = ""mesonpy""\n # Minimum requirements for the build system to execute.\n requires = [\n-    ""meson-python>=0.16.0"",\n-    ""Cython>=3.0.10"",\n-    ""numpy>=2"",\n-    ""scipy>=1.8.0"",\n+    ""meson-python>=0.16.0,<0.19.0"",\n+    ""Cython>=3.0.10,<3.0.13"",\n+    ""numpy>=2,<2.3.0"",\n+    ""scipy>=1.8.0,<1.16.0"",\n ]",1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431671,scikit-learn/scikit-learn,2505387311,2081538338,ogrisel,89061,2025-05-09T12:10:11+00:00,2025-05-09T12:10:12+00:00,"Since the nightly builds are now green with 3.1.0rc1, I think it's fine.",true,pyproject.toml,NaN,"@@ -97,10 +97,10 @@ maintenance = [""conda-lock==2.5.7""]\n build-backend = ""mesonpy""\n # Minimum requirements for the build system to execute.\n requires = [\n-    ""meson-python>=0.16.0"",\n-    ""Cython>=3.0.10"",\n-    ""numpy>=2"",\n-    ""scipy>=1.8.0"",\n+    ""meson-python>=0.16.0,<0.19.0"",\n+    ""Cython>=3.0.10,<3.0.13"",\n+    ""numpy>=2,<2.3.0"",\n+    ""scipy>=1.8.0,<1.16.0"",\n ]",1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431672,scikit-learn/scikit-learn,2504089045,2077418544,adrinjalali,1663864,2025-05-07T11:28:57+00:00,2025-05-07T11:29:39+00:00,We also talked about expanding this error message to include a hint on how to handle this with `catch_warnings`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431673,scikit-learn/scikit-learn,2504089045,2077456944,StefanieSenger,91849487,2025-05-07T11:53:42+00:00,2025-05-07T11:53:43+00:00,"Yes, I've expanded the UndefinedMetricWarning messages to mention `warnings.catch_warnings()`.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431674,scikit-learn/scikit-learn,2504089045,2077418544,adrinjalali,1663864,2025-05-07T11:28:57+00:00,2025-05-07T11:29:39+00:00,We also talked about expanding this error message to include a hint on how to handle this with `catch_warnings`,true,sklearn/metrics/_classification.py,61,"@@ -2220,28 +2213,11 @@ class are present in `y_true`): both likelihood ratios are undefined.\n         ""`UndefinedMetricWarning` will always be raised in case of a division by zero ""\n         ""and the value set with the `replace_undefined_by` param will be returned.""",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431675,scikit-learn/scikit-learn,2504089045,2077456944,StefanieSenger,91849487,2025-05-07T11:53:42+00:00,2025-05-07T11:53:43+00:00,"Yes, I've expanded the UndefinedMetricWarning messages to mention `warnings.catch_warnings()`.",true,sklearn/metrics/_classification.py,61,"@@ -2220,28 +2213,11 @@ class are present in `y_true`): both likelihood ratios are undefined.\n         ""`UndefinedMetricWarning` will always be raised in case of a division by zero ""\n         ""and the value set with the `replace_undefined_by` param will be returned.""",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431676,scikit-learn/scikit-learn,2503680126,2077345599,glemaitre,7454015,2025-05-07T10:43:53+00:00,2025-05-07T10:46:41+00:00,"you should not need the `--doctest-modules` since it is already set in the `pyproject.toml` from the project, isn't it?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431677,scikit-learn/scikit-learn,2503680126,2077348141,glemaitre,7454015,2025-05-07T10:45:47+00:00,2025-05-07T10:46:41+00:00,I would instead redirect to this section from the `numpydoc` documentation: https://numpydoc.readthedocs.io/en/latest/format.html#sections,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431678,scikit-learn/scikit-learn,2503680126,2077349316,glemaitre,7454015,2025-05-07T10:46:36+00:00,2025-05-07T10:46:41+00:00,We could even say that we want to keep this section as brief as possible.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431679,scikit-learn/scikit-learn,2503680126,2077513784,StefanieSenger,91849487,2025-05-07T12:28:57+00:00,2025-05-07T12:29:16+00:00,```suggestion\r\n    would test its docstring compliance:\r\n```\r\n\r\nJust a little typo nit.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843167a,scikit-learn/scikit-learn,2503680126,2077556927,ArturoAmorQ,86408019,2025-05-07T12:51:38+00:00,2025-05-07T12:52:53+00:00,"At least in my setup I do seem to need the flag (else pytest collects 0 items), but maybe I am missing something?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843167b,scikit-learn/scikit-learn,2503680126,2078031981,glemaitre,7454015,2025-05-07T16:27:18+00:00,2025-05-07T16:27:18+00:00,"Uhm, you are right. Apparently, it is something that we have in some other project (`skrub` or `imbalanced-learn`) but not in `scikit-learn`.\r\n\r\nI assume it makes sense to not have it as a default because it might be to costly. So this proposal seems good then.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843167c,scikit-learn/scikit-learn,2503680126,2078754696,lucyleeow,23182829,2025-05-08T01:41:28+00:00,2025-05-08T01:42:44+00:00,"For my education; are ""Notes"" sections limited to ""class/classmethod/method"" ? e.g., could they be added to a attributes section or functions?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843167d,scikit-learn/scikit-learn,2503680126,2078755185,lucyleeow,23182829,2025-05-08T01:42:16+00:00,2025-05-08T01:42:44+00:00,Maybe we should add that the code should be runable as is - i.e. should include all required imports?,false,NaN,NaN,NaN,1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843167e,scikit-learn/scikit-learn,2503680126,2079370753,ArturoAmorQ,86408019,2025-05-08T09:56:04+00:00,2025-05-08T09:56:04+00:00,"For info, adding a ""Notes"" section using markdown inside an attribute\r\n```\r\nNotes\r\n-----\r\n```\r\nwould raise an `ERROR: Error in ""rubric"" directive: no content permitted.` during build, but not during numpydoc validation.\r\n`",false,NaN,NaN,NaN,1,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843167f,scikit-learn/scikit-learn,2503680126,2079374283,lucyleeow,23182829,2025-05-08T09:57:16+00:00,2025-05-08T09:57:16+00:00,"Interesting, it makes sense its limited to the above list",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431680,scikit-learn/scikit-learn,2503680126,2079376472,lucyleeow,23182829,2025-05-08T09:58:00+00:00,2025-05-08T09:58:02+00:00,"Who raises the error? autodoc, sphinx...?",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431681,scikit-learn/scikit-learn,2503680126,2079394065,ArturoAmorQ,86408019,2025-05-08T10:04:46+00:00,2025-05-08T10:04:47+00:00,"Not sure, I think it's sphinx right after reading the sources. Here is the traceback from my local experiment:\r\n```\r\nwriting output... \r\nbuilding [html]: targets for 45 source files that are out of date\r\nupdating environment: 0 added, 49 changed, 0 removed\r\nreading sources... [100%] modules/generated/sklearn.model_selection.TunedThresholdClassifierCV\r\n/home/arturo/scikit-learn/sklearn/model_selection/_classification_threshold.py:docstring of sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV:126: ERROR: Error in ""rubric"" directive:\r\nno content permitted.\r\n\r\n.. rubric:: Notes\r\n\r\n    Only defined if the underlying estimator exposes such an attribute when\r\n    fit. [docutils]\r\nlooking for now-outdated files... none found\r\n```",false,NaN,NaN,NaN,1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431682,scikit-learn/scikit-learn,2503680126,2077345599,glemaitre,7454015,2025-05-07T10:43:53+00:00,2025-05-07T10:46:41+00:00,"you should not need the `--doctest-modules` since it is already set in the `pyproject.toml` from the project, isn't it?",true,doc/developers/contributing.rst,10,"@@ -726,6 +726,16 @@ We are glad to accept any sort of documentation:\n \n .. dropdown:: Guidelines for writing docstrings\n \n+  * You can use `pytest` to test docstrings, e.g. assuming the\n+    `RandomForestClassifier` docstring has been modified, the following command\n+    would test it's docstring compliance:\n+\n+    .. prompt:: bash\n+\n+      pytest --doctest-modules sklearn/ensemble/_forest.py -k RandomForestClassifier",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431683,scikit-learn/scikit-learn,2503680126,2077348141,glemaitre,7454015,2025-05-07T10:45:47+00:00,2025-05-07T10:46:41+00:00,I would instead redirect to this section from the `numpydoc` documentation: https://numpydoc.readthedocs.io/en/latest/format.html#sections,true,doc/developers/contributing.rst,NaN,"@@ -726,6 +726,16 @@ We are glad to accept any sort of documentation:\n \n .. dropdown:: Guidelines for writing docstrings\n \n+  * You can use `pytest` to test docstrings, e.g. assuming the\n+    `RandomForestClassifier` docstring has been modified, the following command\n+    would test it's docstring compliance:\n+\n+    .. prompt:: bash\n+\n+      pytest --doctest-modules sklearn/ensemble/_forest.py -k RandomForestClassifier\n+\n+  * The correct order of sections is: Parameters, Returns, See Also, Notes, Examples",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431684,scikit-learn/scikit-learn,2503680126,2077349316,glemaitre,7454015,2025-05-07T10:46:36+00:00,2025-05-07T10:46:41+00:00,We could even say that we want to keep this section as brief as possible.,true,doc/developers/contributing.rst,NaN,"@@ -791,7 +801,11 @@ We are glad to accept any sort of documentation:\n       SelectKBest : Select features based on the k highest scores.\n       SelectFpr : Select features based on a false positive rate test.\n \n-  * Add one or two snippets of code in ""Example"" section to show how it can be used.\n+  * The ""Notes"" section is optional. It is meant to provide information on\n+    specific behavior of the class/classmethod/method\n+\n+  * Add one or two **snippets** of code in ""Example"" section to show how it can\n+    be used.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431685,scikit-learn/scikit-learn,2503680126,2077513784,StefanieSenger,91849487,2025-05-07T12:28:57+00:00,2025-05-07T12:29:16+00:00,```suggestion\r\n    would test its docstring compliance:\r\n```\r\n\r\nJust a little typo nit.,true,doc/developers/contributing.rst,NaN,"@@ -726,6 +726,16 @@ We are glad to accept any sort of documentation:\n \n .. dropdown:: Guidelines for writing docstrings\n \n+  * You can use `pytest` to test docstrings, e.g. assuming the\n+    `RandomForestClassifier` docstring has been modified, the following command\n+    would test it's docstring compliance:",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431686,scikit-learn/scikit-learn,2503680126,2077556927,ArturoAmorQ,86408019,2025-05-07T12:51:38+00:00,2025-05-07T12:52:53+00:00,"At least in my setup I do seem to need the flag (else pytest collects 0 items), but maybe I am missing something?",true,doc/developers/contributing.rst,10,"@@ -726,6 +726,16 @@ We are glad to accept any sort of documentation:\n \n .. dropdown:: Guidelines for writing docstrings\n \n+  * You can use `pytest` to test docstrings, e.g. assuming the\n+    `RandomForestClassifier` docstring has been modified, the following command\n+    would test it's docstring compliance:\n+\n+    .. prompt:: bash\n+\n+      pytest --doctest-modules sklearn/ensemble/_forest.py -k RandomForestClassifier",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431687,scikit-learn/scikit-learn,2503680126,2078031981,glemaitre,7454015,2025-05-07T16:27:18+00:00,2025-05-07T16:27:18+00:00,"Uhm, you are right. Apparently, it is something that we have in some other project (`skrub` or `imbalanced-learn`) but not in `scikit-learn`.\r\n\r\nI assume it makes sense to not have it as a default because it might be to costly. So this proposal seems good then.",true,doc/developers/contributing.rst,10,"@@ -726,6 +726,16 @@ We are glad to accept any sort of documentation:\n \n .. dropdown:: Guidelines for writing docstrings\n \n+  * You can use `pytest` to test docstrings, e.g. assuming the\n+    `RandomForestClassifier` docstring has been modified, the following command\n+    would test it's docstring compliance:\n+\n+    .. prompt:: bash\n+\n+      pytest --doctest-modules sklearn/ensemble/_forest.py -k RandomForestClassifier",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431688,scikit-learn/scikit-learn,2503680126,2078754696,lucyleeow,23182829,2025-05-08T01:41:28+00:00,2025-05-08T01:42:44+00:00,"For my education; are ""Notes"" sections limited to ""class/classmethod/method"" ? e.g., could they be added to a attributes section or functions?",true,doc/developers/contributing.rst,NaN,"@@ -791,7 +804,11 @@ We are glad to accept any sort of documentation:\n       SelectKBest : Select features based on the k highest scores.\n       SelectFpr : Select features based on a false positive rate test.\n \n-  * Add one or two snippets of code in ""Example"" section to show how it can be used.\n+  * The ""Notes"" section is optional. It is meant to provide information on\n+    specific behavior of the class/classmethod/method.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431689,scikit-learn/scikit-learn,2503680126,2078755185,lucyleeow,23182829,2025-05-08T01:42:16+00:00,2025-05-08T01:42:44+00:00,Maybe we should add that the code should be runable as is - i.e. should include all required imports?,true,doc/developers/contributing.rst,31,"@@ -791,7 +804,11 @@ We are glad to accept any sort of documentation:\n       SelectKBest : Select features based on the k highest scores.\n       SelectFpr : Select features based on a false positive rate test.\n \n-  * Add one or two snippets of code in ""Example"" section to show how it can be used.\n+  * The ""Notes"" section is optional. It is meant to provide information on\n+    specific behavior of the class/classmethod/method.\n+\n+  * Add one or two **snippets** of code in ""Example"" section to show how it can",1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843168a,scikit-learn/scikit-learn,2503680126,2079370753,ArturoAmorQ,86408019,2025-05-08T09:56:04+00:00,2025-05-08T09:56:04+00:00,"For info, adding a ""Notes"" section using markdown inside an attribute\r\n```\r\nNotes\r\n-----\r\n```\r\nwould raise an `ERROR: Error in ""rubric"" directive: no content permitted.` during build, but not during numpydoc validation.\r\n`",true,doc/developers/contributing.rst,NaN,"@@ -791,7 +804,11 @@ We are glad to accept any sort of documentation:\n       SelectKBest : Select features based on the k highest scores.\n       SelectFpr : Select features based on a false positive rate test.\n \n-  * Add one or two snippets of code in ""Example"" section to show how it can be used.\n+  * The ""Notes"" section is optional. It is meant to provide information on\n+    specific behavior of the class/classmethod/method.",1,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843168b,scikit-learn/scikit-learn,2503680126,2079374283,lucyleeow,23182829,2025-05-08T09:57:16+00:00,2025-05-08T09:57:16+00:00,"Interesting, it makes sense its limited to the above list",true,doc/developers/contributing.rst,NaN,"@@ -791,7 +804,11 @@ We are glad to accept any sort of documentation:\n       SelectKBest : Select features based on the k highest scores.\n       SelectFpr : Select features based on a false positive rate test.\n \n-  * Add one or two snippets of code in ""Example"" section to show how it can be used.\n+  * The ""Notes"" section is optional. It is meant to provide information on\n+    specific behavior of the class/classmethod/method.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843168c,scikit-learn/scikit-learn,2503680126,2079376472,lucyleeow,23182829,2025-05-08T09:58:00+00:00,2025-05-08T09:58:02+00:00,"Who raises the error? autodoc, sphinx...?",true,doc/developers/contributing.rst,NaN,"@@ -791,7 +804,11 @@ We are glad to accept any sort of documentation:\n       SelectKBest : Select features based on the k highest scores.\n       SelectFpr : Select features based on a false positive rate test.\n \n-  * Add one or two snippets of code in ""Example"" section to show how it can be used.\n+  * The ""Notes"" section is optional. It is meant to provide information on\n+    specific behavior of the class/classmethod/method.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843168d,scikit-learn/scikit-learn,2503680126,2079394065,ArturoAmorQ,86408019,2025-05-08T10:04:46+00:00,2025-05-08T10:04:47+00:00,"Not sure, I think it's sphinx right after reading the sources. Here is the traceback from my local experiment:\r\n```\r\nwriting output... \r\nbuilding [html]: targets for 45 source files that are out of date\r\nupdating environment: 0 added, 49 changed, 0 removed\r\nreading sources... [100%] modules/generated/sklearn.model_selection.TunedThresholdClassifierCV\r\n/home/arturo/scikit-learn/sklearn/model_selection/_classification_threshold.py:docstring of sklearn.model_selection._classification_threshold.TunedThresholdClassifierCV:126: ERROR: Error in ""rubric"" directive:\r\nno content permitted.\r\n\r\n.. rubric:: Notes\r\n\r\n    Only defined if the underlying estimator exposes such an attribute when\r\n    fit. [docutils]\r\nlooking for now-outdated files... none found\r\n```",true,doc/developers/contributing.rst,NaN,"@@ -791,7 +804,11 @@ We are glad to accept any sort of documentation:\n       SelectKBest : Select features based on the k highest scores.\n       SelectFpr : Select features based on a false positive rate test.\n \n-  * Add one or two snippets of code in ""Example"" section to show how it can be used.\n+  * The ""Notes"" section is optional. It is meant to provide information on\n+    specific behavior of the class/classmethod/method.",1,1,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843168e,scikit-learn/scikit-learn,2501476538,2075705362,ArturoAmorQ,86408019,2025-05-06T15:17:23+00:00,2025-05-06T15:22:56+00:00,LearningCurveDisplay.from_model doesn't exit\r\n\r\n```suggestion\r\n    See also\r\n    --------\r\n    LearningCurveDisplay.from_estimator : Plot a learning curve using an\r\n        estimator and data.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843168f,scikit-learn/scikit-learn,2501476538,2084174308,ArturoAmorQ,86408019,2025-05-12T08:53:07+00:00,2025-05-12T08:53:07+00:00,```suggestion\r\n    See Also\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431690,scikit-learn/scikit-learn,2501476538,2075705362,ArturoAmorQ,86408019,2025-05-06T15:17:23+00:00,2025-05-06T15:22:56+00:00,LearningCurveDisplay.from_model doesn't exit\r\n\r\n```suggestion\r\n    See also\r\n    --------\r\n    LearningCurveDisplay.from_estimator : Plot a learning curve using an\r\n        estimator and data.\r\n```,true,sklearn/model_selection/_validation.py,NaN,"@@ -1929,6 +1929,13 @@ def learning_curve(\n         Times spent for scoring in seconds. Only present if ``return_times``\n         is True.\n \n+    See also:\n+    --------\n+    LearningCurveDisplay.from_estimator : Create a learning curve\n+        display from an estimator\n+\n+    LearningCurveDisplay.from_model : Learn Curve visualization",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431691,scikit-learn/scikit-learn,2501476538,2084174308,ArturoAmorQ,86408019,2025-05-12T08:53:07+00:00,2025-05-12T08:53:07+00:00,```suggestion\r\n    See Also\r\n```,true,sklearn/model_selection/_validation.py,NaN,"@@ -1929,6 +1929,11 @@ def learning_curve(\n         Times spent for scoring in seconds. Only present if ``return_times``\n         is True.\n \n+    See also",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431692,scikit-learn/scikit-learn,2500976992,2075098176,ogrisel,89061,2025-05-06T09:36:06+00:00,2025-05-06T09:36:06+00:00,"```suggestion\r\n    assert ""lbfgs failed to converge after 1 iteration(s)"" in warn_msg\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431693,scikit-learn/scikit-learn,2500976992,2075691207,Copilot,175728472,2025-05-06T15:10:14+00:00,2025-05-06T15:10:14+00:00,"Consider ensuring that 'max_iter' does not become negative after subtracting 'self.iteration'. For example, using max(1, self.max_iter - self.iteration) can safeguard the value passed to _check_optimize_result.\n```suggestion\n        max_iter = max(1, self.max_iter - self.iteration)\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431694,scikit-learn/scikit-learn,2500976992,2075691229,Copilot,175728472,2025-05-06T15:10:14+00:00,2025-05-06T15:14:57+00:00,"[nitpick] Consider adding an inline comment explaining why the recommendation to increase iterations is appended only when n_iter_i equals max_iter. This can help improve the maintainability and clarity of the warning logic.\r\n```suggestion\r\n            )\r\n            # Append a recommendation to increase iterations only when the\r\n            # number of iterations reaches the maximum allowed (max_iter),\r\n            # as this suggests the optimization may have been prematurely\r\n            # terminated due to the iteration limit.\r\n            #\r\n            # Furthermore, this recommandation should only be displayed\r\n            # when max_iter is provided hence assumed settable by the\r\n            # user-facing API.\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431695,scikit-learn/scikit-learn,2500976992,2075702334,ogrisel,89061,2025-05-06T15:15:59+00:00,2025-05-06T15:15:59+00:00,"I don't think this kind of overly defensive coding is needed, unless we find a bug in a dependency that mandates it.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431696,scikit-learn/scikit-learn,2500976992,2076510685,thomasjpfan,5402633,2025-05-06T23:14:45+00:00,2025-05-06T23:18:05+00:00,Nit: I think the first part of the comment is enough.\r\n\r\n```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431697,scikit-learn/scikit-learn,2500976992,2075098176,ogrisel,89061,2025-05-06T09:36:06+00:00,2025-05-06T09:36:06+00:00,"```suggestion\r\n    assert ""lbfgs failed to converge after 1 iteration(s)"" in warn_msg\r\n```",true,sklearn/linear_model/tests/test_logistic.py,NaN,"@@ -444,7 +444,7 @@ def test_logistic_regression_path_convergence_fail():\n \n     assert len(record) == 1\n     warn_msg = record[0].message.args[0]\n-    assert ""lbfgs failed to converge"" in warn_msg\n+    assert ""lbfgs failed to converge after 1 iteration(s) toto"" in warn_msg",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431698,scikit-learn/scikit-learn,2500976992,2075691207,Copilot,175728472,2025-05-06T15:10:14+00:00,2025-05-06T15:10:14+00:00,"Consider ensuring that 'max_iter' does not become negative after subtracting 'self.iteration'. For example, using max(1, self.max_iter - self.iteration) can safeguard the value passed to _check_optimize_result.\n```suggestion\n        max_iter = max(1, self.max_iter - self.iteration)\n```",true,sklearn/linear_model/_glm/_newton_solver.py,4,"@@ -178,21 +178,22 @@ def fallback_lbfgs_solve(self, X, y, sample_weight):\n             - self.coef\n             - self.converged\n         """"""\n+        max_iter = self.max_iter - self.iteration",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec78431699,scikit-learn/scikit-learn,2500976992,2075691229,Copilot,175728472,2025-05-06T15:10:14+00:00,2025-05-06T15:14:57+00:00,"[nitpick] Consider adding an inline comment explaining why the recommendation to increase iterations is appended only when n_iter_i equals max_iter. This can help improve the maintainability and clarity of the warning logic.\r\n```suggestion\r\n            )\r\n            # Append a recommendation to increase iterations only when the\r\n            # number of iterations reaches the maximum allowed (max_iter),\r\n            # as this suggests the optimization may have been prematurely\r\n            # terminated due to the iteration limit.\r\n            #\r\n            # Furthermore, this recommandation should only be displayed\r\n            # when max_iter is provided hence assumed settable by the\r\n            # user-facing API.\r\n```",true,sklearn/utils/optimize.py,21,"@@ -352,25 +352,33 @@ def _check_optimize_result(solver, result, max_iter=None, extra_warning_msg=None\n     """"""\n     # handle both scipy and scikit-learn solver names\n     if solver == ""lbfgs"":\n-        if result.status != 0:\n-            result_message = result.message\n+        if max_iter is not None:\n+            # In scipy <= 1.0.0, nit may exceed maxiter for lbfgs.\n+            # See https://github.com/scipy/scipy/issues/7854\n+            n_iter_i = min(result.nit, max_iter)\n+        else:\n+            n_iter_i = result.nit\n \n+        if result.status != 0:\n             warning_msg = (\n-                ""{} failed to converge (status={}):\n{}.\n\n""\n-                ""Increase the number of iterations (max_iter) ""\n-                ""or scale the data as shown in:\n""\n+                f""{solver} failed to converge after {n_iter_i} iteration(s) ""\n+                f""(status={result.status}):\n""\n+                f""{result.message}\n""\n+            )",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843169a,scikit-learn/scikit-learn,2500976992,2075702334,ogrisel,89061,2025-05-06T15:15:59+00:00,2025-05-06T15:15:59+00:00,"I don't think this kind of overly defensive coding is needed, unless we find a bug in a dependency that mandates it.",true,sklearn/linear_model/_glm/_newton_solver.py,4,"@@ -178,21 +178,22 @@ def fallback_lbfgs_solve(self, X, y, sample_weight):\n             - self.coef\n             - self.converged\n         """"""\n+        max_iter = self.max_iter - self.iteration",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843169b,scikit-learn/scikit-learn,2500976992,2076510685,thomasjpfan,5402633,2025-05-06T23:14:45+00:00,2025-05-06T23:18:05+00:00,Nit: I think the first part of the comment is enough.\r\n\r\n```suggestion\r\n```,true,sklearn/utils/optimize.py,NaN,"@@ -352,25 +352,41 @@ def _check_optimize_result(solver, result, max_iter=None, extra_warning_msg=None\n     """"""\n     # handle both scipy and scikit-learn solver names\n     if solver == ""lbfgs"":\n-        if result.status != 0:\n-            result_message = result.message\n+        if max_iter is not None:\n+            # In scipy <= 1.0.0, nit may exceed maxiter for lbfgs.\n+            # See https://github.com/scipy/scipy/issues/7854\n+            n_iter_i = min(result.nit, max_iter)\n+        else:\n+            n_iter_i = result.nit\n \n+        if result.status != 0:\n             warning_msg = (\n-                ""{} failed to converge (status={}):\n{}.\n\n""\n-                ""Increase the number of iterations (max_iter) ""\n-                ""or scale the data as shown in:\n""\n+                f""{solver} failed to converge after {n_iter_i} iteration(s) ""\n+                f""(status={result.status}):\n""\n+                f""{result.message}\n""\n+            )\n+            # Append a recommendation to increase iterations only when the\n+            # number of iterations reaches the maximum allowed (max_iter),\n+            # as this suggests the optimization may have been prematurely\n+            # terminated due to the iteration limit.\n+            #\n+            # Furthermore, this recommandation should only be displayed\n+            # when max_iter is provided hence assumed settable by the\n+            # user-facing API.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843169c,scikit-learn/scikit-learn,2499716934,2074998138,ogrisel,89061,2025-05-06T08:38:07+00:00,2025-05-06T08:46:26+00:00,"I would just directly link to `ValidationCurveDisplay.from_estimator` and not to the `ValidationCurveDisplay` class itself, as using the class constructor is not recommended to link to use the constructor.\r\n\r\n```suggestion\r\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843169d,scikit-learn/scikit-learn,2499716934,2075012607,ogrisel,89061,2025-05-06T08:45:31+00:00,2025-05-06T08:46:26+00:00,"This method does not exist in the code. If you use an LLM-based coding assistant, please always double-check the correctness of the suggested output.\r\n\r\n```suggestion\r\n```\r\n\r\nIn this case I spotted the mistake by checking the rendered HTML in the CI report (the ""Check the rendered docs here!"" link in particular).\r\n\r\n![image](https://github.com/user-attachments/assets/fd3334cb-130c-493c-8798-a82551f5cfe7)\r\n\r\nYou can see that the second item is not rendered as a link by sphinx because the API target does not exist.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843169e,scikit-learn/scikit-learn,2499716934,2075069856,TheAyos,34219939,2025-05-06T09:19:05+00:00,2025-05-06T09:19:05+00:00,"Hey ! Thank you for the feedback. I do not use an LLM-based coding assistant, I actually just added the second reference after attending a workshop (UNAITE x :probabl. yesterday), where it was mentioned that the goal for this See Also section was to mirror the one in [confusion_matrix](https://github.com/scikit-learn/scikit-learn/blob/98ed9dc73a86f5f11781a0e21f24c8f47979ec67/sklearn/metrics/_classification.py#L301-L307).\r\nThat said, I should have double-checked whether the method actually existed!",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec7843169f,scikit-learn/scikit-learn,2499716934,2074998138,ogrisel,89061,2025-05-06T08:38:07+00:00,2025-05-06T08:46:26+00:00,"I would just directly link to `ValidationCurveDisplay.from_estimator` and not to the `ValidationCurveDisplay` class itself, as using the class constructor is not recommended to link to use the constructor.\r\n\r\n```suggestion\r\n```",true,sklearn/model_selection/_validation.py,NaN,"@@ -2383,6 +2383,14 @@ def validation_curve(\n \n         .. versionadded:: 1.6\n \n+    See Also\n+    --------\n+    ValidationCurveDisplay.from_estimator : Plot the validation curve\n+        given an estimator, the data, and the parameter to vary.\n+    ValidationCurveDisplay.from_predictions : Plot the validation curve\n+        given the training and test scores.\n+    ValidationCurveDisplay : Validation curve visualization.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a0,scikit-learn/scikit-learn,2499716934,2075012607,ogrisel,89061,2025-05-06T08:45:31+00:00,2025-05-06T08:46:26+00:00,"This method does not exist in the code. If you use an LLM-based coding assistant, please always double-check the correctness of the suggested output.\r\n\r\n```suggestion\r\n```\r\n\r\nIn this case I spotted the mistake by checking the rendered HTML in the CI report (the ""Check the rendered docs here!"" link in particular).\r\n\r\n![image](https://github.com/user-attachments/assets/fd3334cb-130c-493c-8798-a82551f5cfe7)\r\n\r\nYou can see that the second item is not rendered as a link by sphinx because the API target does not exist.",true,sklearn/model_selection/_validation.py,NaN,"@@ -2383,6 +2383,14 @@ def validation_curve(\n \n         .. versionadded:: 1.6\n \n+    See Also\n+    --------\n+    ValidationCurveDisplay.from_estimator : Plot the validation curve\n+        given an estimator, the data, and the parameter to vary.\n+    ValidationCurveDisplay.from_predictions : Plot the validation curve\n+        given the training and test scores.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a1,scikit-learn/scikit-learn,2499716934,2075069856,TheAyos,34219939,2025-05-06T09:19:05+00:00,2025-05-06T09:19:05+00:00,"Hey ! Thank you for the feedback. I do not use an LLM-based coding assistant, I actually just added the second reference after attending a workshop (UNAITE x :probabl. yesterday), where it was mentioned that the goal for this See Also section was to mirror the one in [confusion_matrix](https://github.com/scikit-learn/scikit-learn/blob/98ed9dc73a86f5f11781a0e21f24c8f47979ec67/sklearn/metrics/_classification.py#L301-L307).\r\nThat said, I should have double-checked whether the method actually existed!",true,sklearn/model_selection/_validation.py,NaN,"@@ -2383,6 +2383,14 @@ def validation_curve(\n \n         .. versionadded:: 1.6\n \n+    See Also\n+    --------\n+    ValidationCurveDisplay.from_estimator : Plot the validation curve\n+        given an estimator, the data, and the parameter to vary.\n+    ValidationCurveDisplay.from_predictions : Plot the validation curve\n+        given the training and test scores.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a2,scikit-learn/scikit-learn,2499633401,2074989720,ogrisel,89061,2025-05-06T08:33:04+00:00,2025-05-06T08:33:46+00:00,```suggestion\r\n    sklearn.calibration.CalibrationDisplay : Calibration curve visualization\r\n    based on matplotlib.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a3,scikit-learn/scikit-learn,2499633401,2075020635,ogrisel,89061,2025-05-06T08:50:21+00:00,2025-05-06T08:50:21+00:00,"Actually, rather than linking to the class itself, I think we should directly link to the `sklearn.calibration.CalibrationDisplay.from_estimator` and `from_predictions` factory methods.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a4,scikit-learn/scikit-learn,2499633401,2075559967,metlouf,107807424,2025-05-06T14:05:55+00:00,2025-05-06T14:05:55+00:00,"I modified as suggested, I copied the sklearn.calibration.CalibrationDisplay.from_estimator and from_predictions description from another function for consistency, waiting for test to get approved",false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a5,scikit-learn/scikit-learn,2499633401,2075677562,metlouf,107807424,2025-05-06T15:03:01+00:00,2025-05-06T15:03:02+00:00,![image](https://github.com/user-attachments/assets/8b4dd095-fb81-461a-b614-c6a1706c617a)\r\nHere it is !,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a6,scikit-learn/scikit-learn,2499633401,2074989720,ogrisel,89061,2025-05-06T08:33:04+00:00,2025-05-06T08:33:46+00:00,```suggestion\r\n    sklearn.calibration.CalibrationDisplay : Calibration curve visualization\r\n    based on matplotlib.\r\n```,true,sklearn/calibration.py,NaN,"@@ -1005,6 +1005,10 @@ def calibration_curve(\n     prob_pred : ndarray of shape (n_bins,) or smaller\n         The mean predicted probability in each bin.\n \n+    See Also\n+    --------\n+    sklearn.calibration.CalibrationDisplay : Calibration curve visualization.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a7,scikit-learn/scikit-learn,2499633401,2075020635,ogrisel,89061,2025-05-06T08:50:21+00:00,2025-05-06T08:50:21+00:00,"Actually, rather than linking to the class itself, I think we should directly link to the `sklearn.calibration.CalibrationDisplay.from_estimator` and `from_predictions` factory methods.",true,sklearn/calibration.py,NaN,"@@ -1005,6 +1005,10 @@ def calibration_curve(\n     prob_pred : ndarray of shape (n_bins,) or smaller\n         The mean predicted probability in each bin.\n \n+    See Also\n+    --------\n+    sklearn.calibration.CalibrationDisplay : Calibration curve visualization.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a8,scikit-learn/scikit-learn,2499633401,2075559967,metlouf,107807424,2025-05-06T14:05:55+00:00,2025-05-06T14:05:55+00:00,"I modified as suggested, I copied the sklearn.calibration.CalibrationDisplay.from_estimator and from_predictions description from another function for consistency, waiting for test to get approved",true,sklearn/calibration.py,NaN,"@@ -1005,6 +1005,10 @@ def calibration_curve(\n     prob_pred : ndarray of shape (n_bins,) or smaller\n         The mean predicted probability in each bin.\n \n+    See Also\n+    --------\n+    sklearn.calibration.CalibrationDisplay : Calibration curve visualization.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316a9,scikit-learn/scikit-learn,2499633401,2075677562,metlouf,107807424,2025-05-06T15:03:01+00:00,2025-05-06T15:03:02+00:00,![image](https://github.com/user-attachments/assets/8b4dd095-fb81-461a-b614-c6a1706c617a)\r\nHere it is !,true,sklearn/calibration.py,NaN,"@@ -1005,6 +1005,10 @@ def calibration_curve(\n     prob_pred : ndarray of shape (n_bins,) or smaller\n         The mean predicted probability in each bin.\n \n+    See Also\n+    --------\n+    sklearn.calibration.CalibrationDisplay : Calibration curve visualization.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316aa,scikit-learn/scikit-learn,2499509256,2077934845,ArturoAmorQ,86408019,2025-05-07T15:35:02+00:00,2025-05-07T15:35:02+00:00,This should fix the failing CI.\r\n```suggestion\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316ab,scikit-learn/scikit-learn,2499509256,2077934845,ArturoAmorQ,86408019,2025-05-07T15:35:02+00:00,2025-05-07T15:35:02+00:00,This should fix the failing CI.\r\n```suggestion\r\n```,true,sklearn/metrics/_plot/det_curve.py,NaN,"@@ -96,7 +99,12 @@ def from_estimator(\n     ):\n         """"""Plot DET curve given an estimator and data.\n \n-        Read more in the :ref:`User Guide <det_curve>`.\n+        Plot DET curve given an estimator and data.\n+",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316ac,scikit-learn/scikit-learn,2499472518,2077921409,ArturoAmorQ,86408019,2025-05-07T15:27:39+00:00,2025-05-07T15:29:44+00:00,```suggestion\r\n    :ref:`Model Evaluation Guide <confusion_matrix>`.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316ad,scikit-learn/scikit-learn,2499472518,2077922021,ArturoAmorQ,86408019,2025-05-07T15:27:59+00:00,2025-05-07T15:29:44+00:00,```suggestion\r\n        :ref:`Model Evaluation Guide <confusion_matrix>`.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316ae,scikit-learn/scikit-learn,2499472518,2077923816,ArturoAmorQ,86408019,2025-05-07T15:28:53+00:00,2025-05-07T15:29:44+00:00,```suggestion\r\n        :ref:`Model Evaluation Guide <confusion_matrix>`.\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784316af,scikit-learn/scikit-learn,2499472518,2077921409,ArturoAmorQ,86408019,2025-05-07T15:27:39+00:00,2025-05-07T15:29:44+00:00,```suggestion\r\n    :ref:`Model Evaluation Guide <confusion_matrix>`.\r\n```,true,sklearn/metrics/_plot/confusion_matrix.py,NaN,"@@ -21,7 +21,10 @@ class ConfusionMatrixDisplay:\n     create a :class:`ConfusionMatrixDisplay`. All parameters are stored as\n     attributes.\n \n-    Read more in the :ref:`User Guide <visualizations>`.\n+    For general information regarding `scikit-learn` visualization tools, see\n+    the :ref:`Visualization Guide <visualizations>`.\n+    For guidance on interpreting these plots, refer to the\n+    :ref:`Confusion Matrix Guide <confusion_matrix>`.",0,0,0,0,0,0,0,scikit-learn_scikit-learn
6824cd910ce5dcec784319f0,streamlit/streamlit,2512434329,2085846547,mayagbarnes,63436329,2025-05-13T03:12:17+00:00,2025-05-13T03:12:17+00:00,Removing this note as it is legacy from st version `0.47.0` (see [here](https://github.com/streamlit/streamlit/blob/0.47.0/frontend/src/components/core/Sidebar/Sidebar.tsx)). The `tabIndex` was removed in `0.48.0`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f1,streamlit/streamlit,2512434329,2085846547,mayagbarnes,63436329,2025-05-13T03:12:17+00:00,2025-05-13T03:12:17+00:00,Removing this note as it is legacy from st version `0.47.0` (see [here](https://github.com/streamlit/streamlit/blob/0.47.0/frontend/src/components/core/Sidebar/Sidebar.tsx)). The `tabIndex` was removed in `0.48.0`,true,frontend/app/src/components/Sidebar/Sidebar.tsx,65,"@@ -284,7 +284,6 @@ const Sidebar: React.FC<SidebarProps> = ({\n   // If header decoration visible, move sidebar down so decoration doesn't go below it\n   const sidebarAdjust = !isEmbedded && headerDecorationVisible()\n \n-  // The tabindex is required to support scrolling by arrow keys.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f2,streamlit/streamlit,2511567613,2085032428,sfc-gh-lwilby,103002751,2025-05-12T16:19:42+00:00,2025-05-12T16:19:43+00:00,"These snapshot changes below are unexpected, but the difference in snapshots seems imperceptible to me. ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f3,streamlit/streamlit,2511567613,2085063823,graphite-app[bot],96075541,2025-05-12T16:39:43+00:00,2025-05-12T16:39:44+00:00,"The comment in the test case is incorrect - it states that columns receive a ""large"" gap arg, but this test is specifically for the ""none"" gap setting. The comment should be updated to:\n\n```\n# 4 elements will be created: 1 horizontal block, 3 columns, each receives\n# ""none"" gap arg\n```\n\nThis will accurately reflect what the test is verifying.\n```suggestion\n+        # 4 elements will be created: 1 horizontal block, 3 columns, each receives\n+        # ""none"" gap arg\n```\n  \n\n*Spotted by [Diamond](https://app.graphite.dev/diamond/?org=streamlit&ref=ai-review-comment)*<i class='graphite__hidden'><br /><br />Is this helpful? React 👍 or 👎 to let us know.</i>",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f4,streamlit/streamlit,2511567613,2085155875,sfc-gh-bnisco,178842601,2025-05-12T17:41:12+00:00,2025-05-12T22:07:26+00:00,"+1, seems like a slight letter rendering change, but nothing that is concerning/relevant ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f5,streamlit/streamlit,2511567613,2085604101,sfc-gh-bnisco,178842601,2025-05-12T22:06:16+00:00,2025-05-12T22:07:26+00:00,**suggestion:** I believe the CSS `calc` is unnecessary here and thus this could be simplified:\r\n\r\n```suggestion\r\n        ? `${percentage}%`\r\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f6,streamlit/streamlit,2511567613,2085032428,sfc-gh-lwilby,103002751,2025-05-12T16:19:42+00:00,2025-05-12T16:19:43+00:00,"These snapshot changes below are unexpected, but the difference in snapshots seems imperceptible to me. ",true,e2e_playwright/__snapshots__/linux/st_columns_test/st_columns-nested_one_level[chromium].png,1,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f7,streamlit/streamlit,2511567613,2085063823,graphite-app[bot],96075541,2025-05-12T16:39:43+00:00,2025-05-12T16:39:44+00:00,"The comment in the test case is incorrect - it states that columns receive a ""large"" gap arg, but this test is specifically for the ""none"" gap setting. The comment should be updated to:\n\n```\n# 4 elements will be created: 1 horizontal block, 3 columns, each receives\n# ""none"" gap arg\n```\n\nThis will accurately reflect what the test is verifying.\n```suggestion\n+        # 4 elements will be created: 1 horizontal block, 3 columns, each receives\n+        # ""none"" gap arg\n```\n  \n\n*Spotted by [Diamond](https://app.graphite.dev/diamond/?org=streamlit&ref=ai-review-comment)*<i class='graphite__hidden'><br /><br />Is this helpful? React 👍 or 👎 to let us know.</i>",true,lib/tests/streamlit/elements/layouts_test.py,NaN,"@@ -234,6 +238,52 @@ def test_columns_with_large_gap(self):\n                 GapSize.LARGE,\n             )\n \n+    def test_columns_with_none_gap(self):\n+        """"""Test that it works correctly with ""none"" gap argument""""""\n+\n+        st.columns(3, gap=""none"")\n+\n+        all_deltas = self.get_all_deltas_from_queue()\n+\n+        horizontal_container = all_deltas[0]\n+        columns_blocks = all_deltas[1:4]\n+\n+        # 4 elements will be created: 1 horizontal block, 3 columns, each receives\n+        # ""large"" gap arg",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f8,streamlit/streamlit,2511567613,2085155875,sfc-gh-bnisco,178842601,2025-05-12T17:41:12+00:00,2025-05-12T22:07:26+00:00,"+1, seems like a slight letter rendering change, but nothing that is concerning/relevant ",true,e2e_playwright/__snapshots__/linux/st_columns_test/st_columns-nested_one_level[chromium].png,1,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319f9,streamlit/streamlit,2511567613,2085604101,sfc-gh-bnisco,178842601,2025-05-12T22:06:16+00:00,2025-05-12T22:07:26+00:00,**suggestion:** I believe the CSS `calc` is unnecessary here and thus this could be simplified:\r\n\r\n```suggestion\r\n        ? `${percentage}%`\r\n```,true,frontend/lib/src/components/core/Block/styled-components.ts,NaN,"@@ -99,7 +101,10 @@ export const StyledColumn = styled.div<StyledColumnProps>(\n     const { VerticalAlignment } = BlockProto.Column\n     const percentage = weight * 100\n     const gapWidth = translateGapWidth(gap, theme)\n-    const width = `calc(${percentage}% - ${gapWidth})`\n+    const width =\n+      gapWidth === theme.spacing.none\n+        ? `calc(${percentage}%)`",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319fa,streamlit/streamlit,2508411089,2080745825,Copilot,175728472,2025-05-09T01:41:37+00:00,2025-05-09T01:41:42+00:00,Corrected the typo from 'indentifying' to 'identifying'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319fb,streamlit/streamlit,2508411089,2080745829,Copilot,175728472,2025-05-09T01:41:37+00:00,2025-05-09T01:41:42+00:00,Fixed the spelling mistake 'hexidecimal' to 'hexadecimal'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319fc,streamlit/streamlit,2508411089,2080745830,Copilot,175728472,2025-05-09T01:41:38+00:00,2025-05-09T01:41:42+00:00,Fixed the repeated typo 'hexidecimal' to 'hexadecimal'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319fd,streamlit/streamlit,2508411089,2080745832,Copilot,175728472,2025-05-09T01:41:38+00:00,2025-05-09T01:41:42+00:00,Corrected 'approriate' to 'appropriate'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319fe,streamlit/streamlit,2508411089,2080745836,Copilot,175728472,2025-05-09T01:41:38+00:00,2025-05-09T01:41:42+00:00,Corrected 'accomodate' to 'accommodate'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec784319ff,streamlit/streamlit,2508411089,2080745841,Copilot,175728472,2025-05-09T01:41:38+00:00,2025-05-09T01:41:42+00:00,Fixed the typo from 'soley' to 'solely'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a00,streamlit/streamlit,2508411089,2080745842,Copilot,175728472,2025-05-09T01:41:39+00:00,2025-05-09T01:41:42+00:00,Corrected 'behvior' to 'behavior'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a01,streamlit/streamlit,2508411089,2080745846,Copilot,175728472,2025-05-09T01:41:39+00:00,2025-05-09T01:41:42+00:00,Corrected 'precendence' to 'precedence'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a02,streamlit/streamlit,2508411089,2080745849,Copilot,175728472,2025-05-09T01:41:39+00:00,2025-05-09T01:41:42+00:00,Fixed the typo 'shoud' to 'should' in the test description.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a03,streamlit/streamlit,2508411089,2080745851,Copilot,175728472,2025-05-09T01:41:39+00:00,2025-05-09T01:41:42+00:00,"Fixed the typo 'shoud' to 'should' in the test description.\n```suggestion\n  it(""should not update ui value if values are the same"", async () => {\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a04,streamlit/streamlit,2508411089,2080745855,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:42+00:00,Corrected 'overlayed' to 'overlaid' in the comment.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a05,streamlit/streamlit,2508411089,2080745857,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:42+00:00,Corrected 'precendence' to 'precedence'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a06,streamlit/streamlit,2508411089,2080745858,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:42+00:00,"Corrected 'thats' to ""that's"" and 'usuable' to 'usable'.\n```suggestion\n        // 6rem was manually determined as the smallest size that's still somewhat usable:\n```",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a07,streamlit/streamlit,2508411089,2080745860,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:43+00:00,Corrected 'weired' to 'weird'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a08,streamlit/streamlit,2508411089,2080745862,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:43+00:00,Corrected 'alignes' to 'aligns' in the test description.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a09,streamlit/streamlit,2508411089,2080745866,Copilot,175728472,2025-05-09T01:41:41+00:00,2025-05-09T01:41:43+00:00,Corrected 'programatically' to 'programmatically'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a0a,streamlit/streamlit,2508411089,2080745869,Copilot,175728472,2025-05-09T01:41:41+00:00,2025-05-09T01:41:43+00:00,Fixed the typo 'shoud' to 'should' in the test description.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a0b,streamlit/streamlit,2508411089,2080745872,Copilot,175728472,2025-05-09T01:41:41+00:00,2025-05-09T01:41:43+00:00,Corrected 'absolut' to 'absolute'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a0c,streamlit/streamlit,2508411089,2080745877,Copilot,175728472,2025-05-09T01:41:41+00:00,2025-05-09T01:41:43+00:00,Corrected 'interals' to 'intervals'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a0d,streamlit/streamlit,2508411089,2080745881,Copilot,175728472,2025-05-09T01:41:42+00:00,2025-05-09T01:41:43+00:00,Corrected 'substract' to 'subtract'.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a0e,streamlit/streamlit,2508411089,2080999287,mayagbarnes,63436329,2025-05-09T06:11:16+00:00,2025-05-09T06:11:16+00:00,Nit: think the correction should be `internals`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a0f,streamlit/streamlit,2508411089,2081005201,mayagbarnes,63436329,2025-05-09T06:17:04+00:00,2025-05-09T06:17:05+00:00,Nit: think the correction should be `of`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a10,streamlit/streamlit,2508411089,2081006093,mayagbarnes,63436329,2025-05-09T06:17:56+00:00,2025-05-09T06:17:56+00:00,Nit: think the correction should be `dependent`,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a11,streamlit/streamlit,2508411089,2081053893,co63oc,4617245,2025-05-09T06:57:58+00:00,2025-05-09T06:57:58+00:00,fixed,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a12,streamlit/streamlit,2508411089,2081054224,co63oc,4617245,2025-05-09T06:58:11+00:00,2025-05-09T06:58:11+00:00,fixed,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a13,streamlit/streamlit,2508411089,2081054477,co63oc,4617245,2025-05-09T06:58:22+00:00,2025-05-09T06:58:22+00:00,fixed,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a14,streamlit/streamlit,2508411089,2080745825,Copilot,175728472,2025-05-09T01:41:37+00:00,2025-05-09T01:41:42+00:00,Corrected the typo from 'indentifying' to 'identifying'.,true,lib/streamlit/elements/lib/image_utils.py,5,"@@ -360,7 +360,7 @@ def marshall_images(\n     Parameters\n     ----------\n     coordinates\n-        A string indentifying the images' location in the frontend.\n+        A string identifying the images' location in the frontend.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a15,streamlit/streamlit,2508411089,2080745829,Copilot,175728472,2025-05-09T01:41:37+00:00,2025-05-09T01:41:42+00:00,Fixed the spelling mistake 'hexidecimal' to 'hexadecimal'.,true,lib/streamlit/elements/lib/column_types.py,5,"@@ -432,7 +432,7 @@ def NumberColumn(\n         - ``""engineering""``: Show the number in engineering notation (e.g. ""1.235E3"").\n         - printf-style format string: Format the number with a printf\n           specifier, like ``""%d""`` to show a signed integer (e.g. ""1234"") or\n-          ``""%X""`` to show an unsigned hexidecimal integer (e.g. ""4D2""). You\n+          ``""%X""`` to show an unsigned hexadecimal integer (e.g. ""4D2""). You",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a16,streamlit/streamlit,2508411089,2080745830,Copilot,175728472,2025-05-09T01:41:38+00:00,2025-05-09T01:41:42+00:00,Fixed the repeated typo 'hexidecimal' to 'hexadecimal'.,true,lib/streamlit/elements/lib/column_types.py,14,"@@ -2007,7 +2007,7 @@ def ProgressColumn(\n         - ``""engineering""``: Show the number in engineering notation (e.g. ""1.235E3"").\n         - printf-style format string: Format the number with a printf\n           specifier, like ``""%d""`` to show a signed integer (e.g. ""1234"") or\n-          ``""%X""`` to show an unsigned hexidecimal integer (e.g. ""4D2""). You\n+          ``""%X""`` to show an unsigned hexadecimal integer (e.g. ""4D2""). You",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a17,streamlit/streamlit,2508411089,2080745832,Copilot,175728472,2025-05-09T01:41:38+00:00,2025-05-09T01:41:42+00:00,Corrected 'approriate' to 'appropriate'.,true,lib/streamlit/elements/lib/built_in_chart_utils.py,5,"@@ -964,7 +964,7 @@ def _get_color_encoding(\n \n             return alt.ColorValue(to_css_color(cast(""Any"", color_value)))\n \n-        # If the color value is a list of colors of approriate length, return that.\n+        # If the color value is a list of colors of appropriate length, return that.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a18,streamlit/streamlit,2508411089,2080745836,Copilot,175728472,2025-05-09T01:41:38+00:00,2025-05-09T01:41:42+00:00,Corrected 'accomodate' to 'accommodate'.,true,lib/streamlit/elements/code.py,5,"@@ -67,7 +67,7 @@ def code(\n             Desired height of the code block expressed in pixels. If ``height``\n             is ``None`` (default), Streamlit sets the element's height to fit\n             its content. Vertical scrolling within the element is enabled when\n-            the height does not accomodate all lines.\n+            the height does not accommodate all lines.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a19,streamlit/streamlit,2508411089,2080745841,Copilot,175728472,2025-05-09T01:41:38+00:00,2025-05-09T01:41:42+00:00,Fixed the typo from 'soley' to 'solely'.,true,lib/streamlit/commands/navigation.py,5,"@@ -318,7 +318,7 @@ def _navigation(\n \n             script_hash = page._script_hash\n             if script_hash in pagehash_to_pageinfo:\n-                # The page script hash is soley based on the url path\n+                # The page script hash is solely based on the url path",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a1a,streamlit/streamlit,2508411089,2080745842,Copilot,175728472,2025-05-09T01:41:39+00:00,2025-05-09T01:41:42+00:00,Corrected 'behvior' to 'behavior'.,true,lib/streamlit/commands/execution_control.py,5,"@@ -70,7 +70,7 @@ def _new_fragment_id_queue(\n         # If st.rerun(scope=""fragment"") is called during a full script run, we raise an\n         # exception. This occurs, of course, if st.rerun(scope=""fragment"") is called\n         # outside of a fragment, but it somewhat surprisingly occurs if it gets called\n-        # from within a fragment during a run of the full script. While this behvior may\n+        # from within a fragment during a run of the full script. While this behavior may",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a1b,streamlit/streamlit,2508411089,2080745846,Copilot,175728472,2025-05-09T01:41:39+00:00,2025-05-09T01:41:42+00:00,Corrected 'precendence' to 'precedence'.,true,frontend/lib/src/util/utils.test.ts,5,"@@ -375,7 +375,7 @@ describe(""getLoadingScreenType"", () => {\n     expect(getLoadingScreenType()).toBe(LoadingScreenType.V2)\n   })\n \n-  it(""should give precendence to 'hide'"", () => {\n+  it(""should give precedence to 'hide'"", () => {",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a1c,streamlit/streamlit,2508411089,2080745849,Copilot,175728472,2025-05-09T01:41:39+00:00,2025-05-09T01:41:42+00:00,Fixed the typo 'shoud' to 'should' in the test description.,true,frontend/lib/src/hooks/useUpdateUiValue.test.ts,5,"@@ -25,13 +25,13 @@ describe(""useUpdateUiValue"", () => {\n     await waitFor(() => expect(callback).toHaveBeenCalledWith(4))\n   })\n \n-  it(""shoud not update ui value if values are different and ui value is dirty"", async () => {\n+  it(""should not update ui value if values are different and ui value is dirty"", async () => {",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a1d,streamlit/streamlit,2508411089,2080745851,Copilot,175728472,2025-05-09T01:41:39+00:00,2025-05-09T01:41:42+00:00,"Fixed the typo 'shoud' to 'should' in the test description.\n```suggestion\n  it(""should not update ui value if values are the same"", async () => {\n```",true,frontend/lib/src/hooks/useUpdateUiValue.test.ts,12,"@@ -25,13 +25,13 @@ describe(""useUpdateUiValue"", () => {\n     await waitFor(() => expect(callback).toHaveBeenCalledWith(4))\n   })\n \n-  it(""shoud not update ui value if values are different and ui value is dirty"", async () => {\n+  it(""should not update ui value if values are different and ui value is dirty"", async () => {\n     const callback = vi.fn()\n     renderHook(() => useUpdateUiValue(4, 2, callback, true))\n     await waitFor(() => expect(callback).not.toHaveBeenCalled())\n   })\n \n-  it(""shoud not update ui value if values are same"", async () => {\n+  it(""should not update ui value if values are same"", async () => {",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a1e,streamlit/streamlit,2508411089,2080745855,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:42+00:00,Corrected 'overlayed' to 'overlaid' in the comment.,true,frontend/lib/src/components/widgets/NumberInput/styled-components.ts,5,"@@ -31,7 +31,7 @@ export const StyledInputContainer = styled.div(({ theme }) => ({\n   transitionProperty: ""border"",\n   transitionTimingFunction: ""cubic-bezier(0.2, 0.8, 0.4, 1)"",\n   borderRadius: theme.radii.default,\n-  overflow: ""hidden"", // Fix rounded corner being overlayed with corner of internal input.\n+  overflow: ""hidden"", // Fix rounded corner being overlaid with corner of internal input.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a1f,streamlit/streamlit,2508411089,2080745857,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:42+00:00,Corrected 'precendence' to 'precedence'.,true,frontend/lib/src/components/widgets/Multiselect/Multiselect.tsx,5,"@@ -358,7 +358,7 @@ const Multiselect: FC<Props> = props => {\n                       // Using !important because the alternative would be\n                       // uglier: we'd have to put it under a selector like\n                       // ""&[role=""button""]:not(:disabled)"" in order to win in\n-                      // the order of the precendence.\n+                      // the order of the precedence.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a20,streamlit/streamlit,2508411089,2080745858,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:42+00:00,"Corrected 'thats' to ""that's"" and 'usuable' to 'usable'.\n```suggestion\n        // 6rem was manually determined as the smallest size that's still somewhat usable:\n```",true,frontend/lib/src/components/widgets/DataFrame/styled-components.ts,5,"@@ -51,7 +51,7 @@ export const StyledResizableContainer =\n         // 19rem is the closest rem without decimals to the original size:\n         maxWidth: ""19rem"",\n         width: ""80%"",\n-        // 6rem was manually determined as the smallest size thats still somewhat usuable:\n+        // 6rem was manually determined as the smallest size thats still somewhat usable:",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a21,streamlit/streamlit,2508411089,2080745860,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:43+00:00,Corrected 'weired' to 'weird'.,true,frontend/lib/src/components/widgets/DataFrame/styled-components.ts,14,"@@ -63,7 +63,7 @@ export const StyledResizableContainer =\n         },\n         ""& .gdg-search-progress"": {\n           // We are disabling the search progress bar since it\n-          // looks a bit weired in its current state and doesn't work\n+          // looks a bit weird in its current state and doesn't work",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a22,streamlit/streamlit,2508411089,2080745862,Copilot,175728472,2025-05-09T01:41:40+00:00,2025-05-09T01:41:43+00:00,Corrected 'alignes' to 'aligns' in the test description.,true,frontend/lib/src/components/widgets/DataFrame/columns/NumberColumn.test.ts,5,"@@ -101,7 +101,7 @@ describe(""NumberColumn"", () => {\n     expect((mockCell as NumberCell).data).toEqual(1.234)\n   })\n \n-  it(""alignes numbers to the right"", () => {\n+  it(""aligns numbers to the right"", () => {",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a23,streamlit/streamlit,2508411089,2080745866,Copilot,175728472,2025-05-09T01:41:41+00:00,2025-05-09T01:41:43+00:00,Corrected 'programatically' to 'programmatically'.,true,frontend/lib/src/components/widgets/BaseWidget/WidgetLabel.tsx,5,"@@ -34,7 +34,7 @@ export interface LabelProps {\n   // Used to specify whether widget is visible or not.\n   labelVisibility?: LabelVisibilityOptions\n \n-  // Associates the label with the input field programatically. Makes it possible to focus input by clicking on label.\n+  // Associates the label with the input field programmatically. Makes it possible to focus input by clicking on label.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a24,streamlit/streamlit,2508411089,2080745869,Copilot,175728472,2025-05-09T01:41:41+00:00,2025-05-09T01:41:43+00:00,Fixed the typo 'shoud' to 'should' in the test description.,true,frontend/lib/src/components/elements/GraphVizChart/GraphVizChart.test.tsx,5,"@@ -117,7 +117,7 @@ describe(""GraphVizChart Element"", () => {\n     expect(graphviz).toHaveBeenCalledTimes(1)\n   })\n \n-  it(""shoud render with height and width set to auto"", () => {\n+  it(""should render with height and width set to auto"", () => {",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a25,streamlit/streamlit,2508411089,2080745872,Copilot,175728472,2025-05-09T01:41:41+00:00,2025-05-09T01:41:43+00:00,Corrected 'absolut' to 'absolute'.,true,frontend/app/src/components/AppView/AppView.tsx,5,"@@ -283,7 +283,7 @@ function AppView(props: AppViewProps): ReactElement {\n           <Profiler id=""Bottom"">\n             {/* We add spacing here to make sure that the sticky bottom is\n            always pinned the bottom. Using sticky layout here instead of\n-           absolut / fixed is a trick to automatically account for the bottom\n+           absolute / fixed is a trick to automatically account for the bottom",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a26,streamlit/streamlit,2508411089,2080745877,Copilot,175728472,2025-05-09T01:41:41+00:00,2025-05-09T01:41:43+00:00,Corrected 'interals' to 'intervals'.,true,frontend/app/src/App.test.tsx,NaN,"@@ -3283,7 +3283,7 @@ describe(""App"", () => {\n       // make sure that no new messages were sent after switching the page\n       // despite advancing the timer. We could check whether clearInterval\n       // was called, but this check is more observing the behavior than checking\n-      // the exact interals.\n+      // the exact intervals.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a27,streamlit/streamlit,2508411089,2080745881,Copilot,175728472,2025-05-09T01:41:42+00:00,2025-05-09T01:41:43+00:00,Corrected 'substract' to 'subtract'.,true,e2e_playwright/st_expander_test.py,5,"@@ -52,7 +52,7 @@ def test_expander_collapses_and_expands(app: Page):\n     main_expanders = main_container.get_by_test_id(""stExpander"")\n     expect(main_expanders).to_have_count(\n         NUMBER_OF_EXPANDERS - 1\n-    )  # -1 to substract sidebar\n+    )  # -1 to subtract sidebar",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a28,streamlit/streamlit,2508411089,2080999287,mayagbarnes,63436329,2025-05-09T06:11:16+00:00,2025-05-09T06:11:16+00:00,Nit: think the correction should be `internals`,true,frontend/app/src/App.test.tsx,NaN,"@@ -3283,7 +3283,7 @@ describe(""App"", () => {\n       // make sure that no new messages were sent after switching the page\n       // despite advancing the timer. We could check whether clearInterval\n       // was called, but this check is more observing the behavior than checking\n-      // the exact interals.\n+      // the exact intervals.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a29,streamlit/streamlit,2508411089,2081005201,mayagbarnes,63436329,2025-05-09T06:17:04+00:00,2025-05-09T06:17:05+00:00,Nit: think the correction should be `of`,true,lib/tests/streamlit/elements/exception_test.py,NaN,"@@ -317,7 +317,7 @@ def test_st_exception(self, show_error_details: bool):\n         """"""Test st.exception.""""""\n         # client.showErrorDetails has no effect on code that calls\n         # st.exception directly. This test should have the same result\n-        # regardless fo the config option.\n+        # regardless to the config option.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a2a,streamlit/streamlit,2508411089,2081006093,mayagbarnes,63436329,2025-05-09T06:17:56+00:00,2025-05-09T06:17:56+00:00,Nit: think the correction should be `dependent`,true,lib/tests/streamlit/streamlit_test.py,NaN,"@@ -192,7 +192,7 @@ def test_importtime_median_under_threshold():\n     and check if it's under a static threshold.\n     """"""\n     # Define an acceptable threshold for import time (in microseconds).\n-    # This value is also depenend a bit on the machine it's run on,\n+    # This value is also depend a bit on the machine it's run on,",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a2b,streamlit/streamlit,2508411089,2081053893,co63oc,4617245,2025-05-09T06:57:58+00:00,2025-05-09T06:57:58+00:00,fixed,true,lib/tests/streamlit/streamlit_test.py,NaN,"@@ -192,7 +192,7 @@ def test_importtime_median_under_threshold():\n     and check if it's under a static threshold.\n     """"""\n     # Define an acceptable threshold for import time (in microseconds).\n-    # This value is also depenend a bit on the machine it's run on,\n+    # This value is also depend a bit on the machine it's run on,",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a2c,streamlit/streamlit,2508411089,2081054224,co63oc,4617245,2025-05-09T06:58:11+00:00,2025-05-09T06:58:11+00:00,fixed,true,lib/tests/streamlit/elements/exception_test.py,NaN,"@@ -317,7 +317,7 @@ def test_st_exception(self, show_error_details: bool):\n         """"""Test st.exception.""""""\n         # client.showErrorDetails has no effect on code that calls\n         # st.exception directly. This test should have the same result\n-        # regardless fo the config option.\n+        # regardless to the config option.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a2d,streamlit/streamlit,2508411089,2081054477,co63oc,4617245,2025-05-09T06:58:22+00:00,2025-05-09T06:58:22+00:00,fixed,true,frontend/app/src/App.test.tsx,NaN,"@@ -3283,7 +3283,7 @@ describe(""App"", () => {\n       // make sure that no new messages were sent after switching the page\n       // despite advancing the timer. We could check whether clearInterval\n       // was called, but this check is more observing the behavior than checking\n-      // the exact interals.\n+      // the exact intervals.",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a2e,streamlit/streamlit,2508015053,2080442806,raethlein,3775781,2025-05-08T20:45:36+00:00,2025-05-08T20:53:44+00:00,nit: Could we have a small comment here about why we use `-`? Something like `// we use the negative score here because we want the list to be reversed. We don't reverse it in the end because that can lead to UI shuffling.` (if I understood it correctly from your PR description),false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a2f,streamlit/streamlit,2508015053,2080445334,raethlein,3775781,2025-05-08T20:47:53+00:00,2025-05-08T20:53:44+00:00,```suggestion\r\n    /* app/models/order is better than app/models/zrder  */\r\n```\r\n\r\nnit: Could you leave a comment here about the _why_ it is better? I assume its because the `o` in `order` comes before the `z` in `zrder`.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a30,streamlit/streamlit,2508015053,2080448999,raethlein,3775781,2025-05-08T20:51:15+00:00,2025-05-08T20:53:44+00:00,"nit: if these tests are copied 1:1 from fuzzy, I think we should mention it here",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a31,streamlit/streamlit,2508015053,2080450207,raethlein,3775781,2025-05-08T20:52:15+00:00,2025-05-08T20:53:44+00:00,"if this is a substantial copy of the code, I think we should keep the MIT license notice or mention  the MIT-licensing here.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a32,streamlit/streamlit,2508015053,2080442806,raethlein,3775781,2025-05-08T20:45:36+00:00,2025-05-08T20:53:44+00:00,nit: Could we have a small comment here about why we use `-`? Something like `// we use the negative score here because we want the list to be reversed. We don't reverse it in the end because that can lead to UI shuffling.` (if I understood it correctly from your PR description),true,frontend/lib/src/components/shared/Dropdown/Selectbox.tsx,34,"@@ -68,9 +67,10 @@ export function fuzzyFilterSelectOptions(\n   const filteredOptions = options.filter((opt: SelectOption) =>\n     hasMatch(pattern, opt.label)\n   )\n-  return sortBy(filteredOptions, (opt: SelectOption) =>\n-    score(pattern, opt.label)\n-  ).reverse()\n+  return sortBy(\n+    filteredOptions,\n+    (opt: SelectOption) => -score(pattern, opt.label, true)",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a33,streamlit/streamlit,2508015053,2080445334,raethlein,3775781,2025-05-08T20:47:53+00:00,2025-05-08T20:53:44+00:00,```suggestion\r\n    /* app/models/order is better than app/models/zrder  */\r\n```\r\n\r\nnit: Could you leave a comment here about the _why_ it is better? I assume its because the `o` in `order` comes before the `z` in `zrder`.,true,frontend/lib/src/util/fuzzySearch.test.ts,NaN,"@@ -0,0 +1,231 @@\n+/**\n+ * Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the ""License"");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an ""AS IS"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import * as fzy from ""./fuzzySearch""\n+\n+const score = fzy.score\n+const positions = fzy.positions\n+\n+const SCORE_MIN = fzy.SCORE_MIN\n+const SCORE_MAX = fzy.SCORE_MAX\n+\n+const SCORE_GAP_LEADING = fzy.SCORE_GAP_LEADING\n+const SCORE_GAP_TRAILING = fzy.SCORE_GAP_TRAILING\n+const SCORE_GAP_INNER = fzy.SCORE_GAP_INNER\n+const SCORE_MATCH_CONSECUTIVE = fzy.SCORE_MATCH_CONSECUTIVE\n+const SCORE_MATCH_SLASH = fzy.SCORE_MATCH_SLASH\n+const SCORE_MATCH_CAPITAL = fzy.SCORE_MATCH_CAPITAL\n+const SCORE_MATCH_DOT = fzy.SCORE_MATCH_DOT\n+\n+/* score(needle, haystack) */\n+describe(""score"", () => {\n+  it(""should prefer starts of words"", function () {\n+    /* App/Models/Order is better than App/MOdels/zRder  */",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a34,streamlit/streamlit,2508015053,2080448999,raethlein,3775781,2025-05-08T20:51:15+00:00,2025-05-08T20:53:44+00:00,"nit: if these tests are copied 1:1 from fuzzy, I think we should mention it here",true,frontend/lib/src/util/fuzzySearch.test.ts,NaN,"@@ -0,0 +1,231 @@\n+/**\n+ * Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the ""License"");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an ""AS IS"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a35,streamlit/streamlit,2508015053,2080450207,raethlein,3775781,2025-05-08T20:52:15+00:00,2025-05-08T20:53:44+00:00,"if this is a substantial copy of the code, I think we should keep the MIT license notice or mention  the MIT-licensing here.",true,frontend/lib/src/util/fuzzySearch.ts,NaN,"@@ -0,0 +1,231 @@\n+/**\n+ * Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the ""License"");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an ""AS IS"" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// Thank you to fzy.js for the original implementation",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a36,streamlit/streamlit,2507812247,2080433058,raethlein,3775781,2025-05-08T20:40:17+00:00,2025-05-08T20:41:56+00:00,nit: in which cases can the `id` not be set?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a37,streamlit/streamlit,2507812247,2080549924,kmcgrady,69432,2025-05-08T22:34:41+00:00,2025-05-08T22:34:54+00:00,"It should always be defined for use cases, but Virtual Dropdown is generic enough that I want to make sure it always works, so having the value as a fallback makes sense.\r\n\r\nI added a comment for clarity",false,NaN,NaN,NaN,1,1,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a38,streamlit/streamlit,2507812247,2080433058,raethlein,3775781,2025-05-08T20:40:17+00:00,2025-05-08T20:41:56+00:00,nit: in which cases can the `id` not be set?,true,frontend/lib/src/components/shared/Dropdown/VirtualDropdown.tsx,12,"@@ -120,9 +120,11 @@ const VirtualDropdown = React.forwardRef<any, any>((props, ref) => {\n         height={height}\n         itemCount={children.length}\n         itemData={children}\n-        itemKey={(index: number, data: { props: OptionListProps }[]) =>\n-          data[index].props.item.value\n-        }\n+        itemKey={(index: number, data: { props: OptionListProps }[]) => {\n+          const { id, value } = data[index].props.item\n+\n+          return id ?? value",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a39,streamlit/streamlit,2507812247,2080549924,kmcgrady,69432,2025-05-08T22:34:41+00:00,2025-05-08T22:34:54+00:00,"It should always be defined for use cases, but Virtual Dropdown is generic enough that I want to make sure it always works, so having the value as a fallback makes sense.\r\n\r\nI added a comment for clarity",true,frontend/lib/src/components/shared/Dropdown/VirtualDropdown.tsx,12,"@@ -120,9 +120,11 @@ const VirtualDropdown = React.forwardRef<any, any>((props, ref) => {\n         height={height}\n         itemCount={children.length}\n         itemData={children}\n-        itemKey={(index: number, data: { props: OptionListProps }[]) =>\n-          data[index].props.item.value\n-        }\n+        itemKey={(index: number, data: { props: OptionListProps }[]) => {\n+          const { id, value } = data[index].props.item\n+\n+          return id ?? value",1,1,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a3a,streamlit/streamlit,2506789117,2080101093,kmcgrady,69432,2025-05-08T16:52:02+00:00,2025-05-08T16:52:03+00:00,Does it really need to be in React state? It feels like it's prone to continue to cause errors due to some of the async nature under the hood in React (especially with React 19). Might be easier as a flag of some sort?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a3b,streamlit/streamlit,2506789117,2080111928,lukasmasuch,2852129,2025-05-08T16:59:31+00:00,2025-05-08T17:00:11+00:00,"> Does it really need to be in React state? It feels like it's prone to continue to cause errors due to some of the async nature under the hood in React (especially with React 19). Might be easier as a flag of some sort?\r\n\r\nIs there a good alternative that wouldn't require state? I think a more bulletproof solution could be to add some kind of identifiable information to the proto messages to check if a finish message is actually associated with the latest rerun triggered in the frontend. But even that would likely require keeping some info in frontend state, and I'm not too excited about adding more information to our protos. ",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a3c,streamlit/streamlit,2506789117,2080115913,lukasmasuch,2852129,2025-05-08T17:02:28+00:00,2025-05-08T17:02:29+00:00,"However, if we could associate messages with an initial rerun message, we could just ignore all incoming messages in the frontend that are not associated with the latest rerun. But that's likely a slightly bigger change.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a3d,streamlit/streamlit,2506789117,2080121388,kmcgrady,69432,2025-05-08T17:05:35+00:00,2025-05-08T17:05:35+00:00,Would an instance variable be bad? It's like the equivalent of a ref? Or do you think that creates more opportunites to save the cache?,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a3e,streamlit/streamlit,2506789117,2080176157,lukasmasuch,2852129,2025-05-08T17:43:15+00:00,2025-05-08T17:43:15+00:00,"oh, yes... You are correct! Using an instance variable here is a lot better. Updated it 👍",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a3f,streamlit/streamlit,2506789117,2080101093,kmcgrady,69432,2025-05-08T16:52:02+00:00,2025-05-08T16:52:03+00:00,Does it really need to be in React state? It feels like it's prone to continue to cause errors due to some of the async nature under the hood in React (especially with React 19). Might be easier as a flag of some sort?,true,frontend/app/src/App.tsx,NaN,"@@ -201,6 +201,11 @@ interface State {\n   appConfig: AppConfig\n   autoReruns: NodeJS.Timeout[]\n   inputsDisabled: boolean\n+  // Whether we have received a NewSession message after the latest rerun request.\n+  // This is used to ensure that we only increment the message cache run count after\n+  // we have received a NewSession message after the latest rerun request.\n+  // This will allow us to ignore finished messages from previous script runs.\n+  hasReceivedNewSession: boolean",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a40,streamlit/streamlit,2506789117,2080111928,lukasmasuch,2852129,2025-05-08T16:59:31+00:00,2025-05-08T17:00:11+00:00,"> Does it really need to be in React state? It feels like it's prone to continue to cause errors due to some of the async nature under the hood in React (especially with React 19). Might be easier as a flag of some sort?\r\n\r\nIs there a good alternative that wouldn't require state? I think a more bulletproof solution could be to add some kind of identifiable information to the proto messages to check if a finish message is actually associated with the latest rerun triggered in the frontend. But even that would likely require keeping some info in frontend state, and I'm not too excited about adding more information to our protos. ",true,frontend/app/src/App.tsx,NaN,"@@ -201,6 +201,11 @@ interface State {\n   appConfig: AppConfig\n   autoReruns: NodeJS.Timeout[]\n   inputsDisabled: boolean\n+  // Whether we have received a NewSession message after the latest rerun request.\n+  // This is used to ensure that we only increment the message cache run count after\n+  // we have received a NewSession message after the latest rerun request.\n+  // This will allow us to ignore finished messages from previous script runs.\n+  hasReceivedNewSession: boolean",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a41,streamlit/streamlit,2506789117,2080115913,lukasmasuch,2852129,2025-05-08T17:02:28+00:00,2025-05-08T17:02:29+00:00,"However, if we could associate messages with an initial rerun message, we could just ignore all incoming messages in the frontend that are not associated with the latest rerun. But that's likely a slightly bigger change.",true,frontend/app/src/App.tsx,NaN,"@@ -201,6 +201,11 @@ interface State {\n   appConfig: AppConfig\n   autoReruns: NodeJS.Timeout[]\n   inputsDisabled: boolean\n+  // Whether we have received a NewSession message after the latest rerun request.\n+  // This is used to ensure that we only increment the message cache run count after\n+  // we have received a NewSession message after the latest rerun request.\n+  // This will allow us to ignore finished messages from previous script runs.\n+  hasReceivedNewSession: boolean",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a42,streamlit/streamlit,2506789117,2080121388,kmcgrady,69432,2025-05-08T17:05:35+00:00,2025-05-08T17:05:35+00:00,Would an instance variable be bad? It's like the equivalent of a ref? Or do you think that creates more opportunites to save the cache?,true,frontend/app/src/App.tsx,NaN,"@@ -201,6 +201,11 @@ interface State {\n   appConfig: AppConfig\n   autoReruns: NodeJS.Timeout[]\n   inputsDisabled: boolean\n+  // Whether we have received a NewSession message after the latest rerun request.\n+  // This is used to ensure that we only increment the message cache run count after\n+  // we have received a NewSession message after the latest rerun request.\n+  // This will allow us to ignore finished messages from previous script runs.\n+  hasReceivedNewSession: boolean",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a43,streamlit/streamlit,2506789117,2080176157,lukasmasuch,2852129,2025-05-08T17:43:15+00:00,2025-05-08T17:43:15+00:00,"oh, yes... You are correct! Using an instance variable here is a lot better. Updated it 👍",true,frontend/app/src/App.tsx,NaN,"@@ -201,6 +201,11 @@ interface State {\n   appConfig: AppConfig\n   autoReruns: NodeJS.Timeout[]\n   inputsDisabled: boolean\n+  // Whether we have received a NewSession message after the latest rerun request.\n+  // This is used to ensure that we only increment the message cache run count after\n+  // we have received a NewSession message after the latest rerun request.\n+  // This will allow us to ignore finished messages from previous script runs.\n+  hasReceivedNewSession: boolean",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a44,streamlit/streamlit,2505434874,2085080220,sfc-gh-bnisco,178842601,2025-05-12T16:50:49+00:00,2025-05-12T21:53:10+00:00,"**question:** Do we foresee there to be additional usages of `containerElement` in the future? If not, it would make sense to render the `StyleLayoutWrapper` in the return block of `node.deltaBlock.chatMessage` so we keep the variable scope contained in that block.",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a45,streamlit/streamlit,2505434874,2085614412,sfc-gh-lwilby,103002751,2025-05-12T22:19:07+00:00,2025-05-12T22:19:30+00:00,"My plan is to wrap all of these containers with `StyleLayoutWrapper` eventually, and originally I made the change to all of them here, but then I decided to restrict it to just `st.chat_message` to start so I could validate the change for each of them individually along with adding the width/height properties. ",false,NaN,NaN,NaN,1,1,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a46,streamlit/streamlit,2505434874,2085080220,sfc-gh-bnisco,178842601,2025-05-12T16:50:49+00:00,2025-05-12T21:53:10+00:00,"**question:** Do we foresee there to be additional usages of `containerElement` in the future? If not, it would make sense to render the `StyleLayoutWrapper` in the return block of `node.deltaBlock.chatMessage` so we keep the variable scope contained in that block.",true,frontend/lib/src/components/core/Block/Block.tsx,33,"@@ -282,6 +290,8 @@ const BlockNodeRenderer = (props: BlockPropsWithoutWidth): ReactElement => {\n     />\n   )\n \n+  let containerElement: ReactElement | undefined",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a47,streamlit/streamlit,2505434874,2085614412,sfc-gh-lwilby,103002751,2025-05-12T22:19:07+00:00,2025-05-12T22:19:30+00:00,"My plan is to wrap all of these containers with `StyleLayoutWrapper` eventually, and originally I made the change to all of them here, but then I decided to restrict it to just `st.chat_message` to start so I could validate the change for each of them individually along with adding the width/height properties. ",true,frontend/lib/src/components/core/Block/Block.tsx,33,"@@ -282,6 +290,8 @@ const BlockNodeRenderer = (props: BlockPropsWithoutWidth): ReactElement => {\n     />\n   )\n \n+  let containerElement: ReactElement | undefined",1,1,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a48,streamlit/streamlit,2505322559,2078590292,github-advanced-security[bot],62310815,2025-05-07T22:43:16+00:00,2025-05-07T22:43:16+00:00,## Useless conditional\n\nThis constructor call to DateTimeFormat always evaluates to true.\n\n[Show more details](https://github.com/streamlit/streamlit/security/code-scanning/10118),false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a49,streamlit/streamlit,2505322559,2079668474,lukasmasuch,2852129,2025-05-08T12:58:48+00:00,2025-05-08T12:58:48+00:00,Directly using `navigator.languages` here might be even slightly better since it gives us a list of configured locales in the user browser ordered by priority. The number formatter iterates through those to match the first supported locale. ,false,NaN,NaN,NaN,1,1,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a4a,streamlit/streamlit,2505322559,2082353471,lukasmasuch,2852129,2025-05-09T19:00:36+00:00,2025-05-09T19:00:37+00:00,"I think we might also need to apply the locales to the other formats using Intl.NumberFormat as well since those are also expected to be localized to the user's locale. \r\n\r\nBy the way, since catching all these errors could get a bit annoying, maybe we could also define `en` as a fallback in the locales array. Based on ChatGPT, `en` or `en-US` should be supported very broadly. \r\n\r\n```typescript\r\nconst locales = [...navigator.languages, ""en""] \r\n\r\n...\r\n\r\nreturn new Intl.NumberFormat(locales).format(value)\r\n\r\n...\r\n\r\nreturn new Intl.NumberFormat(undefined, {\r\n      style: ""percent"",\r\n      minimumFractionDigits: 0,\r\n      maximumFractionDigits: 2,\r\n    }).format(value)\r\n    \r\n```\r\n\r\n\r\n\r\n\r\n\r\n",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a4b,streamlit/streamlit,2505322559,2082354294,Copilot,175728472,2025-05-09T19:01:20+00:00,2025-05-09T19:01:20+00:00,[nitpick] The extraction of navigator.languages is repeated in both formatNumber and formatMoment; consider refactoring this into a shared helper to reduce duplication.\n```suggestion\n    const locales = getNavigatorLanguages()\n```,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a4c,streamlit/streamlit,2505322559,2083447693,mayagbarnes,63436329,2025-05-11T08:10:39+00:00,2025-05-11T08:10:39+00:00,"Makes sense! Consolidated to helper to avoid a bunch of error catches - also asked which is the better fallback, passing `en-US` vs `undefined` and cursor said `undefined` vs adding en-US` to the locales array",false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a4d,streamlit/streamlit,2505322559,2078590292,github-advanced-security[bot],62310815,2025-05-07T22:43:16+00:00,2025-05-07T22:43:16+00:00,## Useless conditional\n\nThis constructor call to DateTimeFormat always evaluates to true.\n\n[Show more details](https://github.com/streamlit/streamlit/security/code-scanning/10118),true,frontend/lib/src/components/widgets/DataFrame/columns/utils.ts,NaN,"@@ -613,10 +622,18 @@\n   momentKind: ""date"" | ""time"" | ""datetime"" = ""datetime""\n ): string {\n   if (format === ""localized"") {\n-    return new Intl.DateTimeFormat(undefined, {\n-      dateStyle: momentKind === ""time"" ? undefined : ""medium"",\n-      timeStyle: momentKind === ""date"" ? undefined : ""medium"",\n-    }).format(momentDate.toDate())\n+    const locale = getLocaleLanguage()\n+    // If the locale is not supported, use the default locale as fallback\n+    const date =\n+      new Intl.DateTimeFormat(locale, {\n+        dateStyle: momentKind === ""time"" ? undefined : ""medium"",\n+        timeStyle: momentKind === ""date"" ? undefined : ""medium"",\n+      }) ||",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a4e,streamlit/streamlit,2505322559,2079668474,lukasmasuch,2852129,2025-05-08T12:58:48+00:00,2025-05-08T12:58:48+00:00,Directly using `navigator.languages` here might be even slightly better since it gives us a list of configured locales in the user browser ordered by priority. The number formatter iterates through those to match the first supported locale. ,true,frontend/lib/src/components/widgets/DataFrame/columns/utils.ts,NaN,"@@ -555,7 +559,17 @@ export function formatNumber(\n       trimMantissa: true,\n     })\n   } else if (format === ""localized"") {\n-    return new Intl.NumberFormat().format(value)\n+    const locale = getLocaleLanguage()\n+    try {\n+      return new Intl.NumberFormat(locale).format(value)",1,1,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a4f,streamlit/streamlit,2505322559,2082353471,lukasmasuch,2852129,2025-05-09T19:00:36+00:00,2025-05-09T19:00:37+00:00,"I think we might also need to apply the locales to the other formats using Intl.NumberFormat as well since those are also expected to be localized to the user's locale. \r\n\r\nBy the way, since catching all these errors could get a bit annoying, maybe we could also define `en` as a fallback in the locales array. Based on ChatGPT, `en` or `en-US` should be supported very broadly. \r\n\r\n```typescript\r\nconst locales = [...navigator.languages, ""en""] \r\n\r\n...\r\n\r\nreturn new Intl.NumberFormat(locales).format(value)\r\n\r\n...\r\n\r\nreturn new Intl.NumberFormat(undefined, {\r\n      style: ""percent"",\r\n      minimumFractionDigits: 0,\r\n      maximumFractionDigits: 2,\r\n    }).format(value)\r\n    \r\n```\r\n\r\n\r\n\r\n\r\n\r\n",true,frontend/lib/src/components/widgets/DataFrame/columns/utils.ts,42,"@@ -555,7 +555,17 @@ export function formatNumber(\n       trimMantissa: true,\n     })\n   } else if (format === ""localized"") {\n-    return new Intl.NumberFormat().format(value)\n+    const locales = navigator.languages\n+    try {\n+      return new Intl.NumberFormat(locales).format(value)\n+    } catch (error) {\n+      // If the locale is not supported, the above throws a RangeError\n+      // In this case we use default locale as fallback\n+      if (error instanceof RangeError) {\n+        return new Intl.NumberFormat().format(value)\n+      }\n+      throw error\n+    }\n   } else if (format === ""percent"") {",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a50,streamlit/streamlit,2505322559,2082354294,Copilot,175728472,2025-05-09T19:01:20+00:00,2025-05-09T19:01:20+00:00,[nitpick] The extraction of navigator.languages is repeated in both formatNumber and formatMoment; consider refactoring this into a shared helper to reduce duplication.\n```suggestion\n    const locales = getNavigatorLanguages()\n```,true,frontend/lib/src/components/widgets/DataFrame/columns/utils.ts,NaN,"@@ -555,7 +555,17 @@ export function formatNumber(\n       trimMantissa: true,\n     })\n   } else if (format === ""localized"") {\n-    return new Intl.NumberFormat().format(value)\n+    const locales = navigator.languages",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a51,streamlit/streamlit,2505322559,2083447693,mayagbarnes,63436329,2025-05-11T08:10:39+00:00,2025-05-11T08:10:39+00:00,"Makes sense! Consolidated to helper to avoid a bunch of error catches - also asked which is the better fallback, passing `en-US` vs `undefined` and cursor said `undefined` vs adding en-US` to the locales array",true,frontend/lib/src/components/widgets/DataFrame/columns/utils.ts,42,"@@ -555,7 +555,17 @@ export function formatNumber(\n       trimMantissa: true,\n     })\n   } else if (format === ""localized"") {\n-    return new Intl.NumberFormat().format(value)\n+    const locales = navigator.languages\n+    try {\n+      return new Intl.NumberFormat(locales).format(value)\n+    } catch (error) {\n+      // If the locale is not supported, the above throws a RangeError\n+      // In this case we use default locale as fallback\n+      if (error instanceof RangeError) {\n+        return new Intl.NumberFormat().format(value)\n+      }\n+      throw error\n+    }\n   } else if (format === ""percent"") {",0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a52,streamlit/streamlit,2502895623,2077367466,Copilot,175728472,2025-05-07T10:58:48+00:00,2025-05-07T10:58:48+00:00,Consider rate-limiting or sampling the error log to prevent potential log flooding if many authentication cookies exceed the size limit.,false,NaN,NaN,NaN,0,0,0,0,0,0,0,streamlit_streamlit
6824cd910ce5dcec78431a53,streamlit/streamlit,2502895623,2077367466,Copilot,175728472,2025-05-07T10:58:48+00:00,2025-05-07T10:58:48+00:00,Consider rate-limiting or sampling the error log to prevent potential log flooding if many authentication cookies exceed the size limit.,true,lib/streamlit/web/server/oauth_authlib_routes.py,6,"@@ -74,6 +74,13 @@ def redirect_to_base(self) -> None:\n \n     def set_auth_cookie(self, user_info: dict[str, Any]) -> None:\n         serialized_cookie_value = json.dumps(user_info)\n+\n+        # log error if cookie value is larger than 4096 bytes\n+        if len(serialized_cookie_value.encode()) > 4096:",0,0,0,0,0,0,0,streamlit_streamlit
