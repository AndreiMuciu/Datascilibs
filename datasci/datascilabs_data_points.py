# -*- coding: utf-8 -*-
"""datascilabs-data-points.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AJ5VGrEK4g4FZvqIrbGClQGRoSHdXguE

# Collect Data Points
"""

!pip install PyGithub

"""Load secrets and init some constants"""

from github import Github
from google.colab import userdata
GH_TOKEN = userdata.get('GH_TOKEN')
gh = Github(login_or_token=GH_TOKEN)
TOP_HOW_MANY = 50
SEARCH_QUERIES = ('topic:data-science', 'topic:datascience',
                  'data-science', 'datascience',
                  'data science') #search terms for data science repos

"""Function to search on [GitHub Search Page](https://github.com/search) using the rest api.

[Here](https://github.com/search?q=topic%3Adata-science&type=repositories) is the example of the results of **topic:data-science** search query.
"""

def execute_gh_search_query(gh: Github, query: str, limit: int):
  return gh.search_repositories(query, sort='stars', order='desc')[:limit]

"""Append all search results from all used search queries."""

results = []
for query in SEARCH_QUERIES:
  temp = execute_gh_search_query(gh, query, TOP_HOW_MANY)
  for repo in temp:
    results.append(repo)

"""Deduplicating, then sorting and then selecting the top results."""

results = list(set(results))
results_sorted = sorted(results, key=lambda repo: repo.stargazers_count, reverse=True)
results_sorted = results_sorted[:TOP_HOW_MANY]

"""Build shallow csv.

"""

df = pd.DataFrame([{
    'full_name': repo.full_name,
    'url': repo.html_url,
    'stars': repo.stargazers_count,
}])
df.insert(0, 'rank', range(1, len(df) + 1))
df.head()
df.to_csv('shallow_data.csv', index=False)

"""# Build full csv.

Some utility functions
"""

from datetime import datetime, timedelta, timezone
import pandas as pd

def get_contribs_in_last_6_months(repo):
    since = datetime.now() - timedelta(days=180)
    contributors = set()
    count = 0
    try:
        for commit in repo.get_commits(since=since):
            if count >= 1000:
                break
            author = commit.author
            if author:
                contributors.add(author.login)
            count += 1
    except Exception as e:
        print(f"Error getting contributors for {repo.full_name}: {e}")
    return len(contributors)

def get_commit_freq(repo, months=6):
    since = datetime.now() - timedelta(days=months*30)
    try:
        commits = []
        for commit in repo.get_commits(sha=repo.default_branch, since=since):
            commits.append(commit)
        if not commits:
            return 0
        return len(commits) / months
    except Exception as e:
        print(f"Error getting commit frequency for {repo.full_name}: {e}")
        return 0

def get_last_commit_date(repo):
    try:
        default_branch = repo.default_branch
        commits = repo.get_commits(sha=default_branch)
        for commit in commits:
            return commit.commit.author.date  # Return the first (latest) commit date
        return None  # No commits found
    except Exception as e:
        print(f"Error getting last commit date for {repo.full_name}: {e}")
        return None


def get_pr_counts(repo, since_days=30):
    from github.PaginatedList import PaginatedList
    since = datetime.now(timezone.utc) - timedelta(days=since_days)
    open_prs = closed_prs = merged_prs = 0

    def count_prs(state):
        count = merged = 0
        # Use per_page=100 for fewer requests
        prs: PaginatedList = repo.get_pulls(state=state, sort='created', direction='desc')
        for pr in prs:
            if pr.created_at < since:
                break  # Stop if PR is older than 'since'
            count += 1
            if state == 'closed' and pr.merged_at and pr.merged_at >= since:
                merged += 1
        return count, merged

    try:
        open_prs, _ = count_prs('open')
        closed_prs, merged_prs = count_prs('closed')
    except Exception as e:
        print(f"Error getting PR counts for {repo.full_name}: {e}")

    return open_prs, closed_prs, merged_prs

"""Building the csv with all datapoints. (this takes a while)"""

from datetime import datetime, timedelta
import pandas as pd

repo_data = []
total_len = len(results_sorted)
for i, repo in enumerate(results_sorted):
    try:
        print(f'Processing {i + 1}/{total_len}...')
        try:
            closed_issues = repo.get_issues(state='closed').totalCount
        except Exception as e:
            print(f"Error getting closed issues for {repo.full_name}: {e}")
            closed_issues = None
        try:
            number_of_releases = repo.get_releases().totalCount
        except Exception as e:
            print(f"Error getting releases for {repo.full_name}: {e}")
            number_of_releases = None
        try:
            total_contributors = repo.get_contributors().totalCount
        except Exception as e:
            print(f"Error getting contributors for {repo.full_name}: {e}")
            total_contributors = None
        try:
            total_languages = len(repo.get_languages())
        except Exception as e:
            print(f"Error getting languages for {repo.full_name}: {e}")
            total_languages = None

        open_prs, closed_prs, merged_prs = get_pr_counts(repo)

        repo_data.append({
            'full_name': repo.full_name,
            'url': repo.html_url,
            'owner': repo.owner.login,
            'license': repo.license.name if repo.license else None,
            'stars': repo.stargazers_count,
            'forks': repo.forks_count,
            'open_issues': repo.open_issues_count,
            'closed_issues': closed_issues,
            'open_prs': open_prs,
            'closed_prs': closed_prs,
            'merged_prs': merged_prs,
            'date_of_last_commit': get_last_commit_date(repo),
            'number_of_releases': number_of_releases,
            'total_contributors': total_contributors,
            'recent_contributors': get_contribs_in_last_6_months(repo),
            'avg_commits_per_month': get_commit_freq(repo, months=6),
        })
    except Exception as e:
        print(f"Error processing {repo.full_name}: {e}")

df = pd.DataFrame(repo_data)
df.insert(0, 'rank', range(1, len(df) + 1))
df.head()

df.to_csv('data.csv', index=False)