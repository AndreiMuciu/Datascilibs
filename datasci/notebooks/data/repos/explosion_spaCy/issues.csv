repo_full_name,issue_id,number,title,body,user_login,user_id,state,locked,comments_count,created_at,updated_at,closed_at,labels,reactions_total,reactions_plus1,reactions_minus1,reactions_laugh,reactions_hooray,reactions_confused,reactions_heart
explosion/spaCy,3054770441,13817,arabic model not found,"I can't install """"ar_core_web_sm"""", it's not on PyPI, and the releases are missing from the explosion/spacy-models GitHub releases page.
",abduljaleel02,206993293,open,False,0,2025-05-11T06:35:09+00:00,2025-05-11T06:35:09+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3051766509,13816,"fix: match hyphenated words to lemmas in index_table (e.g. ""co-author…","Previously, words with hyphens such as ""co-authored"" could not be matched correctly against the index_table, even though their lemma forms like ""co-author"" existed.
This fix normalizes such words by lemmatizing and aligning them with their base forms, ensuring accurate lookup and processing.

Example:

Input: ""co-authored""
Lemma: ""co-author""
Result: Now correctly matched in index_table

This improves the robustness of word matching, especially for common hyphenated expressions.

",d0ngw,602027,open,False,0,2025-05-09T10:43:07+00:00,2025-05-09T10:58:34+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3051549687,13815,Error in spacy pip installation,"Hi guys,
im new in python, i try to install spacy on my local machine but i reach this error

![Image](https://github.com/user-attachments/assets/6c58cad0-fea0-4bb0-9b67-a41f5419e3ea)

can anyone helps me 
thanks
bye
mauro",mauroruffinocsi,47896538,open,False,3,2025-05-09T09:13:29+00:00,2025-05-12T02:18:19+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3042609647,13813,Fix typos,Fix typos,omahs,73983677,open,False,0,2025-05-06T12:06:06+00:00,2025-05-06T12:06:06+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3041221493,13812,Why is this tokenization weird?,"On My MacPro Sequoria, spacy 3.7.5 with en_core_web_trf model, the following sentence:

`How should writes to T012K and T012T be handled?`

Is tokenized as: 

```
T012K => T012 and K
T012T => T012 and T
```

Other tokens look fine. this is a very normal sentence and how can it split them?",lingvisa,7905135,open,False,0,2025-05-06T01:23:01+00:00,2025-05-06T01:23:01+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3039377493,13811,"Update embeddings-transformers.mdx, update trf_data examples info","Update trf_data examples info in Runtime usage

<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",2513502304,129171955,closed,False,1,2025-05-05T11:29:15+00:00,2025-05-12T05:47:12+00:00,2025-05-12T05:47:12+00:00,docs,0,0,0,0,0,0,0
explosion/spaCy,3023321468,13809,"vscode doesn't like  displacy.render(sentence1, style=""dep"")","<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
---------------------------------------------------------------------------
```
ImportError                               Traceback (most recent call last)
Cell In[44], line 2
      1 from spacy import displacy
----> 2 displacy.render(sentence1, style=""dep"")

File c:\Users\kundeng\scoop\persist\mambaforge\envs\spacy\Lib\site-packages\spacy\displacy\__init__.py:69, in render(docs, style, page, minify, jupyter, options, manual)
     65     html = RENDER_WRAPPER(html)
     66 if jupyter or (jupyter is None and is_in_jupyter()):
     67     # return HTML rendered by IPython display()
     68     # See #4840 for details on span wrapper to disable mathjax
---> 69     from IPython.core.display import HTML, display
     71     return display(HTML('<span class=""tex2jax_ignore"">{}</span>'.format(html)))
     72 return html

ImportError: cannot import name 'display' from 'IPython.core.display' (c:\Users\xxx\scoop\persist\mambaforge\envs\spacy\Lib\site-packages\IPython\core\display.py)
```
## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 11
* Python Version Used: python 3.12.10
* spaCy Version Used:  3.8.5
* Environment Information: Conda Mamaforge
",kundeng,89032,open,False,3,2025-04-27T21:03:40+00:00,2025-05-10T00:32:58+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3023248810,13807,Added Haitian Creole (ht) Language Support to spaCy,"<!--- Added Haitian Creole (ht) Language Support to spaCy -->

## Description
This PR adds official support for Haitian Creole (ht) to spaCy's spacy/lang module.
It includes:

- Added all core language data files for spacy/lang/ht:
  - tokenizer_exceptions.py
  - punctuation.py
  - lex_attrs.py
  - syntax_iterators.py
  - lemmatizer.py
  - stop_words.py
  - tag_map.py

- Unit tests for tokenizer and noun chunking (test_tokenizer.py, test_noun_chunking.py, etc.). Passed all 58 pytest spacy/tests/lang/ht tests that I've created.
- Basic tokenizer rules adapted for Haitian Creole orthography and informal contractions.
- Custom like_num atrribute supporting Haitian number formats (e.g., ""3yèm"").
- Support for common informal apostrophe usage (e.g., ""m'ap"", ""n'ap"", ""di'm"").
- Ensured no breakages in other language modules.
- Followed spaCy coding style (PEP8, Black).



This provides a foundation for Haitian Creole NLP development using spaCy.


### Type of change
My PR covers the addition of a new language (new feature).

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.

## Additional Notes

- Haitian Creole does not have an official XPOS tagset, so UPOS (Universal POS) tags are used.
- The tokenizer was carefully adapted for informal orthographic contractions (m'ap, l'ap, etc.).
- Minimal stop_words were compiled, based on common function words and expressions.
- The contribution focuses on making ht available in the core library, and future models can be trained later based on this work.
- Trained using valid UD CoNLL-U data and received a final LAS score of 0.52 (based on a train set of 2670 sentences and dev set of 333 sentences). Looking to increase the treebank size over time and add on to this foundational ht spaCy module either myself or with the help of other collaborators that are fluent in Haitian Creole. I went with 96 hidden width, 10000 max steps, .25 dropout, 1 accumalate gradient, and a batch size of 50.

## Thanks
I'm very excited to get the ball rolling for a low-resource language like Haitian Creole  and contribute to an amazing library like spaCy!

## Example Usage
```
import spacy

nlp = spacy.blank(""ht"")

# text = ""Map manje gato a pandan map gade televizyon lem lakay mwen.""
# text = ""M'ap vini, eske wap la avek lajan'm? Si ou, di'l non pou fre'w.""
# text = ""M ap teste sa (pou kounye a).""
# text = ""Si'm ka vini, m'ap pale ak li.""
# text = ""\""regre lanmò twò bonè\""""
text = """"""Onè ap fèt pou ansyen lidè Pati Travayè Britanik

Moun atravè lemond ap voye onè pou ansyen lidè
Pati Travayè a, John Smith, ki mouri pi bonè jodi a apre li te fè yon gwo kriz kadyak a laj 55 an.

Nan Washington, Depatman Deta Etazini pibliye yon deklarasyon ki eksprime ""regre lanmò twò bonè"" avoka ak palmantè eskoze a.

""Misye Smith, pandan tout karyè li ki te make ak distenksyon""""""

doc = nlp(text)

print(""Tokens:"")
print(len(doc))
for token in doc:
    print(f""{token.text} | {token.orth_} | {token.norm_} | {token.whitespace_}"")
```",JephteyAdolphe,63896861,open,False,2,2025-04-27T18:31:08+00:00,2025-05-12T12:55:42+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3017599303,13806,sample did not work,"![Image](https://github.com/user-attachments/assets/bc3f37d8-22ca-48e7-90c0-262c263f37ce)


```python
import torch 
print(torch.__version__)

!pip install spacy

!python -m spacy download en_core_web_trf

import spacy
nlp = spacy.load(""en_core_web_trf"")
import en_core_web_trf
nlp = en_core_web_trf.load()
doc = nlp(""This is a sentence."")
print([(w.text, w.pos_) for w in doc])
```

error

```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[7], line 4
      1 get_ipython().system('python -m spacy download en_core_web_trf')
      3 import spacy
----> 4 nlp = spacy.load(""en_core_web_trf"")
      5 import en_core_web_trf
      6 nlp = en_core_web_trf.load()

File [~\AppData\Local\Programs\Python\Python310\lib\site-packages\spacy\__init__.py:51](http://localhost:8888/lab/tree/~/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/__init__.py#line=50), in load(name, vocab, disable, enable, exclude, config)
     27 def load(
     28     name: Union[str, Path],
     29     *,
   (...)
     34     config: Union[Dict[str, Any], Config] = util.SimpleFrozenDict(),
     35 ) -> Language:
     36     """"""Load a spaCy model from an installed package or a local path.
     37 
     38     name (str): Package name or model path.
   (...)
     49     RETURNS (Language): The loaded nlp object.
     50     """"""
---> 51     return util.load_model(
     52         name,
     53         vocab=vocab,
     54         disable=disable,
     55         enable=enable,
     56         exclude=exclude,
     57         config=config,
     58     )
```",donhuvy,1328316,open,False,1,2025-04-24T15:05:22+00:00,2025-04-24T15:07:02+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3011213011,13805,load('zh_core_web_trf-3.8.0') cause Can't find factory for 'curated_transformer' for language Chinese (zh),"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
1、i download the zh_core_web_trf-3.8.0 from https://github.com/explosion/spacy-models/releases/tag/zh_core_web_trf-3.8.0
2、i unzip the download file and write the code like below
```
 local_path = './../zh_core_web_trf-3.8.0'
 nlp = spacy.load(local_path)
       
doc = nlp(text)
```
then the error happen:
Can't find factory for 'curated_transformer' for language Chinese (zh). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel
## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:MACOS 15.1.1 (24B2091)
* Python Version Used: Python 3.10.11
* spaCy Version Used:
* Name: spacy
Version: 3.8.5
Summary: Industrial-strength Natural Language Processing (NLP) in Python
Home-page: https://spacy.io
Author: Explosion
Author-email: contact@explosion.ai
License: MIT
Location: /Users/chenzujie/work/Autobots/autobots_backend/.venv/lib/python3.10/site-packages
Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel

* Environment Information:
",TChengZ,2180654,closed,False,1,2025-04-22T14:08:09+00:00,2025-04-28T02:01:16+00:00,2025-04-28T02:01:15+00:00,,0,0,0,0,0,0,0
explosion/spaCy,3009264427,13804,Spacy Language components initialization extremely slow during cli training due to corpus train size.,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
run cli train for a over 10g of training data (DocBin). it takes several hours to initialize. the train hasn't started.
My use case is transformer+ner. I have about 1000 entity labels with 2 million training docs. After 10 hours I had to abort.
## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: linux
* Python Version Used: 3.12
* spaCy Version Used: latest v4
* Environment Information:GCP A1-ULTRA   16 cores 170gb MEM , A100 80GB GPU 
```
ython3 -m cProfile -o output.prof -m spacy train ./DONOT_MODIFY_en_core_web_custom_ent.cfg  --paths.train ./train_spacy_4m_small  --paths.dev ./test_spacy_4m   --training.max_epochs 5 --gpu-id 0 --output ./custom_ent_4mtrain --verbose
[2025-04-21 18:52:36,710] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev', 'training.max_epochs']
ℹ Saving to output directory: custom_ent_4mtrain
ℹ Using GPU: 0

=========================== Initializing pipeline ===========================
[2025-04-21 18:52:38,416] [INFO] Set up nlp object from config
[2025-04-21 18:52:38,457] [DEBUG] Loading corpus from path: test_spacy_4m
[2025-04-21 18:52:38,461] [DEBUG] Loading corpus from path: train_spacy_4m_small
[2025-04-21 18:52:38,461] [INFO] Pipeline: ['transformer', 'ner', 'doc_cleaner']
[2025-04-21 18:52:38,485] [DEBUG] Loading lookups from spacy-lookups-data: ['lexeme_norm']
[2025-04-21 18:52:38,507] [INFO] Added vocab lookups: lexeme_norm
[2025-04-21 18:52:38,507] [INFO] Created vocabulary
[2025-04-21 18:52:38,507] [INFO] Finished initializing nlp object
----- took 10 hours after which I had to abort it.
```
possible issue identified [here](https://github.com/explosion/spaCy/blob/acb44f8e73ef04b4b019637d5e72e6ad92508e73/spacy/language.py#L1455-L1464) 
```
            self.tokenizer.initialize(get_examples, nlp=self, **tok_settings)  # type: ignore[union-attr]
        for name, proc in self.pipeline:
            if isinstance(proc, ty.InitializableComponent):
                p_settings = I[""components""].get(name, {})
                if labels is not None and name in labels:
                    p_settings[""labels""] = labels[name]
                p_settings = validate_init_settings(
                    proc.initialize, p_settings, section=""components"", name=name
                )
                proc.initialize(get_examples, nlp=self, **p_settings)
        pretrain_cfg = config.get(""pretraining"")
```
request to run multiprocess to read through the examples for initialize . example below to speed up json to DocBin conversion from several hours to few minutes using multiprocess.

```
import spacy
from spacy.tokens import DocBin
from google.cloud import storage
import json

def batch_objects(objects, batch_size):
    for i in range(0, len(objects), batch_size):
        yield objects[i:i + batch_size]


def process_batch(indx_mini_batch):
    try:
        nlp = spacy.blank(""en"")
        db = DocBin()
        for text, annotations in indx_mini_batch[1]:
            if len(text) > 10 and ""entities"" in annotations.keys() and len(annotations[""entities""]) > 0:
                
                doc = nlp.make_doc(text)
                ents = []
                for start, end, label in annotations['entities']:
                    span = doc.char_span(start, end, label=label)
                    if span:
                        ents.append(span)           
                doc.ents = ents
                db.add(doc)
        db.to_disk(f""/root/train_spacy_500k_v2/{indx_mini_batch[0]}.spacy"")
        return True
    except Exception as e:
        raise ValueError(f""{e}"")

def run_all_batches(TRAIN_DATA):
    objects = TRAIN_DATA  # High volume of objects 
    batch_size = 100  # Define the batch size

    # Create batches
    batches = [x for x in  enumerate(list(batch_objects(objects, batch_size)))]
    with concurrent.futures.ProcessPoolExecutor(max_workers=10) as executor:
        results= executor.map(process_batch, batches)       
        for result in results:
            print(f""Result: {result}"")

# Run the main function
run_all_batches(TRAIN_DATA_SPL)

```",smamidik,58484438,open,False,0,2025-04-21T20:39:59+00:00,2025-04-21T20:58:52+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,3006334025,13802,Loading docbin from a previous saved transformer pipeline do not reconstruct `trf_data`,"## How to reproduce the behaviour
```
import spacy
from spacy.tokens import Doc, DocBin

def get_emb_size(d):
    return d._.trf_data.last_hidden_layer_state.data.shape
Doc.set_extension(""emb_size"", getter=get_emb_size, force=True)

def get_num_words(d):
    return len(d)
Doc.set_extension(""num_words"", getter=get_num_words, force=True)

# Loading pipeline to test extension
nlp = spacy.load(""en_core_web_trf"")
text = ""I like to eat pizza""
doc = nlp(text)
# All right, doc contains extension `emb_size` and `num_words`
print(""Num words:"", doc._.num_words)
print(""Emb size:"", doc._.emb_size)

# Saving do disk
docbin = DocBin(store_user_data=True, docs=[doc])
docbin.to_disk(""my_docbin.spacy"")
# Loading from disk
my_docbin = DocBin().from_disk(""my_docbin.spacy"")
docs_loaded = list(my_docbin.get_docs(nlp.vocab))
doc_loaded = docs_loaded[0]

print(""Num words (after loading):"", doc_loaded._.num_words)
print(""Emb size (after loading):"", doc_loaded._.emb_size)
```

The last instructions returns

```
Num words (after loading): 5


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[8], line 2
      1 print(""Num words (after loading):"", doc_loaded._.num_words)
----> 2 print(""Emb size (after loading):"", doc_loaded._.emb_size)

File ~/.pyenv/versions/3.12.9/envs/gah/lib/python3.12/site-packages/spacy/tokens/underscore.py:51, in Underscore.__getattr__(self, name)
     49 default, method, getter, setter = self._extensions[name]
     50 if getter is not None:
---> 51     return getter(self._obj)
     52 elif method is not None:
     53     method_partial = functools.partial(method, self._obj)

Cell In[2], line 2, in get_emb_size(d)
      1 def get_emb_size(d):
----> 2     return d._.trf_data.last_hidden_layer_state.data.shape

AttributeError: 'NoneType' object has no attribute 'last_hidden_layer_state'
```

Sint the loaded document `doc_loaded`, does not contain trf_data.



## Your Environment
## Info about spaCy

- **spaCy version:** 3.8.5
- **Platform:** Linux-6.14.2-arch1-1-x86_64-with-glibc2.41
- **Python version:** 3.12.9
- **Pipelines:** en_core_web_sm (3.8.0), en_core_web_md (3.8.0), en_core_web_lg (3.8.0), en_core_web_trf (3.8.0)

```
spacy                       3.8.5
spacy-alignments            0.9.1
spacy-curated-transformers  0.3.0
spacy-experimental          0.6.4
spacy-huggingface-pipelines 0.0.4
spacy-legacy                3.0.12
spacy-llm                   0.7.3
spacy-loggers               1.0.5
spacy-lookups-data          1.0.5
spacy-stanza                1.0.4
spacy-transformers          1.3.8
```

",igormorgado,2663394,closed,False,1,2025-04-19T09:49:45+00:00,2025-04-20T22:34:06+00:00,2025-04-20T20:56:23+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2994580749,13800,Add TeNs plugin,"<!--- Provide a general summary of your changes in the title. -->

## Description
Added the TeNs plugin to the spaCy's universe.

TeNs is a plugin that excels in handling real-world text by **identifying** not only standard date representations but also **abbreviated, informal, or even misspelled temporal expressions.**

### Types of change
The change represents an enhancement to the spaCy's universe.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [ ] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",iliedorobat,40547052,open,False,0,2025-04-15T00:05:10+00:00,2025-04-15T00:14:07+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2975261351,13791,Spacy produces random results during inference even when the random seed is fixed,"## How to reproduce the behaviour
Here is the minimal script:

```
import spacy
import truecase

text = truecase.get_true_case('ball,wanna get your train out ? I wanna play soccer ball . .')
print(text)

spacy.util.fix_random_seed(326)
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(text)

cleaned = [t.text if t.ent_type_ not in ['GPE', 'PERSON', 'ORG'] else t.ent_type_ for t in doc]
print(cleaned)
```
Run it multiple times: `for i in {1..5}; do python temp.py; done`

I get random results:

```
Ball, wanNA get your train out? I wanNA play Soccer ball . .
['PERSON', ',', 'PERSON', 'get', 'your', 'train', 'out', '?', 'I', 'wanNA', 'play', 'Soccer', 'ball', '.', '.']
Ball, wanNA get your train out? I wanNA play soccer ball . .
['PERSON', ',', 'PERSON', 'get', 'your', 'train', 'out', '?', 'I', 'PERSON', 'play', 'soccer', 'ball', '.', '.']
Ball, wanNA get your train out? I wanNA play Soccer ball . .
['PERSON', ',', 'PERSON', 'get', 'your', 'train', 'out', '?', 'I', 'wanNA', 'play', 'Soccer', 'ball', '.', '.']
Ball, wanNA get your train out? I wanNA play soccer ball . .
['PERSON', ',', 'PERSON', 'get', 'your', 'train', 'out', '?', 'I', 'PERSON', 'play', 'soccer', 'ball', '.', '.']
Ball, wanNA get your train out? I wanNA play soccer ball . .
['PERSON', ',', 'PERSON', 'get', 'your', 'train', 'out', '?', 'I', 'PERSON', 'play', 'soccer', 'ball', '.', '.']
```

## Your Environment
- **spaCy version:** 3.8.2
- **Platform:** Linux-4.15.0-159-generic-x86_64-with-glibc2.27
- **Python version:** 3.12.3
- **Pipelines:** en_core_web_sm (3.8.0), en_core_web_trf (3.8.0)

",jiangtianli91,43689484,open,False,1,2025-04-06T21:53:48+00:00,2025-04-16T10:01:36+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2974938515,13790,I can't open spaCy lessons,"I click on the lessons on the course page (https://course.spacy.io/en/) but nothing happens.
",Bla-bla-bla-bla-bla,84030501,open,False,0,2025-04-06T12:55:26+00:00,2025-04-06T12:55:26+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2973353444,13788,Support Cython 3,Need to support Cython 3 for Python 3.13,honnibal,8059750,open,False,0,2025-04-04T20:55:50+00:00,2025-04-05T10:09:26+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2973018933,13787,Fix: Update display import for compatibility with latest IPython Fixes: #13763,"<!--- Provide a general summary of your changes in the title. -->

**Fix: Update display import for compatibility with latest IPython**

---

## Description

This PR updates the import statement in `displacy.render()` to fix an `ImportError` caused by breaking changes in IPython 9+.

Replaced:

```python
from IPython.core.display import HTML, display
```

With:

```python
from IPython.display import HTML, display
```

The `display` function is no longer available in `IPython.core.display` in IPython 9 and above. This change ensures `displacy.render()` works correctly in Jupyter and notebook environments using newer versions of IPython.

**Tested in:**
- Python 3.12.9 (venv)
- IPython 9.0.2
- JupyterLab
- Confirmed `displacy.render()` now works without error.

---

### Types of change

- [x] Bug fix

---

## Checklist

- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.

---

Fixes: #13763 ",rnihesh,153965219,open,False,4,2025-04-04T17:39:45+00:00,2025-05-12T11:13:38+00:00,,bug;third-party;feat / visualizers,0,0,0,0,0,0,0
explosion/spaCy,2966670090,13785,Infixes Update Not Applying Properly to Tokenizer,"###  Infixes Update Not Applying Properly to Tokenizer  

#### **Description**  
I tried updating the **infix patterns** in **spaCy**, but the changes are not applying correctly to the tokenizer. Specifically, I'm trying to modify how apostrophes and other symbols ( `'`) are handled. However, even after setting a new regex, the tokenizer does not reflect these changes.  

#### **Steps to Reproduce**  
Here are the two approaches I tried:  

1️⃣ **Removing apostrophe-related rules from `infixes` and recompiling:**  
```python
default_infixes = [pattern for pattern in nlp.Defaults.infixes if ""'"" not in pattern]
infix_re = compile_infix_regex(default_infixes)
nlp.tokenizer.infix_finditer = infix_re.finditer  
```
**Issue:** Even after modifying the infix rules, contractions like `""can't""` still split incorrectly.  

2️⃣ **Manually adding new infix rules (including hyphens, plus signs, and dollar signs):**  
```python
infixes = nlp.Defaults.infixes + [r""'"",]  
infixe_regex = spacy.util.compile_infix_regex(infixes)  
nlp.tokenizer.infix_finditer = infixe_regex.finditer
```

#### **Expected Behavior**  
- The tokenizer should correctly apply the **new infix rules**.  

#### **Actual Behavior**  
- Changes to **`nlp.tokenizer.infix_finditer`** do not seem to take effect.  

#### **Question**  
Am I missing something in how infix rules should be updated? Is there a correct way to override **infix splitting**?  

Thanks for your help! ",Rayan-Allali,108684980,open,False,0,2025-04-02T14:58:15+00:00,2025-04-02T14:58:40+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2964636140,13784,Update spaCy Universe entry for ChatterBot to use the correct name casing,"## Description

This is a very small change to update the `title` attribute of the [spaCy Universe entry for ChatterBot](https://spacy.io/universe/project/Chatterbot) to use the correct casing for the name so that it matches the logo and casing of the [ChatterBot GitHub repository](https://github.com/gunthercox/chatterbot) and [documentation](https://docs.chatterbot.us/).

Current:

![image](https://github.com/user-attachments/assets/97d55ff0-05c5-448e-a33b-49dcfaf2a2bc)

Expected:

![image](https://github.com/user-attachments/assets/13278baf-0d37-4ee3-9190-e46ab50692fa)

### Types of change

Documentation update

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.

***

_P.S. The spaCy Universe collection is wonderful and I was quite happy and surprised when I recently stumbled upon the fact that ChatterBot had an entry there._",gunthercox,2030578,closed,False,1,2025-04-01T21:42:07+00:00,2025-05-12T05:47:53+00:00,2025-05-12T05:47:51+00:00,docs,0,0,0,0,0,0,0
explosion/spaCy,2963651068,13783,Speed regression on GPU inference after upgrading from spaCy 3.3.1 to 3.7.5,"Dear spaCy Team,

I recently upgraded from spaCy v3.3.1 to v3.7.5 and observed a significant inference slowdown on GPU when using the same model and input data.

Upon investigating, the issue appears related to the introduction of **spacy_curated_transformers** starting from v3.7.0. Specifically, the last_transformer_layer_listener_forward function in spacy_curated_transformers/models/listeners.py:382 takes approximately 1 second per call in v3.7.5, whereas the equivalent functionality in spaCy v3.3.1 (spacy_transformers) took only about 0.1 second.

This slowdown consistently occurs with transformer-based models such as en_core_web_trf, de_dep_news_trf, and zh_core_web_trf, while non-transformer models like xx_ent_wiki_sm remain unaffected.

Could you please confirm whether this slowdown is an unintended regression or if this performance difference is expected? Thank you in advance!

## Steps to Reproduce
1. Create two virtual environments:

- Environment A (spaCy v3.3.1): Install spaCy 3.3.1 with CUDA 11.8 support and its dependencies, download the “en_core_web_trf” model with python -m spacy download en_core_web_trf. Then install torch version torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 from https://download.pytorch.org/whl/torch_stable.html

- Environment B (spaCy v3.7.5): Install spaCy 3.7.5 and its dependencies, download the same “en_core_web_trf” model, then install the same torch version.

2. Run the following code snippet in both environments and compare the inference times:
````
import time
import spacy
import cProfile
import pstats

def load_model():
    s1 = time.time()
    nlp=spacy.load(""en_core_web_trf"")
    e1 = time.time()
    print(f""Load model took {(e1-s1)*1000:.2f} ms "")
    return nlp

def proc(nlp, texts):
    s = time.time()
    processed_docs = list(nlp.pipe(texts, batch_size=5))
    e = time.time()
    print(f""{(e-s)*1000:.2f} ms"")

text = """"""""""""
While some of spaCy's features work independently, others require trained pipelines to be loaded, which enable spaCy to predict linguistic annotations for example, whether a word is a verb or a noun. A trained pipeline can consist of multiple components that use a statistical model trained on labeled data. spaCy currently offers trained pipelines for a variety of languages, which can be installed as individual Python modules. Pipeline packages can differ in size, speed, memory usage, accuracy and the data they include. The package you choose always depends on your use case and the texts you're working with. For a general-purpose use case, the small, default packages are always a good start. They typically include the following components: Binary weights for the part-of-speech tagger, dependency parser and named entity recognizer to predict those annotations in context.
Lexical entries in the vocabulary, i.e. words and their context-independent attributes like the shape or spelling.
Data files like lemmatization rules and lookup tables.
Word vectors, i.e. multi-dimensional meaning representations of words that let you determine how similar they are to each other.
Configuration options, like the language and processing pipeline settings and model implementations to use, to put spaCy in the correct state when you load the pipeline.""""""
""""""

spacy.require_gpu(1)
nlp_gpu = load_model()
profiler = cProfile.Profile()
profiler.enable()
proc(nlp_gpu, [text])
profiler.disable()
stats = pstats.Stats(profiler).sort_stats('cumtime')
stats.print_stats(50)
````

## Your Environment
* Operating System: Ubuntu 20.04.4
* Python Version Used: 3.9.15
* GPU: NVIDIA GeForce RTX 3090, CUDA 11.8 
#### Environment A (spaCy v3.3.1) Packages:
 ```
Package                  Version
------------------------ ------------
blis                     0.7.11
catalogue                2.0.10
certifi                  2025.1.31
charset-normalizer       3.4.1
click                    8.1.8
cupy-cuda112             10.6.0
cymem                    2.0.11
en-core-web-trf          3.3.0
fastrlock                0.8.3
filelock                 3.18.0
fsspec                   2025.3.0
huggingface-hub          0.29.3
idna                     3.10
Jinja2                   3.1.6
langcodes                3.5.0
language_data            1.3.0
marisa-trie              1.2.1
MarkupSafe               3.0.2
mpmath                   1.3.0
murmurhash               1.0.12
networkx                 3.2.1
numpy                    1.24.4
nvidia-cublas-cu12       12.1.3.1
nvidia-cuda-cupti-cu12   12.1.105
nvidia-cuda-nvrtc-cu12   12.1.105
nvidia-cuda-runtime-cu12 12.1.105
nvidia-cudnn-cu12        8.9.2.26
nvidia-cufft-cu12        11.0.2.54
nvidia-curand-cu12       10.3.2.106
nvidia-cusolver-cu12     11.4.5.107
nvidia-cusparse-cu12     12.1.0.106
nvidia-nccl-cu12         2.18.1
nvidia-nvjitlink-cu12    12.8.93
nvidia-nvtx-cu12         12.1.105
packaging                24.2
pathlib_abc              0.1.1
pathy                    0.11.0
pillow                   11.1.0
pip                      25.0.1
preshed                  3.0.9
pydantic                 1.8.2
PyYAML                   6.0.2
regex                    2024.11.6
requests                 2.32.3
setuptools               78.1.0
smart-open               6.4.0
spacy                    3.3.1
spacy-alignments         0.9.1
spacy-legacy             3.0.12
spacy-loggers            1.0.5
spacy-transformers       1.1.7
srsly                    2.5.1
sympy                    1.13.3
thinc                    8.0.17
tokenizers               0.12.1
torch                      2.0.1+cu118
torchaudio                 2.0.2+cu118
torchvision                0.15.2+cu118
tqdm                     4.67.1
transformers             4.20.1
triton                   2.0.0
typer                    0.4.2
typing_extensions        4.5.0
typing-inspection          0.4.0
urllib3                  2.3.0
wasabi                   0.10.1
wheel                    0.45.1
```
#### Environment B (spaCy v3.7.5) packages:
```
Package                    Version
-------------------------- ------------
annotated-types            0.7.0
blis                       0.7.11
catalogue                  2.0.10
certifi                    2025.1.31
charset-normalizer         3.4.1
click                      8.1.8
cloudpathlib               0.21.0
confection                 0.1.5
cupy-cuda11x               13.4.1
curated-tokenizers         0.0.9
curated-transformers       0.1.1
cymem                      2.0.11
en-core-web-trf            3.7.3
fastrlock                  0.8.3
filelock                   3.13.1
fsspec                     2024.6.1
idna                       3.10
Jinja2                     3.1.6
langcodes                  3.5.0
language_data              1.3.0
marisa-trie                1.2.1
markdown-it-py             3.0.0
MarkupSafe                 3.0.2
mdurl                      0.1.2
mpmath                     1.3.0
murmurhash                 1.0.12
networkx                   3.2.1
numpy                      1.26.4
nvidia-cublas-cu11         11.11.3.6
nvidia-cuda-cupti-cu11     11.8.87
nvidia-cuda-nvrtc-cu11     11.8.89
nvidia-cuda-runtime-cu11   11.8.89
nvidia-cudnn-cu11          8.7.0.84
nvidia-cufft-cu11          10.9.0.58
nvidia-curand-cu11         10.3.0.86
nvidia-cusolver-cu11       11.4.1.48
nvidia-cusparse-cu11       11.7.5.86
nvidia-nccl-cu11           2.20.5
nvidia-nvtx-cu11           11.8.86
packaging                  24.2
pillow                     11.0.0
pip                        25.0.1
preshed                    3.0.9
pydantic                   2.11.1
pydantic_core              2.33.0
Pygments                   2.19.1
regex                      2024.11.6
requests                   2.32.3
rich                       14.0.0
setuptools                 78.1.0
shellingham                1.5.4
smart-open                 7.1.0
spacy                      3.7.5
spacy-curated-transformers 0.2.2
spacy-legacy               3.0.12
spacy-loggers              1.0.5
srsly                      2.5.1
sympy                      1.13.1
thinc                      8.2.5
torch                      2.0.1+cu118
torchaudio                 2.0.2+cu118
torchvision                0.15.2+cu118
tqdm                       4.67.1
triton                     2.0.0
typer                      0.15.2
typing_extensions          4.13.0
typing-inspection          0.4.0
urllib3                    2.3.0
wasabi                     1.1.3
weasel                     0.4.1
wheel                      0.45.1
wrapt                      1.17.2
```

## Observed Behavior:
   * Environment A (v3.3.1): approximately 2300 ms inference time
 
````
53596 function calls (52689 primitive calls) in 2.369 seconds

  Ordered by: cumulative time
  List reduced from 558 to 50 due to restriction <50>

  ncalls tottime percall cumtime percall filename:lineno(function)
    1  0.000  0.000  2.369  2.369 /data/debug-spacy-speed/debug.py:16(proc)
    2  0.000  0.000  2.369  1.185 /data/venv-spacy33/lib/python3.9/site-packages/spacy/language.py:1499(pipe)
   12/2  0.008  0.001  2.369  1.184 /data/venv-spacy33/lib/python3.9/site-packages/spacy/util.py:1603(_pipe)
   12/4  0.000  0.000  2.365  0.591 /data/venv-spacy33/lib/python3.9/site-packages/spacy/util.py:1549(minibatch)
   4/2  0.000  0.000  2.365  1.182 spacy/pipeline/pipe.pyx:41(pipe)
    2  0.000  0.000  2.299  1.150 spacy/pipeline/trainable_pipe.pyx:58(pipe)
    4  0.000  0.000  2.282  0.571 /data/venv-spacy33/lib/python3.9/site-packages/thinc/model.py:311(predict)
   23/7  0.000  0.000  2.163  0.309 /data/venv-spacy33/lib/python3.9/site-packages/thinc/model.py:288(__call__)
   6/3  0.000  0.000  2.076  0.692 /data/venv-spacy33/lib/python3.9/site-packages/thinc/layers/chain.py:48(forward)
    1  0.000  0.000  2.075  2.075 spacy/pipeline/tagger.pyx:129(predict)
    1  0.000  0.000  2.063  2.063 /data/venv-spacy33/lib/python3.9/site-packages/thinc/layers/with_array.py:28(forward)
    1  0.000  0.000  2.063  2.063 /data/venv-spacy33/lib/python3.9/site-packages/thinc/layers/with_array.py:68(_list_forward)
    1  0.000  0.000  2.061  2.061 /data/venv-spacy33/lib/python3.9/site-packages/thinc/layers/softmax.py:56(forward)
    1  0.000  0.000  2.061  2.061 /data/venv-spacy33/lib/python3.9/site-packages/thinc/backends/ops.py:220(affine)
    5  0.000  0.000  2.056  0.411 /data/venv-spacy33/lib/python3.9/site-packages/thinc/backends/cupy_ops.py:59(gemm)
    5  0.000  0.000  2.056  0.411 /data/venv-spacy33/lib/python3.9/site-packages/cupy/linalg/_product.py:45(dot)
    5  2.056  0.411  2.056  0.411 {method 'dot' of 'cupy._core.core.ndarray' objects}
    2  0.000  0.000  0.223  0.112 /data/venv-spacy33/lib/python3.9/site-packages/spacy_transformers/pipeline_component.py:196(pipe)
    1  0.000  0.000  0.205  0.205 /data/venv-spacy33/lib/python3.9/site-packages/spacy_transformers/pipeline_component.py:215(predict)
    1  0.000  0.000  0.203  0.203 /data/venv-spacy33/lib/python3.9/site-packages/spacy_transformers/layers/transformer_model.py:161(forward)
````

* Environment B (v3.7.5): approximately 4800 ms inference time on the same text
    
````
56357 function calls (55121 primitive calls) in 4.836 seconds

  Ordered by: cumulative time
  List reduced from 619 to 100 due to restriction <100>

  ncalls tottime percall cumtime percall filename:lineno(function)
    1  0.000  0.000  4.836  4.836 /data/debug-spacy-speed/debug.py:16(proc)
    2  0.000  0.000  4.836  2.418 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy/language.py:1534(pipe)
   12/2  0.004  0.000  4.836  2.418 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy/util.py:1693(_pipe)
   12/4  0.000  0.000  4.832  1.208 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy/util.py:1639(minibatch)
   4/2  0.000  0.000  4.832  2.416 spacy/pipeline/pipe.pyx:43(pipe)
    2  0.000  0.000  4.792  2.396 spacy/pipeline/trainable_pipe.pyx:58(pipe)
   7/4  0.000  0.000  4.760  1.190 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/model.py:330(predict)
   22/7  0.000  0.000  4.758  0.680 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/model.py:307(__call__)
    4  0.000  0.000  4.732  1.183 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/layers/chain.py:48(forward)
    1  0.000  0.000  2.591  2.591 spacy/pipeline/tagger.pyx:124(predict)
    3  0.000  0.000  2.589  0.863 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy_curated_transformers/models/listeners.py:382(last_transformer_layer_listener_forward)
    3  0.000  0.000  2.589  0.863 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy_curated_transformers/models/pooling.py:91(with_ragged_last_layer_forward)
    14  0.000  0.000  2.585  0.185 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/cupy/cuda/compiler.py:517(_compile_module_with_cache)
    14  0.001  0.000  2.585  0.185 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/cupy/cuda/compiler.py:548(_compile_with_cache_cuda)
    3  0.000  0.000  2.583  0.861 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/layers/reduce_mean.py:17(forward)
    3  0.000  0.000  2.583  0.861 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/backends/cupy_ops.py:286(reduce_mean)
    3  0.001  0.000  2.583  0.861 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/backends/_custom_kernels.py:435(reduce_mean)
    3  0.000  0.000  2.567  0.856 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/backends/_custom_kernels.py:96(__call__)
    3  0.000  0.000  2.567  0.856 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/backends/_custom_kernels.py:100(_compile_kernel)
    1  0.000  0.000  2.566  2.566 {method 'get_function' of 'cupy._core.raw.RawModule' objects}
    1  0.000  0.000  2.562  2.562 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/cupy/cuda/compiler.py:335(compile_using_nvrtc)
    1  0.000  0.000  2.560  2.560 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/cupy/cuda/compiler.py:338(_compile)
    1  0.000  0.000  2.560  2.560 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/cupy/cuda/compiler.py:732(compile)
    1  2.560  2.560  2.560  2.560 {built-in method cupy_backends.cuda.libs.nvrtc.compileProgram}
    2  0.000  0.000  2.200  1.100 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy_curated_transformers/pipeline/transformer.py:195(pipe)
    1  0.000  0.000  2.165  2.165 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy_curated_transformers/pipeline/transformer.py:214(predict)
    1  0.000  0.000  2.164  2.164 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy_curated_transformers/models/architectures.py:648(transformer_model_forward)
    1  0.000  0.000  2.164  2.164 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy_curated_transformers/models/with_non_ws_tokens.py:67(with_non_ws_tokens_forward)
    1  0.000  0.000  2.031  2.031 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/spacy_curated_transformers/models/with_strided_spans.py:91(with_strided_spans_forward)
    1  0.000  0.000  2.018  2.018 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/layers/pytorchwrapper.py:217(forward)
    1  0.000  0.000  2.014  2.014 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/shims/pytorch.py:93(__call__)
    1  0.000  0.000  2.014  2.014 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/thinc/shims/pytorch.py:107(predict)
  176/1  0.000  0.000  2.006  2.006 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/torch/nn/modules/module.py:1528(_wrapped_call_impl)
  176/1  0.001  0.000  2.006  2.006 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/torch/nn/modules/module.py:1534(_call_impl)
    1  0.000  0.000  2.005  2.005 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/curated_transformers/models/curated_transformer.py:27(forward)
    1  0.000  0.000  2.005  2.005 /data/venv-spacy37-cuda118/lib/python3.9/site-packages/curated_transformers/models/roberta/encoder.py:33(forward)
````
* While inference times might differ depending on hardware (CPU or GPU), spaCy v3.7.5 consistently shows around twice the inference time compared to spaCy v3.3.1 across different hardware setups tested.",mingzhu-wu,5382908,open,False,4,2025-04-01T14:33:07+00:00,2025-04-16T16:27:10+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2963059130,13782,Can't load model from config file: no [nlp] section found,"I have a config.cfg file very simple

[nlp]
lang = ""el""
pipeline = [""transformer"", ""ner""]
components = [""transformer"", ""ner""]

[components]

[components.transformer]
factory = ""transformer""
model = ""amichailidis/greek_legal_bert_v2-finetuned-ner""

[components.ner]
factory = ""ner""
source = ""transformer""

and when I try to load it 

config_path = ""/config.cfg""
 
print(f""Loading model 'amichailidis/greek_legal_bert_v2-finetuned-ner' via config: {config_path}"")

# Load the pipeline from the configuration file
# This will download the model from Hugging Face Hub if not cached
nlp = spacy.util.load_model_from_config(config_path)

print(""Model loaded successfully."")

I receive this error:

File ""/home/luis/projects/spacy/spacy-greek.py"", line 13, in <module>
    nlp = spacy.util.load_model_from_config(config_path)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: [E985] Can't load model from config file: no [nlp] section found.

/config.cfg

but as you can see the config file has a nlp section
",micuentadecasa,912298,open,False,0,2025-04-01T10:49:12+00:00,2025-04-01T10:49:12+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2957958733,13781,fix cannot import name 'display' from 'IPython.core.display',,wildfluss,120495783,closed,False,2,2025-03-29T11:38:05+00:00,2025-05-12T05:51:20+00:00,2025-05-12T05:51:20+00:00,feat / visualizers,0,0,0,0,0,0,0
explosion/spaCy,2929022943,13773,Unable to use picamera2 after downloading Spacy on raspberry pi,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour


Create a test.py as follow to use picamera2 : 

```
from picamera2 import Picamera2, Preview
import time

picam2 = Picamera2()
picam2.start_preview(Preview.QTGL)
camera_config = picam2.create_preview_configuration()
picam2.configure(camera_config)

picam2.start()
time.sleep(5)
picam2.close() #Remove the sleep and close if you want to close it manually with ctrl+C.
```

Once it's done, run it : it works ! 
Download spacy and it crashs. Here's the log : 
```
(Spotty) admin@raspberrypi:~/Desktop $ python test.py
[2:24:41.338237926] [33775]  INFO Camera camera_manager.cpp:327 libcamera v0.4.0+53-29156679
[2:24:41.346274570] [33781]  INFO RPI pisp.cpp:720 libpisp version v1.1.0 e7974a156008 27-01-2025 (21:50:51)
[2:24:41.355908746] [33781]  INFO RPI pisp.cpp:1179 Registered camera /base/axi/pcie@120000/rp1/i2c@88000/imx708@1a to CFE device /dev/media2 and ISP device /dev/media0 using PiSP variant BCM2712_D0
[2:24:41.749562537] [33775]  INFO Camera camera.cpp:1202 configuring streams: (0) 640x480-XBGR8888 (1) 1536x864-BGGR_PISP_COMP1
[2:24:41.749724235] [33781]  INFO RPI pisp.cpp:1484 Sensor: /base/axi/pcie@120000/rp1/i2c@88000/imx708@1a - Selected sensor format: 1536x864-SBGGR10_1X10 - Selected CFE format: 1536x864-PC1B
(Spotty) admin@raspberrypi:~/Desktop $ python Spacy.py
Traceback (most recent call last):
  File ""/home/admin/Desktop/Spacy.py"", line 1, in <module>
    import spacy
ModuleNotFoundError: No module named 'spacy'
(Spotty) admin@raspberrypi:~/Desktop $ uname -m
aarch64
(Spotty) admin@raspberrypi:~/Desktop $ conda install -c conda-forge spacy
Collecting package metadata (current_repodata.json): done
Solving environment: done


==> WARNING: A newer version of conda exists. <==
  current version: 23.3.1
  latest version: 25.1.1

Please update conda by running

    $ conda update -n base -c defaults conda

Or to minimize the number of packages updated during conda update use

     conda install conda=25.1.1



## Package Plan ##

  environment location: /home/admin/anaconda3/envs/Spotty

  added / updated specs:
    - spacy


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    _openmp_mutex-4.5          |            2_gnu          23 KB  conda-forge
    annotated-types-0.7.0      |     pyhd8ed1ab_1          18 KB  conda-forge
    brotli-python-1.1.0        |  py311h89d996e_2         349 KB  conda-forge
    bzip2-1.0.8                |       h68df207_7         185 KB  conda-forge
    ca-certificates-2025.1.31  |       hcefe29a_0         155 KB  conda-forge
    catalogue-2.0.10           |  py311hec3470c_1          43 KB  conda-forge
    certifi-2025.1.31          |     pyhd8ed1ab_0         159 KB  conda-forge
    cffi-1.17.1                |  py311h14e8bb7_0         312 KB  conda-forge
    charset-normalizer-3.4.1   |     pyhd8ed1ab_0          46 KB  conda-forge
    click-8.1.8                |     pyh707e725_0          83 KB  conda-forge
    cloudpathlib-0.20.0        |     pyhd8ed1ab_0          43 KB  conda-forge
    cloudpickle-3.1.1          |     pyhd8ed1ab_0          25 KB  conda-forge
    colorama-0.4.6             |     pyhd8ed1ab_1          26 KB  conda-forge
    confection-0.1.5           |     pyhecae5ae_0          37 KB  conda-forge
    cymem-2.0.11               |  py311h89d996e_0          50 KB  conda-forge
    cython-blis-1.0.1          |  py311hec9beba_0         1.3 MB  conda-forge
    h2-4.2.0                   |     pyhd8ed1ab_0          53 KB  conda-forge
    hpack-4.1.0                |     pyhd8ed1ab_0          30 KB  conda-forge
    hyperframe-6.1.0           |     pyhd8ed1ab_0          17 KB  conda-forge
    idna-3.10                  |     pyhd8ed1ab_1          49 KB  conda-forge
    jinja2-3.1.6               |     pyhd8ed1ab_0         110 KB  conda-forge
    langcodes-3.3.0            |     pyhd3eb1b0_0         151 KB
    ld_impl_linux-aarch64-2.43 |       h80caac9_4         683 KB  conda-forge
    libblas-3.9.0              |31_h1a9f1db_openblas          17 KB  conda-forge
    libcblas-3.9.0             |31_hab92f65_openblas          16 KB  conda-forge
    libexpat-2.6.4             |       h5ad3122_0          71 KB  conda-forge
    libffi-3.4.6               |       he21f813_0          50 KB  conda-forge
    libgcc-14.2.0              |       he277a41_2         523 KB  conda-forge
    libgcc-ng-14.2.0           |       he9431aa_2          52 KB  conda-forge
    libgfortran-14.2.0         |       he9431aa_2          52 KB  conda-forge
    libgfortran5-14.2.0        |       hb6113d0_2         1.0 MB  conda-forge
    libgomp-14.2.0             |       he277a41_2         452 KB  conda-forge
    liblapack-3.9.0            |31_h411afd4_openblas          16 KB  conda-forge
    liblzma-5.6.4              |       h86ecc28_0         121 KB  conda-forge
    libnsl-2.0.1               |       h31becfc_0          34 KB  conda-forge
    libopenblas-0.3.29         |pthreads_h9d3fd7e_0         4.6 MB  conda-forge
    libsqlite-3.49.1           |       h5eb1b54_2         895 KB  conda-forge
    libstdcxx-14.2.0           |       h3f4de04_2         3.6 MB  conda-forge
    libuuid-2.38.1             |       hb4cce97_0          35 KB  conda-forge
    libxcrypt-4.4.36           |       h31becfc_1         112 KB  conda-forge
    libzlib-1.3.1              |       h86ecc28_2          65 KB  conda-forge
    markdown-it-py-3.0.0       |     pyhd8ed1ab_1          63 KB  conda-forge
    markupsafe-3.0.2           |  py311ha09ea12_1          25 KB  conda-forge
    mdurl-0.1.2                |     pyhd8ed1ab_1          14 KB  conda-forge
    murmurhash-1.0.10          |  py311h89d996e_2          36 KB  conda-forge
    ncurses-6.5                |       ha32ae93_3         904 KB  conda-forge
    numpy-2.2.4                |  py311h6c2b7b4_0         7.5 MB  conda-forge
    openssl-3.4.1              |       hd08dc88_0         3.3 MB  conda-forge
    packaging-24.2             |     pyhd8ed1ab_2          59 KB  conda-forge
    pip-25.0.1                 |     pyh8b19718_0         1.2 MB  conda-forge
    preshed-3.0.9              |  py311h89d996e_2         131 KB  conda-forge
    pycparser-2.22             |     pyh29332c3_1         108 KB  conda-forge
    pydantic-2.0.3             |     pyhd8ed1ab_1         245 KB  conda-forge
    pydantic-core-2.3.0        |  py311h2bdde80_0         1.3 MB  conda-forge
    pygments-2.19.1            |     pyhd8ed1ab_0         868 KB  conda-forge
    pysocks-1.7.1              |     pyha55dd90_7          21 KB  conda-forge
    python-3.11.11             |h1683364_2_cpython        14.5 MB  conda-forge
    python_abi-3.11            |          5_cp311           6 KB  conda-forge
    readline-8.2               |       h8382b9d_2         285 KB  conda-forge
    requests-2.32.3            |     pyhd8ed1ab_1          57 KB  conda-forge
    rich-13.9.4                |     pyhd8ed1ab_1         181 KB  conda-forge
    setuptools-75.8.2          |     pyhff2d567_0         760 KB  conda-forge
    shellingham-1.5.4          |     pyhd8ed1ab_1          14 KB  conda-forge
    smart-open-7.1.0           |       hd8ed1ab_0           7 KB  conda-forge
    smart_open-7.1.0           |     pyhd8ed1ab_0          51 KB  conda-forge
    spacy-3.8.2                |  py311hb9acf69_0         5.4 MB  conda-forge
    spacy-legacy-3.0.12        |     pyhd8ed1ab_0          28 KB  conda-forge
    spacy-loggers-1.0.5        |     pyhd8ed1ab_0          21 KB  conda-forge
    srsly-2.5.1                |  py311h89d996e_1         599 KB  conda-forge
    thinc-8.3.2                |  py311hb9acf69_1         959 KB  conda-forge
    tk-8.6.13                  |       h194ca79_0         3.2 MB  conda-forge
    tqdm-4.67.1                |     pyhd8ed1ab_1          87 KB  conda-forge
    typer-0.15.2               |     pyhff008b6_0          74 KB  conda-forge
    typer-slim-0.15.2          |     pyh29332c3_0          45 KB  conda-forge
    typer-slim-standard-0.15.2 |       h801b22e_0           5 KB  conda-forge
    typing-extensions-4.12.2   |       hd8ed1ab_1          10 KB  conda-forge
    typing_extensions-4.12.2   |     pyha770c72_1          39 KB  conda-forge
    tzdata-2025a               |       h78e105d_0         120 KB  conda-forge
    ujson-5.10.0               |  py311h89d996e_1          51 KB  conda-forge
    urllib3-2.3.0              |     pyhd8ed1ab_0          98 KB  conda-forge
    wasabi-1.1.3               |     pyhd8ed1ab_1          28 KB  conda-forge
    weasel-0.4.1               |     pyhd8ed1ab_2          42 KB  conda-forge
    wheel-0.45.1               |     pyhd8ed1ab_1          61 KB  conda-forge
    wrapt-1.17.2               |  py311ha879c10_0          64 KB  conda-forge
    zstandard-0.23.0           |  py311ha879c10_1         681 KB  conda-forge
    ------------------------------------------------------------
                                           Total:        58.8 MB

The following NEW packages will be INSTALLED:

  _openmp_mutex      conda-forge/linux-aarch64::_openmp_mutex-4.5-2_gnu 
  annotated-types    conda-forge/noarch::annotated-types-0.7.0-pyhd8ed1ab_1 
  brotli-python      conda-forge/linux-aarch64::brotli-python-1.1.0-py311h89d996e_2 
  bzip2              conda-forge/linux-aarch64::bzip2-1.0.8-h68df207_7 
  ca-certificates    conda-forge/linux-aarch64::ca-certificates-2025.1.31-hcefe29a_0 
  catalogue          conda-forge/linux-aarch64::catalogue-2.0.10-py311hec3470c_1 
  certifi            conda-forge/noarch::certifi-2025.1.31-pyhd8ed1ab_0 
  cffi               conda-forge/linux-aarch64::cffi-1.17.1-py311h14e8bb7_0 
  charset-normalizer conda-forge/noarch::charset-normalizer-3.4.1-pyhd8ed1ab_0 
  click              conda-forge/noarch::click-8.1.8-pyh707e725_0 
  cloudpathlib       conda-forge/noarch::cloudpathlib-0.20.0-pyhd8ed1ab_0 
  cloudpickle        conda-forge/noarch::cloudpickle-3.1.1-pyhd8ed1ab_0 
  colorama           conda-forge/noarch::colorama-0.4.6-pyhd8ed1ab_1 
  confection         conda-forge/noarch::confection-0.1.5-pyhecae5ae_0 
  cymem              conda-forge/linux-aarch64::cymem-2.0.11-py311h89d996e_0 
  cython-blis        conda-forge/linux-aarch64::cython-blis-1.0.1-py311hec9beba_0 
  h2                 conda-forge/noarch::h2-4.2.0-pyhd8ed1ab_0 
  hpack              conda-forge/noarch::hpack-4.1.0-pyhd8ed1ab_0 
  hyperframe         conda-forge/noarch::hyperframe-6.1.0-pyhd8ed1ab_0 
  idna               conda-forge/noarch::idna-3.10-pyhd8ed1ab_1 
  jinja2             conda-forge/noarch::jinja2-3.1.6-pyhd8ed1ab_0 
  langcodes          pkgs/main/noarch::langcodes-3.3.0-pyhd3eb1b0_0 
  ld_impl_linux-aar~ conda-forge/linux-aarch64::ld_impl_linux-aarch64-2.43-h80caac9_4 
  libblas            conda-forge/linux-aarch64::libblas-3.9.0-31_h1a9f1db_openblas 
  libcblas           conda-forge/linux-aarch64::libcblas-3.9.0-31_hab92f65_openblas 
  libexpat           conda-forge/linux-aarch64::libexpat-2.6.4-h5ad3122_0 
  libffi             conda-forge/linux-aarch64::libffi-3.4.6-he21f813_0 
  libgcc             conda-forge/linux-aarch64::libgcc-14.2.0-he277a41_2 
  libgcc-ng          conda-forge/linux-aarch64::libgcc-ng-14.2.0-he9431aa_2 
  libgfortran        conda-forge/linux-aarch64::libgfortran-14.2.0-he9431aa_2 
  libgfortran5       conda-forge/linux-aarch64::libgfortran5-14.2.0-hb6113d0_2 
  libgomp            conda-forge/linux-aarch64::libgomp-14.2.0-he277a41_2 
  liblapack          conda-forge/linux-aarch64::liblapack-3.9.0-31_h411afd4_openblas 
  liblzma            conda-forge/linux-aarch64::liblzma-5.6.4-h86ecc28_0 
  libnsl             conda-forge/linux-aarch64::libnsl-2.0.1-h31becfc_0 
  libopenblas        conda-forge/linux-aarch64::libopenblas-0.3.29-pthreads_h9d3fd7e_0 
  libsqlite          conda-forge/linux-aarch64::libsqlite-3.49.1-h5eb1b54_2 
  libstdcxx          conda-forge/linux-aarch64::libstdcxx-14.2.0-h3f4de04_2 
  libuuid            conda-forge/linux-aarch64::libuuid-2.38.1-hb4cce97_0 
  libxcrypt          conda-forge/linux-aarch64::libxcrypt-4.4.36-h31becfc_1 
  libzlib            conda-forge/linux-aarch64::libzlib-1.3.1-h86ecc28_2 
  markdown-it-py     conda-forge/noarch::markdown-it-py-3.0.0-pyhd8ed1ab_1 
  markupsafe         conda-forge/linux-aarch64::markupsafe-3.0.2-py311ha09ea12_1 
  mdurl              conda-forge/noarch::mdurl-0.1.2-pyhd8ed1ab_1 
  murmurhash         conda-forge/linux-aarch64::murmurhash-1.0.10-py311h89d996e_2 
  ncurses            conda-forge/linux-aarch64::ncurses-6.5-ha32ae93_3 
  numpy              conda-forge/linux-aarch64::numpy-2.2.4-py311h6c2b7b4_0 
  openssl            conda-forge/linux-aarch64::openssl-3.4.1-hd08dc88_0 
  packaging          conda-forge/noarch::packaging-24.2-pyhd8ed1ab_2 
  pip                conda-forge/noarch::pip-25.0.1-pyh8b19718_0 
  preshed            conda-forge/linux-aarch64::preshed-3.0.9-py311h89d996e_2 
  pycparser          conda-forge/noarch::pycparser-2.22-pyh29332c3_1 
  pydantic           conda-forge/noarch::pydantic-2.0.3-pyhd8ed1ab_1 
  pydantic-core      conda-forge/linux-aarch64::pydantic-core-2.3.0-py311h2bdde80_0 
  pygments           conda-forge/noarch::pygments-2.19.1-pyhd8ed1ab_0 
  pysocks            conda-forge/noarch::pysocks-1.7.1-pyha55dd90_7 
  python             conda-forge/linux-aarch64::python-3.11.11-h1683364_2_cpython 
  python_abi         conda-forge/linux-aarch64::python_abi-3.11-5_cp311 
  readline           conda-forge/linux-aarch64::readline-8.2-h8382b9d_2 
  requests           conda-forge/noarch::requests-2.32.3-pyhd8ed1ab_1 
  rich               conda-forge/noarch::rich-13.9.4-pyhd8ed1ab_1 
  setuptools         conda-forge/noarch::setuptools-75.8.2-pyhff2d567_0 
  shellingham        conda-forge/noarch::shellingham-1.5.4-pyhd8ed1ab_1 
  smart-open         conda-forge/noarch::smart-open-7.1.0-hd8ed1ab_0 
  smart_open         conda-forge/noarch::smart_open-7.1.0-pyhd8ed1ab_0 
  spacy              conda-forge/linux-aarch64::spacy-3.8.2-py311hb9acf69_0 
  spacy-legacy       conda-forge/noarch::spacy-legacy-3.0.12-pyhd8ed1ab_0 
  spacy-loggers      conda-forge/noarch::spacy-loggers-1.0.5-pyhd8ed1ab_0 
  srsly              conda-forge/linux-aarch64::srsly-2.5.1-py311h89d996e_1 
  thinc              conda-forge/linux-aarch64::thinc-8.3.2-py311hb9acf69_1 
  tk                 conda-forge/linux-aarch64::tk-8.6.13-h194ca79_0 
  tqdm               conda-forge/noarch::tqdm-4.67.1-pyhd8ed1ab_1 
  typer              conda-forge/noarch::typer-0.15.2-pyhff008b6_0 
  typer-slim         conda-forge/noarch::typer-slim-0.15.2-pyh29332c3_0 
  typer-slim-standa~ conda-forge/noarch::typer-slim-standard-0.15.2-h801b22e_0 
  typing-extensions  conda-forge/noarch::typing-extensions-4.12.2-hd8ed1ab_1 
  typing_extensions  conda-forge/noarch::typing_extensions-4.12.2-pyha770c72_1 
  tzdata             conda-forge/noarch::tzdata-2025a-h78e105d_0 
  ujson              conda-forge/linux-aarch64::ujson-5.10.0-py311h89d996e_1 
  urllib3            conda-forge/noarch::urllib3-2.3.0-pyhd8ed1ab_0 
  wasabi             conda-forge/noarch::wasabi-1.1.3-pyhd8ed1ab_1 
  weasel             conda-forge/noarch::weasel-0.4.1-pyhd8ed1ab_2 
  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 
  wrapt              conda-forge/linux-aarch64::wrapt-1.17.2-py311ha879c10_0 
  zstandard          conda-forge/linux-aarch64::zstandard-0.23.0-py311ha879c10_1 


Proceed ([y]/n)? y


Downloading and Extracting Packages
                                                                                
Preparing transaction: done                                                     
Verifying transaction: done                                                     
Executing transaction: done                                                     
(Spotty) admin@raspberrypi:~/Desktop $ python -m spacy download fr_core_news_md
Looking in indexes: https://pypi.org/simple, https://www.piwheels.org/simple    
Collecting fr-core-news-md==3.8.0                                               
  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)                
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.8/45.8 MB 8.8 MB/s eta 0:00:00
Installing collected packages: fr-core-news-md                                  
Successfully installed fr-core-news-md-3.8.0                                    
✔ Download and installation successful                                          
You can now load the package via spacy.load('fr_core_news_md')                  
(Spotty) admin@raspberrypi:~/Desktop $ python Spacy.py
Traceback (most recent call last):                                              
  File ""/home/admin/Desktop/Spacy.py"", line 4, in <module>                      
    from picamera2 import Picamera2                                             
ModuleNotFoundError: No module named 'picamera2'                                
(Spotty) admin@raspberrypi:~/Desktop $ ^C                                       
(Spotty) admin@raspberrypi:~/Desktop $ python test.py                           
Traceback (most recent call last):                                              
  File ""/home/admin/Desktop/test.py"", line 1, in <module>                       
    from picamera2 import Picamera2, Preview
ModuleNotFoundError: No module named 'picamera2'
```


## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Linux-6.6.52
* Python Version Used: python-3.11.11
* spaCy Version Used: 3.8.2
* Environment Information: downloaded picamera2 and spacy. Working on conda. 
",Rob-Nafous,173673960,open,False,0,2025-03-18T15:56:58+00:00,2025-03-18T15:56:58+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2928391779,13772,"In requirements.txt thinc>=8.3.4,<8.4.0,which was not found so I changed it to  thinc>=8.3.4,<8.4.0 but it is giving error that failed building wheel for thinc","
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
(dlenv) [manshika@lappy spaCy]$ pip install -r requirements.txt
Collecting spacy-legacy<3.1.0,>=3.0.11 (from -r requirements.txt (line 2))
  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)
Collecting spacy-loggers<2.0.0,>=1.0.0 (from -r requirements.txt (line 3))
  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)
Collecting cymem<2.1.0,>=2.0.2 (from -r requirements.txt (line 4))
  Using cached cymem-2.0.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)
Collecting preshed<3.1.0,>=3.0.2 (from -r requirements.txt (line 5))
  Using cached preshed-3.0.9-cp313-cp313-linux_x86_64.whl
ERROR: Ignored the following yanked versions: 6.10.4.dev0, 7.4.4
ERROR: Could not find a version that satisfies the requirement thinc<8.4.0,>=8.3.4 (from versions: 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.41, 1.42, 1.60, 1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67, 1.68, 1.69, 1.70, 1.71, 1.72, 1.73, 1.74, 1.75, 1.76, 2.0, 3.0, 3.1, 3.2, 3.3, 3.4.1, 4.0.0, 4.1.0, 4.2.0, 5.0.0, 5.0.1, 5.0.2, 5.0.3, 5.0.4, 5.0.5, 5.0.6, 5.0.7, 5.0.8, 6.0.0, 6.1.0, 6.1.1, 6.1.2, 6.1.3, 6.2.0, 6.3.0, 6.4.0, 6.5.0, 6.5.2, 6.6.0, 6.7.0, 6.7.1, 6.7.2, 6.7.3, 6.8.0, 6.8.1, 6.8.2, 6.9.0, 6.10.0, 6.10.1.dev0, 6.10.1, 6.10.2.dev0, 6.10.2.dev1, 6.10.2, 6.10.3.dev0, 6.10.3.dev1, 6.10.3, 6.11.0.dev2, 6.11.1.dev0, 6.11.1.dev1, 6.11.1.dev2, 6.11.1.dev3, 6.11.1.dev4, 6.11.1.dev6, 6.11.1.dev7, 6.11.1.dev10, 6.11.1.dev11, 6.11.1.dev12, 6.11.1.dev13, 6.11.1.dev15, 6.11.1.dev16, 6.11.1.dev17, 6.11.1.dev18, 6.11.1.dev19, 6.11.1.dev20, 6.11.1, 6.11.2.dev0, 6.11.2, 6.11.3.dev1, 6.11.3.dev2, 6.12.0, 6.12.1, 7.0.0.dev0, 7.0.0.dev1, 7.0.0.dev2, 7.0.0.dev3, 7.0.0.dev4, 7.0.0.dev5, 7.0.0.dev6, 7.0.0.dev8, 7.0.0, 7.0.1.dev0, 7.0.1.dev1, 7.0.1.dev2, 7.0.1, 7.0.2, 7.0.3, 7.0.4.dev0, 7.0.4, 7.0.5.dev0, 7.0.5, 7.0.6, 7.0.7, 7.0.8, 7.1.0.dev0, 7.1.0, 7.1.1, 7.2.0.dev3, 7.2.0, 7.3.0.dev0, 7.3.0, 7.3.1, 7.4.0.dev0, 7.4.0.dev1, 7.4.0.dev2, 7.4.0, 7.4.1, 7.4.2, 7.4.3, 7.4.5, 7.4.6, 8.0.0.dev0, 8.0.0.dev2, 8.0.0.dev4, 8.0.0a0, 8.0.0a1, 8.0.0a2, 8.0.0a3, 8.0.0a6, 8.0.0a8, 8.0.0a9, 8.0.0a11, 8.0.0a12, 8.0.0a13, 8.0.0a14, 8.0.0a16, 8.0.0a17, 8.0.0a18, 8.0.0a19, 8.0.0a20, 8.0.0a21, 8.0.0a22, 8.0.0a23, 8.0.0a24, 8.0.0a25, 8.0.0a26, 8.0.0a27, 8.0.0a28, 8.0.0a29, 8.0.0a30, 8.0.0a31, 8.0.0a32, 8.0.0a33, 8.0.0a34, 8.0.0a35, 8.0.0a36, 8.0.0a40, 8.0.0a41, 8.0.0a42, 8.0.0a43, 8.0.0a44, 8.0.0rc0, 8.0.0rc1, 8.0.0rc2, 8.0.0rc3, 8.0.0rc4, 8.0.0rc5, 8.0.0rc6.dev0, 8.0.0rc6, 8.0.0, 8.0.1, 8.0.2, 8.0.3, 8.0.4, 8.0.5, 8.0.6, 8.0.7, 8.0.8, 8.0.9, 8.0.10, 8.0.11, 8.0.12, 8.0.13, 8.0.14.dev0, 8.0.14, 8.0.15, 8.0.16, 8.0.17, 8.1.0.dev0, 8.1.0.dev1, 8.1.0.dev2, 8.1.0.dev3, 8.1.0, 8.1.1, 8.1.2, 8.1.3, 8.1.4, 8.1.5, 8.1.6, 8.1.7, 8.1.8, 8.1.9, 8.1.10, 8.1.11, 8.1.12, 8.2.0, 8.2.1, 8.2.2, 8.2.3, 8.2.4, 8.2.5, 8.3.0, 8.3.1, 8.3.2, 9.0.0.dev0, 9.0.0.dev1, 9.0.0.dev2, 9.0.0.dev3, 9.0.0.dev4, 9.0.0.dev5, 9.0.0, 9.1.0, 9.1.1)
ERROR: No matching distribution found for thinc<8.4.0,>=8.3.4
(dlenv) [manshika@lappy spaCy]$ pip install -r requirements.txt
Collecting spacy-legacy<3.1.0,>=3.0.11 (from -r requirements.txt (line 2))
  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)
Collecting spacy-loggers<2.0.0,>=1.0.0 (from -r requirements.txt (line 3))
  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)
Collecting cymem<2.1.0,>=2.0.2 (from -r requirements.txt (line 4))
  Using cached cymem-2.0.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)
Collecting preshed<3.1.0,>=3.0.2 (from -r requirements.txt (line 5))
  Using cached preshed-3.0.9-cp313-cp313-linux_x86_64.whl
Collecting thinc<8.4.0,>=8.3.0 (from -r requirements.txt (line 6))
  Using cached thinc-8.3.2.tar.gz (193 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Preparing metadata (pyproject.toml) ... done
Collecting ml_datasets<0.3.0,>=0.2.0 (from -r requirements.txt (line 7))
  Using cached ml_datasets-0.2.0-py3-none-any.whl.metadata (7.5 kB)
Collecting murmurhash<1.1.0,>=0.28.0 (from -r requirements.txt (line 8))
  Using cached murmurhash-1.0.12-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting wasabi<1.2.0,>=0.9.1 (from -r requirements.txt (line 9))
  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)
Collecting srsly<3.0.0,>=2.4.3 (from -r requirements.txt (line 10))
  Using cached srsly-2.5.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)
Collecting catalogue<2.1.0,>=2.0.6 (from -r requirements.txt (line 11))
  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)
Collecting typer<1.0.0,>=0.3.0 (from -r requirements.txt (line 12))
  Using cached typer-0.15.2-py3-none-any.whl.metadata (15 kB)
Collecting weasel<0.5.0,>=0.1.0 (from -r requirements.txt (line 13))
  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)
Requirement already satisfied: numpy<3.0.0,>=2.0.0 in /home/manshika/.virtualenvs/dlenv/lib/python3.13/site-packages (from -r requirements.txt (line 15)) (2.2.4)
Collecting requests<3.0.0,>=2.13.0 (from -r requirements.txt (line 16))
  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting tqdm<5.0.0,>=4.38.0 (from -r requirements.txt (line 17))
  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)
Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from -r requirements.txt (line 18))
  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)
Collecting jinja2 (from -r requirements.txt (line 19))
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting langcodes<4.0.0,>=3.2.0 (from -r requirements.txt (line 20))
  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)
Requirement already satisfied: setuptools in /home/manshika/.virtualenvs/dlenv/lib/python3.13/site-packages (from -r requirements.txt (line 22)) (76.1.0)
Collecting packaging>=20.0 (from -r requirements.txt (line 23))
  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)
Collecting pre-commit>=2.13.0 (from -r requirements.txt (line 25))
  Using cached pre_commit-4.1.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting cython<3.0,>=0.25 (from -r requirements.txt (line 26))
  Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
Collecting pytest!=7.1.0,>=5.2.0 (from -r requirements.txt (line 27))
  Using cached pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)
Collecting pytest-timeout<2.0.0,>=1.3.0 (from -r requirements.txt (line 28))
  Using cached pytest_timeout-1.4.2-py2.py3-none-any.whl.metadata (11 kB)
Collecting mock<3.0.0,>=2.0.0 (from -r requirements.txt (line 29))
  Using cached mock-2.0.0-py2.py3-none-any.whl.metadata (3.2 kB)
Collecting flake8<6.0.0,>=3.8.0 (from -r requirements.txt (line 30))
  Using cached flake8-5.0.4-py2.py3-none-any.whl.metadata (4.1 kB)
Collecting hypothesis<7.0.0,>=3.27.0 (from -r requirements.txt (line 31))
  Using cached hypothesis-6.129.4-py3-none-any.whl.metadata (4.4 kB)
Collecting mypy<1.6.0,>=1.5.0 (from -r requirements.txt (line 32))
  Using cached mypy-1.5.1-py3-none-any.whl.metadata (1.7 kB)
Collecting types-mock>=0.1.1 (from -r requirements.txt (line 33))
  Using cached types_mock-5.2.0.20250306-py3-none-any.whl.metadata (2.0 kB)
Collecting types-setuptools>=57.0.0 (from -r requirements.txt (line 34))
  Using cached types_setuptools-76.0.0.20250313-py3-none-any.whl.metadata (2.2 kB)
Collecting types-requests (from -r requirements.txt (line 35))
  Using cached types_requests-2.32.0.20250306-py3-none-any.whl.metadata (2.3 kB)
Collecting black==22.3.0 (from -r requirements.txt (line 37))
  Using cached black-22.3.0-py3-none-any.whl.metadata (45 kB)
Collecting cython-lint>=0.15.0 (from -r requirements.txt (line 38))
  Using cached cython_lint-0.16.6-py3-none-any.whl.metadata (4.9 kB)
Collecting isort<6.0,>=5.0 (from -r requirements.txt (line 39))
  Using cached isort-5.13.2-py3-none-any.whl.metadata (12 kB)
Collecting click>=8.0.0 (from black==22.3.0->-r requirements.txt (line 37))
  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)
Collecting platformdirs>=2 (from black==22.3.0->-r requirements.txt (line 37))
  Using cached platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)
Collecting pathspec>=0.9.0 (from black==22.3.0->-r requirements.txt (line 37))
  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
Collecting mypy-extensions>=0.4.3 (from black==22.3.0->-r requirements.txt (line 37))
  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)
Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->-r requirements.txt (line 6))
  Using cached blis-1.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)
Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->-r requirements.txt (line 6))
  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)
Collecting numpy<3.0.0,>=2.0.0 (from -r requirements.txt (line 15))
  Using cached numpy-2.0.2-cp313-cp313-linux_x86_64.whl
Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/manshika/.virtualenvs/dlenv/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->-r requirements.txt (line 12)) (4.12.2)
Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->-r requirements.txt (line 12))
  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)
Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->-r requirements.txt (line 12))
  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)
Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->-r requirements.txt (line 13))
  Using cached cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)
Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->-r requirements.txt (line 13))
  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)
Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->-r requirements.txt (line 16))
  Using cached charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)
Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->-r requirements.txt (line 16))
  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)
Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->-r requirements.txt (line 16))
  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)
Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->-r requirements.txt (line 16))
  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)
Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->-r requirements.txt (line 18))
  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)
Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->-r requirements.txt (line 18))
  Using cached pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2->-r requirements.txt (line 19))
  Using cached MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->-r requirements.txt (line 20))
  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)
Collecting cfgv>=2.0.0 (from pre-commit>=2.13.0->-r requirements.txt (line 25))
  Using cached cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)
Collecting identify>=1.0.0 (from pre-commit>=2.13.0->-r requirements.txt (line 25))
  Using cached identify-2.6.9-py2.py3-none-any.whl.metadata (4.4 kB)
Collecting nodeenv>=0.11.1 (from pre-commit>=2.13.0->-r requirements.txt (line 25))
  Using cached nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)
Collecting pyyaml>=5.1 (from pre-commit>=2.13.0->-r requirements.txt (line 25))
  Using cached PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting virtualenv>=20.10.0 (from pre-commit>=2.13.0->-r requirements.txt (line 25))
  Using cached virtualenv-20.29.3-py3-none-any.whl.metadata (4.5 kB)
Collecting iniconfig (from pytest!=7.1.0,>=5.2.0->-r requirements.txt (line 27))
  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)
Collecting pluggy<2,>=1.5 (from pytest!=7.1.0,>=5.2.0->-r requirements.txt (line 27))
  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)
Collecting pbr>=0.11 (from mock<3.0.0,>=2.0.0->-r requirements.txt (line 29))
  Using cached pbr-6.1.1-py2.py3-none-any.whl.metadata (3.4 kB)
Collecting six>=1.9 (from mock<3.0.0,>=2.0.0->-r requirements.txt (line 29))
  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)
Collecting mccabe<0.8.0,>=0.7.0 (from flake8<6.0.0,>=3.8.0->-r requirements.txt (line 30))
  Using cached mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)
Collecting pycodestyle<2.10.0,>=2.9.0 (from flake8<6.0.0,>=3.8.0->-r requirements.txt (line 30))
  Using cached pycodestyle-2.9.1-py2.py3-none-any.whl.metadata (31 kB)
Collecting pyflakes<2.6.0,>=2.5.0 (from flake8<6.0.0,>=3.8.0->-r requirements.txt (line 30))
  Using cached pyflakes-2.5.0-py2.py3-none-any.whl.metadata (3.8 kB)
Collecting attrs>=22.2.0 (from hypothesis<7.0.0,>=3.27.0->-r requirements.txt (line 31))
  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)
Collecting sortedcontainers<3.0.0,>=2.1.0 (from hypothesis<7.0.0,>=3.27.0->-r requirements.txt (line 31))
  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)
Collecting tokenize-rt>=3.2.0 (from cython-lint>=0.15.0->-r requirements.txt (line 38))
  Using cached tokenize_rt-6.1.0-py2.py3-none-any.whl.metadata (4.1 kB)
Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->-r requirements.txt (line 20))
  Using cached marisa_trie-1.2.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)
Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->-r requirements.txt (line 12))
  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)
Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->-r requirements.txt (line 12))
  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)
Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->-r requirements.txt (line 13))
  Using cached wrapt-1.17.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)
Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit>=2.13.0->-r requirements.txt (line 25))
  Using cached distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)
Collecting filelock<4,>=3.12.2 (from virtualenv>=20.10.0->pre-commit>=2.13.0->-r requirements.txt (line 25))
  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)
Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->-r requirements.txt (line 12))
  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)
Using cached black-22.3.0-py3-none-any.whl (153 kB)
Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)
Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)
Using cached cymem-2.0.11-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (222 kB)
Using cached ml_datasets-0.2.0-py3-none-any.whl (15 kB)
Using cached murmurhash-1.0.12-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)
Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)
Using cached srsly-2.5.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)
Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)
Using cached typer-0.15.2-py3-none-any.whl (45 kB)
Using cached weasel-0.4.1-py3-none-any.whl (50 kB)
Using cached requests-2.32.3-py3-none-any.whl (64 kB)
Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)
Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)
Using cached pydantic_core-2.27.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)
Using cached packaging-24.2-py3-none-any.whl (65 kB)
Using cached pre_commit-4.1.0-py2.py3-none-any.whl (220 kB)
Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)
Using cached pytest-8.3.5-py3-none-any.whl (343 kB)
Using cached pytest_timeout-1.4.2-py2.py3-none-any.whl (10 kB)
Using cached mock-2.0.0-py2.py3-none-any.whl (56 kB)
Using cached flake8-5.0.4-py2.py3-none-any.whl (61 kB)
Using cached hypothesis-6.129.4-py3-none-any.whl (489 kB)
Using cached mypy-1.5.1-py3-none-any.whl (2.5 MB)
Using cached types_mock-5.2.0.20250306-py3-none-any.whl (10 kB)
Using cached types_setuptools-76.0.0.20250313-py3-none-any.whl (65 kB)
Using cached types_requests-2.32.0.20250306-py3-none-any.whl (20 kB)
Using cached cython_lint-0.16.6-py3-none-any.whl (12 kB)
Using cached isort-5.13.2-py3-none-any.whl (92 kB)
Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)
Using cached attrs-25.3.0-py3-none-any.whl (63 kB)
Using cached blis-1.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.2 MB)
Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)
Using cached cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)
Using cached charset_normalizer-3.4.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)
Using cached click-8.1.8-py3-none-any.whl (98 kB)
Using cached cloudpathlib-0.21.0-py3-none-any.whl (52 kB)
Using cached confection-0.1.5-py3-none-any.whl (35 kB)
Using cached identify-2.6.9-py2.py3-none-any.whl (99 kB)
Using cached idna-3.10-py3-none-any.whl (70 kB)
Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)
Using cached MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)
Using cached mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)
Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)
Using cached nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)
Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)
Using cached pbr-6.1.1-py2.py3-none-any.whl (108 kB)
Using cached platformdirs-4.3.6-py3-none-any.whl (18 kB)
Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)
Using cached pycodestyle-2.9.1-py2.py3-none-any.whl (41 kB)
Using cached pyflakes-2.5.0-py2.py3-none-any.whl (66 kB)
Using cached PyYAML-6.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)
Using cached rich-13.9.4-py3-none-any.whl (242 kB)
Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)
Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)
Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)
Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)
Using cached tokenize_rt-6.1.0-py2.py3-none-any.whl (6.0 kB)
Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)
Using cached virtualenv-20.29.3-py3-none-any.whl (4.3 MB)
Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)
Using cached distlib-0.3.9-py2.py3-none-any.whl (468 kB)
Using cached filelock-3.18.0-py3-none-any.whl (16 kB)
Using cached marisa_trie-1.2.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)
Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)
Using cached pygments-2.19.1-py3-none-any.whl (1.2 MB)
Using cached wrapt-1.17.2-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)
Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)
Building wheels for collected packages: thinc
  Building wheel for thinc (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Building wheel for thinc (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [380 lines of output]
      Cythonizing sources
      running bdist_wheel
      running build
      running build_py
      creating build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/util.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/types.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/schedules.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/optimizers.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/mypy.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/model.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/loss.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/initializers.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/config.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/compat.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/api.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/about.py -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc
      creating build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/util.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_util.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_types.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_serialize.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_schedules.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_optimizers.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_loss.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_initializers.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_indexing.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_import__all__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_examples.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/test_config.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/strategies.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/enable_tensorflow.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/enable_mxnet.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/conftest.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      copying thinc/tests/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests
      creating build/lib.linux-x86_64-cpython-313/thinc/shims
      copying thinc/shims/torchscript.py -> build/lib.linux-x86_64-cpython-313/thinc/shims
      copying thinc/shims/tensorflow.py -> build/lib.linux-x86_64-cpython-313/thinc/shims
      copying thinc/shims/shim.py -> build/lib.linux-x86_64-cpython-313/thinc/shims
      copying thinc/shims/pytorch_grad_scaler.py -> build/lib.linux-x86_64-cpython-313/thinc/shims
      copying thinc/shims/pytorch.py -> build/lib.linux-x86_64-cpython-313/thinc/shims
      copying thinc/shims/mxnet.py -> build/lib.linux-x86_64-cpython-313/thinc/shims
      copying thinc/shims/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/shims
      creating build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_signpost_interval.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_reshape.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_ragged.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_padded.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_nvtx_range.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_list.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_getitem.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_flatten_v2.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_flatten.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_debug.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_cpu.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_array2d.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/with_array.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/uniqued.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/tuplify.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/torchscriptwrapper.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/tensorflowwrapper.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/swish.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/strings2arrays.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/softmax_activation.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/softmax.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/sigmoid_activation.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/sigmoid.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/siamese.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/resizable.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/residual.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/remap_ids.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/relu.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/reduce_sum.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/reduce_mean.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/reduce_max.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/reduce_last.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/reduce_first.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/ragged2list.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/pytorchwrapper.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/parametricattention_v2.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/parametricattention.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/padded2list.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/noop.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/mxnetwrapper.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/multisoftmax.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/mish.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/maxout.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/map_list.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/lstm.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/logistic.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/list2ragged.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/list2padded.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/list2array.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/linear.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/layernorm.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/hashembed.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/hard_swish_mobilenet.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/hard_swish.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/gelu.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/expand_window.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/embed.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/dropout.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/dish.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/concatenate.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/clone.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/clipped_linear.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/chain.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/cauchysimilarity.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/bidirectional.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/array_getitem.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/add.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/layers
      creating build/lib.linux-x86_64-cpython-313/thinc/extra
      copying thinc/extra/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/extra
      creating build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/ops.py -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/mps_ops.py -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/cupy_ops.py -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/_param_server.py -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/_custom_kernels.py -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/_cupy_allocators.py -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/backends
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/shims
      copying thinc/tests/shims/test_pytorch_grad_scaler.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/shims
      copying thinc/tests/shims/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/shims
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/regression
      copying thinc/tests/regression/test_issue564.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/regression
      copying thinc/tests/regression/test_issue208.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/regression
      copying thinc/tests/regression/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/regression
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/mypy
      copying thinc/tests/mypy/test_mypy.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy
      copying thinc/tests/mypy/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/model
      copying thinc/tests/model/test_validation.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/model
      copying thinc/tests/model/test_model.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/model
      copying thinc/tests/model/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/model
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_with_transforms.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_with_flatten.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_with_debug.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_uniqued.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_transforms.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_torchscriptwrapper.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_tensorflow_wrapper.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_sparse_linear.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_softmax.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_shim.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_resizable.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_reduce.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_pytorch_wrapper.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_parametric_attention_v2.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_mxnet_wrapper.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_mnist.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_mappers.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_lstm.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_linear.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_layers_api.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_hash_embed.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_feed_forward.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_combinators.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/test_basic_tagger.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      copying thinc/tests/layers/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/layers
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/extra
      copying thinc/tests/extra/test_beam_search.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/extra
      copying thinc/tests/extra/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/extra
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/backends
      copying thinc/tests/backends/test_ops.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/backends
      copying thinc/tests/backends/test_mem.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/backends
      copying thinc/tests/backends/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/backends
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/regression/issue519
      copying thinc/tests/regression/issue519/test_issue519.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/regression/issue519
      copying thinc/tests/regression/issue519/program.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/regression/issue519
      copying thinc/tests/regression/issue519/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/regression/issue519
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/modules
      copying thinc/tests/mypy/modules/success_plugin.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/modules
      copying thinc/tests/mypy/modules/success_no_plugin.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/modules
      copying thinc/tests/mypy/modules/fail_plugin.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/modules
      copying thinc/tests/mypy/modules/fail_no_plugin.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/modules
      copying thinc/tests/mypy/modules/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/modules
      creating build/lib.linux-x86_64-cpython-313/thinc/extra/tests
      copying thinc/extra/tests/__init__.py -> build/lib.linux-x86_64-cpython-313/thinc/extra/tests
      running egg_info
      writing thinc.egg-info/PKG-INFO
      writing dependency_links to thinc.egg-info/dependency_links.txt
      writing entry points to thinc.egg-info/entry_points.txt
      writing requirements to thinc.egg-info/requires.txt
      writing top-level names to thinc.egg-info/top_level.txt
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/arrayobject.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/arrayscalars.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/ndarrayobject.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/ndarraytypes.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/ufuncobject.h won't be automatically included in the manifest: the path must be relative
      dependency /usr/include/python3.13/Python.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/arrayobject.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/arrayscalars.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/ndarrayobject.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/ndarraytypes.h won't be automatically included in the manifest: the path must be relative
      dependency /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include/numpy/ufuncobject.h won't be automatically included in the manifest: the path must be relative
      dependency /usr/include/python3.13/Python.h won't be automatically included in the manifest: the path must be relative
      reading manifest file 'thinc.egg-info/SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      no previously-included directories found matching 'tmp'
      adding license file 'LICENSE'
      writing manifest file 'thinc.egg-info/SOURCES.txt'
      /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'thinc.tests.mypy.configs' is absent from the `packages` configuration.
      !!
      
              ********************************************************************************
              ############################
              # Package would be ignored #
              ############################
              Python recognizes 'thinc.tests.mypy.configs' as an importable package[^1],
              but it is absent from setuptools' `packages` configuration.
      
              This leads to an ambiguous overall configuration. If you want to distribute this
              package, please make sure that 'thinc.tests.mypy.configs' is explicitly added
              to the `packages` configuration field.
      
              Alternatively, you can also rely on setuptools' discovery methods
              (for example by using `find_namespace_packages(...)`/`find_namespace:`
              instead of `find_packages(...)`/`find:`).
      
              You can read more about ""package discovery"" on setuptools documentation page:
      
              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
      
              If you don't want 'thinc.tests.mypy.configs' to be distributed and are
              already explicitly excluding 'thinc.tests.mypy.configs' via
              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,
              you can try to use `exclude_package_data`, or `include-package-data=False` in
              combination with a more fine grained `package-data` configuration.
      
              You can read more about ""package data files"" on setuptools documentation page:
      
              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html
      
      
              [^1]: For Python, any directory (with suitable naming) can be imported,
                    even if it does not contain any `.py` files.
                    On the other hand, currently there is no concept of package data
                    directory, all directories are treated like packages.
              ********************************************************************************
      
      !!
        check.warn(importable)
      /tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/setuptools/command/build_py.py:212: _Warning: Package 'thinc.tests.mypy.outputs' is absent from the `packages` configuration.
      !!
      
              ********************************************************************************
              ############################
              # Package would be ignored #
              ############################
              Python recognizes 'thinc.tests.mypy.outputs' as an importable package[^1],
              but it is absent from setuptools' `packages` configuration.
      
              This leads to an ambiguous overall configuration. If you want to distribute this
              package, please make sure that 'thinc.tests.mypy.outputs' is explicitly added
              to the `packages` configuration field.
      
              Alternatively, you can also rely on setuptools' discovery methods
              (for example by using `find_namespace_packages(...)`/`find_namespace:`
              instead of `find_packages(...)`/`find:`).
      
              You can read more about ""package discovery"" on setuptools documentation page:
      
              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html
      
              If you don't want 'thinc.tests.mypy.outputs' to be distributed and are
              already explicitly excluding 'thinc.tests.mypy.outputs' via
              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,
              you can try to use `exclude_package_data`, or `include-package-data=False` in
              combination with a more fine grained `package-data` configuration.
      
              You can read more about ""package data files"" on setuptools documentation page:
      
              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html
      
      
              [^1]: For Python, any directory (with suitable naming) can be imported,
                    even if it does not contain any `.py` files.
                    On the other hand, currently there is no concept of package data
                    directory, all directories are treated like packages.
              ********************************************************************************
      
      !!
        check.warn(importable)
      copying thinc/__init__.pxd -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/py.typed -> build/lib.linux-x86_64-cpython-313/thinc
      copying thinc/layers/premap_ids.pyx -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/layers/sparselinear.pyx -> build/lib.linux-x86_64-cpython-313/thinc/layers
      copying thinc/extra/__init__.pxd -> build/lib.linux-x86_64-cpython-313/thinc/extra
      copying thinc/extra/search.pxd -> build/lib.linux-x86_64-cpython-313/thinc/extra
      copying thinc/extra/search.pyx -> build/lib.linux-x86_64-cpython-313/thinc/extra
      copying thinc/backends/__init__.pxd -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/_custom_kernels.cu -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/_murmur3.cu -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/cblas.pxd -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/cblas.pyx -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/cpu_kernels.hh -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/linalg.pxd -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/linalg.pyx -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/numpy_ops.pxd -> build/lib.linux-x86_64-cpython-313/thinc/backends
      copying thinc/backends/numpy_ops.pyx -> build/lib.linux-x86_64-cpython-313/thinc/backends
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/configs
      copying thinc/tests/mypy/configs/mypy-default.ini -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/configs
      copying thinc/tests/mypy/configs/mypy-plugin.ini -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/configs
      creating build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/outputs
      copying thinc/tests/mypy/outputs/fail-no-plugin.txt -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/outputs
      copying thinc/tests/mypy/outputs/fail-plugin.txt -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/outputs
      copying thinc/tests/mypy/outputs/success-no-plugin.txt -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/outputs
      copying thinc/tests/mypy/outputs/success-plugin.txt -> build/lib.linux-x86_64-cpython-313/thinc/tests/mypy/outputs
      copying thinc/extra/tests/c_test_search.pyx -> build/lib.linux-x86_64-cpython-313/thinc/extra/tests
      running build_ext
      building 'thinc.backends.cblas' extension
      creating build/temp.linux-x86_64-cpython-313/thinc/backends
      g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O3 -Wall -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fexceptions -Wp,-D_FORTIFY_SOURCE=3 -Wformat -Werror=format-security -fstack-clash-protection -fcf-protection -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -g -ffile-prefix-map=/build/python/src=/usr/src/debug/python -flto=auto -ffat-lto-objects -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fexceptions -Wp,-D_FORTIFY_SOURCE=3 -Wformat -Werror=format-security -fstack-clash-protection -fcf-protection -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -g -ffile-prefix-map=/build/python/src=/usr/src/debug/python -flto=auto -march=x86-64 -mtune=generic -O3 -pipe -fno-plt -fexceptions -Wp,-D_FORTIFY_SOURCE=3 -Wformat -Werror=format-security -fstack-clash-protection -fcf-protection -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -g -ffile-prefix-map=/build/python/src=/usr/src/debug/python -flto=auto -fPIC -I/tmp/pip-build-env-w27chjg7/overlay/lib/python3.13/site-packages/numpy/_core/include -I/usr/include/python3.13 -I/home/manshika/.virtualenvs/dlenv/include -I/usr/include/python3.13 -c thinc/backends/cblas.cpp -o build/temp.linux-x86_64-cpython-313/thinc/backends/cblas.o -O3 -Wno-strict-prototypes -Wno-unused-function -std=c++11
      cc1plus: warning: command-line option ‘-Wno-strict-prototypes’ is valid for C/ObjC but not for C++
      thinc/backends/cblas.cpp:871:72: warning: ‘Py_UNICODE’ is deprecated [-Wdeprecated-declarations]
        871 | static CYTHON_INLINE size_t __Pyx_Py_UNICODE_strlen(const Py_UNICODE *u) {
            |                                                                        ^
      In file included from /usr/include/python3.13/unicodeobject.h:1014,
                       from /usr/include/python3.13/Python.h:79,
                       from thinc/backends/cblas.cpp:24:
      /usr/include/python3.13/cpython/unicodeobject.h:10:37: note: declared here
         10 | Py_DEPRECATED(3.13) typedef wchar_t Py_UNICODE;
            |                                     ^~~~~~~~~~
      thinc/backends/cblas.cpp: In function ‘size_t __Pyx_Py_UNICODE_strlen(const Py_UNICODE*)’:
      thinc/backends/cblas.cpp:872:23: warning: ‘Py_UNICODE’ is deprecated [-Wdeprecated-declarations]
        872 |     const Py_UNICODE *u_end = u;
            |                       ^~~~~
      /usr/include/python3.13/cpython/unicodeobject.h:10:37: note: declared here
         10 | Py_DEPRECATED(3.13) typedef wchar_t Py_UNICODE;
            |                                     ^~~~~~~~~~
      thinc/backends/cblas.cpp: In function ‘int __Pyx_PyList_Extend(PyObject*, PyObject*)’:
      thinc/backends/cblas.cpp:1908:22: error: ‘_PyList_Extend’ was not declared in this scope; did you mean ‘PyList_Extend’?
       1908 |     PyObject* none = _PyList_Extend((PyListObject*)L, v);
            |                      ^~~~~~~~~~~~~~
            |                      PyList_Extend
      thinc/backends/cblas.cpp: In function ‘void __Pyx_init_assertions_enabled()’:
      thinc/backends/cblas.cpp:1946:39: error: ‘_PyInterpreterState_GetConfig’ was not declared in this scope; did you mean ‘PyInterpreterState_GetID’?
       1946 |     __pyx_assertions_enabled_flag = ! _PyInterpreterState_GetConfig(__Pyx_PyThreadState_Current->interp)->optimization_level;
            |                                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            |                                       PyInterpreterState_GetID
      thinc/backends/cblas.cpp: In function ‘int __Pyx_PyInt_As_int(PyObject*)’:
      thinc/backends/cblas.cpp:20354:46: error: too few arguments to function ‘int _PyLong_AsByteArray(PyLongObject*, unsigned char*, size_t, int, int, int)’
      20354 |                 int ret = _PyLong_AsByteArray((PyLongObject *)v,
            |                           ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~
      20355 |                                               bytes, sizeof(val),
            |                                               ~~~~~~~~~~~~~~~~~~~
      20356 |                                               is_little, !is_unsigned);
            |                                               ~~~~~~~~~~~~~~~~~~~~~~~~
      In file included from /usr/include/python3.13/longobject.h:107,
                       from /usr/include/python3.13/Python.h:81:
      /usr/include/python3.13/cpython/longobject.h:111:17: note: declared here
        111 | PyAPI_FUNC(int) _PyLong_AsByteArray(PyLongObject* v,
            |                 ^~~~~~~~~~~~~~~~~~~
      thinc/backends/cblas.cpp: In function ‘long int __Pyx_PyInt_As_long(PyObject*)’:
      thinc/backends/cblas.cpp:20550:46: error: too few arguments to function ‘int _PyLong_AsByteArray(PyLongObject*, unsigned char*, size_t, int, int, int)’
      20550 |                 int ret = _PyLong_AsByteArray((PyLongObject *)v,
            |                           ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~
      20551 |                                               bytes, sizeof(val),
            |                                               ~~~~~~~~~~~~~~~~~~~
      20552 |                                               is_little, !is_unsigned);
            |                                               ~~~~~~~~~~~~~~~~~~~~~~~~
      /usr/include/python3.13/cpython/longobject.h:111:17: note: declared here
        111 | PyAPI_FUNC(int) _PyLong_AsByteArray(PyLongObject* v,
            |                 ^~~~~~~~~~~~~~~~~~~
      thinc/backends/cblas.cpp: In function ‘char __Pyx_PyInt_As_char(PyObject*)’:
      thinc/backends/cblas.cpp:20822:46: error: too few arguments to function ‘int _PyLong_AsByteArray(PyLongObject*, unsigned char*, size_t, int, int, int)’
      20822 |                 int ret = _PyLong_AsByteArray((PyLongObject *)v,
            |                           ~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~
      20823 |                                               bytes, sizeof(val),
            |                                               ~~~~~~~~~~~~~~~~~~~
      20824 |                                               is_little, !is_unsigned);
            |                                               ~~~~~~~~~~~~~~~~~~~~~~~~
      /usr/include/python3.13/cpython/longobject.h:111:17: note: declared here
        111 | PyAPI_FUNC(int) _PyLong_AsByteArray(PyLongObject* v,
            |                 ^~~~~~~~~~~~~~~~~~~
      error: command '/usr/bin/g++' failed with exit code 1
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for thinc
Failed to build thinc
ERROR: Failed to build installable wheels for some pyproject.toml based projects (thinc)


## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Arch Linux
* Python Version Used: Python 3.13.2
* spaCy Version Used:Latest
* Environment Information: virtual environment
",manshika13,102040383,open,False,4,2025-03-18T12:44:49+00:00,2025-04-10T14:12:25+00:00,,,2,2,0,0,0,0,0
explosion/spaCy,2926020654,13771,Error: A module that was compiled using NumPy 1.x cannot be run in,"I see an error message and stack trace when running `python -m spacy download en_core_web_lg` using a ""simple"" pyenv setup.  

It appears to conduct the download, but Spacey isn't able to find it thereafter.  

> A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.3 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

>If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

>Traceback (most recent call last):  File ""<frozen runpy>"", line 189, in _run_module_as_main
  File ""<frozen runpy>"", line 148, in _get_module_details
  File ""<frozen runpy>"", line 112, in _get_module_details
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/spacy/__init__.py"", line 6, in <module>
    from .errors import setup_default_warnings
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/spacy/errors.py"", line 3, in <module>
    from .compat import Literal
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/spacy/compat.py"", line 4, in <module>
    from thinc.util import copy_array
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/thinc/__init__.py"", line 5, in <module>
    from .config import registry
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/thinc/config.py"", line 5, in <module>
    from .types import Decorator
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/thinc/types.py"", line 25, in <module>
    from .compat import cupy, has_cupy
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/thinc/compat.py"", line 35, in <module>
    import torch
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/torch/__init__.py"", line 1477, in <module>
    from .functional import *  # noqa: F403
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/torch/functional.py"", line 9, in <module>
    import torch.nn.functional as F
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/torch/nn/__init__.py"", line 1, in <module>
    from .modules import *  # noqa: F403
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/torch/nn/modules/__init__.py"", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File ""/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py"", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/Users/u/dev/monorepo/poc/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
Collecting en-core-web-lg==3.8.0
  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 400.7/400.7 MB 31.9 MB/s eta 0:00:00
✔ Download and installation successful
You can now load the package via spacy.load('en_core_web_lg')

Here's my pip freeze if it helps
```
$  pip freeze
...
annotated-types==0.7.0
anyio==4.8.0
asttokens==3.0.0
blis==1.2.0
catalogue==2.0.10
certifi==2025.1.31
charset-normalizer==3.4.1
click==8.1.8
cloudpathlib==0.21.0
confection==0.1.5
cymem==2.0.11
decorator==5.2.0
diskcache==5.6.3
dnspython==2.7.0
email_validator==2.2.0
en_core_web_lg @ https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl#sha256=293e9547a655b25499198ab15a525b05b9407a75f10255e405e8c3854329ab63
executing==2.2.0
fastapi==0.115.8
fastapi-cli==0.0.7
filelock==3.18.0
fsspec==2025.3.0
h11==0.14.0
httpcore==1.0.7
httptools==0.6.4
httpx==0.28.1
idna==3.10
# Editable Git install (inferencecontroller==0.1) with either a deleted local remote or invalid URI:
# 'git@gitlab.njax.org:/Apps/job_hunter.git'
-e /Users/john/dev/apps/job_hunter/inference-controller
iniconfig==2.0.0
ipdb==0.13.13
ipython==8.32.0
itsdangerous==2.2.0
jedi==0.19.2
Jinja2==3.1.5
langcodes==3.5.0
language_data==1.3.0
llama_cpp_python==0.3.7
marisa-trie==1.2.1
markdown-it-py==3.0.0
MarkupSafe==3.0.2
matplotlib-inline==0.1.7
mdurl==0.1.2
mpmath==1.3.0
murmurhash==1.0.12
networkx==3.4.2
nose==1.3.7
numpy==2.2.3
orjson==3.10.15
packaging==24.2
parso==0.8.4
pexpect==4.9.0
pillow==11.1.0
pipdeptree==2.25.1
pluggy==1.5.0
preshed==3.0.9
prompt_toolkit==3.0.50
ptyprocess==0.7.0
pure_eval==0.2.3
pydantic==2.10.6
pydantic-extra-types==2.10.2
pydantic-settings==2.8.0
pydantic_core==2.27.2
Pygments==2.19.1
pytest==8.3.4
python-dotenv==1.0.1
python-multipart==0.0.20
PyYAML==6.0.2
requests==2.32.3
rich==13.9.4
rich-toolkit==0.13.2
shellingham==1.5.4
smart-open==7.1.0
sniffio==1.3.1
spacy==3.8.4
spacy-legacy==3.0.12
spacy-loggers==1.0.5
srsly==2.5.1
stack-data==0.6.3
starlette==0.45.3
sympy==1.13.3
thinc==8.3.4
torch==2.2.2
torchaudio==2.2.2
torchvision==0.17.2
tqdm==4.67.1
traitlets==5.14.3
typer==0.15.1
typing_extensions==4.12.2
ujson==5.10.0
urllib3==2.3.0
uvicorn==0.34.0
uvloop==0.21.0
wasabi==1.1.3
watchfiles==1.0.4
wcwidth==0.2.13
weasel==0.4.1
websockets==15.0
wrapt==1.17.2
...
```

Also, supporting more recent versions of Numpy, as silly as it sounds, may constitute a ""breaking change"" and may indicate a major version bump, rather than a minor or patch assuming you aren't reserving the major version for something else.  
",TheNotary,799247,closed,True,2,2025-03-17T18:25:26+00:00,2025-04-17T00:03:18+00:00,2025-03-17T18:55:27+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2914761028,13769,Bug in Span.sents,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->
When a `Doc`'s entity is in the second to the last sentence, and the last sentence consists only of one token, `entity.sents` includes that last 1-token sentence (even though the entity is fully contained by the previous sentence.

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
```
text = ""This is a sentence. This is another sentence. Third""
doc = nlp.tokenizer(text)
doc[0].is_sent_start = True
doc[5].is_sent_start = True
doc[10].is_sent_start = True
doc.ents = [('ENTITY', 7, 9)] # ""another sentence"" phrase in the second sentence
entity = doc.ents[0]
print(f""Entity: {entity}. Sentence: {entity.sent} Sentences: {list(entity.sents)}"")
```
Output:
```
Entity: another sentence. Sentence: This is another sentence. Sentences: [This is another sentence., Third]
```
## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
",nrodnova,4512370,open,False,0,2025-03-12T18:03:13+00:00,2025-03-12T18:03:13+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2908026166,13768,fix: import `display` from `ipython.display`,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->
fixes #13763

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->
bug fix

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",superleesa,88019950,open,False,0,2025-03-10T17:57:27+00:00,2025-03-10T18:03:57+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2901533009,13766,Update mock import to work with Python 3.13,"
## Description

There's no `mock` import in Python 3.13 so need to update to `unittest`

### Types of change

Update `mock` import to `unittest.mock`

## Checklist
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [ ] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",davispuh,651800,open,False,0,2025-03-06T22:07:00+00:00,2025-03-06T22:07:00+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2901473172,13765,Support Cython 3 (v4 branch),"## Description

Cython 0.x doesn't support Python 3.13 so for that need to update to Cython 3

### Types of change

Update to build with Cython 3 for `v4` branch.

## Checklist
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [ ] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",davispuh,651800,open,False,0,2025-03-06T21:28:35+00:00,2025-03-06T21:28:35+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2901467402,13764,Support Cython 3,"
## Description

Cython 0.x doesn't support Python 3.13 so for that need to update to Cython 3

### Types of change

Update to build with Cython 3

## Checklist
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [ ] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",davispuh,651800,open,False,1,2025-03-06T21:25:00+00:00,2025-04-11T18:55:52+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2893975693,13763,"""from IPython.core.display import HTML, display"" no more supported for latest IPython","Please consider replace ```from IPython.core.display import HTML, display```  by ```from IPython.display import HTML, display```.",ZhihuiLuo,16853336,open,False,1,2025-03-04T11:45:05+00:00,2025-04-04T17:50:38+00:00,,,8,8,0,0,0,0,0
explosion/spaCy,2893229014,13762,displacy: fix import path for ipython 9.0.1,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

With IPython 9.0.1, they [removed the `IPython.core.display` shim](https://github.com/ipython/ipython/commit/8a0533f31abcc4cb4341858bdeee86e2061c4fd9) that `displacy.render()` relied on. This was apparently deprecated in [IPython 8.0.0](https://github.com/ipython/ipython/commit/43a01a21101ea65dbd52cf760c1e4f149bfab588).

This fixes it by trying the newer import path if possible, falling back to the older one.

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->
Bug fix

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [ ] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.

I am unable to install the packages in requirements.txt due to `thinc<8.4.0,>=8.3.4` being unsatisfied since there's only nearby version 8 package is `8.3.2` and then it skips to `9.0.0.dev0`. However, editing this line locally fixed the import issue.",1480c1,8345542,open,False,0,2025-03-04T07:25:06+00:00,2025-03-04T07:25:06+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2887324790,13761,Empty coarse-grained POS tags for number in the large Romanian model,"Hi, I noticed that the coarse-grained POS tags for numbers in the large Romanian model (`ro_core_news_lg`) is empty rather than `X`. Is this the expected behavior?

## How to reproduce the behaviour
```
import spacy
nlp = spacy.load('ro_core_news_lg')
for token in nlp('2025'):
    print(len(token.pos_), token.pos_)
    print(len(token.tag_), token.tag_)
```

## Your Environment
* Operating System: Windows 11 x64
* Python Version Used: 3.11.9
* spaCy Version Used: 3.8.4
",BLKSerene,23695743,open,False,0,2025-02-28T14:39:49+00:00,2025-02-28T14:39:49+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2886390644,13760,Remove dependency on langcodes,"## Description
This PR removes the dependency on [`langcodes`](https://github.com/rspeer/langcodes) introduced in #9342.

While the introduction of [`langcodes`](https://github.com/rspeer/langcodes) allows a significantly wider range of language codes, there are some unexpected side effects:

1. `zh-Hant` (Traditional Chinese) should be mapped to `zh` intead of `None`, as spaCy's Chinese model is based on `pkuseg` which supports tokenization of both Simplified and Traditional Chinese.
2. Since it is possible that spaCy may have a model for Norwegian Nynorsk in the future, mapping `no` (macrolanguage Norwegian) to `nb` (Norwegian Bokmål) might be misleading. In that case, the user should be asked to specify `nb` or `nn` (Norwegian Nynorsk) specifically or consult the doc.
3. Same as above for regional variants of languages such as `en_gb` and `en_us`.

Overall, IMHO, introducing an extra dependency just for the conversion of language codes is an overkill. It is possible that most user just need the conversion between 2/3-letter ISO codes and a simple dictionary lookup should suffice.

With this PR, [ISO 639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) and [ISO 639-3](https://en.wikipedia.org/wiki/List_of_ISO_639-3_codes) codes are supported. [ISO 639-2/B](https://en.wikipedia.org/wiki/ISO_639-2#B_and_T_codes) (bibliographic codes which are not favored and used in ISO 639-3) and [deprecated ISO 639-1/2 codes](https://www.loc.gov/standards/iso639-2/php/code_changes.php) are also supported to maximize backward compatibility.

### Types of change
Dependency.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",BLKSerene,23695743,open,False,0,2025-02-28T07:10:39+00:00,2025-02-28T08:19:56+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2886156869,13759,Switch to typer-slim,"## Description
This PR changes the dependency on [`typer`](https://github.com/fastapi/typer) to [`typer-slim`](https://github.com/fastapi/typer?tab=readme-ov-file#typer-slim) which has lighter dependencies.

Relates to https://github.com/explosion/spaCy/discussions/13538.

### Types of change
Dependency.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",BLKSerene,23695743,open,False,0,2025-02-28T04:23:17+00:00,2025-02-28T04:23:17+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2863077722,13756,[Enhancement]: add scores to Displacy entity markup,"## Description
This PR adds the ability to pass scores along with entities to displacy with the manual mode. If no score is passed there is no change.

It uses the template below: 
```
# Doing it with a template similar to TPL_KB_LINK so that we don't get empty '()' parenthesis if no score is present
TPL_SCORE = ""({score:.2f})""
```
not sure if should allow modification of template|precision through options?

### Example
So far only tested with `manual=True` mode on jupyter, but if anyone else is willing to complete it feel free~

```
from spacy import displacy

text=""Hello world with scores""
formatted_entities =[  {'start': 0,
                        'end': 5,
                        'label': 'greeting',
                        'score': 0.90},
                        {'start': 5,
                        'end': 11,
                        'label': 'planet',
                        'score': 0.86666},
                        {'start': 17,
                        'end': 23,
                        'label': 'yeay',
                        'score': 0.5},
                        ]
displacy.render([{""text"": text, ""ents"": formatted_entities, ""title"": None}], style=""ent"", manual=True,jupyter=True)
```
<img width=""439"" alt=""image"" src=""https://github.com/user-attachments/assets/c1f2b7b6-39d0-4cc6-bded-0bfa2156a260"" />

If a 'score' doesn't exist it isn't shown:
<img width=""393"" alt=""image"" src=""https://github.com/user-attachments/assets/93fa8936-4b00-4be5-9c3b-9c4e8024cb1e"" />
 

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [ ] I ran the tests, and all new and existing tests passed.
- [ ] My changes don't require a change to the documentation, or if they do, I've added all required information.
",cceyda,15624271,open,False,0,2025-02-19T11:41:39+00:00,2025-02-19T11:42:22+00:00,,,1,1,0,0,0,0,0
explosion/spaCy,2857956828,13753,Compiler cl cannot compile programs.,"
### Discussed in https://github.com/explosion/spaCy/discussions/8226

## Environment

- Windows 11
- Python 3.13.2
- Pip 25.0.1
- Visual Studio 2022
   - SDK do Windows 11
   - MSVC v143 - VS 2022 C++ x64/x86 build tools

## How to reproduce the problem

1. python -m venv venv
2. pip install spacy

## Output of the error

```sh
Collecting spacy
  Using cached spacy-3.8.2.tar.gz (1.3 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error
  
  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [96 lines of output]
      Ignoring numpy: markers 'python_version < ""3.9""' don't match your environment
      Collecting setuptools
        Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)
      Collecting cython<3.0,>=0.25
        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)
      Collecting preshed<3.1.0,>=3.0.2
        Using cached preshed-3.0.9.tar.gz (14 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'done'
        Getting requirements to build wheel: started
        Getting requirements to build wheel: finished with status 'done'
        Preparing metadata (pyproject.toml): started
        Preparing metadata (pyproject.toml): finished with status 'done'
      Collecting murmurhash<1.1.0,>=0.28.0
        Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)
      Collecting thinc<8.4.0,>=8.3.0
        Using cached thinc-8.3.2.tar.gz (193 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'error'  
        error: subprocess-exited-with-error

        pip subprocess to install build dependencies did not run successfully.
        exit code: 1

        [58 lines of output]
        Ignoring numpy: markers 'python_version < ""3.9""' don't match your environment
        Collecting setuptools
          Using cached setuptools-75.8.0-py3-none-any.whl.metadata (6.7 kB)
        Collecting cython<3.0,>=0.25
          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
        Collecting murmurhash<1.1.0,>=1.0.2
          Using cached murmurhash-1.0.12-cp313-cp313-win_amd64.whl.metadata (2.2 kB)
        Collecting cymem<2.1.0,>=2.0.2
          Using cached cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)
        Collecting preshed<3.1.0,>=3.0.2
          Using cached preshed-3.0.9.tar.gz (14 kB)
          Installing build dependencies: started
          Installing build dependencies: finished with status 'done' 
          Getting requirements to build wheel: started
          Getting requirements to build wheel: finished with status 'done'
          Preparing metadata (pyproject.toml): started
          Preparing metadata (pyproject.toml): finished with status 'done'
        Collecting blis<1.1.0,>=1.0.0
          Using cached blis-1.0.2-cp313-cp313-win_amd64.whl.metadata (7.8 kB)
        Collecting numpy<2.1.0,>=2.0.0
          Using cached numpy-2.0.2.tar.gz (18.9 MB)
          Installing build dependencies: started
          Installing build dependencies: finished with status 'done' 
          Getting requirements to build wheel: started
          Getting requirements to build wheel: finished with status 'done'
          Installing backend dependencies: started
          Installing backend dependencies: finished with status 'done'
          Preparing metadata (pyproject.toml): started
          Preparing metadata (pyproject.toml): finished with status 'error'
          error: subprocess-exited-with-error

          Preparing metadata (pyproject.toml) did not run successfully.
          exit code: 1

          [12 lines of output]
          + C:\Users\M361-1\Developer\TRT23\nlp\venv_spacy\Scripts\python.exe C:\Users\M361-1\AppData\Local\Temp\pip-install-mlt_1vm3\numpy_59db6106a8f04eba871fd92c04f756d5\vendored-meson\meson\meson.py setup C:\Users\M361-1\AppData\Local\Temp\pip-install-mlt_1vm3\numpy_59db6106a8f04eba871fd92c04f756d5 C:\Users\M361-1\AppData\Local\Temp\pip-install-mlt_1vm3\numpy_59db6106a8f04eba871fd92c04f756d5\.mesonpy-gf7b5yu8 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\Users\M361-1\AppData\Local\Temp\pip-install-mlt_1vm3\numpy_59db6106a8f04eba871fd92c04f756d5\.mesonpy-gf7b5yu8\meson-python-native-file.ini
          The Meson build system
          Version: 1.4.99
          Source dir: C:\Users\M361-1\AppData\Local\Temp\pip-install-mlt_1vm3\numpy_59db6106a8f04eba871fd92c04f756d5
          Build dir: C:\Users\M361-1\AppData\Local\Temp\pip-install-mlt_1vm3\numpy_59db6106a8f04eba871fd92c04f756d5\.mesonpy-gf7b5yu8     
          Build type: native build
          Project name: NumPy
          Project version: 2.0.2

          ..\meson.build:1:0: ERROR: Compiler cl cannot compile programs.

          A full log can be found at C:\Users\M361-1\AppData\Local\Temp\pip-install-mlt_1vm3\numpy_59db6106a8f04eba871fd92c04f756d5\.mesonpy-gf7b5yu8\meson-logs\meson-log.txt
          [end of output]

          note: This error originates from a subprocess, and is likely not a problem with pip.

        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: subprocess-exited-with-error

      pip subprocess to install build dependencies did not run successfully.
      exit code: 1

      See above for output.

      note: This error originates from a subprocess, and is likely not a problem with pip.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
```


",MullerEsposito,20048394,closed,True,2,2025-02-17T14:03:03+00:00,2025-03-21T00:03:07+00:00,2025-02-18T13:52:09+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2848226927,13751,Apple and Docker support,"I'm using Spacy 3.8.4 on a MacMin with M4PRO chip and this works great.
When trying to build a Docker container it fails because of blis and thinc compatiblity.
SO i can run it from my virtual environment which works but on our development platform we use a MacBook Pro with M1Max chip and pip install spacy and pip install -U ""spacy[apple]"" do NOT work.
Questions:
1. Is there a wheel to download for M1Max
2. Can you make it useable within Docker

",arikivandeput,132348981,open,False,2,2025-02-12T13:22:27+00:00,2025-03-20T11:39:33+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2843989482,13749,docs: fix README.md for compatible Python versions,"## Description
SpaCy 3.8 is not compatible with Python 3.13 yet but the doc says otherwise, which can lead for users to loose time on that if they are ill-informed.
See https://github.com/explosion/spaCy/issues/13658

### Types of change
Update README.md to fix compatible Python versions.
",bolinocroustibat,4353767,closed,False,2,2025-02-11T00:05:24+00:00,2025-04-11T18:56:53+00:00,2025-04-11T18:56:53+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2840479211,13747,Avoid using pip for download models,"I use misaki package from pypi which uses spacy.
When I run the script that uses it, it tried first to download something from pypi using pip but I use `uv` which doesn't have `pip` by default.

I can see in 

- https://github.com/explosion/spaCy/blob/b3c46c315eb16ce644bddd106d31c3dd349f6bb2/spacy/cli/download.py#L161

That you spawn pip for fetch models.
Can you fetch it with requests instead or letting the user decide where and when to fetch it?",thewh1teagle,61390950,open,False,7,2025-02-09T06:57:16+00:00,2025-05-01T09:37:01+00:00,,,2,2,0,0,0,0,0
explosion/spaCy,2832357035,13743,remove reference to python 3.13 in setup.cfg,"## Description
https://github.com/explosion/spaCy/commit/85cc763006c6cbe69c31d41ac53fa20699551801 changed the supported python range to exclude 3.13 but left this entry in `classifiers` after it had been previously added.

related:
https://github.com/explosion/spaCy/commit/a6317b3836966f1298bfc95a604662d4fdb9ce60 https://github.com/explosion/spaCy/commit/343f4f21d706971be93757b430bad39e9ed49ed3

I'm not familiar with the reasons for the back and forth on this but it confused me that its mentioned in setup.cfg but not actually pip installable in python 3.13.

### Types of change
Should be a noop, just tidying.

tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",slaupster,9283885,open,False,0,2025-02-05T09:14:02+00:00,2025-02-21T21:35:57+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2826220398,13740,Change Twitter to X,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

Change Twitter to X, in both the Connect area and the Contribution Guidelines.

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Change to the documentation

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",star1327p,5897944,closed,False,1,2025-02-03T02:11:11+00:00,2025-02-03T08:30:21+00:00,2025-02-03T08:30:21+00:00,docs,0,0,0,0,0,0,0
explosion/spaCy,2823966861,13739,Inconsistent handling of whitespace tokens in Scorer.score_token_attr,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->
I've been training tagger and parser on heavily augmented data and was surprised by poor performance (comparing to what I calculated manually on the test dataset). I narrowed it down to `Scorer.score_token_attr`. 

## How to reproduce the behaviour
Sorry, I don't have a code example, but it's pretty straight-forward. In this function, in the gold dataset, all tokens (except for those with a missing attribute) are included in evaluation. However, in the predicted dataset, whitespace tokens are excluded. If the gold dataset contains whitespace tokens (which is true in my case), we are not comparing apples to apples here and inflate the error rate.

I just created my own scorer for now, but this behavior is kind of unexpected.

Let me know if you want me to change the behavior, and I will do a PR. My own fix to the function was to add `exclude_spaces` parameter, defaulting to the current behavior (i.e. `True`), and either include or exclude whitespace tokens in **both** datasets.

<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
```
# line 240 of scorer.py
for gold_i, token in enumerate(gold_doc):
        value = getter(token, attr)
        if value not in missing_values:
            gold_tags.add((gold_i, getter(token, attr)))
        else:
            missing_indices.add(gold_i)
    pred_tags = set()
    for token in pred_doc:
        if token.orth_.isspace(): # HERE: excluding whitespace tokens
            continue
        if align.x2y.lengths[token.i] == 1:
            gold_i = align.x2y[token.i][0]
```
",nrodnova,4512370,open,False,1,2025-01-31T16:44:50+00:00,2025-04-11T18:52:32+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2819046063,13737,Add calamanCy to universe.json,"Add calamanCy to universe.json


### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

Website change

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [ ] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",ljvmiranda921,12949683,open,False,1,2025-01-29T19:16:50+00:00,2025-01-30T01:34:28+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2810768089,13734,en_core_web_trf (3.8.0) ORG predictions seem  inaccurate compared to en_core_web_trf (3.6.1),"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

en_core_web_trf (3.8.0) labels CARDINAL tokens as ORG.  This happens in the affiliation sections for many scientific manuscript I tried out.   Interestingly, only the transformer pipeline has this new unexpected behavior,  NER  from en_core_web_lg (3.8.0) works as expected.

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
```python
from pprint import pprint

import spacy

text = ""Kelly E. Williams 1,2,3* , Kathryn P. Huyvaert 2 , Kurt C. Vercauteren 1 , Amy J. Davis 1 , Antoinette J. Piaggio 1\n1 USDA, Wildlife Services, National Wildlife Research Center, Wildlife Genetics Lab, 4101 Laporte Avenue, Fort Collins, CO, USA\n2 Department of Fish, Wildlife, and Conservation Biology, Colorado State University, Fort Collins, CO, 80523, USA\n3 School of Environmental and Forest Sciences, University of Washington, Seattle, WA, USA""
nlp = spacy.load(""en_core_web_trf"")
doc = nlp(text)
pprint([ent for ent in doc.ents if ent.label_ == ""ORG""])
```

 spaCy version  3.6.1 ( en_core_web_trf (3.6.1) ) returns:
```
[USDA,
 Wildlife Services,
 National Wildlife Research Center,
 Wildlife Genetics Lab,
 Department of Fish, Wildlife, and Conservation Biology,
 Colorado State University,
 School of Environmental and Forest Sciences,
 University of Washington]
```

spaCy version 3.8.4 (en_core_web_trf (3.8.0)) returns:
```
[USDA,
 Wildlife Services,
 National Wildlife Research Center,
 Wildlife Genetics Lab,
 USA
,
 2 Department of Fish, Wildlife,,       <=== ORG instead ORDINAL for ""2""
 Colorado State University,
 USA
,
 3 School of Environmental and Forest Sciences,       <=== ORG instead ORDINAL for ""3""
 University of Washington]
``` 



## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
## Info about spaCy

- **spaCy version:** 3.8.4
- **Platform:** Linux-6.8.0-51-generic-x86_64-with-glibc2.39
- **Platform:** macOS-15.2-arm64-arm-64bit
 - **Python version:** 3.11.11
- **Pipelines:** en_core_web_trf (3.8.0)
",vitaly-d,912437,open,False,1,2025-01-25T04:56:21+00:00,2025-01-26T04:29:23+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2809751679,13733,Japanese Tokenizer`RuntimeError: Already borrowed`,"* Japanese tokenizer specifically began throwing `RuntimeError: Already borrowed` when receiving multi-threaded requests.
* We noticed this error began on 2025-01-15, and did not previously struggle with this.
* Other language tokenizers are still able to handle multi-threading (see english and chinese succeed in example below, while japanese fails)

## How to reproduce the behaviour
`python -m spacy download en_core_web_md`
`python -m spacy download zh_core_web_md `
`python -m spacy download ja_core_news_md `

`python3` (open shell, paste below in):
```
import spacy
import pandas as pd
from multiprocessing.dummy import Pool as ThreadPool

data = ['string'] * 10
langs = [
    (""English"", spacy.load(""en_core_web_md"")), 
    (""Chinese"", spacy.load(""zh_core_web_md"")), 
    (""Japanese"", spacy.load(""ja_core_news_md""))
]

for (lang, model) in langs:
    print(lang)
    pool = ThreadPool(10)
    pool.map(model, data)
    pool.close()
    pool.join()
```

Output:
```
English
[string, string, string, string, string, string, string, string, string, string]
Chinese
[string, string, string, string, string, string, string, string, string, string]
Japanese
Traceback (most recent call last):
  File ""<stdin>"", line 4, in <module>
  File ""/home/jbartlett/.pyenv/versions/3.11.11/lib/python3.11/multiprocessing/pool.py"", line 367, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jbartlett/.pyenv/versions/3.11.11/lib/python3.11/multiprocessing/pool.py"", line 774, in get
    raise self._value
  File ""/home/jbartlett/.pyenv/versions/3.11.11/lib/python3.11/multiprocessing/pool.py"", line 125, in worker
    result = (True, func(*args, **kwds))
                    ^^^^^^^^^^^^^^^^^^^
  File ""/home/jbartlett/.pyenv/versions/3.11.11/lib/python3.11/multiprocessing/pool.py"", line 48, in mapstar
    return list(map(*args))
           ^^^^^^^^^^^^^^^^
  File ""/home/jbartlett/data-airflow/workspace/ml-services/slack-ml/.venv/lib/python3.11/site-packages/spacy/language.py"", line 1037, in __call__
    doc = self._ensure_doc(text)
          ^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jbartlett/data-airflow/workspace/ml-services/slack-ml/.venv/lib/python3.11/site-packages/spacy/language.py"", line 1128, in _ensure_doc
    return self.make_doc(doc_like)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/jbartlett/data-airflow/workspace/ml-services/slack-ml/.venv/lib/python3.11/site-packages/spacy/language.py"", line 1120, in make_doc
    return self.tokenizer(text)
           ^^^^^^^^^^^^^^^^^^^^
  File ""/home/jbartlett/data-airflow/workspace/ml-services/slack-ml/.venv/lib/python3.11/site-packages/spacy/lang/ja/__init__.py"", line 56, in __call__
    sudachipy_tokens = self.tokenizer.tokenize(text)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Already borrowed
```

## Info about spaCy
python -m spacy info --markdown

- **spaCy version:** 3.7.5
- **Platform:** Linux-6.8.0-1021-aws-x86_64-with-glibc2.35
- **Python version:** 3.11.11
- **Pipelines:** ja_core_news_lg (3.7.0), ja_core_news_md (3.7.0), en_core_web_md (3.7.1), zh_core_web_sm (3.7.0), ja_core_news_sm (3.7.0), zh_core_web_md (3.7.0)",joelbartlett20,106328751,open,False,0,2025-01-24T15:40:48+00:00,2025-01-24T15:42:43+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2789994244,13730,Inconsistent Token Alignment Between Fast and Slow Tokenizers in spacy-transformers,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

### Description
When using `spacy-transformers` with various HuggingFace models, I've discovered inconsistencies in the token alignment data (`doc._.trf_data.align`) between fast and slow tokenizer implementations. This issue particularly affects DeBERTa models and, to a lesser extent, RoBERTa-based models.

### Key Observations
1. **DeBERTa Models**: The alignment IDs are being duplicated when using the fast tokenizer:
   - Fast tokenizer produces pairs of duplicate IDs: `(1, 2, 2, 3, 3, 4, 4, ...)`
   - Slow tokenizer produces sequential IDs: `(1, 2, 3, 4, ...)`

2. **RoBERTa Models**: Shows minor differences in alignment:
   - Different `align_data` between fast/slow implementations
   - Fast tokenizer: `(4, 1, 1, 1, ..., 1, 1, 1, 1, 3, ...)`
   - Slow tokenizer: `(4, 1, 1, 1, ..., 1, 0, 1, 1, 3, ...)`

### Verification
- The issue appears to be specific to spacy-transformers, as direct usage of HuggingFace transformers shows no such discrepancies
- The differences affect both alignment data and lengths

### Reproduction Steps
1. Run the attached script which tests multiple models with both fast and slow tokenizer implementations
2. Compare the alignment data and lengths between fast/slow tokenizer variants
3. Note the systematic duplication in DeBERTa models and the alignment shifts in RoBERTa models

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

Run the following script:

```python
import warnings

warnings.simplefilter(""ignore"")

import spacy
from rich import print
from transformers import AutoTokenizer

MODELS = [
    ""distilroberta-base"",
    # ""roberta-base"",
    # ""intfloat/e5-small-v2"",
    ""BAAI/bge-small-en-v1.5"",
    ""microsoft/deberta-v3-xsmall"",
    # ""microsoft/deberta-v3-small"",
    ""microsoft/Multilingual-MiniLM-L12-H384"",
    # ""microsoft/deberta-v3-large"",
]

model_max_length = 1024

text = """"""Copenhagen is the capital and most populous city of Denmark,
with a population of 1.4 million in the urban area.""""""

for model_name in MODELS:
    wordpieces_strings = []
    wordpieces_input_ids = []
    wordpieces_attention_mask = []

    model_output_last_hidden_state = []
    align_data = []
    align_lengths = []

    print(f""[bold blue]Model: {model_name}[/bold blue]"")
    for use_fast in [True, False]:
        nlp = spacy.blank(""en"")
        config = {
            ""model"": {
                ""@architectures"": ""spacy-transformers.TransformerModel.v3"",
                ""name"": model_name,
                ""tokenizer_config"": {
                    ""use_fast"": use_fast,
                    ""model_max_length"": model_max_length,
                },
                ""get_spans"": {""@span_getters"": ""spacy-transformers.doc_spans.v1""},
            },
        }
        nlp.add_pipe(""transformer"", config=config)
        nlp.initialize()

        # tokenizer = nlp.get_pipe(""transformer"").model.tokenizer
        # print(f""[bold blue]Tokenizer: {type(tokenizer)}[/bold blue]"")

        doc = nlp(text)

        wordpieces_strings.append(doc._.trf_data.wordpieces.strings[0])
        wordpieces_input_ids.append(
            tuple(doc._.trf_data.wordpieces.input_ids[0].tolist())
        )
        wordpieces_attention_mask.append(
            tuple(doc._.trf_data.wordpieces.attention_mask[0].tolist())
        )

        model_output_last_hidden_state.append(
            doc._.trf_data.model_output[""last_hidden_state""].squeeze(0).shape
        )
        align_data.append(tuple(doc._.trf_data.align.data.flatten().tolist()))
        align_lengths.append(tuple(doc._.trf_data.align.lengths.tolist()))

    if wordpieces_strings[0] != wordpieces_strings[1]:
        print(""[red]Different wordpieces_strings[/red]"")

    if wordpieces_input_ids[0] != wordpieces_input_ids[1]:
        print(""[red]Different wordpieces_input_ids[/red]"")

    if wordpieces_attention_mask[0] != wordpieces_attention_mask[1]:
        print(""[red]Different wordpieces_attention_mask[/red]"")

    if model_output_last_hidden_state[0] != model_output_last_hidden_state[1]:
        print(""[red]Different model_output_last_hidden_state[/red]"")

    if align_data[0] != align_data[1]:
        print(align_data[0])
        print(align_data[1])
        print(""[red]Different align_data[/red]"")

    if align_lengths[0] != align_lengths[1]:
        print(align_lengths[0])
        print(align_lengths[1])
        print(""[red]Different align_lengths[/red]"")

    print()

print(""[bold purple]Pure huggingface transformers:[/bold purple]"")
print()
for model_name in MODELS:
    print(f""[bold blue]Model: {model_name}[/bold blue]"")
    inp = []
    att = []
    for use_fast in [True, False]:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            model_max_length=model_max_length,
            use_fast=use_fast,
        )

        inputs = tokenizer(text)

        input_ids = tuple(inputs[""input_ids""])
        attention_mask = tuple(inputs[""attention_mask""])

        inp.append(input_ids)
        att.append(attention_mask)

    if inp[0] != inp[1]:
        print(""[red]Different input_ids[/red]"")

    if att[0] != att[1]:
        print(""[red]Different attention masks[/red]"")

```

Output:

```shell
Model: distilroberta-base
(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28)
(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28)
Different align_data
(4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1)
(4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1)
Different align_lengths

Model: BAAI/bge-small-en-v1.5

Model: microsoft/deberta-v3-xsmall
(1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24)
(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24)
Different align_data
(2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 4, 2, 2, 2, 2, 1, 1)
(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1)
Different align_lengths

Model: microsoft/Multilingual-MiniLM-L12-H384
(1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 9, 9, 10, 10, 11, 11, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23)
(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23)
Different align_data
(2, 2, 2, 2, 2, 2, 3, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1)
(1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
Different align_lengths

Pure huggingface transformers:

Model: distilroberta-base
Model: BAAI/bge-small-en-v1.5
Model: microsoft/deberta-v3-xsmall
Model: microsoft/Multilingual-MiniLM-L12-H384
```

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: ubuntu 24
* Python Version Used: 3.12.3
* spaCy Version Used: 3.8.3
* Environment Information:

```
uv pip list | grep ""spacy""                  
spacy                                 3.8.3
spacy-alignments                      0.9.1
spacy-curated-transformers            0.3.0
spacy-legacy                          3.0.12
spacy-loggers                         1.0.5
spacy-lookups-data                    1.0.5
spacy-transformers                    1.3.5
spacy-utils                           0.1.0

uv pip list | grep ""transformers""           
curated-transformers                  0.1.1
spacy-curated-transformers            0.3.0
spacy-transformers                    1.3.5
transformers                          4.36.2
```

## Info about spaCy

- **spaCy version:** 3.8.3
- **Platform:** Linux-6.8.0-51-generic-x86_64-with-glibc2.39
- **Python version:** 3.12.3
- **Pipelines:** en_core_web_trf (3.8.0), en_core_web_sm (3.8.0), en_core_web_lg (3.8.0), en_core_web_md (3.8.0)
",Rassibassi,2855550,open,False,4,2025-01-15T14:26:10+00:00,2025-01-29T19:12:11+00:00,,,1,0,0,0,0,0,0
explosion/spaCy,2789150981,13729,German model lemmatizes punctuation inconsistently,"I'm on Python 3.11 with spacy 3.7.4 and noted an [inconsistent behavior](https://stackoverflow.com/questions/79330953/lemma-of-puncutation-in-spacy/79331038) when I lemmatize my German text.

## How to reproduce the behaviour
```python
import spacy

nlp = spacy.load(""de_core_news_sm"")
doc = nlp(""(Das ist ein Test!)"")
for token in doc:
    print(f""Text: '{token.text}', Lemma: '{token.lemma_}'"")
```

Output:

> Text: '(', Lemma: '--'
> Text: 'Das', Lemma: 'der'
> Text: 'ist', Lemma: 'sein'
> Text: 'ein', Lemma: 'ein'
> Text: 'Test', Lemma: 'Test'
> Text: '!', Lemma: '--'
> Text: ')', Lemma: '--'

However, note the English standard model:

```python
import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""(This is a test!)"")
for token in doc:
    print(f""Text: '{token.text}', Lemma: '{token.lemma_}'"")
```

Output:

> Text: '(', Lemma: '('
> Text: 'This', Lemma: 'this'
> Text: 'is', Lemma: 'be'
> Text: 'a', Lemma: 'a'
> Text: 'test', Lemma: 'test'
> Text: '!', Lemma: '!'
> Text: ')', Lemma: ')'

On StackOverlow, the answer-giver reported the Dutch model treats punctuation as in English.

## Your Environment
* Operating System: Windows
* Python Version Used: 3.11
* spaCy Version Used: 3.7.4
* Environment Information: locally (no venv or container)
",Michael-E-Rose,7095926,open,False,0,2025-01-15T08:33:27+00:00,2025-01-15T08:34:29+00:00,,,1,1,0,0,0,0,0
explosion/spaCy,2783897331,13727,"Update requirements, fixing windows crashes","Previous releases have used newer versions of our `blis` wrapper, but this has had intermittent crashes on Windows I haven't been able to resolve.

I've therefore released a new version of `blis` that allows wider numpy compatibility and vendors the known-good v0.7 version of `blis` we were using before.

This took surprisingly long to do, because I had a lot of trouble getting that version of `blis` to compile on Windows, due to conflicts between `intrin.h` and `cpuid.h`.",honnibal,8059750,closed,False,0,2025-01-13T12:59:03+00:00,2025-01-13T15:39:47+00:00,2025-01-13T15:39:47+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2773016576,13726,Advance NLP with spaCy load issue,"In the the online course https://course.spacy.io/ 
 
import spacy failing to load in exercises (see Section 2 Getting Started)
",natandatasci,194115303,open,False,0,2025-01-07T14:43:26+00:00,2025-01-07T14:43:26+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2759571538,13725,Empty MorphAnalysis Hash differs from Token.morph.key,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->
Hello,

I've trained a Morphologizer and i saw that empty MorphAnalysis (`""""`) actually have the hash value of `""_""`. Is it by design? Because the documentation doesn't mention this.

> key `int` | The hash of the features string.

```python
for i in doc:
    nlp.vocab.strings[i.morph.key] == str(i.morph)
False
False
False
True
False
True
True
False
```

As i use the hash values in a lookup for something, it produced `KeyError`.

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Linux (Debian 12)
* Python Version Used: 3.11
* spaCy Version Used: 3.7.3 (i will train my Morphologizer soon on 3.8.3 to see if that change)
* Environment Information:
",thjbdvlt,109964512,open,False,0,2024-12-26T10:07:16+00:00,2024-12-26T10:07:40+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2753907535,13724,AssertionError when using nlp.rehearse() with `en_core_web_trf`,"## How to reproduce the behaviour
I want to use rehearsal to correct some wrong dependency parsing tree produced by `en_core_web_trf`

I used the code directly from: https://github.com/explosion/spaCy/blob/be29216fe2451adead7f56ccd1db494fd8549dae/spacy/tests/training/test_rehearse.py

I modifed `test_rehearse` to be:
```python
def test_rehearse(component):
    nlp = spacy.load(""en_core_web_trf"")
    _optimize(nlp, component, REHEARSE_DATA, True)
```
and called it with `test_rehearse(""parser"")`

## Your Environment
- **spaCy version:** 3.8.2
- **Platform:** Linux-6.8.0-51-generic-x86_64-with-glibc2.39
- **Python version:** 3.12.7
- **Pipelines:** en_core_web_sm (3.8.0), en_core_web_lg (3.8.0), en_core_web_trf (3.8.0)

## Error Message

I received the following error message:
<img width=""1353"" alt=""image"" src=""https://github.com/user-attachments/assets/3c816829-d1d9-4236-a286-cf0a699101b4"" />",jama1017,32912869,open,False,0,2024-12-21T11:15:50+00:00,2024-12-21T11:15:50+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2752681041,13721,Lazily load cli with module __getattr__,"This is a reland of #12962. That PR broke accesses to `spacy.cli`:
```py
import spacy
spacy.cli # NameError
```
This avoids such a  problem by using module `__getattr__`.",hoodmane,8739626,open,False,6,2024-12-20T12:51:07+00:00,2025-02-13T17:48:41+00:00,,,3,3,0,0,0,0,0
explosion/spaCy,2747655485,13720,Textcat fails for large datasets,"I have a very simple task of binary text classification. What I noticed is that it works as a charm for small dataset (~500-1000 samples), but when I asked my team to annotate more examples (> 3,000), it crashes with GPU OOM error. This is puzzling as I was under impression that the entire dataset should be split into batches (I'm using the same batch size for both large and) and then fed into the model. So, if the model works with a particular batch size, then it should be working with the same batch size regardless of the total number of samples (large or small). Playing with batch size and the `trainer.batcher.size` has had only a minor effect. 

What happens when I use a large data set, it starts training, but then after a few epochs it throws the OOM error. I am also monitoring the GPU memory consumption and it increases with every epoch until it reaches its limit and then throws an error.

I am using `trf` version of textcategorizer with exclusive (binary) labels.  

Obviously I am overlooking something and will be absolutely grateful for any recommendations.


## Your Environment
spaCy version: 3.7.6
Platform: Linux--4.14.285-215.501.amzn2.x86_64-x86-64-with-glibc2.26
Python version: 3.9.4
Pipeline: en_core_web_trf (3.7.3), en_core_web_sm (3.7.1), en_core_web_lg (3.7.1)



",kormilitzin,20815939,closed,True,1,2024-12-18T12:15:18+00:00,2025-01-18T00:02:57+00:00,2024-12-18T13:59:01+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2736995110,13716,[WIP] Investigate Windows crash,,honnibal,8059750,open,False,0,2024-12-12T22:26:23+00:00,2024-12-16T14:17:52+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2730785326,13713,Fix allocation of non-transient strings in StringStore,"* Fix bug in memory-zone code when adding non-transient strings. The error could result in segmentation faults or other memory errors during memory zones if new labels were added to the model.
* Fix handling of new morphological labels within memory zones. Addresses second issue reported in #13684",honnibal,8059750,closed,False,0,2024-12-10T17:48:23+00:00,2024-12-11T12:06:53+00:00,2024-12-11T12:06:53+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2727671172,13712,thinc conflicting versions,"## How to reproduce the behaviour
I had an issue with thinc when updated I numpy: 
`thinc 8.3.2 has requirement numpy<2.1.0,>=2.0.0; python_version >= ""3.9"", but you have numpy 2.1.3.`

Therefore I updated thinc to its latest version `9.1.1` but now I'm having an issue with spaCy 
`spacy 3.8.2 depends on thinc<8.4.0 and >=8.3.0`
## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: WSL Ubuntu 20.04.6
* Python Version Used: Python 3.11
* spaCy Version Used: spacy 3.8.2

",kimlizette,23389957,open,False,1,2024-12-09T16:43:01+00:00,2024-12-10T16:43:18+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2722066769,13710,Unable to finetune transformer based ner model after initial tuning,"### Discussed in https://github.com/explosion/spaCy/discussions/13394

<div type='discussions-op-text'>

<sup>Originally posted by **jlustgarten** March 23, 2024</sup>
<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
Create a transformer ner model
Train it on data using the cfg and cli which auto-saves it
Create a new cfg file that points to your existing model
Try triggering the training using the CLI
You will get a missing config.json error

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
- **spaCy version:** 3.7.2
- **Platform:** Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
- **Python version:** 3.10.13

</div>

This still is occurring with the same text:
Config:
[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = ""pytorch""
seed = 0

[nlp]
lang = ""en""
pipeline = [""transformer"",""ner""]
batch_size = 128
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {""@Tokenizers"":""spacy.Tokenizer.v1""}
vectors = {""@vectors"":""spacy.Vectors.v1""}

[components]

[components.ner]
factory = ""ner""
incorrect_spans_key = null
moves = null
scorer = {""https://github.com/scorers"":""spacy.ner_scorer.v1""}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = ""spacy.TransitionBasedParser.v2""
state_type = ""ner""
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = false
nO = null

[components.ner.model.tok2vec]
@architectures = ""spacy-transformers.TransformerListener.v1""
grad_factor = 1.0
pooling = {""https://github.com/layers"":""reduce_mean.v1""}
upstream = ""*""

[components.transformer]
factory = ""transformer""
max_batch_items = 4096
set_extra_annotations = {""@annotation_setters"":""spacy-transformers.null_annotation_setter.v1""}

[components.transformer.model]
@architectures = ""spacy-transformers.TransformerModel.v3""
name = ""/home/user/Coding/PatientHistory/original_pt_hist_ner""
mixed_precision = false

[components.transformer.model.get_spans]
@span_getters = ""spacy-transformers.strided_spans.v1""
window = 128
stride = 96

[components.transformer.model.grad_scaler_config]

[components.transformer.model.tokenizer_config]
use_fast = true

[components.transformer.model.transformer_config]

[corpora]

[corpora.dev]
@readers = ""spacy.Corpus.v1""
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = ""spacy.Corpus.v1""
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
accumulate_gradient = 4
dev_corpus = ""corpora.dev""
train_corpus = ""corpora.train""
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
patience = 2000
max_epochs = 0
max_steps = 80000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null
before_update = null

[training.batcher]
@batchers = ""spacy.batch_by_padded.v1""
discard_oversize = false
size = 2000
buffer = 256
get_length = null

[training.logger]
@Loggers = ""spacy.ConsoleLogger.v1""
progress_bar = false

[training.optimizer]
https://github.com/optimizers = ""Adam.v1""
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001

[training.optimizer.learn_rate]
https://github.com/schedules = ""warmup_linear.v1""
warmup_steps = 250
total_steps = 200000
initial_rate = 0.00005

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]

Here's the CLI:
python -m spacy train '/home/user/Coding/PatientHistory/refine_pt_hist_ner.cfg' --output '/home/user/Coding/PatientHistory/improved_pt_hist_3_22_2024' --paths.train '/home/user/Coding/PatientHistory/train.spacy' --paths.dev '/home/user/Coding/PatientHistory/test.spacy' --gpu-id 0
Here's the output:
ℹ Saving to output directory:
/home/user/Coding/PatientHistory/improved_pt_hist_3_22_2024
ℹ Using GPU: 0

=========================== Initializing pipeline ===========================
/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
_torch_pytree._register_pytree_node(
/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
_torch_pytree._register_pytree_node(
Traceback (most recent call last):
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
return _run_code(code, main_globals, None,
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/runpy.py"", line 86, in _run_code
exec(code, run_globals)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy/main.py"", line 4, in
setup_cli()
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy/cli/_util.py"", line 87, in setup_cli
command(prog_name=COMMAND)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/click/core.py"", line 1157, in call
return self.main(*args, **kwargs)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/typer/core.py"", line 778, in main
return _main(
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/typer/core.py"", line 216, in _main
rv = self.invoke(ctx)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/click/core.py"", line 1688, in invoke
return _process_result(sub_ctx.command.invoke(sub_ctx))
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/click/core.py"", line 1434, in invoke
return ctx.invoke(self.callback, **ctx.params)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/click/core.py"", line 783, in invoke
return __callback(*args, **kwargs)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/typer/main.py"", line 683, in wrapper
return callback(**use_params) # type: ignore
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy/cli/train.py"", line 54, in train_cli
train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy/cli/train.py"", line 81, in train
nlp = init_nlp(config, use_gpu=use_gpu)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy/training/initialize.py"", line 95, in init_nlp
nlp.initialize(lambda: train_corpus(nlp), sgd=optimizer)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy/language.py"", line 1349, in initialize
proc.initialize(get_examples, nlp=self, **p_settings)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy_transformers/pipeline_component.py"", line 351, in initialize
self.model.initialize(X=docs)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/thinc/model.py"", line 318, in initialize
self.init(self, X=X, Y=Y)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy_transformers/layers/transformer_model.py"", line 131, in init
hf_model = huggingface_from_pretrained(name, tok_cfg, trf_cfg)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/spacy_transformers/layers/transformer_model.py"", line 267, in huggingface_from_pretrained
tokenizer = tokenizer_cls.from_pretrained(str_path, **tok_config)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 752, in from_pretrained
config = AutoConfig.from_pretrained(
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py"", line 1082, in from_pretrained
config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/transformers/configuration_utils.py"", line 644, in get_config_dict
config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/transformers/configuration_utils.py"", line 699, in _get_config_dict
resolved_config_file = cached_file(
File ""/home/user/miniconda3/envs/patienthistoryclassifier/lib/python3.10/site-packages/transformers/utils/hub.py"", line 360, in cached_file
raise EnvironmentError(
OSError: /home/user/Coding/PatientHistory/original_pt_hist_ner does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/user/Coding/PatientHistory/original_pt_hist_ner/None' for available files.",jlustgarten,65241780,open,False,0,2024-12-06T04:55:54+00:00,2024-12-06T04:55:54+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2722050310,13709,Unable to fine-tune previously trained transformer based spaCy NER.,"## How to reproduce the behaviour
Use spacy to fine-tune a base model with a transformer from hugging face: 
python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy

Collect new tagged entries under new sets and set your model location to the output/model-last in a new config:
python -m spacy train fine_tune_config.cfg --output ./fine_tune_output --paths.train ./newtrain.spacy --paths.dev ./newdev.spacy

You will get an error about a missing config.json.  Even replacing this will then lead to an error of a missing tokenizer.

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 11
- **spaCy version:** 3.7.2
- **Platform:** Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35
- **Python version:** 3.10.13
",jlustgarten,65241780,open,False,1,2024-12-06T04:46:11+00:00,2024-12-06T04:57:26+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2720042313,13708,en_core_web_md and en_core_web_lg make Python crash silently on Windows,"When I try to use the models `en_core_web_md `or `en_core_web_lg` Python silently crashes.
However `en_core_web_sm ` runs fine with no problems!
There are no Python exceptions (tried Debugging).
But the crash causes an entry in the Windows Event Protocol:
```
Faulting application name: python3.10.exe, version: 3.10.11150.1013, timestamp: 0x642cc42e
Faulting module name: cy.cp310-win_amd64.pyd, version: 0.0.0.0, timestamp: 0x66e36da3
Exception code: 0xc0000005
Fault offset: 0x000000000008e9ea
Faulting process ID: 0x5ed4
Faulting application start time: 0x01db4497b25c334c
Faulting application path: C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\python3.10.exe
Faulting module path: c:\Users\Felix\Documents\Entwicklung\usecase-scoring\clean_env\lib\site-packages\blis\cy.cp310-win_amd64.pyd
Report ID: 8dbf93b3-cb53-46c1-ba02-e51170db537a
Full package name: PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0
Application ID relative to faulting package: Python
```
So it has something to do with the Windows version of the blis-Dependency for spaCy, already tried different versions of it without success.

## How to reproduce the behaviour
I'm just trying to do a basic check if a String consists of a noun and a verb, there appears to be no error:
```
    def _internalValidation(self, s: str) -> bool:
        # Put space between camel case word combinations
        s = re.sub(r'(?<=[a-z])(?=[A-Z])', ' ', s)
        
        # Load string into natural language processing model
        doc = self.nlp(s)
        
        # Tokenize the string
        words = [token for token in doc]
        
        # Invalid if there are less than 2 words
        if len(words) < 2:
            return False
        
        # Check if there is a noun and a verb
        hasNoun = any(x.pos_ == 'NOUN' for x in words)
        hasVerb = any(x.pos_ == 'VERB' for x in words)

        return hasNoun and hasVerb
```


## Your Environment
**Info about spaCy**

- **spaCy version:** 3.8.2
- **Platform:** Windows-10-10.0.19045-SP0
- **Python version:** 3.12.7
- **Pipelines:** en_core_web_sm (3.8.0), en_core_web_sm (3.8.0), en_core_web_lg (3.8.0)

**System specs:**
- AMD Ryzen 3700X
- 32 GB RAM
- NVIDIA GeForce RTX 3070
",felixlennart,54703850,open,False,1,2024-12-05T10:41:13+00:00,2024-12-17T15:48:16+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2719819411,13707,numpy requirements/compatibility,"I noticed that conda-forge is struggling with some of the current numpy specifications to the point that they're having to patch the requirements (https://github.com/conda-forge/thinc-feedstock/pull/123).

To improve this and restore numpy v1 compatibility, could you consider using numpy's suggested build+install requirements as described here? https://numpy.org/doc/stable/dev/depending_on_numpy.html#numpy-2-0-specific-advice

For python 3.9+ only, I think this could look like:

```
[build-system]
requires = [
    ""numpy>=2.0.0"",  # or additionally with <3.0 if you want
]
build-backend = ""setuptools.build_meta""
```

```
install_requires =
    numpy>=1.19.3
```

And if you wanted to support earlier python, you could use `oldest-supported-numpy` for simplicity (which is less concerning now that it's effectively archived/frozen, plus restricted to EOL python):

```
[build-system]
requires = [
    ""oldest-supported-numpy; python_version < '3.9'"",
    ""numpy>=2.0.0; python_version >= '3.9'"",
]
build-backend = ""setuptools.build_meta""
```

(And if you're already taking a look at requirements, I think you could also consider restricting blis to `<0.9` for windows only? The segfaults are a known issue and we found some but not all of the related bugs back then, which is why we reverted all the attempted blis upgrades after a short time.)",adrianeboyd,5794899,open,False,2,2024-12-05T09:13:23+00:00,2024-12-16T06:31:18+00:00,,,1,1,0,0,0,0,0
explosion/spaCy,2718224981,13706,Numpy is not available error running spaCy 3.8.2,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

Getting intermittent ""Numpy is not available"" error when utilizing the spaCy 3.8.2 library. It is unclear exactly where this error is occurring.

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
- **spaCy version:** 3.8.2
- **Platform:** Windows-2022Server-10.0.20348-SP0
- **Python version:** 3.12.3
- **Pipelines:** en_core_web_lg (3.8.0), en_core_web_md (3.8.0), en_core_web_sm (3.8.0), en_core_web_trf (3.8.0)

Thanks.",rshahrabani,37419900,open,False,0,2024-12-04T16:15:36+00:00,2024-12-04T16:15:36+00:00,,,2,2,0,0,0,0,0
explosion/spaCy,2717368488,13705, Improving Korean NER model accuracy by including particles in entity annotations,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->
Body:

When training a Named Entity Recognition (NER) model for Korean using spaCy, I've encountered an interesting phenomenon that significantly affects the model's accuracy. I'm using the updated NER pipeline to create the configuration file.

Issue:
The model's accuracy improves substantially when I include particles (조사, josa) along with the annotated nouns in the training data.

Example:

    Lower accuracy: <PERSON>김철수</PERSON>가 왔다.
    Higher accuracy: <PERSON>김철수가</PERSON> 왔다.


In the second example, the particle ""가"" is included within the entity annotation.

Questions:

    Is this a known behavior for Korean NER models in spaCy?
    Are there any best practices or recommendations for handling particles in Korean NER annotations?
    How might this affect the model's performance on texts where particles may vary or be omitted?
    Are there any potential drawbacks to this approach that I should be aware of?


I would appreciate any insights, explanations, or suggestions on how to best approach this issue while maintaining the integrity and flexibility of the NER model for Korean language processing.

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
config.cfg：

[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = ""ko""
pipeline = [""tok2vec"",""tagger"",""morphologizer"",""parser"",""lemmatizer"",""senter"",""attribute_ruler"",""ner""]
disabled = [""senter""]
before_creation = null
after_creation = null
after_pipeline_creation = null
batch_size = 256
tokenizer = {""@tokenizers"":""spacy.Tokenizer.v1""}
vectors = {""@vectors"":""spacy.Vectors.v1""}

[components]

[components.attribute_ruler]
source = ""ko_core_news_lg""

[components.lemmatizer]
source = ""ko_core_news_lg""

[components.morphologizer]
source = ""ko_core_news_lg""

[components.ner]
source = ""ko_core_news_lg""
replace_listeners = [""model.tok2vec""]

[components.parser]
source = ""ko_core_news_lg""

[components.senter]
source = ""ko_core_news_lg""

[components.tagger]
source = ""ko_core_news_lg""

[components.tok2vec]
source = ""ko_core_news_lg""

[corpora]

[corpora.dev]
@readers = ""spacy.Corpus.v1""
path = ${paths.dev}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

[corpora.train]
@readers = ""spacy.Corpus.v1""
path = ${paths.train}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

[training]
train_corpus = ""corpora.train""
dev_corpus = ""corpora.dev""
seed = ${system:seed}
gpu_allocator = ${system:gpu_allocator}
dropout = 0.1
accumulate_gradient = 1
patience = 5000
max_epochs = 0
max_steps = 100000
eval_frequency = 1000
frozen_components = [""tok2vec"",""tagger"",""morphologizer"",""parser"",""lemmatizer"",""senter"",""attribute_ruler""]
before_to_disk = null
annotating_components = []
before_update = null

[training.batcher]
@batchers = ""spacy.batch_by_words.v1""
discard_oversize = false
tolerance = 0.2
get_length = null

[training.batcher.size]
@schedules = ""compounding.v1""
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = ""spacy.ConsoleLogger.v3""
console_output = true
output_file = ""/media//trainer_log.jsonl""

[training.optimizer]
@optimizers = ""Adam.v1""
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = true
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
tag_acc = 0.1
pos_acc = 0.1
morph_acc = 0.09
morph_per_feat = null
dep_uas = 0.0
dep_las = 0.29
dep_las_per_type = null
sents_p = null
sents_r = null
sents_f = 0.04
lemma_acc = 0.1
ents_f = 0.29
ents_p = 0.0
ents_r = 0.0
ents_per_type = null
speed = 0.0

[pretraining]

[initialize]
vocab_data = null
vectors = null
init_tok2vec = ${paths.init_tok2vec}
after_init = null
lookups = null

[initialize.before_init]
@callbacks = ""spacy.copy_from_base_model.v1""
tokenizer = ""ko_core_news_lg""
vocab = ""ko_core_news_lg""

[initialize.components]

[initialize.components.lemmatizer]

[initialize.components.lemmatizer.labels]
@readers = ""spacy.read_labels.v1""
path = ""corpus/labels/trainable_lemmatizer.json""
require = false

[initialize.components.morphologizer]

[initialize.components.morphologizer.labels]
@readers = ""spacy.read_labels.v1""
path = ""corpus/labels/morphologizer.json""
require = false

[initialize.components.ner]

[initialize.components.ner.labels]
@readers = ""spacy.read_labels.v1""
path = ""corpus/labels/ner.json""
require = false

[initialize.components.parser]

[initialize.components.parser.labels]
@readers = ""spacy.read_labels.v1""
path = ""corpus/labels/parser.json""
require = false

[initialize.components.tagger]

[initialize.components.tagger.labels]
@readers = ""spacy.read_labels.v1""
path = ""corpus/labels/tagger.json""
require = false

[initialize.tokenizer]

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: ubuntu 22.04
* Python Version Used: 3.10
* spaCy Version Used: 3.8
* Environment Information:
## Info about spaCy

- **spaCy version:** 3.8.2
- **Platform:** Linux-6.8.0-49-generic-x86_64-with-glibc2.35
- **Python version:** 3.10.15
- **Pipelines:** ko_core_news_lg (3.8.0)


",peacefulluo,34934135,open,False,1,2024-12-04T11:07:11+00:00,2024-12-20T07:17:36+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2716758331,13704,Idle shell restarts when loading en_core_web_sm on Macbook Pro (M4 Pro),"I just got a MB Pro (M4Pro) and migrated from MB Air (2018). I have installed spacy 3.9.6 with en_core_web_sm (3.8.0). On the new MB, I had to downgrade Numpy to 1.21.0. In the Python IDLE, I can import spacy as:
`import spacy`
However, the shell restarts once I run the command:
`nlp = spacy.load('en_core_web_sm')`
Is there anything I can do?",gio3b2,91661309,closed,True,2,2024-12-04T07:14:52+00:00,2025-03-29T00:03:05+00:00,2025-02-26T09:44:01+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2707915143,13702,remove linalg backend as per #13701,"https://github.com/explosion/spaCy/issues/13701

I have not tested this due to some external issues, but the edits were compared against https://github.com/explosion/spaCy/pull/11292/files#diff-f19f042dd7350fc29ab39ab36483a189e482a86592f20bb5b07f0d085306bd9d",envolution,12188773,open,False,2,2024-11-30T19:53:09+00:00,2025-03-06T21:05:06+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2707898916,13701,reference to obsolete thinc backend linalg,"```
ml/parser_model.pyx
line8: from thinc.backends.linalg cimport Vec, VecVec
```
https://github.com/explosion/thinc/pull/742

was obsoleted a couple of years ago, which is causing some downstream issues:
```
  File ""/usr/lib/python3.12/site-packages/auralis/models/xttsv2/config/tokenizer.py"", line 12, in <module>
    from spacy.lang.ar import Arabic
  File ""/usr/lib/python3.12/site-packages/spacy/__init__.py"", line 13, in <module>
    from . import pipeline  # noqa: F401
    ^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.12/site-packages/spacy/pipeline/__init__.py"", line 2, in <module>
    from .dep_parser import DependencyParser
  File ""spacy/pipeline/dep_parser.pyx"", line 1, in init spacy.pipeline.dep_parser
  File ""spacy/pipeline/transition_parser.pyx"", line 1, in init spacy.pipeline.transition_parser
  File ""/usr/lib/python3.12/site-packages/spacy/ml/__init__.py"", line 2, in <module>
    from .models import *  # noqa: F401, F403
    ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.12/site-packages/spacy/ml/models/__init__.py"", line 3, in <module>
    from .parser import *  # noqa
    ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/lib/python3.12/site-packages/spacy/ml/models/parser.py"", line 11, in <module>
    from ..tb_framework import TransitionModel
  File ""/usr/lib/python3.12/site-packages/spacy/ml/tb_framework.py"", line 4, in <module>
    from .parser_model import ParserStepModel
  File ""spacy/ml/parser_model.pyx"", line 1, in init spacy.ml.parser_model
ModuleNotFoundError: No module named 'thinc.backends.linalg'
```
## Your Environment
Arch linux
Name: spacy
Version: 3.8.2
Summary: Industrial-strength Natural Language Processing (NLP) in Python
Home-page: https://spacy.io
Author: Explosion
Author-email: contact@explosion.ai
License: MIT
Location: /usr/lib/python3.12/site-packages
Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel
Required-by: auralis

* Operating System: Arch Linux
* Python Version Used: 3.12.7
* spaCy Version Used: 3.8.2 binary from pypi
* Environment Information:
",envolution,12188773,closed,True,2,2024-11-30T19:27:58+00:00,2025-01-11T00:03:05+00:00,2024-12-11T12:55:44+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2705779382,13700,"Error: Cannot use GPU, Cupy is not installed ",I already installed but showing same error: how to resolve this issue,Gouss-shaikh,113688039,open,False,0,2024-11-29T17:38:28+00:00,2024-11-29T17:38:28+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2688983454,13694,Improve Ligurian tokenization,"## Description

* Corrected one spelling mistake in the example text.
* Improved suffixes/prefixes:
  * support elision in years (e.g. _1990_ -> _’90_),
  * don't split the degree symbol when it's part of a degree unit (e.g. _°C_),
  * don't split left apostrophes for the few words that can have elision occur on the left (_’na_, _’n_, _’n’_).
* Improved handling of special cases: 
  * handle compound prepositions (e.g. _a-a_, _co-i_) in a way that doesn't break compatibility with how they're dealt with in Universal Dependencies (using `NORM` as described in https://github.com/explosion/spaCy/issues/1460),
  * handle cases such as _°C_, and generate the correct `NORM` forms for cases such as _’na_.
* Added tests for all of the above.
*  Added a few extra stop words, including variants with the curly quote / typographic apostrophe (commonly found in Ligurian corpora).

### Types of change

Enhanced support for the Ligurian language.

## Checklist
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",jeanm,107696,open,False,0,2024-11-25T03:56:36+00:00,2024-11-25T03:56:36+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2673943210,13692,[v3.8.2] Segmentation Fault when running lemmatisation (Windows),"## Update
I've done some digging and this only seems to affect v3.8. Downgrading to v3.7 fixes the problem.

The only 3.8 version I've tried is 3.8.2 so I'm unsure if 3.8.0 / 3.8.1 are also affected.

## Overview
When running `spacy.language.Language` in a script on Windows, it randomly produces a segmentation fault (the behaviour in powershell is to stop execution of script and you need to run it in bash to see the ""segmentation fault"" error). This error does NOT appear on macOS, even in identical environments.

There appears to be no link between the text input and the crashes since:
- It crashes on a different sentence each time
- It processes the same sentence perfectly fine on subsequent runs

I've tracked it down to `spacy.language.Language` by isolating it with `logging` statements on either side of the function call. The error is not caught by a try/except block.

Three examples of sentences that it has crashed on:

```
le do la camera del sole al primo piano.
marco is offering you a drink.
non c'è niente qua giù.
```

The model being used is `it-core-news-lg==3.8.0`.
Update: The crash occurs on the English large model too.

Any advice is appreciated!

## How to reproduce the behaviour

Simplified lemmatizer class:

```python
import spacy

class ClassName():
    _nlp: spacy.language.Language

    def __init__(self, model_name: str):
        self._nlp = spacy.load(name=model_name)

    def lemmatize(self, input_str : str):

        # Random crashes on this line
        # Try / except doesn't make any difference
        doc = self._nlp(text=input_str) 
        
        # Do stuff
        return stuff 
```

Simplified application code

``` python
lemmatizer = ClassName(model_name=""it-core-news-lg"")
for sentence in big_sentence_list:
    x = lemmatizer.lemmatize(sentence)
```

Actual class being used is [here](https://github.com/jonathanfox5/lemon_tizer/blob/main/src/lemon_tizer/main.py)
Actual application code is [here](https://github.com/jonathanfox5/gogadget/blob/main/gogadget/frequency_analyser.py), within the `generate_frequency_analysis` function.

The code crashes on ~20% of the runs, even with identical input data. Each run has subtitles from ~100 minutes worth of mixed Italian / English content.

## Your Environment
- **spaCy version:** 3.8.2
- **Platform:** Windows-11-10.0.26100-SP0
- **Python version:** 3.12.7
- **Model**:  `it-core-news-lg==3.8.0`
- **System**: Running on CPU: Ryzen 5600X, 32 GB RAM, Running on GPU: RTX 3070 Ti
- **pip list** (from application code):

```
aiohappyeyeballs              2.4.3
aiohttp                       3.11.6
aiosignal                     1.3.1
alembic                       1.14.0
annotated-types               0.7.0
antlr4-python3-runtime        4.9.3
argos-spacy-compatibility     0.1.0
asteroid-filterbanks          0.4.0
attrs                         24.2.0
audioread                     3.0.1
av                            12.3.0
blis                          1.0.1
cached-property               2.0.1
catalogue                     2.0.10
certifi                       2024.8.30
cffi                          1.17.1
charset-normalizer            3.4.0
chevron                       0.14.0
click                         8.1.7
cloudpathlib                  0.20.0
colorama                      0.4.6
coloredlogs                   15.0.1
colorlog                      6.9.0
confection                    0.1.5
contourpy                     1.3.1
ctranslate2                   4.5.0
cycler                        0.12.1
cymem                         2.0.8
decorator                     5.1.1
docopt                        0.6.2
einops                        0.8.0
et-xmlfile                    2.0.0
faster-whisper                1.0.3
ffmpeg-python                 0.2.0
filelock                      3.16.1
flatbuffers                   24.3.25
fonttools                     4.55.0
frozendict                    2.4.6
frozenlist                    1.5.0
fsspec                        2024.10.0
future                        1.0.0
genanki                       0.13.1
gogadget                      0.2.2
greenlet                      3.1.1
huggingface-hub               0.26.2
humanfriendly                 10.0
hyperpyyaml                   1.2.2
idna                          3.10
jinja2                        3.1.4
joblib                        1.4.2
julius                        0.2.7
kiwisolver                    1.4.7
langcodes                     3.5.0
language-data                 1.3.0
lazy-loader                   0.4
lemon-tizer                   0.0.5
librosa                       0.10.2.post1
lightning                     2.4.0
lightning-utilities           0.11.9
llvmlite                      0.43.0
mako                          1.3.6
marisa-trie                   1.2.1
markdown-it-py                3.0.0
markupsafe                    3.0.2
matplotlib                    3.9.2
mdurl                         0.1.2
mpmath                        1.3.0
msgpack                       1.1.0
multidict                     6.1.0
murmurhash                    1.0.10
networkx                      3.4.2
nltk                          3.9.1
numba                         0.60.0
numpy                         2.0.2
omegaconf                     2.3.0
onnxruntime                   1.20.0
openpyxl                      3.1.5
optuna                        4.1.0
packaging                     24.2
pandas                        2.2.3
pillow                        11.0.0
pip                           24.3.1
platformdirs                  4.3.6
pooch                         1.8.2
preshed                       3.0.9
primepy                       1.3
propcache                     0.2.0
protobuf                      5.28.3
pyannote-audio                3.3.2
pyannote-core                 5.0.0
pyannote-database             5.1.0
pyannote-metrics              3.2.1
pyannote-pipeline             3.0.1
pycparser                     2.22
pydantic                      2.9.2
pydantic-core                 2.23.4
pygments                      2.18.0
pyparsing                     3.2.0
pyreadline3                   3.5.4
pysubs2                       1.7.3
python-dateutil               2.9.0.post0
pytorch-lightning             2.4.0
pytorch-metric-learning       2.7.0
pytz                          2024.2
pyyaml                        6.0.2
regex                         2024.11.6
requests                      2.32.3
rich                          13.9.4
rtoml                         0.11.0
ruamel-yaml                   0.18.6
ruamel-yaml-clib              0.2.12
sacremoses                    0.0.53
safetensors                   0.4.5
scikit-learn                  1.5.2
scipy                         1.14.1
semver                        3.0.2
sentencepiece                 0.2.0
setuptools                    75.5.0
shellingham                   1.5.4
six                           1.16.0
smart-open                    7.0.5
sortedcontainers              2.4.0
soundfile                     0.12.1
soxr                          0.5.0.post1
spacy                         3.8.2
spacy-legacy                  3.0.12
spacy-loggers                 1.0.5
speechbrain                   1.0.2
sqlalchemy                    2.0.36
srsly                         2.4.8
sympy                         1.13.1
tabulate                      0.9.0
tensorboardx                  2.6.2.2
thinc                         8.3.2
threadpoolctl                 3.5.0
tokenizers                    0.20.3
tomlkit                       0.13.2
torch                         2.5.1
torch-audiomentations         0.11.1
torch-pitch-shift             1.2.5
torchaudio                    2.5.1
torchmetrics                  1.6.0
tqdm                          4.67.0
transformers                  4.46.3
typer                         0.13.1
typing-extensions             4.12.2
tzdata                        2024.2
urllib3                       2.2.3
wasabi                        1.1.3
weasel                        0.4.1
whisperx-numpy2-compatibility 0.1.0
wrapt                         1.16.0
yarl                          1.17.2
yt-dlp                        2024.11.18
```
",jonathanfox5,32023524,open,False,2,2024-11-19T23:59:47+00:00,2024-12-11T12:57:47+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2655565493,13690,"403 Server Error: Downloading `en_core_web_sm` fails due to ""Compatibility table not found for Spacy v3.7.5""","<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
Not sure if it's easily reproducible (it isn't even for me consistently), but during `docker build`, downloading the ` en_core_web_sm` package fails with a 403 Server error, on occasion. With the `--no-cache` option during `docker build`, it still fails occasionally, and retrying the build often leads to a success, but not always. This has only started happening since sometime early this week or late last week, there's been no code change at all for the build or for requirements, and I've never faced this issue prior to now. spaCy claims it can't find the compatibility table, but clearly it can when it doesn't fail.

### Failing command
```
python -m spacy download en_core_web_sm
```

### Error message
```
81.32 ✘ Server error (403)
81.32 Couldn't fetch compatibility table. Please find a package for your spaCy
81.32 installation (v3.7.5), and download it manually. For more details, see the
81.32 documentation: https://spacy.io/usage/models
```

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Ubuntu 24.04 LTS
* Python Version Used: 3.11
* spaCy Version Used: 3.7.5
* Environment Information: Conda environment within Docker, spaCy is downloaded with `pip install -r requirements.txt` 


",mkh1991,14204128,open,False,9,2024-11-13T13:40:51+00:00,2024-12-09T08:52:27+00:00,,,7,7,0,0,0,0,0
explosion/spaCy,2648418523,13689,"""invalid whitespace entity spans"" error while validation training and test data for NER ","<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour

I have use the following piece of code to convert json to spacy while validationg using spacy --debug i get whitespace error:

![image](https://github.com/user-attachments/assets/b2176cf3-35d1-4d0d-81d0-94c284bc219d)


please help me how to resolve this

for text, annot in tqdm(TRAIN_DATA['annotations']):
    doc = nlp.make_doc(text)
    ents = []
    for start, end, label in annot[""entities""]:
        span = doc.char_span(start, end, label=label, alignment_mode=""contract"")
        if span is None:
            print(""Skipping entity"")
        else:
            ents.append(span)
    doc.ents = ents
    db.add(doc)
db.to_disk(""train_data.spacy"")


## Info about spaCy

- **spaCy version:** 3.7.5
- **Platform:** Linux-6.1.85+-x86_64-with-glibc2.35
- **Python version:** 3.10.12
- **Pipelines:** en_core_web_lg (3.7.1), en_core_web_sm (3.7.1)

* Operating System:
* Python Version Used:
* spaCy Version Used:
* Environment Information:
",abrarsharif66,89507014,open,False,1,2024-11-11T07:25:54+00:00,2024-11-11T07:27:36+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2639141635,13686,"ValueError: spacy.strings.StringStore size changed, may indicate binary incompatibility without python version or packages change","## How to reproduce the behaviour
It happened suddenly after an image rebuild.

Our dockerfile looks something like this, from nvidia CUDA 12.6.2 ubuntu 22.04 image

```Dockerfile
FROM nvidia/cuda:12.6.2-cudnn-runtime-ubuntu22.04

# Upgrade ubuntu packages
# Install dependencies with pip base on pyproject.toml (see complete dump at the end)

CMD [""poetry"", ""run"", ""gunicorn"", ""nlp.api.predict:app"", ""-c"" ,""config/gunicorn.config.py""]
```

With the python file `predict.py` (2 files simplified into a single one)
```python
import spacy
from fastapi import FastAPI

class Pipeline:
    def __init__(self) -> None:
        self.model_path = None
        self.nlp = None
        self.get_and_load_model()

    def get_and_load_model(self):
        self.get_model_from_bucket()

    def load_model(self):
        spacy.require_gpu(0)
        self.nlp = spacy.load(self.model_path)

    def get_model_from_bucket(self) -> spacy.Language:
        self.model_path = self.download_folder_files()
        self.load_model()

    def download_folder_files(self) -> str:
        self.model_path = ""/tmp/model""
        # Download ML model from bucket to local folder /tmp/model/

app = FastAPI()
pipeline = Pipeline()
```

I get the following error while spawning the server:
```shell
Traceback (most recent call last):
  File ""/usr/local/lib/python3.10/dist-packages/gunicorn/arbiter.py"", line 609, in spawn_worker
    worker.init_process()
  File ""/usr/local/lib/python3.10/dist-packages/uvicorn/workers.py"", line 66, in init_process
    super(UvicornWorker, self).init_process()
  File ""/usr/local/lib/python3.10/dist-packages/gunicorn/workers/base.py"", line 134, in init_process
    self.load_wsgi()
  File ""/usr/local/lib/python3.10/dist-packages/gunicorn/workers/base.py"", line 146, in load_wsgi
    self.wsgi = self.app.wsgi()
  File ""/usr/local/lib/python3.10/dist-packages/gunicorn/app/base.py"", line 67, in wsgi
    self.callable = self.load()
  File ""/usr/local/lib/python3.10/dist-packages/gunicorn/app/wsgiapp.py"", line 58, in load
    return self.load_wsgiapp()
  File ""/usr/local/lib/python3.10/dist-packages/gunicorn/app/wsgiapp.py"", line 48, in load_wsgiapp
    return util.import_app(self.app_uri)
  File ""/usr/local/lib/python3.10/dist-packages/gunicorn/util.py"", line 371, in import_app
    mod = importlib.import_module(module)
  File ""/usr/lib/python3.10/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/opt/app/nlp/api/predict.py"", line 17, in <module>
    pipeline = Pipeline()
  File ""/opt/app/nlp/api/pipeline.py"", line 40, in __init__
    self.get_and_load_model()
  File ""/opt/app/nlp/api/pipeline.py"", line 55, in get_and_load_model
    self.get_model_from_bucket()
  File ""/opt/app/nlp/api/pipeline.py"", line 77, in get_model_from_bucket
    self.load_model()
  File ""/opt/app/nlp/api/pipeline.py"", line 63, in load_model
    self.nlp = spacy.load(self.model_path)
  File ""/usr/local/lib/python3.10/dist-packages/spacy/__init__.py"", line 51, in load
    return util.load_model(
  File ""/usr/local/lib/python3.10/dist-packages/spacy/util.py"", line 467, in load_model
    return load_model_from_path(Path(name), **kwargs)  # type: ignore[arg-type]
  File ""/usr/local/lib/python3.10/dist-packages/spacy/util.py"", line 539, in load_model_from_path
    nlp = load_model_from_config(
  File ""/usr/local/lib/python3.10/dist-packages/spacy/util.py"", line 587, in load_model_from_config
    nlp = lang_cls.from_config(
  File ""/usr/local/lib/python3.10/dist-packages/spacy/language.py"", line 1855, in from_config
    nlp = lang_cls(
  File ""/usr/local/lib/python3.10/dist-packages/spacy/language.py"", line 188, in __init__
    util.registry._entry_point_factories.get_all()
  File ""/usr/local/lib/python3.10/dist-packages/catalogue/__init__.py"", line 110, in get_all
    result.update(self.get_entry_points())
  File ""/usr/local/lib/python3.10/dist-packages/catalogue/__init__.py"", line 125, in get_entry_points
    result[entry_point.name] = entry_point.load()
  File ""/usr/local/lib/python3.10/dist-packages/importlib_metadata/__init__.py"", line 189, in load
    module = import_module(match.group('module'))
  File ""/usr/lib/python3.10/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1050, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 1027, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 1006, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 688, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 883, in exec_module
  File ""<frozen importlib._bootstrap>"", line 241, in _call_with_frames_removed
  File ""/opt/app/nlp/architecture/ner.py"", line 7, in <module>
    from .entity_recognizer import EntityRecognizer
  File ""nlp/architecture/entity_recognizer.pyx"", line 1, in init nlp.architecture.entity_recognizer
    # cython: infer_types=True, profile=True, binding=True
ValueError: spacy.strings.StringStore size changed, may indicate binary incompatibility. Expected 96 from C header, got 64 from PyObject
```

We tried: 
- Upgrading Spacy to 3.8.2 but it does not work because this version is incompatible with `cupy-cuda12x` because of Numpy, see https://github.com/explosion/spaCy/issues/13669
- Downgrade numpy to 1.22, not working better than 1.26.4 even though https://github.com/explosion/spaCy/issues/13528 raises the same error


## Your Environment
- **spaCy version:** 3.7.5
- **Platform:** Linux-6.10.4-linuxkit-x86_64-with-glibc2.35
- **Python version:** 3.10.12
- **Python packages:**
```shell
Package                           Version       Editable project location
--------------------------------- ------------- -------------------------
accelerate                        0.25.0
aiohttp                           3.9.5
aiosignal                         1.3.1
altair                            5.3.0
anyio                             4.4.0
async-timeout                     4.0.3
attrs                             23.2.0
black                             24.4.2
blinker                           1.8.2
blis                              0.7.11
build                             1.2.2.post1
CacheControl                      0.14.1
cachetools                        5.4.0
catalogue                         2.0.10
certifi                           2024.7.4
cffi                              1.16.0
charset-normalizer                3.3.2
cleo                              2.1.0
click                             8.1.7
cloudpathlib                      0.18.1
confection                        0.1.5
coverage                          7.6.0
crashtest                         0.4.1
cryptography                      43.0.1
cupy-cuda12x                      12.3.0
cymem                             2.0.8
Cython                            0.29.37
datasets                          2.20.0
dill                              0.3.8
distlib                           0.3.9
docstring_parser                  0.16
dulwich                           0.21.7
exceptiongroup                    1.2.2
fastapi                           0.110.3
fastjsonschema                    2.20.0
fastrlock                         0.8.2
filelock                          3.15.4
freezegun                         1.5.1
frozenlist                        1.4.1
fsspec                            2024.5.0
gitdb                             4.0.11
GitPython                         3.1.43
google-api-core                   2.19.1
google-api-python-client          2.137.0
google-auth                       2.32.0
google-auth-httplib2              0.2.0
google-cloud-aiplatform           1.59.0
google-cloud-appengine-logging    1.4.4
google-cloud-audit-log            0.2.5
google-cloud-bigquery             3.25.0
google-cloud-core                 2.4.1
google-cloud-error-reporting      1.11.0
google-cloud-kms                  2.24.1
google-cloud-logging              3.10.0
google-cloud-pipeline-components  2.15.0
google-cloud-profiler             4.1.0
google-cloud-resource-manager     1.12.4
google-cloud-secret-manager       2.20.1
google-cloud-storage              2.17.0
google-crc32c                     1.5.0
google-resumable-media            2.7.1
googleapis-common-protos          1.63.2
grpc-google-iam-v1                0.13.1
grpcio                            1.64.1
grpcio-status                     1.62.2
gunicorn                          22.0.0
h11                               0.14.0
httplib2                          0.22.0
huggingface-hub                   0.23.4
idna                              3.7
importlib_metadata                8.5.0
iniconfig                         2.0.0
installer                         0.7.0
jaraco.classes                    3.4.0
jeepney                           0.8.0
Jinja2                            3.1.4
jsonschema                        4.23.0
jsonschema-specifications         2023.12.1
keyring                           24.3.1
kfp                               2.7.0
kfp-pipeline-spec                 0.3.0
kfp-server-api                    2.0.5
kubernetes                        26.1.0
langcodes                         3.4.0
language_data                     1.2.0
marisa-trie                       1.2.0
markdown-it-py                    3.0.0
MarkupSafe                        2.1.5
mdurl                             0.1.2
mock                              5.1.0
more-itertools                    10.5.0
mpmath                            1.3.0
msgpack                           1.1.0
multidict                         6.0.5
multiprocess                      0.70.16
murmurhash                        1.0.10
mypy-extensions                   1.0.0
networkx                          3.3
nlp                               0.3.6         /opt/app
numpy                             1.26.4
nvidia-cublas-cu12                12.1.3.1
nvidia-cuda-cupti-cu12            12.1.105
nvidia-cuda-nvrtc-cu12            12.1.105
nvidia-cuda-runtime-cu12          12.1.105
nvidia-cudnn-cu12                 8.9.2.26
nvidia-cufft-cu12                 11.0.2.54
nvidia-curand-cu12                10.3.2.106
nvidia-cusolver-cu12              11.4.5.107
nvidia-cusparse-cu12              12.1.0.106
nvidia-nccl-cu12                  2.19.3
nvidia-nvjitlink-cu12             12.5.82
nvidia-nvtx-cu12                  12.1.105
oauthlib                          3.2.2
packaging                         24.1
pandas                            2.2.2
pathspec                          0.12.1
pexpect                           4.9.0
pillow                            10.4.0
pip                               24.3.1
pkginfo                           1.11.2
platformdirs                      4.2.2
pluggy                            1.5.0
poetry                            1.8.4
poetry-core                       1.9.1
poetry-plugin-export              1.8.0
preshed                           3.0.9
prometheus_client                 0.20.0
prometheus-fastapi-instrumentator 5.11.2
proto-plus                        1.24.0
protobuf                          4.25.3
psutil                            6.0.0
ptyprocess                        0.7.0
pyarrow                           16.1.0
pyarrow-hotfix                    0.6
pyasn1                            0.6.0
pyasn1_modules                    0.4.0
pycparser                         2.22
pydantic                          1.10.17
pydeck                            0.9.1
Pygments                          2.18.0
pynvml                            11.5.2
pyparsing                         3.1.2
pyproject_hooks                   1.2.0
pytest                            7.4.4
pytest-cov                        4.1.0
pytest-mock                       3.14.0
python-dateutil                   2.9.0.post0
python-decouple                   3.8
python-dotenv                     1.0.1
pytz                              2024.1
PyYAML                            6.0.1
RapidFuzz                         3.10.1
referencing                       0.35.1
regex                             2024.5.15
requests                          2.32.3
requests-oauthlib                 2.0.0
requests-toolbelt                 0.10.1
rich                              13.7.1
rpds-py                           0.19.0
rsa                               4.9
safetensors                       0.4.3
SecretStorage                     3.3.3
sentencepiece                     0.1.99
setuptools                        70.3.0
shapely                           2.0.5
shellingham                       1.5.4
simplejson                        3.19.2
six                               1.16.0
smart-open                        7.0.4
smmap                             5.0.1
sniffio                           1.3.1
spacy                             3.7.5
spacy-alignments                  0.9.1
spacy-legacy                      3.0.12
spacy-loggers                     1.0.5
spacy-transformers                1.3.5
srsly                             2.4.8
starlette                         0.37.2
streamlit                         1.36.0
sympy                             1.13.0
tabulate                          0.9.0
tenacity                          8.5.0
thinc                             8.2.5
tokenizers                        0.15.2
toml                              0.10.2
tomli                             2.0.1
tomlkit                           0.13.2
toolz                             0.12.1
torch                             2.2.0
tornado                           6.4.1
tqdm                              4.66.4
transformers                      4.36.2
triton                            2.2.0
trove-classifiers                 2024.10.21.16
typer                             0.9.4
typing_extensions                 4.12.2
tzdata                            2024.1
uritemplate                       4.1.1
urllib3                           1.26.19
uvicorn                           0.25.0
virtualenv                        20.27.1
wasabi                            1.1.3
watchdog                          4.0.1
weasel                            0.4.1
websocket-client                  1.8.0
wheel                             0.42.0
wrapt                             1.16.0
xxhash                            3.4.1
yarl                              1.9.4
zipp                              3.20.2
```
",antoninguilet,155458656,open,False,0,2024-11-06T20:27:25+00:00,2024-11-06T20:27:25+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2633527685,13684,Memory leak of MorphAnalysis object.,"I have encountered a crucial bug, which makes running a continuous tokenization using Japanese tokenizer close to impossible. It's all due so memory leak of MorphAnalysis
## How to reproduce the behaviour
```
import spacy
import tracemalloc


tracemalloc.start()
tokenizer = spacy.blank(""ja"")
tokenizer.add_pipe(""sentencizer"")

for _ in range(1000):
    text = "" "".join([""a""] * 1000)
    snapshot = tracemalloc.take_snapshot()
    with tokenizer.memory_zone():
        doc = tokenizer(text)
        tokenizer.max_length = len(text) + 10
    import gc
    gc.collect()
    snapshot2 = tracemalloc.take_snapshot()
    # Compare the two snapshots
    p_stats = snapshot2.compare_to(snapshot, ""lineno"")
    # Pretty print the top 10 differences
    print(""[ Top 10 ]"")
    # Stop here with pdb
    for stat in p_stats[:10]:
        if stat.size_diff > 0:


            print(stat)
```

Run this script and observe how memory keeps growing:
![image](https://github.com/user-attachments/assets/9c6ef7b2-c243-43bd-af28-71da92dcbe21)
It all happens due to the this line:
`token.morph = MorphAnalysis(self.vocab, morph)`. I have checked the implementation itself and there is neither code for dealocation implemented, nor it supports the memory_zone.
",hynky1999,39408646,open,False,3,2024-11-04T18:18:58+00:00,2024-12-28T14:07:29+00:00,,,1,1,0,0,0,0,0
explosion/spaCy,2624341915,13681,SpaCy 3.8.2 CUDA extras are not compatible with numpy 2.0,"We wanted to use GPUs for spacy after recently upgrading to `spacy v3.8`, but this was impossible due to conflicting requirements. The `spacy[cudaXXX]` still depend on `cupy < 13.0.0`, which is incompatible with numpy 2.0. Support for this has been added in `cupy 13.2` back in june.

## How to reproduce the behaviour
Try to install `spacy[cuda12x]==3.8.2`. 
<details><summary>Details</summary>
<p>

```
ERROR: Cannot install spacy and spacy[cuda12x]==3.8.2 because these package versions have conflicting dependencies.

The conflict is caused by:
    spacy 3.8.2 depends on numpy>=1.19.0; python_version >= ""3.9""
    spacy[cuda12x] 3.8.2 depends on numpy>=1.19.0; python_version >= ""3.9""
    cupy-cuda12x 11.5.0 depends on numpy<1.26 and >=1.20
    thinc 8.3.2 depends on numpy<2.1.0 and >=2.0.0; python_version >= ""3.9""
    spacy 3.8.2 depends on numpy>=1.19.0; python_version >= ""3.9""
    spacy[cuda12x] 3.8.2 depends on numpy>=1.19.0; python_version >= ""3.9""
    cupy-cuda12x 11.5.0 depends on numpy<1.26 and >=1.20
    thinc 8.3.1 depends on numpy<2.1.0 and >=2.0.0; python_version >= ""3.9""
    spacy 3.8.2 depends on numpy>=1.19.0; python_version >= ""3.9""
    spacy[cuda12x] 3.8.2 depends on numpy>=1.19.0; python_version >= ""3.9""
    cupy-cuda12x 11.5.0 depends on numpy<1.26 and >=1.20
    thinc 8.3.0 depends on numpy<2.1.0 and >=2.0.0; python_version >= ""3.9""
```
</p>
</details> 

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Ubuntu
* Python Version Used: 3.10
* spaCy Version Used: 3.8.2 (failed to install)
* Environment Information:
",CptCaptain,1939455,open,False,2,2024-10-30T14:43:52+00:00,2024-11-09T05:06:29+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2618349236,13680,Spaces impacting tag/pos,"## How to reproduce the behaviour

Notice the double space in front of `sourire` in the first case vs. the single space in the second case

`Les publics avec un  sourire chaleureux et`

<img width=""1277"" alt=""image"" src=""https://github.com/user-attachments/assets/9bdb2aca-8741-41d5-995e-2333aa392158"">

https://demos.explosion.ai/displacy?text=Les%20publics%20avec%20un%20%20sourire%20chaleureux%20%20et&model=fr_core_news_sm

vs.

`Les publics avec un sourire chaleureux et`

<img width=""1282"" alt=""image"" src=""https://github.com/user-attachments/assets/e43870e6-115a-42ca-9d2b-39c9446ed212"">

https://demos.explosion.ai/displacy?text=Les%20publics%20avec%20un%20sourire%20chaleureux%20%20et&model=fr_core_news_sm

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:
* Python Version Used: 3.12
* spaCy Version Used: v3.5 (displacy) but also in v3.7
* Environment Information:

Semi-related: Any guidance on how to modify the tokenizer so that a double spaces would be placed into `whitespace_` (ie. ` `) and not lead to a `SPACE` token? I did take note of https://github.com/explosion/spaCy/issues/1707 though putting the additional spaces into `whitespace_` seems more logical to me.

## Research

a) Maybe related https://github.com/explosion/spaCy/issues/621
b) Semi-related https://stephantul.github.io/spacy/2019/05/01/tokenizationspacy/
c) Semi-related https://github.com/explosion/spaCy/discussions/9978",lsmith77,300279,open,False,1,2024-10-28T12:55:22+00:00,2024-11-12T04:11:26+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2617577404,13679,Pipeline for opentapioca not working,"### Discussed in https://github.com/explosion/spaCy/discussions/13678

<div type='discussions-op-text'>

<sup>Originally posted by **piaschwarz** October 25, 2024</sup>
I am using opentapioca for entity linking. 
The code worked before but now calling the opentapioca endpoint throws an error (happens with _and_ without the endpoint url in the config variable).

Is it a problem on my side or is the endpoint not working?

**Versions:**
Python 3.10.6
spacy                        3.3.1
spacy-transformers           1.1.7
spacyopentapioca             0.1.7

**My code**:
```
import spacy

nlp_spacy_trf = spacy.load('de_dep_news_trf')
dummy_text = ""Christian Drosten arbeitet an der Charité in Berlin.""

nlp_spacy_trf.add_pipe('opentapioca', config={""url"": ""https://opentapioca.wordlift.io/api/annotate?lc=de""})
doc = nlp_spacy_trf(dummy_text)
for span in doc.ents:
     print((span.text, span.kb_id_, span.label_, span._.description, span._.score))
```

keeps throwing this **error**: 
```
Traceback (most recent call last):
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 703, in urlopen
    httplib_response = self._make_request(
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 386, in _make_request
    self._validate_conn(conn)
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 1042, in _validate_conn
    conn.connect()
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/urllib3/connection.py"", line 414, in connect
    self.sock = ssl_wrap_socket(
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/urllib3/util/ssl_.py"", line 449, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/urllib3/util/ssl_.py"", line 493, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File ""/usr/lib/python3.10/ssl.py"", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File ""/usr/lib/python3.10/ssl.py"", line 1071, in _create
    self.do_handshake()
  File ""/usr/lib/python3.10/ssl.py"", line 1342, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:997)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/requests/adapters.py"", line 489, in send
    resp = conn.urlopen(
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 787, in urlopen
    retries = retries.increment(
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/urllib3/util/retry.py"", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='opentapioca.wordlift.io', port=443): Max retries exceeded with url: /api/annotate?lc=de (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:997)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/mnt/c/Users/XXXXX/PycharmProjects/EntityLinking/experiments.py"", line 10, in <module>
    doc = nlp_spacy_trf(dummy_text)
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/spacy/language.py"", line 1025, in __call__
    error_handler(name, proc, [doc], e)
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/spacy/util.py"", line 1630, in raise_error
    raise e
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/spacy/language.py"", line 1020, in __call__
    doc = proc(doc, **component_cfg.get(name, {}))  # type: ignore[call-arg]
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/spacyopentapioca/entity_linker.py"", line 101, in __call__
    r = self.make_request(doc)
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/spacyopentapioca/entity_linker.py"", line 93, in make_request
    return requests.post(url=self.url,
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/requests/api.py"", line 115, in post
    return request(""post"", url, data=data, json=json, **kwargs)
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/requests/api.py"", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/requests/sessions.py"", line 587, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/requests/sessions.py"", line 701, in send
    r = adapter.send(request, **kwargs)
  File ""/home/xxxxxxxx/.local/lib/python3.10/site-packages/requests/adapters.py"", line 563, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='opentapioca.wordlift.io', port=443): Max retries exceeded with url: /api/annotate?lc=de (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:997)')))
```</div>",piaschwarz,38730179,open,False,0,2024-10-28T07:40:29+00:00,2024-10-28T07:40:29+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2599636499,13673,`spacy download nl_core_news_sm` downgrades transformers installation,"Since models are in fact pip modules, it makes sense that they have their own dependencies. However, I was very surprised to find out that `nl_core_news_sm` required me to downgrade my `transformers` version. I am running on the main branch of `transformers` so ahead of 4.45.2 (`4.46.0.dev0`). yet when installing `nl-core-news-sm` I find the transformers version to be downgraded to the pip release. To me that sounds like a bug but maybe this is intended behavior to avoid conflicts for the average user.

As a power user that restriction is a bit too strong, though. As per semver, no breaking changes ought to be introduced with minor version bumps so it would be surprising to see major shifts. (Disclaimer: I'm not sure how closely HF follows semver.)

spaCy version    3.8.2
Location         /home/local/vanroy/defgen/.venv/lib/python3.10/site-packages/spacy
Platform         Linux-5.14.0-427.20.1.el9_4.x86_64-x86_64-with-glibc2.34
Python version   3.10.15
Pipelines        nl_core_news_sm (3.8.0)

If the restriction cannot be relieved a bit, do you have another suggestion to by-pass this? I am willing to build things from source if needed, though I am not sure how to do that with the mode files.",BramVanroy,2779410,open,False,0,2024-10-19T21:45:34+00:00,2024-10-19T21:45:34+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2599596578,13672,"Issue with detecting ""être"" in verb lemmatization using SpaCy","**Description of the problem:**

I am encountering an issue with lemmatizing verbs using SpaCy. My goal is to convert all conjugated verbs to their infinitive forms. However, when analyzing sentences containing the verb ""être,"" such as ""Je suis en train de manger,"" the conjugated verb ""suis"" is not detected as a verb. Only the infinitive form of ""manger"" is identified.

**Code snippet:**

```python
import spacy

# Load the French model from SpaCy
nlp = spacy.load(""fr_core_news_md"")

def find_conjugated_verbs_and_infinitives(phrase):
    doc = nlp(phrase)  # Analyze the phrase with SpaCy
    conjugated_verbs = [token.text for token in doc if token.pos_ == ""VERB""]
    infinitives = [token.lemma_ for token in doc if token.pos_ == ""VERB""]
    return conjugated_verbs, infinitives

# Phrase to analyze
phrase = ""Je suis en train de manger""
conjugated_verbs, infinitives = find_conjugated_verbs_and_infinitives(phrase)
print(""Conjugated verbs found:"", conjugated_verbs)
print(""Infinitives found:"", infinitives)
```

**Obtained result:**
Conjugated verbs found: ['manger']
Infinitives found: ['manger']

**Expected result:**
Conjugated verbs found: ['suis', 'manger']
Infinitives found: ['être', 'manger']
",souksili,105323111,closed,True,2,2024-10-19T20:58:04+00:00,2024-12-07T00:03:16+00:00,2024-10-20T16:06:48+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2598105630,13669,Numpy 2.0 update causing issues with other software,"Forcing numpy to 2.0+ forces breaking changes on other software. Unless it is absolutely required to use 2.0+, can this be reverted?

https://github.com/explosion/spaCy/commit/184e508d9c8db6fc89c60e5ec8e94324817259c9",Josh-XT,102809327,open,False,5,2024-10-18T18:05:18+00:00,2024-12-10T16:57:26+00:00,,,4,4,0,0,0,0,0
explosion/spaCy,2597831887,13668,Python 3.12 support in conda ,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->
Hi Team, we are trying to support AutoGluon in Python 3.12 on Conda, but `spaCy`, as one of our dependencies, does not seem to support python 3.12 on Conda yet. Do you have recent plans to add it? Your assistance is much appreciated!

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
conda create --name ag312 python=3.12
conda install -c conda-forge spacy

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Linux
* Python Version Used: Python3.12
* spaCy Version Used: 
* Environment Information:
",suzhoum,11562485,open,False,1,2024-10-18T15:30:19+00:00,2025-01-20T10:34:44+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2588993614,13663,Hyphenated words in French,"## How to reproduce the behaviour
`j'imagine des grands-pères` is tokanized to `j'`, `imagine`, `des`, `grands-`, `pères`

https://demos.explosion.ai/displacy?text=j%27imagine%20des%20grands-p%C3%A8res&model=fr_core_news_sm&cpu=1&cph=1

I would expect it to tokanize to `j'`, `imagine`, `des`, `grands-pères`, ie. not split `grands-pères`.

In English `he is a top-performer` does not split `top-performer`

https://demos.explosion.ai/displacy?text=he%20is%20a%20top-performer&model=en_core_web_sm&cpu=1&cph=1

Is this intended or a bug?
If it is intended, how can I adjust the tokanization to not split hyphened words?

",lsmith77,300279,closed,True,3,2024-10-15T14:39:36+00:00,2024-11-16T00:03:16+00:00,2024-10-16T09:26:02+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2587142063,13662,model load error:Could not find: alms,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
## Info about spaCy

- **spaCy version:** 3.7.5
- **Platform:** macOS-10.16-x86_64-i386-64bit
- **Python version:** 3.9.13
- **Pipelines:** en_core_web_sm (3.7.1), en_core_web_md (3.7.1)

Spacy used to work fine, though I haven't used it recently.

Notebook code

import spacy

import en_core_web_sm
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""Apple is looking at buying U.K. startup for $1 billion"")

returns error:
      3 import en_core_web_sm
----> 4 nlp = spacy.load(""en_core_web_sm"")
      5 doc = nlp(""Apple is looking at buying U.K. startup for $1 billion"")

AttributeError: Could not find: llms

How do I resolve?

Andy",andyhegedus,44418310,open,False,3,2024-10-14T21:52:26+00:00,2024-10-15T16:12:31+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2584637910,13660,Create ErrorCode.py,"Process finished with exit code -1073741819 (0xC0000005) #13659

I have tried to update the code. Please check once if its working. Previously I have not update the code correctly in your repository. Please accept my pull request if its working and fixing the problem. Also please let me know if I can do something further more.
",milansamuel609,75712741,closed,False,0,2024-10-14T03:31:19+00:00,2024-10-15T05:07:04+00:00,2024-10-15T05:07:04+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2583642647,13659,Process finished with exit code -1073741819 (0xC0000005),"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->

```python
nlp = spacy.load('en_core_web_lg')
with open('data/1971 Davis Cup.txt', encoding='utf-8') as file:
    for line in file:
        line = line.strip()
        if not line:
            continue
        doc = nlp(line)
        tokens = [token.text for token in doc]
        dependencies = set()
        for token in doc:
            for child in token.children:
                dependencies.add((token.i, token.dep_, child.i))
        print(tokens, dependencies)
```

The error occurs during parsing of the 2nd line of the file.

[1971 Davis Cup.txt](https://github.com/user-attachments/files/17353154/1971.Davis.Cup.txt)

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->

- **spaCy version:** 3.8.2
- **Platform:** Windows-11-10.0.22631-SP0
- **Python version:** 3.12.7
- **Pipelines:** en_core_web_lg (3.8.0), en_core_web_sm (3.8.0)

```plaintext
Package              Version
-------------------- ---------
annotated-types      0.7.0
beautifulsoup4       4.12.3
blinker              1.8.2
blis                 1.0.1
catalogue            2.0.10
certifi              2024.8.30
charset-normalizer   3.3.2
click                8.1.7
cloudpathlib         0.19.0
colorama             0.4.6
confection           0.1.5
cymem                2.0.8
dash                 2.18.1
dash-core-components 2.0.0
dash-html-components 2.0.0
dash-table           5.0.0
en_core_web_lg       3.8.0
en_core_web_sm       3.8.0
filelock             3.16.1
Flask                3.0.3
fsspec               2024.9.0
gensim               4.3.3
graphviz             0.20.3
idna                 3.10
importlib_metadata   8.5.0
itsdangerous         2.2.0
Jinja2               3.1.4
langcodes            3.4.1
language_data        1.2.0
marisa-trie          1.2.1
markdown-it-py       3.0.0
MarkupSafe           2.1.5
mdurl                0.1.2
mpmath               1.3.0
murmurhash           1.0.10
neo4j                5.25.0
nest-asyncio         1.6.0
networkx             3.4.1
numpy                1.26.4
packaging            24.1
pip                  23.2.1
plotly               5.24.1
preshed              3.0.9
pydantic             2.9.2
pydantic_core        2.23.4
Pygments             2.18.0
pytz                 2024.2
requests             2.32.3
retrying             1.3.4
rich                 13.9.2
scipy                1.13.1
setuptools           75.1.0
shellingham          1.5.4
six                  1.16.0
smart-open           7.0.5
soupsieve            2.6
spacy                3.8.2
spacy-legacy         3.0.12
spacy-loggers        1.0.5
srsly                2.4.8
sympy                1.13.3
tenacity             9.0.0
thinc                8.3.2
torch                2.4.1
tqdm                 4.66.5
typer                0.12.5
typing_extensions    4.12.2
urllib3              2.2.3
wasabi               1.1.3
weasel               0.4.1
Werkzeug             3.0.4
wikipedia            1.4.0
wrapt                1.16.0
zipp                 3.20.2

```",hosford42,4564403,open,False,9,2024-10-13T04:42:11+00:00,2024-12-10T18:30:55+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2582657163,13658,Spacy installation on python 3.13 fails,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
`pip3.13 install spacy`

C:\Users\talta\AppData\Local\Programs\Python\Python313>pip3.13 install spacy

`Collecting spacy
  Downloading spacy-3.8.2.tar.gz (1.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 5.0 MB/s eta 0:00:00
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [118 lines of output]
      Ignoring numpy: markers 'python_version < ""3.9""' don't match your environment
      Collecting setuptools
        Downloading setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)
      Collecting cython<3.0,>=0.25
        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Using cached cymem-2.0.8-cp313-cp313-win_amd64.whl
      Collecting preshed<3.1.0,>=3.0.2
        Using cached preshed-3.0.9-cp313-cp313-win_amd64.whl
      Collecting murmurhash<1.1.0,>=0.28.0
        Using cached murmurhash-1.0.10-cp313-cp313-win_amd64.whl
      Collecting thinc<8.4.0,>=8.3.0
        Downloading thinc-8.3.2.tar.gz (193 kB)
        Installing build dependencies: started
        Installing build dependencies: still running...
        Installing build dependencies: still running...
        Installing build dependencies: still running...
        Installing build dependencies: still running...
        Installing build dependencies: still running...
        Installing build dependencies: finished with status 'error'
        error: subprocess-exited-with-error

        pip subprocess to install build dependencies did not run successfully.
        exit code: 1

        [81 lines of output]
        Ignoring numpy: markers 'python_version < ""3.9""' don't match your environment
        Collecting setuptools
          Using cached setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)
        Collecting cython<3.0,>=0.25
          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
        Collecting murmurhash<1.1.0,>=1.0.2
          Using cached murmurhash-1.0.10-cp313-cp313-win_amd64.whl
        Collecting cymem<2.1.0,>=2.0.2
          Using cached cymem-2.0.8-cp313-cp313-win_amd64.whl
        Collecting preshed<3.1.0,>=3.0.2
          Using cached preshed-3.0.9-cp313-cp313-win_amd64.whl
        Collecting blis<1.1.0,>=1.0.0
          Downloading blis-1.0.1.tar.gz (3.6 MB)
             ---------------------------------------- 3.6/3.6 MB 7.7 MB/s eta 0:00:00
          Installing build dependencies: started
          Installing build dependencies: finished with status 'done'
          Getting requirements to build wheel: started
          Getting requirements to build wheel: finished with status 'done'
          Preparing metadata (pyproject.toml): started
          Preparing metadata (pyproject.toml): finished with status 'done'
        Collecting numpy<2.1.0,>=2.0.0
          Downloading numpy-2.0.2.tar.gz (18.9 MB)
             ---------------------------------------- 18.9/18.9 MB 7.0 MB/s eta 0:00:00
          Installing build dependencies: started
          Installing build dependencies: finished with status 'done'
          Getting requirements to build wheel: started
          Getting requirements to build wheel: finished with status 'done'
          Installing backend dependencies: started
          Installing backend dependencies: finished with status 'done'
          Preparing metadata (pyproject.toml): started
          Preparing metadata (pyproject.toml): still running...
          Preparing metadata (pyproject.toml): still running...
          Preparing metadata (pyproject.toml): still running...
          Preparing metadata (pyproject.toml): still running...
          Preparing metadata (pyproject.toml): finished with status 'done'
        Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)
        Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)
        Building wheels for collected packages: blis, numpy
          Building wheel for blis (pyproject.toml): started
          Building wheel for blis (pyproject.toml): finished with status 'error'
          error: subprocess-exited-with-error

          Building wheel for blis (pyproject.toml) did not run successfully.
          exit code: 1

          [24 lines of output]
          BLIS_COMPILER? None
          running bdist_wheel
          running build
          running build_py
          creating build\lib.win-amd64-cpython-313\blis
          copying blis\about.py -> build\lib.win-amd64-cpython-313\blis
          copying blis\benchmark.py -> build\lib.win-amd64-cpython-313\blis
          copying blis\__init__.py -> build\lib.win-amd64-cpython-313\blis
          creating build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\common.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\conftest.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\test_dotv.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\test_gemm.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\__init__.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\cy.pyx -> build\lib.win-amd64-cpython-313\blis
          copying blis\py.pyx -> build\lib.win-amd64-cpython-313\blis
          copying blis\cy.pxd -> build\lib.win-amd64-cpython-313\blis
          copying blis\__init__.pxd -> build\lib.win-amd64-cpython-313\blis
          running build_ext
          Build options win32 msvc
          BUILD ARCH: x86_64
          {'!EXITCODE': '00000000', 'ACLOCAL_PATH': 'C:\\Program Files\\Git\\mingw64\\share\\aclocal;C:\\Program Files\\Git\\usr\\share\\aclocal', 'AGENT_BUILDDIRECTORY': 'D:\\a\\1', 'AGENT_DISABLELOGPLUGIN_TESTFILEPUBLISHERPLUGIN': 'true', 'AGENT_DISABLELOGPLUGIN_TESTRESULTLOGPLUGIN': 'false', 'AGENT_HOMEDIRECTORY': 'C:\\agents\\2.202.0', 'AGENT_ID': '92',  'TEMP': 'C:\\Users\\VSSADM~1\\AppData\\Local\\Temp', 'TERM': 'xterm-256color', 'TF_BUILD': 'True', 'TMP': 'C:\\Users\\VSSADM~1\\AppData\\Local\\Temp', 'TMPDIR': 'C:\\Users\\VSSADM~1\\AppData\\Local\\Temp', 'USEPYTHONVERSION_PYTHONLOCATION': 'C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64', 'USERDOMAIN': 'WIN-CU8INV6766V', 'USERDOMAIN_ROAMINGPROFILE': 'WIN-CU8INV6766V', 'USERNAME': 'VssAdministrator', 'USERPROFILE': 'C:\\Users\\VssAdministrator', 'VCPKG_INSTALLATION_ROOT': 'C:\\vcpkg', 'VSTS_AGENT_PERFLOG': 'C:\\agents\\perflog', 'VSTS_PROCESS_LOOKUP_ID': 'vsts_175962b3-f397-42b7-b557-a072c6b9de45', 'VSTS_SECRET_VARIABLES': '', 'WINDIR': 'C:\\Windows', 'WIX': 'C:\\Program Files (x86)\\WiX Toolset v3.11\\', '_': 'C:/hostedtoolcache/windows/Python/3.8.10/x64/python', 'AGENT.JOBSTATUS': 'Succeeded', 'NPM_CONFIG_PREFIX': 'C:\\npm\\prefix'}
          [COMMAND] C:\Program Files\LLVM\bin\clang.exe -c C:\Users\talta\AppData\Local\Temp\pip-install-kh08oxiy\blis_df788e7abdee4a1a89b9e3d09171d711\blis\_src\config\bulldozer\bli_cntx_init_bulldozer.c -o C:\Users\talta\AppData\Local\Temp\tmpbfr5y8sq\bli_cntx_init_bulldozer.o -O2 -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.9.0"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude\windows-x86_64 -I.\frame\3\ -I.\frame\1m\ -I.\frame\1f\ -I.\frame\1\ -I.\frame\include -IC:\Users\talta\AppData\Local\Temp\pip-install-kh08oxiy\blis_df788e7abdee4a1a89b9e3d09171d711\blis\_src\include\windows-x86_64
          error: [WinError 2] The system cannot find the file specified
          [end of output]

          note: This error originates from a subprocess, and is likely not a problem with pip.
          ERROR: Failed building wheel for blis
          Building wheel for numpy (pyproject.toml): started
          Building wheel for numpy (pyproject.toml): finished with status 'done'
          Created wheel for numpy: filename=numpy-2.0.2-cp313-cp313-win_amd64.whl size=6700177 sha256=5263dc88014c220c1ebf5dae21f711e712a047da860c13e47e2a172a15ef19fd
          Stored in directory: c:\users\talta\appdata\local\pip\cache\wheels\bc\ef\e8\cf84dd8a34d77dcede062417e099659e1f48d17c8befac28c9
        Successfully built numpy
        Failed to build blis
        ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (blis)
        [end of output]

        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: subprocess-exited-with-error

      pip subprocess to install build dependencies did not run successfully.
      exit code: 1

      See above for output.

      note: This error originates from a subprocess, and is likely not a problem with pip.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.

C:\Users\talta\AppData\Local\Programs\Python\Python313>
C:\Users\talta\AppData\Local\Programs\Python\Python313>pip3.13 install jupyter
Collecting jupyter
  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)
Collecting notebook (from jupyter)
  Downloading notebook-7.2.2-py3-none-any.whl.metadata (10 kB)
Collecting jupyter-console (from jupyter)
  Downloading jupyter_console-6.6.3-py3-none-any.whl.metadata (5.8 kB)
Collecting nbconvert (from jupyter)
  Downloading nbconvert-7.16.4-py3-none-any.whl.metadata (8.5 kB)
Collecting ipykernel (from jupyter)
  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)
Collecting ipywidgets (from jupyter)
  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)
Collecting jupyterlab (from jupyter)
  Downloading jupyterlab-4.2.5-py3-none-any.whl.metadata (16 kB)
Collecting comm>=0.1.1 (from ipykernel->jupyter)
  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)
Collecting debugpy>=1.6.5 (from ipykernel->jupyter)
  Downloading debugpy-1.8.7-cp313-cp313-win_amd64.whl.metadata (1.1 kB)
Collecting ipython>=7.23.1 (from ipykernel->jupyter)
  Downloading ipython-8.28.0-py3-none-any.whl.metadata (5.0 kB)
Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter)
  Downloading jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)
Collecting jupyter-core!=5.0.*,>=4.12 (from ipykernel->jupyter)
  Downloading jupyter_core-5.7.2-py3-none-any.whl.metadata (3.4 kB)
Collecting matplotlib-inline>=0.1 (from ipykernel->jupyter)
  Downloading matplotlib_inline-0.1.7-py3-none-any.whl.metadata (3.9 kB)
Collecting nest-asyncio (from ipykernel->jupyter)
  Downloading nest_asyncio-1.6.0-py3-none-any.whl.metadata (2.8 kB)
Collecting packaging (from ipykernel->jupyter)
  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)
Collecting psutil (from ipykernel->jupyter)
  Downloading psutil-6.0.0-cp37-abi3-win_amd64.whl.metadata (22 kB)
Collecting pyzmq>=24 (from ipykernel->jupyter)
  Downloading pyzmq-26.2.0-cp313-cp313-win_amd64.whl.metadata (6.2 kB)
Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)
  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)
Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter)
  Downloading jupyter_server-2.14.2-py3-none-any.whl.metadata (8.4 kB)
Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter)
  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)
Collecting notebook-shim>=0.2 (from jupyterlab->jupyter)
  Downloading notebook_shim-0.2.4-py3-none-any.whl.metadata (4.0 kB)
Collecting setuptools>=40.1.0 (from jupyterlab->jupyter)
  Using cached setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)
Collecting beautifulsoup4 (from nbconvert->jupyter)
  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)
Collecting bleach!=5.0.0 (from nbconvert->jupyter)
  Downloading bleach-6.1.0-py3-none-any.whl.metadata (30 kB)
Collecting defusedxml (from nbconvert->jupyter)
  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)
Collecting jupyterlab-pygments (from nbconvert->jupyter)
  Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl.metadata (4.4 kB)
Collecting markupsafe>=2.0 (from nbconvert->jupyter)
  Downloading MarkupSafe-3.0.1-cp313-cp313-win_amd64.whl.metadata (4.1 kB)
Collecting mistune<4,>=2.0.3 (from nbconvert->jupyter)
  Downloading mistune-3.0.2-py3-none-any.whl.metadata (1.7 kB)
Collecting nbclient>=0.5.0 (from nbconvert->jupyter)
  Downloading nbclient-0.10.0-py3-none-any.whl.metadata (7.8 kB)
Collecting nbformat>=5.7 (from nbconvert->jupyter)
  Downloading nbformat-5.10.4-py3-none-any.whl.metadata (3.6 kB)
Collecting pandocfilters>=1.4.1 (from nbconvert->jupyter)
  Downloading pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)
Collecting tinycss2 (from nbconvert->jupyter)
  Downloading tinycss2-1.3.0-py3-none-any.whl.metadata (3.0 kB)
Requirement already satisfied: six>=1.9.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from bleach!=5.0.0->nbconvert->jupyter) (1.16.0)
Collecting webencodings (from bleach!=5.0.0->nbconvert->jupyter)
  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)
Collecting anyio (from httpx>=0.25.0->jupyterlab->jupyter)
  Downloading anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)
Collecting certifi (from httpx>=0.25.0->jupyterlab->jupyter)
  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)
Collecting httpcore==1.* (from httpx>=0.25.0->jupyterlab->jupyter)
  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)
Requirement already satisfied: idna in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.8)
Collecting sniffio (from httpx>=0.25.0->jupyterlab->jupyter)
  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)
Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter)
  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)
Collecting decorator (from ipython>=7.23.1->ipykernel->jupyter)
  Using cached decorator-5.1.1-py3-none-any.whl.metadata (4.0 kB)
Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter)
  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)
adata (1.9 kB)
Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)
Downloading ipykernel-6.29.5-py3-none-any.whl (117 kB)
Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)
Downloading jupyter_console-6.6.3-py3-none-any.whl (24 kB)
Downloading jupyterlab-4.2.5-py3-none-any.whl (11.6 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 10.0 MB/s eta 0:00:00
Downloading nbconvert-7.16.4-py3-none-any.whl (257 kB)
Downloading notebook-7.2.2-py3-none-any.whl (5.0 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.0/5.0 MB 15.7 MB/s eta 0:00:00
Downloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)
Downloading bleach-6.1.0-py3-none-any.whl (162 kB)
Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)
Downloading debugpy-1.8.7-cp313-cp313-win_amd64.whl (5.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 15.8 MB/s eta 0:00:00
Downloading httpx-0.27.2-py3-none-any.whl (76 kB)
Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)
Downloading ipython-8.28.0-py3-none-any.whl (819 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 819.5/819.5 kB 13.8 MB/s eta 0:00:00
Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)
Downloading jupyter_client-8.6.3-py3-none-any.whl (106 kB)
Downloading jupyter_core-5.7.2-py3-none-any.whl (28 kB)
Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)
Downloading jupyter_server-2.14.2-py3-none-any.whl (383 kB)
Downloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)
Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)
Downloading MarkupSafe-3.0.1-cp313-cp313-win_amd64.whl (15 kB)
Downloading matplotlib_inline-0.1.7-py3-none-any.whl (9.9 kB)
Downloading mistune-3.0.2-py3-none-any.whl (47 kB)
Downloading nbclient-0.10.0-py3-none-any.whl (25 kB)
Downloading nbformat-5.10.4-py3-none-any.whl (78 kB)
Downloading notebook_shim-0.2.4-py3-none-any.whl (13 kB)
Using cached packaging-24.1-py3-none-any.whl (53 kB)
Downloading pandocfilters-1.5.1-py2.py3-none-any.whl (8.7 kB)
Downloading prompt_toolkit-3.0.48-py3-none-any.whl (386 kB)
Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 14.0 MB/s eta 0:00:00
Downloading pyzmq-26.2.0-cp313-cp313-win_amd64.whl (637 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 637.6/637.6 kB 10.4 MB/s eta 0:00:00
Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)
Downloading tornado-6.4.1-cp38-abi3-win_amd64.whl (438 kB)
Downloading traitlets-5.14.3-py3-none-any.whl (85 kB)
Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 12.8 MB/s eta 0:00:00
Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)
Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)
Downloading jupyterlab_pygments-0.3.0-py3-none-any.whl (15 kB)
Downloading nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 15.3 MB/s eta 0:00:00
Downloading requests-2.32.3-py3-none-any.whl (64 kB)
Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)
Downloading Send2Trash-1.8.3-py3-none-any.whl (18 kB)
Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)
Downloading soupsieve-2.6-py3-none-any.whl (36 kB)
Downloading terminado-0.18.1-py3-none-any.whl (14 kB)
Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)
Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)
Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)
Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)
Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)
Using cached asttokens-2.4.1-py2.py3-none-any.whl (27 kB)
Downloading charset_normalizer-3.4.0-cp313-cp313-win_amd64.whl (102 kB)
Downloading executing-2.1.0-py2.py3-none-any.whl (25 kB)
Using cached h11-0.14.0-py3-none-any.whl (58 kB)
Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)
Downloading parso-0.8.4-py2.py3-none-any.whl (103 kB)
Downloading python_json_logger-2.0.7-py3-none-any.whl (8.1 kB)
Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)
Downloading referencing-0.35.1-py3-none-any.whl (26 kB)
Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)
Downloading rpds_py-0.20.0-cp313-none-win_amd64.whl (214 kB)
Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)
Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-win_amd64.whl (30 kB)
Downloading pure_eval-0.2.3-py3-none-any.whl (11 kB)
Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)
Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)
Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)
Downloading webcolors-24.8.0-py3-none-any.whl (15 kB)
Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)
Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)
Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)
Using cached arrow-1.3.0-py3-none-any.whl (66 kB)
Using cached pycparser-2.22-py3-none-any.whl (117 kB)
Downloading types_python_dateutil-2.9.0.20241003-py3-none-any.whl (9.7 kB)
Building wheels for collected packages: pywinpty
  Building wheel for pywinpty (pyproject.toml) ... done
  Created wheel for pywinpty: filename=pywinpty-2.0.13-cp313-none-win_amd64.whl size=212535 sha256=54c2e44895a13911de518041b6c18d1541ae8fc175578e25f021cefebfef384d
  Stored in directory: c:\users\talta\appdata\local\pip\cache\wheels\a2\49\ee\ee8b8645371f968556431726ee02500e0cb4bcc6d78bcd57e7
Successfully built pywinpty
C:\Users\talta\AppData\Local\Programs\Python\Python313>pip3.13 install jupyterlab
Requirement already satisfied: jupyterlab in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (4.2.5)
Requirement already satisfied: async-lru>=1.0.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (2.0.4)
Requirement already satisfied: httpx>=0.25.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (0.27.2)
Requirement already satisfied: ipykernel>=6.5.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (6.29.5)
Requirement already satisfied: jinja2>=3.0.3 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (3.1.4)
Requirement already satisfied: jupyter-core in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (5.7.2)
Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (2.2.5)
Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (2.14.2)
Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (2.27.3)
Requirement already satisfied: notebook-shim>=0.2 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (0.2.4)
Requirement already satisfied: packaging in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (24.1)
Requirement already satisfied: setuptools>=40.1.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (75.1.0)
Requirement already satisfied: tornado>=6.2.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (6.4.1)
Requirement already satisfied: traitlets in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyterlab) (5.14.3)
talta\appdata\local\programs\python\python313\lib\site-packages (from ipykernel>=6.5.0->jupyterlab) (8.28.0)
Requirement already satisfied: jupyter-client>=6.1.12 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipykernel>=6.5.0->jupyterlab) (8.6.3)
Requirement already satisfied: matplotlib-inline>=0.1 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipykernel>=6.5.0->jupyterlab) (0.1.7)
Requirement already satisfied: nest-asyncio in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipykernel>=6.5.0->jupyterlab) (1.6.0)
Requirement already satisfied: psutil in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipykernel>=6.5.0->jupyterlab) (6.0.0)
Requirement already satisfied: pyzmq>=24 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipykernel>=6.5.0->jupyterlab) (26.2.0)
Requirement already satisfied: MarkupSafe>=2.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jinja2>=3.0.3->jupyterlab) (3.0.1)
Requirement already satisfied: platformdirs>=2.5 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-core->jupyterlab) (4.3.6)
Requirement already satisfied: pywin32>=300 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-core->jupyterlab) (307)
Requirement already satisfied: argon2-cffi>=21.1 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (23.1.0)
Requirement already satisfied: jupyter-events>=0.9.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.10.0)
Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.5.3)
Requirement already satisfied: nbconvert>=6.4.4 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.16.4)
Requirement already satisfied: nbformat>=5.3.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from 
Requirement already satisfied: jedi>=0.16 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.19.1)
Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (3.0.48)
Requirement already satisfied: pygments>=2.4.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (2.18.0)
Requirement already satisfied: stack-data in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.6.3)
Requirement already satisfied: colorama in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from ipython>=7.23.1->ipykernel>=6.5.0->jupyterlab) (0.4.6)
Requirement already satisfied: attrs>=22.2.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (24.2.0)
Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (2024.10.1)
Requirement already satisfied: referencing>=0.28.4 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.35.1)
Requirement already satisfied: rpds-py>=0.7.1 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.20.0)
Requirement already satisfied: python-dateutil>=2.8.2 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-client>=6.1.12->ipykernel>=6.5.0->jupyterlab) (2.9.0.post0)
Requirement already satisfied: python-json-logger>=2.0.4 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.7)
Requirement already satisfied: pyyaml>=5.3 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0.2)
Requirement already satisfied: rfc3339-validator in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.4)
Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.1)
Requirement already satisfied: beautifulsoup4 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.12.3)
Requirement already satisfied: bleach!=5.0.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (6.1.0)
Requirement already satisfied: defusedxml in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)
Requirement already satisfied: jupyterlab-pygments in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.3.0)
Requirement already satisfied: mistune<4,>=2.0.3 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (3.0.2)
Requirement already satisfied: nbclient>=0.5.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.10.0)
Requirement already satisfied: pandocfilters>=1.4.1 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.1)

Requirement already satisfied: pycparser in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab) (2.22)
Requirement already satisfied: arrow>=0.15.0 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)
Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\users\talta\appdata\local\programs\python\python313\lib\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.9.0.20241003)

C:\Users\talta\AppData\Local\Programs\Python\Python313>pip3.13 install spacy
Collecting spacy
  Using cached spacy-3.8.2.tar.gz (1.3 MB)
  Installing build dependencies ... error
  error: subprocess-exited-with-error

  × pip subprocess to install build dependencies did not run successfully.
  │ exit code: 1
  ╰─> [94 lines of output]
      Ignoring numpy: markers 'python_version < ""3.9""' don't match your environment
      Collecting setuptools
        Using cached setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)
      Collecting cython<3.0,>=0.25
        Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
      Collecting cymem<2.1.0,>=2.0.2
        Using cached cymem-2.0.8-cp313-cp313-win_amd64.whl
      Collecting preshed<3.1.0,>=3.0.2
        Using cached preshed-3.0.9-cp313-cp313-win_amd64.whl
      Collecting murmurhash<1.1.0,>=0.28.0
        Using cached murmurhash-1.0.10-cp313-cp313-win_amd64.whl
      Collecting thinc<8.4.0,>=8.3.0
        Using cached thinc-8.3.2.tar.gz (193 kB)
        Installing build dependencies: started
        Installing build dependencies: finished with status 'error'
        error: subprocess-exited-with-error

        pip subprocess to install build dependencies did not run successfully.
        exit code: 1

        [62 lines of output]
        Ignoring numpy: markers 'python_version < ""3.9""' don't match your environment
        Collecting setuptools
          Using cached setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)
        Collecting cython<3.0,>=0.25
          Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)
        Collecting murmurhash<1.1.0,>=1.0.2
          Using cached murmurhash-1.0.10-cp313-cp313-win_amd64.whl
        Collecting cymem<2.1.0,>=2.0.2
          Using cached cymem-2.0.8-cp313-cp313-win_amd64.whl
        Collecting preshed<3.1.0,>=3.0.2
          Using cached preshed-3.0.9-cp313-cp313-win_amd64.whl
        Collecting blis<1.1.0,>=1.0.0
          Using cached blis-1.0.1.tar.gz (3.6 MB)
          Installing build dependencies: started
          Installing build dependencies: finished with status 'done'
          Getting requirements to build wheel: started
          Getting requirements to build wheel: finished with status 'done'
          Preparing metadata (pyproject.toml): started
          Preparing metadata (pyproject.toml): finished with status 'done'
        Collecting numpy<2.1.0,>=2.0.0
          Using cached numpy-2.0.2-cp313-cp313-win_amd64.whl
        Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)
        Using cached Cython-0.29.37-py2.py3-none-any.whl (989 kB)
        Building wheels for collected packages: blis
          Building wheel for blis (pyproject.toml): started
          Building wheel for blis (pyproject.toml): finished with status 'error'
          error: subprocess-exited-with-error

          Building wheel for blis (pyproject.toml) did not run successfully.
          exit code: 1

          [24 lines of output]
          BLIS_COMPILER? None
          running bdist_wheel
          running build
          running build_py
          creating build\lib.win-amd64-cpython-313\blis
          copying blis\about.py -> build\lib.win-amd64-cpython-313\blis
          copying blis\benchmark.py -> build\lib.win-amd64-cpython-313\blis
          copying blis\__init__.py -> build\lib.win-amd64-cpython-313\blis
          creating build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\common.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\conftest.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\test_dotv.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\test_gemm.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\tests\__init__.py -> build\lib.win-amd64-cpython-313\blis\tests
          copying blis\cy.pyx -> build\lib.win-amd64-cpython-313\blis
          copying blis\py.pyx -> build\lib.win-amd64-cpython-313\blis
          copying blis\cy.pxd -> build\lib.win-amd64-cpython-313\blis
          copying blis\__init__.pxd -> build\lib.win-amd64-cpython-313\blis
          running build_ext
          Build options win32 msvc
          BUILD ARCH: x86_64
          {'!EXITCODE': '00000000', 'ACLOCAL_PATH': 'C:\\Program Files\\Git\\mingw64\\share\\aclocal;C:\\Program Files\\Git\\usr\\share\\aclocal', 'AGENT_BUILDDIRECTORY': 'D:\\a\\1', 'AGENT_DISABLELOGPLUGIN_TESTFILEPUBLISHERPLUGIN': 'true', 'AGENT_DISABLELOGPLUGIN_TESTRESULTLOGPLUGIN': 'false', 'AGENT_HOMEDIRECTORY': 'C:\\agents\\2.202.0', 'AGENT_ID': '92', 'AGENT_JOBNAME': 'JSONL Python38Windows', 'AGENT_JOBSTATUS': 'Succeeded', 'AGENT_LOGTOBLOBSTORAGESERVICE': 'true', 'AGENT_MACHINENAME': 'WIN-CU8INV6766V', 'AGENT_NAME': 'Hosted Agent', 'AGENT_OS': 'Windows_NT', 'AGENT_OSARCHITECTURE': 'X64', 'AGENT_READONLYVARIABLES': 'true', 'AGENT_RETAINDEFAULTENCODING': 'false', 'AGENT_ROOTDIRECTORY': 'D:\\a', 'AGENT_SERVEROMDIRECTORY': 'C:\\agents\\2.202.0\\externals\\vstsom', 'AGENT_TASKRESTRICTIONSENFORCEMENTMODE': 'Enabled', 'AGENT_TEMPDIRECTORY': 'D:\\a\\_temp', 'AGENT_TOOLSDIRECTORY': 'C:\\hostedtoolcache\\windows', 'AGENT_USEWORKSPACEID': 'true', 'AGENT_VERSION': '2.202.0', 'AGENT_WORKFOLDER': 'D:\\a', 'ALLUSERSPROFILE': 'C:\\ProgramData', 'ANDROID_HOME': 'C:\\Android\\android-sdk', 'ANDROID_NDK_HOME': 'C:\\Android\\android-sdk\\ndk-bundle', 'ANDROID_NDK_LATEST_HOME': 'C:\\Android\\android-sdk\\ndk\\23.1.7779620', 'ANDROID_NDK_PATH': 'C:\\Android\\android-sdk\\ndk-bundle', 'ANDROID_NDK_ROOT': 'C:\\Android\\android-sdk\\ndk-bundle', 'ANDROID_SDK_ROOT': 'C:\\Android\\android-sdk', 'ANT_HOME': 'C:\\ProgramData\\chocolatey\\lib\\ant\\tools\\apache-ant-1.10.12', 'APPDATA': 'C:\\Users\\VssAdministrator\\AppData\\Roaming', 'AR': 'llvm-ar', 'AS': 'llvm-as', 'AZURE_EXTENSION_DIR': 'C:\\Program Files\\Common Files\\AzureCliExtensionDirectory', 'AZURE_HTTP_USER_AGENT': 'VSTS_116cc368-5c0c-4eb4-bb44-7f3fa5bdce14_build_6_0', 'BUILD_ARTIFACTSTAGINGDIRECTORY': 'D:\\a\\1\\a', 'BUILD_BINARIESDIRECTORY': 'D:\\a\\1\\b', 'BUILD_BUILDID': '17021', 'BUILD_BUILDNUMBER': '20220408.7', 'BUILD_BUILDURI': 'vstfs:///Build/Build/17021', 'BUILD_CONTAINERID': '11809685', 'BUILD_DEFINITIONNAME': 'explosion.cython-blis', 'BUILD_DEFINITIONVERSION': '1', 'BUILD_QUEUEDBY': 'GitHub', 'BUILD_QUEUEDBYID': '38e7e9f7-fc06-4f5a-b6dd-1782f4ef7c25', 'BUILD_REASON': 'PullRequest', 'BUILD_REPOSITORY_GIT_SUBMODULECHECKOUT': 'False', 'BUILD_REPOSITORY_ID': 'explosion/cython-blis', 'BUILD_REPOSITORY_LOCALPATH': 'D:\\a\\1\\s', 'BUILD_REPOSITORY_NAME': 'explosion/cython-blis', 'BUILD_REPOSITORY_PROVIDER': 'GitHub', 'BUILD_REPOSITORY_URI': 'https://github.com/explosion/cython-blis', 'BUILD_REQUESTEDFOR': 'GitHub', 'BUILD_REQUESTEDFOREMAIL': '', 'BUILD_REQUESTEDFORID': '38e7e9f7-fc06-4f5a-b6dd-1782f4ef7c25', 'BUILD_SOURCEBRANCH': 'refs/pull/69/merge', 'BUILD_SOURCEBRANCHNAME': 'merge', 'BUILD_SOURCESDIRECTORY': 'D:\\a\\1\\s', 'BUILD_SOURCEVERSION': '273ec162fa5f042b5d946638cedd954583ff8111', 'BUILD_SOURCEVERSIONAUTHOR': 'Daniël de Kok', 'BUILD_SOURCEVERSIONMESSAGE': 'Merge 1de7a1931422b892af086ce69604e7e3459e9f8e into 6daabf0c925bfe67f7d87874ce014eb3212711e7', 'BUILD_STAGINGDIRECTORY': 'D:\\a\\1\\a', 'CABAL_DIR': 'C:\\cabal', 'CC': 'clang', 'COBERTURA_HOME': 'C:\\cobertura-2.1.1', 'COMMONPROGRAMFILES': 'C:\\Program Files\\Common Files', 'COMMON_TESTRESULTSDIRECTORY': 'D:\\a\\1\\TestResults', 'COMPUTERNAME': 'WIN-CU8INV6766V', 'COMSPEC': 'C:\\Windows\\system32\\cmd.exe', 'CONDA': 'C:\\Miniconda', 'CONFIG_SITE': 'C:/Program Files/Git/etc/config.site', 'CHOCOLATEYINSTALL': 'C:\\ProgramData\\chocolatey', 'CHROMEWEBDRIVER': 'C:\\SeleniumWebDrivers\\ChromeDriver', 'COMMONPROGRAMFILES(X86)': 'C:\\Program Files (x86)\\Common Files', 'COMMONPROGRAMW6432': 'C:\\Program Files\\Common Files', 'DISPLAY': 'needs-to-be-defined', 'DOTNET_MULTILEVEL_LOOKUP': '0', 'DRIVERDATA': 'C:\\Windows\\System32\\Drivers\\DriverData', 'EXEPATH': 'C:\\Program Files\\Git\\bin', 'EDGEWEBDRIVER': 'C:\\SeleniumWebDrivers\\EdgeDriver', 'GCM_INTERACTIVE': 'Never', 'GHCUP_INSTALL_BASE_PREFIX': 'C:\\', 'GHCUP_MSYS2': 'C:\\msys64', 'GIT_TERMINAL_PROMPT': '0', 'GOROOT_1_15_X64': 'C:\\hostedtoolcache\\windows\\go\\1.15.15\\x64', 'GOROOT_1_16_X64': 'C:\\hostedtoolcache\\windows\\go\\1.16.15\\x64', 'GOROOT_1_17_X64': 'C:\\hostedtoolcache\\windows\\go\\1.17.8\\x64', 'GOROOT_1_18_X64': 'C:\\hostedtoolcache\\windows\\go\\1.18.0\\x64', 'GRADLE_HOME': 'C:\\ProgramData\\chocolatey\\lib\\gradle\\tools\\gradle-7.4', 'GECKOWEBDRIVER': 'C:\\SeleniumWebDrivers\\GeckoDriver', 'HOME': 'C:\\Users\\VssAdministrator', 'HOMEDRIVE': 'C:', 'HOMEPATH': '\\Users\\VssAdministrator', 'HOSTNAME': 'WIN-CU8INV6766V', 'IEWEBDRIVER': 'C:\\SeleniumWebDrivers\\IEDriver', 'IMAGENAME': 'windows-latest', 'INFOPATH': 'C:\\Program Files\\Git\\usr\\local\\info;C:\\Program Files\\Git\\usr\\share\\info;C:\\Program Files\\Git\\usr\\info;C:\\Program Files\\Git\\share\\info', 'IMAGEOS': 'win22', 'IMAGEVERSION': '20220330.1', 'JAVA_HOME': 'C:\\hostedtoolcache\\windows\\Java_Temurin-Hotspot_jdk\\8.0.322-6\\x64', 'JAVA_HOME_11_X64': 'C:\\hostedtoolcache\\windows\\Java_Temurin-Hotspot_jdk\\11.0.14-101\\x64', 'JAVA_HOME_17_X64': 'C:\\hostedtoolcache\\windows\\Java_Temurin-Hotspot_jdk\\17.0.2-8\\x64', 'JAVA_HOME_8_X64': 'C:\\hostedtoolcache\\windows\\Java_Temurin-Hotspot_jdk\\8.0.322-6\\x64', 'LANG': 'en_US.UTF-8', 'LOCALAPPDATA': 'C:\\Users\\VssAdministrator\\AppData\\Local', 'LOGONSERVER': '\\\\WIN-CU8INV6766V', 'M2': 'C:\\ProgramData\\chocolatey\\lib\\maven\\apache-maven-3.8.5\\bin', 'M2_REPO': 'C:\\ProgramData\\m2', 'MANPATH': 'C:\\Program Files\\Git\\mingw64\\local\\man;C:\\Program Files\\Git\\mingw64\\share\\man;C:\\Program Files\\Git\\usr\\local\\man;C:\\Program Files\\Git\\usr\\share\\man;C:\\Program Files\\Git\\usr\\man;C:\\Program Files\\Git\\share\\man', 'MAVEN_OPTS': '-Xms256m', 'MINGW_CHOST': 'x86_64-w64-mingw32', 'MINGW_PACKAGE_PREFIX': 'mingw-w64-x86_64', 'MINGW_PREFIX': 'C:/Program Files/Git/mingw64', 'MSDEPLOY_HTTP_USER_AGENT': 'VSTS_116cc368-5c0c-4eb4-bb44-7f3fa5bdce14_build_6_0', 'MSYSTEM': 'MINGW64', 'MSYSTEM_CARCH': 'x86_64', 'MSYSTEM_CHOST': 'x86_64-w64-mingw32', 'MSYSTEM_PREFIX': 'C:/Program Files/Git/mingw64', 'MONAGENTCLIENTLOCATION': 'C:\\Packages\\Plugins\\Microsoft.Azure.Geneva.GenevaMonitoring\\2.35.0.2\\Monitoring\\Agent', 'NUMBER_OF_PROCESSORS': '4', 'OLDPWD': 'D:/a/1/s', 'ORIGINAL_PATH': 'C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Users\\VssAdministrator\\bin;C:\\Program Files\\LLVM\\bin;C:\\Users\\VssAdministrator\\AppData\\Roaming\\Python\\Python38\\Scripts;C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\Scripts;C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64;C:\\agents\\2.202.0\\externals\\git\\cmd;C:\\agents\\2.202.0\\externals\\git\\mingw64\\bin;C:\\Program Files\\MongoDB\\Server\\5.0\\bin;C:\\aliyun-cli;C:\\vcpkg;C:\\Program Files (x86)\\NSIS;C:\\tools\\zstd;C:\\Program Files\\Mercurial;C:\\hostedtoolcache\\windows\\stack\\2.7.5\\x64;C:\\cabal\\bin;C:\\ghcup\\bin;C:\\tools\\ghc-9.2.2\\bin;C:\\Program Files\\dotnet;C:\\mysql\\bin;C:\\Program Files\\R\\R-4.1.3\\bin\\x64;C:\\SeleniumWebDrivers\\GeckoDriver;C:\\Program Files (x86)\\sbt\\bin;C:\\Program Files (x86)\\GitHub CLI;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files (x86)\\pipx_bin;C:\\hostedtoolcache\\windows\\go\\1.16.15\\x64\\bin;C:\\hostedtoolcache\\windows\\Python\\3.9.12\\x64\\Scripts;C:\\hostedtoolcache\\windows\\Python\\3.9.12\\x64;C:\\hostedtoolcache\\windows\\Ruby\\3.0.3\\x64\\bin;C:\\tools\\kotlinc\\bin;C:\\hostedtoolcache\\windows\\Java_Temurin-Hotspot_jdk\\8.0.322-6\\x64\\bin;C:\\npm\\prefix;C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\wbin;C:\\ProgramData\\kind;C:\\Program Files\\Microsoft\\jdk-11.0.12.7-hotspot\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\dotnet;C:\\ProgramData\\Chocolatey\\bin;C:\\Program Files\\Docker;C:\\Program Files\\PowerShell\\7;C:\\Program Files\\Microsoft\\Web Platform Installer;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn;C:\\Program Files\\Microsoft SQL Server\\150\\Tools\\Binn;C:\\Program Files\\nodejs;C:\\Program Files\\OpenSSL\\bin;C:\\Strawberry\\c\\bin;C:\\Strawberry\\perl\\site\\bin;C:\\Strawberry\\perl\\bin;C:\\ProgramData\\chocolatey\\lib\\pulumi\\tools\\Pulumi\\bin;C:\\Program Files\\TortoiseSVN\\bin;C:\\Program Files\\CMake\\bin;C:\\ProgramData\\chocolatey\\lib\\maven\\apache-maven-3.8.5\\bin;C:\\Program Files\\Microsoft Service Fabric\\bin\\Fabric\\Fabric.Code;C:\\Program Files\\Microsoft SDKs\\Service Fabric\\Tools\\ServiceFabricLocalClusterManager;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\GitHub CLI;C:\\tools\\php;C:\\Program Files (x86)\\sbt\\bin;C:\\SeleniumWebDrivers\\ChromeDriver;C:\\SeleniumWebDrivers\\EdgeDriver;C:\\Program Files\\Amazon\\AWSCLIV2;C:\\Program Files\\Amazon\\SessionManagerPlugin\\bin;C:\\Program Files\\Amazon\\AWSSAMCLI\\bin;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn;C:\\Program Files\\LLVM\\bin;C:\\Users\\VssAdministrator\\.dotnet\\tools;C:\\Users\\VssAdministrator\\.cargo\\bin;C:\\Users\\VssAdministrator\\AppData\\Local\\Microsoft\\WindowsApps', 'ORIGINAL_TEMP': 'C:/Users/VSSADM~1/AppData/Local/Temp', 'ORIGINAL_TMP': 'C:/Users/VSSADM~1/AppData/Local/Temp', 'OS': 'windows', 'PATH': 'C:\\Users\\VssAdministrator\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\local\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Users\\VssAdministrator\\bin;C:\\Program Files\\LLVM\\bin;C:\\Users\\VssAdministrator\\AppData\\Roaming\\Python\\Python38\\Scripts;C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\Scripts;C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64;C:\\agents\\2.202.0\\externals\\git\\cmd;C:\\agents\\2.202.0\\externals\\git\\mingw64\\bin;C:\\Program Files\\MongoDB\\Server\\5.0\\bin;C:\\aliyun-cli;C:\\vcpkg;C:\\Program Files (x86)\\NSIS;C:\\tools\\zstd;C:\\Program Files\\Mercurial;C:\\hostedtoolcache\\windows\\stack\\2.7.5\\x64;C:\\cabal\\bin;C:\\ghcup\\bin;C:\\tools\\ghc-9.2.2\\bin;C:\\Program Files\\dotnet;C:\\mysql\\bin;C:\\Program Files\\R\\R-4.1.3\\bin\\x64;C:\\SeleniumWebDrivers\\GeckoDriver;C:\\Program Files (x86)\\sbt\\bin;C:\\Program Files (x86)\\GitHub CLI;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files (x86)\\pipx_bin;C:\\hostedtoolcache\\windows\\go\\1.16.15\\x64\\bin;C:\\hostedtoolcache\\windows\\Python\\3.9.12\\x64\\Scripts;C:\\hostedtoolcache\\windows\\Python\\3.9.12\\x64;C:\\hostedtoolcache\\windows\\Ruby\\3.0.3\\x64\\bin;C:\\tools\\kotlinc\\bin;C:\\hostedtoolcache\\windows\\Java_Temurin-Hotspot_jdk\\8.0.322-6\\x64\\bin;C:\\npm\\prefix;C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\CLI2\\wbin;C:\\ProgramData\\kind;C:\\Program Files\\Microsoft\\jdk-11.0.12.7-hotspot\\bin;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0;C:\\Windows\\System32\\OpenSSH;C:\\Program Files\\dotnet;C:\\ProgramData\\Chocolatey\\bin;C:\\Program Files\\Docker;C:\\Program Files\\PowerShell\\7;C:\\Program Files\\Microsoft\\Web Platform Installer;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn;C:\\Program Files\\Microsoft SQL Server\\150\\Tools\\Binn;C:\\Program Files\\nodejs;C:\\Program Files\\OpenSSL\\bin;C:\\Strawberry\\c\\bin;C:\\Strawberry\\perl\\site\\bin;C:\\Strawberry\\perl\\bin;C:\\ProgramData\\chocolatey\\lib\\pulumi\\tools\\Pulumi\\bin;C:\\Program Files\\TortoiseSVN\\bin;C:\\Program Files\\CMake\\bin;C:\\ProgramData\\chocolatey\\lib\\maven\\apache-maven-3.8.5\\bin;C:\\Program Files\\Microsoft Service Fabric\\bin\\Fabric\\Fabric.Code;C:\\Program Files\\Microsoft SDKs\\Service Fabric\\Tools\\ServiceFabricLocalClusterManager;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Git\\mingw64\\bin;C:\\Program Files\\Git\\usr\\bin;C:\\Program Files\\GitHub CLI;C:\\tools\\php;C:\\Program Files (x86)\\sbt\\bin;C:\\SeleniumWebDrivers\\ChromeDriver;C:\\SeleniumWebDrivers\\EdgeDriver;C:\\Program Files\\Amazon\\AWSCLIV2;C:\\Program Files\\Amazon\\SessionManagerPlugin\\bin;C:\\Program Files\\Amazon\\AWSSAMCLI\\bin;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\Binn;C:\\Program Files\\LLVM\\bin;C:\\Users\\VssAdministrator\\.dotnet\\tools;C:\\Users\\VssAdministrator\\.cargo\\bin;C:\\Users\\VssAdministrator\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Program Files\\Git\\usr\\bin\\vendor_perl;C:\\Program Files\\Git\\usr\\bin\\core_perl', 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.CPL', 'PGBIN': 'C:\\Program Files\\PostgreSQL\\14\\bin', 'PGDATA': 'C:\\Program Files\\PostgreSQL\\14\\data', 'PGPASSWORD': 'root', 'PGROOT': 'C:\\Program Files\\PostgreSQL\\14', 'PGUSER': 'postgres', 'PHPROOT': 'c:\\tools\\php', 'PIPELINE_WORKSPACE': 'D:\\a\\1', 'PIPX_BIN_DIR': 'C:\\Program Files (x86)\\pipx_bin', 'PIPX_HOME': 'C:\\Program Files (x86)\\pipx', 'PKG_CONFIG_PATH': 'C:\\Program Files\\Git\\mingw64\\lib\\pkgconfig;C:\\Program Files\\Git\\mingw64\\share\\pkgconfig', 'PLINK_PROTOCOL': 'ssh', 'POWERSHELL_DISTRIBUTION_CHANNEL': 'Azure-DevOps-win22', 'POWERSHELL_UPDATECHECK': 'Off', 'PROCESSOR_ARCHITECTURE': 'AMD64', 'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 79 Stepping 1, GenuineIntel', 'PROCESSOR_LEVEL': '6', 'PROCESSOR_REVISION': '4f01', 'PROGRAMFILES': 'C:\\Program Files', 'PROMPT': '$P$G', 'PSEXECUTIONPOLICYPREFERENCE': 'Unrestricted', 'PSMODULEPATH': 'C:\\Users\\VssAdministrator\\Documents\\WindowsPowerShell\\Modules;C:\\\\Modules\\azurerm_2.1.0;C:\\\\Modules\\azure_2.1.0;C:\\Users\\packer\\Documents\\WindowsPowerShell\\Modules;C:\\Program Files\\WindowsPowerShell\\Modules;C:\\Windows\\system32\\WindowsPowerShell\\v1.0\\Modules;C:\\Program Files\\Microsoft SQL Server\\130\\Tools\\PowerShell\\Modules\\', 'PUBLIC': 'C:\\Users\\Public', 'PWD': 'D:/a/1/s/flame-blis', 'PYTHON_VERSION': '3.8', 'PROGRAMDATA': 'C:\\ProgramData', 'PROGRAMFILES(X86)': 'C:\\Program Files (x86)', 'PROGRAMW6432': 'C:\\Program Files', 'RANLIB': 'echo', 'RESOURCES_TRIGGERINGALIAS': '', 'RESOURCES_TRIGGERINGCATEGORY': '', 'RTOOLS40_HOME': 'C:\\rtools40', 'RUNNER_TOOLSDIRECTORY': 'C:\\hostedtoolcache\\windows', 'SBT_HOME': 'C:\\Program Files (x86)\\sbt\\', 'SELENIUM_JAR_PATH': 'C:\\selenium\\selenium-server.jar', 'SHELL': 'C:\\Program Files\\Git\\usr\\bin\\bash.exe', 'SHLVL': '2', 'SSH_ASKPASS': 'C:/Program Files/Git/mingw64/bin/git-askpass.exe', 'SYSTEM': 'build', 'SYSTEMDRIVE': 'C:', 'SYSTEMROOT': 'C:\\Windows', 'SYSTEM_ARTIFACTSDIRECTORY': 'D:\\a\\1\\a', 'SYSTEM_COLLECTIONID': '116cc368-5c0c-4eb4-bb44-7f3fa5bdce14', 'SYSTEM_COLLECTIONURI': 'https://dev.azure.com/explosion-ai/', 'SYSTEM_CULTURE': 'en-US', 'SYSTEM_DEBUG': 'false', 'SYSTEM_DEFAULTWORKINGDIRECTORY': 'D:\\a\\1\\s', 'SYSTEM_DEFINITIONID': '6', 'SYSTEM_DEFINITIONNAME': 'explosion.cython-blis', 'SYSTEM_ENABLEACCESSTOKEN': 'SecretVariable', 'SYSTEM_HOSTTYPE': 'build', 'SYSTEM_ISSCHEDULED': 'False', 'SYSTEM_JOBATTEMPT': '1', 'SYSTEM_JOBDISPLAYNAME': 'JSONL Python38Windows', 'SYSTEM_JOBID': 'efb31c5a-ec83-5597-c79c-3c04d0eba6be', 'SYSTEM_JOBIDENTIFIER': 'JSONL.Python38Windows', 'SYSTEM_JOBNAME': 'Python38Windows', 'SYSTEM_JOBPARALLELISMTAG': 'Public', 'SYSTEM_JOBPOSITIONINPHASE': '1', 'SYSTEM_JOBTIMEOUT': '60', 'SYSTEM_PARALLELEXECUTIONTYPE': 'MultiConfiguration', 'SYSTEM_PHASEATTEMPT': '1', 'SYSTEM_PHASEDISPLAYNAME': 'JSONL', 'SYSTEM_PHASEID': 'ecb95708-c2a5-5456-f379-96cd8090c2a6', 'SYSTEM_PHASENAME': 'JSONL', 'SYSTEM_PIPELINESTARTTIME': '2022-04-08 12:45:25+00:00', 'SYSTEM_PLANID': '4bcc5172-4f8e-4a4f-b00f-1b3d5e2fe9dd', 'SYSTEM_POSTLINESSPEED': '10000', 'SYSTEM_PULLREQUEST_ISFORK': 'True', 'SYSTEM_PULLREQUEST_MERGEDAT': '', 'SYSTEM_PULLREQUEST_PULLREQUESTID': '899994381', 'SYSTEM_PULLREQUEST_PULLREQUESTNUMBER': '69', 'SYSTEM_PULLREQUEST_SOURCEBRANCH': 'update-to-blis-0.9.0', 'SYSTEM_PULLREQUEST_SOURCECOMMITID': '1de7a1931422b892af086ce69604e7e3459e9f8e', 'SYSTEM_PULLREQUEST_SOURCEREPOSITORYURI': 'https://github.com/explosion/cython-blis', 'SYSTEM_PULLREQUEST_TARGETBRANCH': 'master', 'SYSTEM_RESTRICTSECRETS': 'True', 'SYSTEM_SERVERTYPE': 'Hosted', 'SYSTEM_STAGEATTEMPT': '1', 'SYSTEM_STAGEDISPLAYNAME': '__default', 'SYSTEM_STAGEID': '96ac2280-8cb4-5df5-99de-dd2da759617d', 'SYSTEM_STAGENAME': '__default', 'SYSTEM_TASKDEFINITIONSURI': 'https://dev.azure.com/explosion-ai/', 'SYSTEM_TASKDISPLAYNAME': 'Generate JSONL (Windows)', 'SYSTEM_TASKINSTANCEID': '4bae54ba-656f-5414-04c0-0cf207e9f5bd', 'SYSTEM_TASKINSTANCENAME': 'CmdLine5', 'SYSTEM_TEAMFOUNDATIONCOLLECTIONURI': 'https://dev.azure.com/explosion-ai/', 'SYSTEM_TEAMFOUNDATIONSERVERURI': 'https://dev.azure.com/explosion-ai/', 'SYSTEM_TEAMPROJECT': 'Public', 'SYSTEM_TEAMPROJECTID': '5c6613e9-6ccf-48bd-81de-dbc3b0a6f957', 'SYSTEM_TIMELINEID': '4bcc5172-4f8e-4a4f-b00f-1b3d5e2fe9dd', 'SYSTEM_TOTALJOBSINPHASE': '1', 'SYSTEM_WORKFOLDER': 'D:\\a', 'TASK_DISPLAYNAME': 'Generate JSONL (Windows)', 'TASK_SKIPTRANSLATORFORCHECKOUT': 'False', 'TEMP': 'C:\\Users\\VSSADM~1\\AppData\\Local\\Temp', 'TERM': 'xterm-256color', 'TF_BUILD': 'True', 'TMP': 'C:\\Users\\VSSADM~1\\AppData\\Local\\Temp', 'TMPDIR': 'C:\\Users\\VSSADM~1\\AppData\\Local\\Temp', 'USEPYTHONVERSION_PYTHONLOCATION': 'C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64', 'USERDOMAIN': 'WIN-CU8INV6766V', 'USERDOMAIN_ROAMINGPROFILE': 'WIN-CU8INV6766V', 'USERNAME': 'VssAdministrator', 'USERPROFILE': 'C:\\Users\\VssAdministrator', 'VCPKG_INSTALLATION_ROOT': 'C:\\vcpkg', 'VSTS_AGENT_PERFLOG': 'C:\\agents\\perflog', 'VSTS_PROCESS_LOOKUP_ID': 'vsts_175962b3-f397-42b7-b557-a072c6b9de45', 'VSTS_SECRET_VARIABLES': '', 'WINDIR': 'C:\\Windows', 'WIX': 'C:\\Program Files (x86)\\WiX Toolset v3.11\\', '_': 'C:/hostedtoolcache/windows/Python/3.8.10/x64/python', 'AGENT.JOBSTATUS': 'Succeeded', 'NPM_CONFIG_PREFIX': 'C:\\npm\\prefix'}
          [COMMAND] C:\Program Files\LLVM\bin\clang.exe -c C:\Users\talta\AppData\Local\Temp\pip-install-wk4qfygk\blis_cf8f906855084da7a11bbd6cbcaf460a\blis\_src\config\bulldozer\bli_cntx_init_bulldozer.c -o C:\Users\talta\AppData\Local\Temp\tmpyzg4ayho\bli_cntx_init_bulldozer.o -O2 -std=c99 -D_POSIX_C_SOURCE=200112L -DBLIS_VERSION_STRING=""0.9.0"" -DBLIS_IS_BUILDING_LIBRARY -Iinclude\windows-x86_64 -I.\frame\3\ -I.\frame\1m\ -I.\frame\1f\ -I.\frame\1\ -I.\frame\include -IC:\Users\talta\AppData\Local\Temp\pip-install-wk4qfygk\blis_cf8f906855084da7a11bbd6cbcaf460a\blis\_src\include\windows-x86_64
          error: [WinError 2] The system cannot find the file specified
          [end of output]

          note: This error originates from a subprocess, and is likely not a problem with pip.
          ERROR: Failed building wheel for blis
        Failed to build blis
        ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (blis)
        [end of output]

        note: This error originates from a subprocess, and is likely not a problem with pip.
      error: subprocess-exited-with-error

      pip subprocess to install build dependencies did not run successfully.
      exit code: 1

      See above for output.

      note: This error originates from a subprocess, and is likely not a problem with pip.
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× pip subprocess to install build dependencies did not run successfully.
│ exit code: 1
╰─> See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.`


## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 11
* Python Version Used: Python 3.13
* spaCy Version Used: spacy-3.8.2
* Environment Information: Command prompt
",RubTalha,150074539,open,False,49,2024-10-12T07:42:28+00:00,2025-05-05T15:24:28+00:00,,,30,30,0,0,0,0,0
explosion/spaCy,2581880512,13657,Fix typo (#13657) [ci skip],"Fixes ""mdodel"" to ""model"". ",paskn,11264346,closed,False,1,2024-10-11T17:39:09+00:00,2024-10-23T10:06:41+00:00,2024-10-23T10:06:36+00:00,docs,0,0,0,0,0,0,0
explosion/spaCy,2579703138,13656,Sentiment analysis logregspacy,"<!--- Provide a general summary of your changes in the title. -->
## Summary of Changes
Implemented a custom Logistic Regression model for sentiment analysis using spaCy without relying on scikit-learn. The model processes datasets to classify text as positive or negative. Additionally, a comprehensive README.md file was created to guide users through installation, usage, and testing of the model.

## Description
This pull request introduces a new feature that includes a custom **Logistic Regression** sentiment analysis model. The implementation is based on spaCy and avoids using scikit-learn, thereby providing a more integrated solution within the spaCy ecosystem.

The model processes the dataset, trains the logistic regression classifier, and outputs classification results. The README file has been updated to include detailed instructions for running the model, testing its performance, and using it within user projects.

### Types of change
- [x] New Feature: Introduced a custom Logistic Regression model for sentiment analysis only using spaCy.
- [x] Documentation: Updated the README.md file to include usage instructions, testing guidelines, and examples.

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.

",samhithamuvva,163280630,open,False,0,2024-10-10T19:49:52+00:00,2024-10-10T19:49:52+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2578495538,13655,Sentiment analysis using logistic regression (only spacy),"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",samhithamuvva,163280630,closed,False,0,2024-10-10T11:05:34+00:00,2024-10-10T16:02:09+00:00,2024-10-10T16:02:03+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2578248394,13654,Sentiment analysis logisticregression,"<!--- Provide a general summary of your changes in the title. -->

## Description
<!--- Use this section to describe your changes. If your changes required
testing, include information about the testing environment and the tests you
ran. If your test fixes a bug reported in an issue, don't forget to include the
issue number. If your PR is still a work in progress, that's totally fine – just
include a note to let us know. -->

### Types of change
<!-- What type of change does your PR cover? Is it a bug fix, an enhancement
or new feature, or a change to the documentation? -->

## Checklist
<!--- Before you submit the PR, go over this checklist and make sure you can
tick off all the boxes. [] -> [x] -->
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",samhithamuvva,163280630,closed,False,2,2024-10-10T09:41:18+00:00,2024-10-10T16:21:28+00:00,2024-10-10T10:12:33+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2571203763,13653,Tight Version Constraints on thinc in spaCy 3.8.2 Causing Dependency Resolution Issues for AutoGluon Users,"
## How to reproduce the behaviour

When attempting to install `autogluon==1.1.1`, the installation fails due to dependency resolution issues involving `spaCy<4.0` and its tight version constraints on `thinc`. This leads `pip` to select incompatible versions of packages like `onnx`, resulting in installation failures.

**Steps to reproduce:**

1. **Create a new virtual environment:**

   ```bash
   python -m venv test_env
   source test_env/bin/activate  # On Windows, use: test_env\Scripts\activate
   ```

2. **Upgrade `pip` to the latest version:**

   ```bash
   pip install --upgrade pip
   ```

3. **Attempt to install `autogluon==1.1.1`:**

   ```bash
   pip install autogluon==1.1.1
   ```

4. **Observe the installation failure**, particularly with the `onnx` package.

**Error Message:**

```
Collecting onnx
  Using cached onnx-1.10.0.tar.gz (10.0 MB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [11 lines of output]
      /tmp/pip-install-xxxxxx/onnx/setup.py:36: DeprecationWarning: Use shutil.which instead of find_executable
        CMAKE = find_executable('cmake3') or find_executable('cmake')
      /tmp/pip-install-xxxxxx/onnx/setup.py:37: DeprecationWarning: Use shutil.which instead of find_executable
        MAKE = find_executable('make')
      fatal: not a git repository (or any of the parent directories): .git
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/tmp/pip-install-xxxxxx/onnx/setup.py"", line 318, in <module>
          raise FileNotFoundError(""Unable to find "" + requirements_file)
      FileNotFoundError: Unable to find requirements.txt
      [end of output]
```

**Additional Information:**

- The issue started occurring after the release of `spaCy==3.8.2` on 2024-10-01.
- Downgrading `spaCy` to version `3.7.5` resolves the installation issue.
- The tight version constraints on `thinc` specified by `spaCy<4.0` (e.g., `thinc<8.4.0,>=8.3.0`) appear to be causing dependency resolution conflicts.
- This affects users who are installing packages that depend on `spaCy<4.0`, such as `autogluon`.

**Explanation:**

- `spaCy<4.0` specifies strict version constraints on `thinc`, requiring versions between `>=8.3.0` and `<8.4.0`.
- This tight constraint causes `pip` to spend a long time resolving dependencies and ultimately selects older, incompatible versions of other packages (like `onnx`), which fail to install.
- Relaxing the version constraints on `thinc` could allow `pip` to find a compatible set of package versions, preventing the installation failure.

## Your Environment

- **Operating System:** Linux (Ubuntu 20.04)
- **Python Version Used:** 3.10
- **spaCy Version Used:** `spaCy==3.8.2`
- **Environment Information:**
  - **pip Version:** 24.2
  - **Other Relevant Packages:**
    - `autogluon==1.1.1`
    - `thinc` versions being considered: `8.3.0`, `8.3.1`, `8.3.2`

---

**Our Request:**

We kindly request the spaCy team to consider adjusting the version constraints on `thinc` in `spaCy<4.0` to be less restrictive, or to release a patch version that relaxes these constraints. This adjustment could help prevent dependency resolution issues for users installing packages that depend on `spaCy<4.0`.

Is it feasible to modify these constraints without impacting spaCy's functionality? Any guidance or assistance on this matter would be greatly appreciated.

Thank you for your time and support.

---

**Note:** If this issue is better suited for a discussion, please let me know, and I'll be happy to open a discussion thread instead.",tonyhoo,2148897,open,False,7,2024-10-07T18:50:44+00:00,2025-04-29T16:15:06+00:00,,,10,10,0,0,0,0,0
explosion/spaCy,2565929425,13652,No compatible packages found for v3.8.2 of spaCy,"It seems like the recently pushed` 3.8.2` version has some issues downloading models.
```
python -m spacy download en_core_web_md
✘ No compatible package found for 'en-core-web-md' (spaCy v3.8.2)
```

Here's my system info.
```
C:\Users\victim\AppData\Local\Programs\Python\Python312\Lib\site-packages\spacy\util.py:910: UserWarning: [W095] Model 'en_core_web_md' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate
  warnings.warn(warn_msg)

============================== Info about spaCy ==============================

spaCy version    3.8.2
Location         C:\Users\victim\AppData\Local\Programs\Python\Python312\Lib\site-packages\spacy
Platform         Windows-11-10.0.22631-SP0
Python version   3.12.6
Pipelines        en_core_web_md (3.7.1)
```
Issue fix: Just make it en_core_web_md instead of en-core-web-md",HydraDragonAntivirus,142328963,closed,True,1,2024-10-04T09:35:19+00:00,2024-11-04T00:03:06+00:00,2024-10-04T09:42:09+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2564654915,13649,Cannot complete a Docker build using v.3.8.2,"It looks like this problem still exists in the newest release, @honnibal :
Issue #13606 

The Docker image build hangs in the same place...
```
#35 42.62 Collecting spacy (from -r requirements.txt (line 15))
#35 42.64   Downloading spacy-3.8.2.tar.gz (1.3 MB)
#35 42.78      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 11.8 MB/s eta 0:00:00
#35 45.06   Installing build dependencies: started
#35 131.6   Installing build dependencies: still running...
#35 218.1   Installing build dependencies: still running...
#35 376.2   Installing build dependencies: still running...
#35 501.0   Installing build dependencies: still running...
#35 715.7   Installing build dependencies: still running...
#35 1017.0   Installing build dependencies: still running...
#35 1082.7   Installing build dependencies: still running...
#35 1306.2   Installing build dependencies: still running...
```

It looks like the specific test run for Python 3.12.5 on the latest Ubuntu was canceled...here:
https://github.com/explosion/spaCy/actions/runs/11127868506/job/30921252535

## How to reproduce the behaviour
Make sure your project pulls the latest spaCy (3.8.2 as of now), then attmept to build a Docker image.  In our case, this happens withing a GitHub workflow, and we build them on latest Ubuntu.  

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System:  Ubuntu, latest
* Python Version Used: 3.12.5
* spaCy Version Used: 3.8.2
* Environment Information:  Issue seen in a GitHub workflow run

Our workaround will be to revert back to a hard pin of spaCy 3.7.5 for now, but we'd certainly like to get back to the latest releases once this issue is sorted.   Thanks!",erikspears,98238295,open,False,5,2024-10-03T17:51:15+00:00,2025-01-31T14:28:58+00:00,,,7,7,0,0,0,0,0
explosion/spaCy,2564642605,13648,spacy[cuda12x] not work with windows 11 torch 2.3.1 cu 12.1 WinError 127 cufftw64_11.dll,"<!-- NOTE: For questions or install related issues, please open a Discussion instead. -->

## How to reproduce the behaviour
<!-- Include a code example or the steps that led to the problem. Please try to be as specific as possible. -->
When I tried to use spacy with torch 2.3.1 cu12.1, I have the following error with whatever version of spacy. (Tried 3.8, 3.7.5, 3.6.1)
![{$F {7_524%D9H~JKQ}HSG8](https://github.com/user-attachments/assets/977a1540-cb78-419b-925c-a15f1193281d)
I tried to install cuda 11.8 and torch 2.3.1 cu118 and solved the issue.

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: Windows 11
* Python Version Used: 3.10
* spaCy Version Used: 3.7.5
* Environment Information: Torch 2.3.1 cu12.1
",lrzjason,7887717,closed,True,3,2024-10-03T17:43:18+00:00,2024-11-04T00:03:09+00:00,2024-10-04T05:03:25+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2560340714,13646,[WIP] Investigate windows CI failures,Need to investigate this,honnibal,8059750,closed,False,1,2024-10-01T22:46:20+00:00,2024-10-08T07:32:43+00:00,2024-10-01T23:01:41+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2560220757,13645,v3.8 fails to build on py3.8,"## How to reproduce the behaviour
Hi!

in the most recent release, the `setup_requires` numpy version was updated to a version which is not published for py3.8
https://github.com/explosion/spaCy/blob/master/setup.cfg#L39

1.24 is the last version available for py3.8 https://endoflife.date/numpy#python-support

The current `setup.cfg` does list 3.8 as still supported https://github.com/explosion/spaCy/blob/master/setup.cfg#L21

If that is true, perhaps the <py3.9 tag [here](https://github.com/explosion/spaCy/blob/master/pyproject.toml#L9) should be downgraded (leaving >3.9 updated)

p.s. i see https://github.com/explosion/spaCy/issues/13624 which looks related

> The only solution is to update your Python, or continue using the version of spaCy that's been working for you.

This is a totally reasonable answer, but if thats the case, perhaps it would be best to remove the advertised support for 3.8 in those classifiers (and any other relevant locations)



Thanks so much for the great project! 

## Your Environment
<!-- Include details of your environment. You can also type `python -m spacy info --markdown` and copy-paste the result here.-->
* Operating System: U20
* Python Version Used: py3.8
* spaCy Version Used: 3.8
* Environment Information:
",markjm,16494982,open,False,1,2024-10-01T21:09:27+00:00,2024-10-01T22:39:38+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2557576832,13644,Convert numpy.floating values in meta.json,Ports over a numpy v2 compatibility change from v3.8,honnibal,8059750,closed,False,0,2024-09-30T20:27:28+00:00,2024-10-01T22:52:12+00:00,2024-10-01T22:52:12+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2557039288,13643,"Usage page on memory management, explaining memory zones and doc_cleaner (#13643) [ci skip]","**Preview:** https://deploy-preview-13643--spacy.netlify.app/usage/memory-management

I'm sure I've gotten all sorts of syntax wrong, but here's a start on a usage page explaining how to use the memory zones. The `doc_cleaner` thing is also important and wasn't in any of the usage pages.

The API pages will also need to be updated.",honnibal,8059750,closed,False,0,2024-09-30T15:40:57+00:00,2024-10-23T10:42:59+00:00,2024-10-23T10:42:54+00:00,docs;perf / memory,0,0,0,0,0,0,0
explosion/spaCy,2556610590,13642,[Documentation] Serializing Pipeline unclear,"## Summary
On this page, it claims to serialize a pipeline, you use the following methods:
```
config = nlp.config
bytes_data = nlp.to_bytes()
```

and that you you must take care of storing both and then loading from disk.

However, it also appears that:
```
nlp.to_disk('directory_name')
```

coupled with:
```
spacy.load('directory_name')
```

works and this is a lot more simple. The code executes and I can call a built nlp object on text successfully.

## Questions

1. Does this approach actually work identically?
     1.   If so, can we update the documentation? The `nlp.config` and `to_bytes` seem like implementation details rather than the API for serializing?
     2. I didn't see a mention on this page that you can load the persisted pipeline from disk with `spacy.load`, should this be added?

2. If this approach doesn't work, I think we should call this out and build a function/method that handles loading and saving to disk with a single call - this seems better than having to write your own disk persistence for the config and bytes object. What do you think?

Thanks!

## Which page or section is this issue related to?
https://spacy.io/usage/saving-loading


",DomHudson,10864294,open,False,2,2024-09-30T13:00:58+00:00,2024-10-01T11:59:20+00:00,,,0,0,0,0,0,0,0
explosion/spaCy,2556534404,13641,[Documentation] Replace broken URL in  _serialization.mdx,"## Description

""Pickle protocol"" text previously linked to https://www.diveinto.org/python3/serializing.html#dump, however diveinto is not currently a valid domain. Updating hyperlink to the Python documentation for pickle.

### Types of change
Documentation.

## Checklist
- [x] I confirm that I have the right to submit this contribution under the project's MIT license.
- [x] I ran the tests, and all new and existing tests passed.
- [x] My changes don't require a change to the documentation, or if they do, I've added all required information.
",DomHudson,10864294,closed,False,0,2024-09-30T12:39:00+00:00,2024-09-30T15:45:51+00:00,2024-09-30T15:45:51+00:00,,0,0,0,0,0,0,0
explosion/spaCy,2553781507,13640,Could we release a new version of  spacy-transformer?,"Right now, it seems we already relaxed transformer version requirement to <4.42.0 in https://github.com/explosion/spacy-transformers/pull/418, could we release a new version v1.3.6?

cc @danieldk",xingjianan,1633398,open,False,0,2024-09-27T21:46:40+00:00,2024-09-27T21:46:40+00:00,,,5,5,0,0,0,0,0
