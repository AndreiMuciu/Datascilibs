repo_full_name,issue_id,number,title,body,user_login,user_id,state,locked,comments_count,created_at,updated_at,closed_at,labels,reactions_total,reactions_plus1,reactions_minus1,reactions_laugh,reactions_hooray,reactions_confused,reactions_heart
microsoft/nni,2512403457,5806,i have a question,"when i run the demo of nni, i found my trial always fail. Did anyone tell me why? 
![Pic](https://github.com/user-attachments/assets/2199591b-29db-40ad-83b1-c3dc55282450)

",coolcoolboy,21376617,open,True,0,2024-09-08T14:01:01+00:00,2024-09-08T14:01:01+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2507248292,5805,About encountering exception while using ConvTranspose2d in nni pruning,"**Describe the issue**:
Report an bug.

When using ConvTranspose2d in config_list to prune my models, I encountered an exception: Unsupported types for ConvTranspose2d.
However, when I check the library files, I see this
![image](https://github.com/user-attachments/assets/71d01ac0-3593-4ea6-bc22-8e4d3adf9977)
in nni/compression/speedup/mask_conflict.py, it deals with ConvTranspose2d here,
whereas does not deal with ConvTranspose2d here, which leads to exception for anyone who uses ConvTranspose2d in their codes
![image](https://github.com/user-attachments/assets/9eca3bbb-3ade-4a39-9c06-361d8daf606f)
However, I have no idea to solve this by coding myself, so I report a bug here.
Hope to see the solution ASAP, thanks!



**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only): Ubuntu
- Python version: 3.9.10
- PyTorch/TensorFlow version: PyTorch version: 1.7.1
- Is conda/virtualenv/venv used?:yes 
- Is running in Docker?:no


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",ChristopherCao,48756688,open,True,0,2024-09-05T09:21:51+00:00,2024-09-05T09:21:51+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2500168226,5804,"Run Kubeflow Training Service Error, ","**Describe the issue**:

I run ""Run Kubeflow Training Service Error, "" with offical example
([Example](https://nni.readthedocs.io/en/stable/experiment/training_service/kubeflow.html))

But Got ImportError exception

**Traceback (most recent call last):
  File ""_nni_k8s._test_.py"", line 2, in <module>
    from nni.experiment import Experiment, K8sNfsConfig, KubeflowRowConfig
ImportError: cannot import name 'KubeflowRowConfig' from 'nni.experiment' (/root/.local/lib/python3.8/site-packages/nni/experiment/__init__.py)**


**Environment**:
- NNI version: 3.0
- Training service:  local
- Client OS:  Unbunt 20.04.6 LTS
- Server OS (for remote mode only):
- Python version:  3.8.10
- PyTorch/TensorFlow version:  torch==1.11.0+cu113, not use tf
- Is conda/virtualenv/venv used: No
- Is running in Docker: Yes


**Configuration**:
None, Beacse it's a ImportError exception


**Log message**:
 None, Beacse it's a ImportError exception
 

**How to reproduce it?**:
Using Offical Example",Yumeka999,5391552,open,True,0,2024-09-02T07:02:42+00:00,2024-09-02T07:02:42+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2486776662,5803,nnictl resume cannot open GUI,"As title, GUI cannot be open after I resume the experiment.
I observe that if the output log does not hang at ""Web portal URLs: ..."", GUI will be unable to open. However, I can't find the way to keep ""nnictl resume ID"" command running.

Complete log of command:

[2024-08-26 13:10:28] Creating experiment, Experiment ID: z39sirw8
[2024-08-26 13:10:28] Starting web server...
[2024-08-26 13:10:29] INFO (main) Start NNI manager
[2024-08-26 13:10:29] INFO (RestServer) Starting REST server at port 8080, URL prefix: ""/""
[2024-08-26 13:10:29] INFO (RestServer) REST server started.
[2024-08-26 13:10:29] INFO (NNIDataStore) Datastore initialization done
[2024-08-26 13:10:29] Setting up...
[2024-08-26 13:10:30] INFO (NNIManager) Resuming experiment: z39sirw8
[2024-08-26 13:10:30] INFO (NNIManager) Setup training service...
[2024-08-26 13:10:30] INFO (NNIManager) Setup tuner...
[2024-08-26 13:10:31] INFO (NNIManager) Number of current submitted trials: 621, where 0 is resuming.
[2024-08-26 13:10:31] INFO (NNIManager) Change NNIManager status from: INITIALIZED to: RUNNING
[2024-08-26 13:10:31] Web portal URLs: http://127.0.0.1:8080 http://172.17.0.2:8080
[2024-08-26 13:10:31] Stopping experiment, please wait...
[2024-08-26 13:10:31] Saving experiment checkpoint...
[2024-08-26 13:10:31] Stopping NNI manager, if any...
[2024-08-26 13:10:31] INFO (ShutdownManager) Initiate shutdown: REST request
[2024-08-26 13:10:31] INFO (RestServer) Stopping REST server.
[2024-08-26 13:10:31] ERROR (ShutdownManager) Error during shutting down NniManager: TypeError: Cannot read properties of undefined (reading 'getBufferedAmount')
    at TunerServer.sendCommand (/usr/local/lib/python3.8/dist-packages/nni_node/core/tuner_command_channel.js:60:26)
    at NNIManager.stopExperimentTopHalf (/usr/local/lib/python3.8/dist-packages/nni_node/core/nnimanager.js:303:25)
    at NNIManager.stopExperiment (/usr/local/lib/python3.8/dist-packages/nni_node/core/nnimanager.js:292:20)
    at /usr/local/lib/python3.8/dist-packages/nni_node/common/globals/shutdown.js:49:23
    at Array.map (<anonymous>)
    at ShutdownManager.shutdown (/usr/local/lib/python3.8/dist-packages/nni_node/common/globals/shutdown.js:47:51)
    at ShutdownManager.initiate (/usr/local/lib/python3.8/dist-packages/nni_node/common/globals/shutdown.js:22:18)
    at /usr/local/lib/python3.8/dist-packages/nni_node/rest_server/restHandler.js:366:40
    at Layer.handle [as handle_request] (/usr/local/lib/python3.8/dist-packages/nni_node/node_modules/express/lib/router/layer.js:95:5)
    at next (/usr/local/lib/python3.8/dist-packages/nni_node/node_modules/express/lib/router/route.js:144:13)
[2024-08-26 13:10:31] INFO (NNIManager) Change NNIManager status from: RUNNING to: STOPPING
[2024-08-26 13:10:31] INFO (NNIManager) Stopping experiment, cleaning up ...
[2024-08-26 13:10:31] INFO (ShutdownManager) Shutdown complete.
[2024-08-26 13:10:31] INFO (RestServer) REST server stopped.
[2024-08-26 13:10:31] Experiment stopped.
root@e44bc2dd4409:/workspace/MediaPipePyTorch# Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.8/dist-packages/nni/__main__.py"", line 85, in <module>
    main()
  File ""/usr/local/lib/python3.8/dist-packages/nni/__main__.py"", line 58, in main
    dispatcher = MsgDispatcher(url, tuner, assessor)
  File ""/usr/local/lib/python3.8/dist-packages/nni/runtime/msg_dispatcher.py"", line 71, in __init__
    super().__init__(command_channel_url)
  File ""/usr/local/lib/python3.8/dist-packages/nni/runtime/msg_dispatcher_base.py"", line 47, in __init__
    self._channel.connect()
  File ""/usr/local/lib/python3.8/dist-packages/nni/runtime/tuner_command_channel/channel.py"", line 58, in connect
    self._channel.connect()
  File ""/usr/local/lib/python3.8/dist-packages/nni/runtime/command_channel/websocket/channel.py"", line 23, in connect
    self._ensure_conn()
  File ""/usr/local/lib/python3.8/dist-packages/nni/runtime/command_channel/websocket/channel.py"", line 75, in _ensure_conn
    self._conn.connect()
  File ""/usr/local/lib/python3.8/dist-packages/nni/runtime/command_channel/websocket/connection.py"", line 65, in connect
    self._ws = _wait(_connect_async(self._url))
  File ""/usr/local/lib/python3.8/dist-packages/nni/runtime/command_channel/websocket/connection.py"", line 121, in _wait
    return future.result()
  File ""/usr/lib/python3.8/concurrent/futures/_base.py"", line 444, in result
    return self.__get_result()
  File ""/usr/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""/usr/local/lib/python3.8/dist-packages/nni/runtime/command_channel/websocket/connection.py"", line 135, in _connect_async
    return await websockets.connect(url, max_size=None)  # type: ignore
  File ""/usr/local/lib/python3.8/dist-packages/websockets/legacy/client.py"", line 655, in __await_impl_timeout__
    return await self.__await_impl__()
  File ""/usr/local/lib/python3.8/dist-packages/websockets/legacy/client.py"", line 659, in __await_impl__
    _transport, _protocol = await self._create_connection()
  File ""/usr/lib/python3.8/asyncio/base_events.py"", line 1033, in create_connection
    raise OSError('Multiple exceptions: {}'.format(
OSError: Multiple exceptions: [Errno 111] Connect call failed ('127.0.0.1', 8080), [Errno 99] Cannot assign requested address
",jimmy133719,39381342,open,True,0,2024-08-26T12:43:00+00:00,2024-08-26T13:20:43+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2458237314,5802,"Once the experiment reaches a certain point, it generally stops running and reports an error.","**Describe the issue**:

""Once the experiment reaches a certain point, it generally stops running and reports an error.""

[2024-08-09 23:59:19] ERROR (nni.runtime.msg_dispatcher_base/Thread-1 (command_queue_worker)) 10
Traceback (most recent call last):
  File ""/root/miniconda3/lib/python3.10/site-packages/nni/runtime/msg_dispatcher_base.py"", line 108, in command_queue_worker
    self.process_command(command, data)
  File ""/root/miniconda3/lib/python3.10/site-packages/nni/runtime/msg_dispatcher_base.py"", line 154, in process_command
    command_handlers[command](data)
  File ""/root/miniconda3/lib/python3.10/site-packages/nni/runtime/msg_dispatcher.py"", line 148, in handle_report_metric_data
    self._handle_final_metric_data(data)
  File ""/root/miniconda3/lib/python3.10/site-packages/nni/runtime/msg_dispatcher.py"", line 201, in _handle_final_metric_data
    self.tuner.receive_trial_result(id_, _trial_params[id_], value, customized=customized,
  File ""/root/miniconda3/lib/python3.10/site-packages/nni/algorithms/hpo/tpe_tuner.py"", line 197, in receive_trial_result
    params = self._running_params.pop(parameter_id)
KeyError: 10

  content: '{""parameter_id"": 12, ""trial_job_id"": ""YbXt7"", ""type"": ""PERIODICAL"", ""sequence"": 199, ""value"": ""0.2895440735801888""}'
}
[2024-08-10 00:00:06] ERROR (WsChannel.__default__) Channel closed. Ignored command {
  type: 'ME',
  content: '{""parameter_id"": 12, ""trial_job_id"": ""YbXt7"", ""type"": ""FINAL"", ""sequence"": 0, ""value"": ""0.2898187191127104""}'
}
[2024-08-10 00:00:07] INFO (NNIManager) Trial job YbXt7 status changed from RUNNING to SUCCEEDED
[2024-08-10 00:00:07] ERROR (WsChannel.__default__) Channel closed. Ignored command {
  type: 'EN',
  content: '{""trial_job_id"":""YbXt7"",""event"":""SUCCEEDED"",""hyper_params"":""{\\""parameter_id\\"": 12, \\""parameter_source\\"": \\""algorithm\\"", \\""parameters\\"": {\\""activate\\"": \\""elu\\"", \\""d_emb\\"": 64, \\""d_hid\\"": 32, \\""drop\\"": 0.3884039376983632, \\""gamma\\"": 6.4905452738897065, \\""l1\\"": 1.4578424787079767, \\""l2\\"": 38.44410448714523, \\""l4\\"": 0.29277084068918136, \\""lr\\"": 9.015207683143664e-05, \\""mask\\"": 0.004542790568841141, \\""mode\\"": \\""GAT\\"", \\""t\\"": 0.6139793721895512, \\""mask_edge\\"": 0.07705512469912157, \\""instance_temperature\\"": 0.6737029785000441, \\""cluster_temperature\\"": 0.5472419195458156}, \\""parameter_index\\"": 0}""}'
}
[2024-08-10 00:00:07] ERROR (WsChannel.__default__) Channel closed. Ignored command { type: 'GE', content: '1' }



**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 


**How to reproduce it?**:",EternityJune25,143079174,open,True,1,2024-08-09T16:04:45+00:00,2024-08-20T16:48:13+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2444102625,5801,Does NNI ModelSpeedupTensorRT support Encoder-Decoder models?,"**Question**:
I have an encoder decoder model, quantized using TensorRT's packages for post-training quantization. It is in the HuggingFace transformers saved model format. The model is a TrOCR model, which is implemented with the HuggingFace VisionEncoderDecoder class. With Transformers, the encoder and decoder are in a single file, but when saving to ONNX format, the encoder and decoder become two different onnx files.

 I am trying to run this model through ModelSpeedupTensorRT, using the tutorial here: https://nni.readthedocs.io/en/stable/tutorials/quantization_speedup.html. When I tried to do `engine.compress_with_calibrator(calib)` with a calibrator I made from a dataloader, I had an error where my CPU RAM was being taken up by the conversion to ONNX format for some reason. To solve this, I had to convert the model myself, using the HuggingFace Optimum interface for ONNX Runtime.

When editing the source code to accomodate for this, I found the implementation of the `build_engine_with_calib()` method being called by `compress_with_calibrator()`:
 
```python3
def build_engine_with_calib(onnx_model_file, calib, input_shape):
    """"""
    Parameters
    ----------
    """"""
    builder = trt.Builder(TRT_LOGGER)
    network = builder.create_network(common.explicit_batch())
    trt_config = builder.create_builder_config()
    parser = trt.OnnxParser(network, TRT_LOGGER)

    builder.max_batch_size = input_shape[0]
    trt_config.max_workspace_size = common.GiB(8)
    trt_config.set_flag(trt.BuilderFlag.INT8)
    trt_config.set_flag(trt.BuilderFlag.FP16)
    trt_config.set_flag(trt.BuilderFlag.PREFER_PRECISION_CONSTRAINTS)
    trt_config.int8_calibrator = calib

    with open(onnx_model_file, 'rb') as model:
        if not parser.parse(model.read()):
            for error in range(parser.num_errors):
                TRT_LOGGER.log(TRT_LOGGER.ERROR, parser.get_error(error))
            raise ValueError('Failed to parse the ONNX file.')

    TRT_LOGGER.log(TRT_LOGGER.INFO, f'input number: {network.num_inputs}')
    TRT_LOGGER.log(TRT_LOGGER.INFO, f'output number: {network.num_outputs}')

    profile = builder.create_optimization_profile()
    input_name = network.get_input(0).name
    profile.set_shape(input_name, min=input_shape, opt=input_shape, max=input_shape)
    trt_config.add_optimization_profile(profile)

    config_network_to_int8(network) # not sure whether it is necessary because trt.BuilderFlag.INT8 is set.

    engine = builder.build_engine(network, trt_config)
    return engine
```
 
I noticed here that the ONNX model is being read as a single file, not from a directory. Because of this, will my Vision Encoder Decoder model not work with the ModelSpeedup, as it is saved as two different files?? Is there any way for me to make it work??",donjuanpond,85766054,open,True,1,2024-08-02T05:43:13+00:00,2024-08-16T03:45:17+00:00,,,3,3,0,0,0,0,0
microsoft/nni,2442788086,5800,How to display error messages,"When I encounter some errors, the process will exit without giving me any information. 
Actually, it is common to have some bugs in my code, but after 
experiment.run(port=8080, wait_completion=False,debug=True)
the new process will not print my debug info onto the terminal. 
For example, if I read a non-existent file, the process will fail sliently, and I have to gauss what has happened.
I attempt to read "" https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md "" but the file does't exist.

当使用nni运行带有bug的代码时，似乎nni会跳过此次trail，并且不报告错误在何处。例如我读取数据的路径出现了错误，在nni的终端里并不会报告此错误。
此外， "" https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md ""  也无法打开。
**Environment**:
- Training service (local|remote|pai|aml|etc): local
- Client OS: Windows 11 / ubuntu 20.04
- Python version: 3.8
- Is conda/virtualenv/venv used?: yes
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 

**How to reproduce it?**:
You can make a bug deliberatly, and you will see that nothing about the bug is reported.",ayachi3,75938599,open,True,0,2024-08-01T15:40:29+00:00,2024-08-01T15:40:29+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2391423139,5799,Pruner does not run in Python 3.7.x,"**Describe the issue**:
Literal is not available in the typing package of Python 3.7.x's. It can be fixed by importing from typing_extensions package. 

```
\anaconda3\envs\compression\lib\site-packages\nni\compression\base\config.py in <module>
      7 from copy import deepcopy
      8 import re
----> 9 from typing import Any, Dict, List, Literal, Tuple
     10 
     11 from schema import Schema, Optional, Or

ImportError: cannot import name 'Literal' from 'typing' (...\lib\typing.py)
```


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: windows 10
- Server OS (for remote mode only):
- Python version: 3.7.9
- PyTorch/TensorFlow version: 1.11.0
- Is conda/virtualenv/venv used?: conda
- Is running in Docker?: No


**Configuration**:
 - Experiment config (remember to remove secrets!):
 config_list = [{
    'op_types': ['Linear', 'Conv2d'], # types of layers to prune
    'exclude_op_names': ['fc3'], # exclude specific layers
    'sparse_ratio': 0.3 # mask 30% of the parameters
}]
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",adhocmaster,7497944,open,True,0,2024-07-04T20:17:52+00:00,2024-07-04T20:18:32+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2389523180,5798,Dispatcher crash with TPE KeyError,"**Describe the issue**:
It seems the dispatcher crashes for me from unknown causes, and when this happens, my experiment stops running.


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: Linux (Ubuntu 22.04)
- Server OS (for remote mode only):
- Python version: 3.10.14
- PyTorch/TensorFlow version: 2.2.1+cu118 (PyTorch)
- Is conda/virtualenv/venv used?: No (pyenv is used)
- Is running in Docker?: No


**Configuration**:
 - Experiment config (remember to remove secrets!):
```
from nni.experiment import Experiment
experiment = Experiment('local')
experiment.config.trial_command = 'python model.py'
experiment.config.trial_code_directory = '.'
experiment.config.search_space = search_space
experiment.config.tuner.name = 'TPE'
experiment.config.tuner.class_args['optimize_mode'] = 'minimize'
experiment.config.max_trial_number = 1000
experiment.config.trial_concurrency = 1
experiment.run(8080)
```
 - Search space:
```
search_space = {
    ""hidden_sizes"": {
        ""_type"": ""choice"",
        ""_value"": [[], [256], [512], [1024], [1024, 512], [1024, 512, 256], [512, 256]]
    },
    ""learning_rate"": {
        ""_type"": ""loguniform"",
        ""_value"": [0.000001, 0.1]
    },
    ""batch_size"": {
        ""_type"": ""choice"",
        ""_value"": [32, 64, 128]
    },
    ""num_epochs"": {
        ""_type"": ""randint"",
        ""_value"": [100, 1000]
    },
    ""dropout_prob"": {
        ""_type"": ""uniform"",
        ""_value"": [0.0, 0.5]
    },
    ""use_batch_norm"": {
        ""_type"": ""choice"",
        ""_value"": [True, False]
    },
    ""activation_fn"": {
        ""_type"": ""choice"",
        ""_value"": [""relu"", ""leaky_relu"", ""sigmoid"", ""tanh"", ""elu"", ""selu""]
    },
    ""patience"": {
        ""_type"": ""randint"",
        ""_value"": [0, 10]
    }
}
```

**Log message**:
 - nnimanager.log:
 (relevant snippet)
```
[2024-07-03 17:10:21] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 46,
  hyperParameters: {
    value: '{""parameter_id"": 46, ""parameter_source"": ""algorithm"", ""parameters"": {""hidden_sizes"": [256], ""learning_rate"": 0.004027533073627928, ""batch_size"": 128, ""num_epochs"": 748, ""dropout_prob"": 0.18980965379785528, ""use_batch_norm"": true, ""activation_fn"": ""selu"", ""patience"": 6}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-07-03 17:10:21] INFO (LocalV3.local) Created trial eDGmO
[2024-07-03 17:10:22] INFO (LocalV3.local) Trial parameter: eDGmO {""parameter_id"": 46, ""parameter_source"": ""algorithm"", ""parameters"": {""hidden_sizes"": [256], ""learning_rate"": 0.004027533073627928, ""batch_size"": 128, ""num_epochs"": 748, ""dropout_prob"": 0.18980965379785528, ""use_batch_norm"": true, ""activation_fn"": ""selu"", ""patience"": 6}, ""parameter_index"": 0}
[2024-07-03 17:10:29] ERROR (WsChannel.__default__) Channel closed. Ignored command {
  type: 'ME',
  content: '{""parameter_id"": 46, ""trial_job_id"": ""eDGmO"", ""type"": ""PERIODICAL"", ""sequence"": 1, ""value"": ""14.616238377757908""}'
}
[2024-07-03 17:10:29] ERROR (WsChannel.__default__) Channel closed. Ignored command {
  type: 'ME',
  content: '{""parameter_id"": 46, ""trial_job_id"": ""eDGmO"", ""type"": ""PERIODICAL"", ""sequence"": 2, ""value"": ""10.664397033219485""}'
}
```
 - dispatcher.log:
```
[2024-07-03 16:45:49] INFO (nni.tuner.tpe/MainThread) Using random seed 668056533
[2024-07-03 16:45:49] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher started
[2024-07-03 16:45:49] INFO (nni.runtime.msg_dispatcher/Thread-1 (command_queue_worker)) Initial search space: {'hidden_sizes': {'_type': 'choice', '_value': [[], [256], [512], [1024], [1024, 512], [1024, 512, 256], [512, 256]]}, 'learning_rate': {'_type': 'loguniform', '_value': [1e-06, 0.1]}, 'batch_size': {'_type': 'choice', '_value': [32, 64, 128]}, 'num_epochs': {'_type': 'randint', '_value': [100, 1000]}, 'dropout_prob': {'_type': 'uniform', '_value': [0, 0.5]}, 'use_batch_norm': {'_type': 'choice', '_value': [True, False]}, 'activation_fn': {'_type': 'choice', '_value': ['relu', 'leaky_relu', 'sigmoid', 'tanh', 'elu', 'selu']}, 'patience': {'_type': 'randint', '_value': [0, 10]}}
[2024-07-03 17:10:21] ERROR (nni.runtime.msg_dispatcher_base/Thread-1 (command_queue_worker)) 45
Traceback (most recent call last):
  File ""/home/josep/.pyenv/versions/3.10.14/lib/python3.10/site-packages/nni/runtime/msg_dispatcher_base.py"", line 108, in command_queue_worker
    self.process_command(command, data)
  File ""/home/josep/.pyenv/versions/3.10.14/lib/python3.10/site-packages/nni/runtime/msg_dispatcher_base.py"", line 154, in process_command
    command_handlers[command](data)
  File ""/home/josep/.pyenv/versions/3.10.14/lib/python3.10/site-packages/nni/runtime/msg_dispatcher.py"", line 148, in handle_report_metric_data
    self._handle_final_metric_data(data)
  File ""/home/josep/.pyenv/versions/3.10.14/lib/python3.10/site-packages/nni/runtime/msg_dispatcher.py"", line 201, in _handle_final_metric_data
    self.tuner.receive_trial_result(id_, _trial_params[id_], value, customized=customized,
  File ""/home/josep/.pyenv/versions/3.10.14/lib/python3.10/site-packages/nni/algorithms/hpo/tpe_tuner.py"", line 197, in receive_trial_result
    params = self._running_params.pop(parameter_id)
KeyError: 45
[2024-07-03 17:10:28] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher exiting...
[2024-07-03 17:10:28] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher terminiated

```


**How to reproduce it?**: 

It happens not just once for me, but occasionally with different experiments. I tried lowering concurrency to 1 in order to avoid it, but it appears nonetheless.

In this example, it was trial 45 evidently which caused the crash. In the web ui, I can see that trial 45 succeeded and there is a recorded metric value for it. Yet, when TPE goes to find its parameters, it seems it cannot find them?",Fripplebubby,3044352,open,True,3,2024-07-03T21:33:41+00:00,2024-08-26T07:01:24+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2388320392,5797,Update README.md,"remove wechat group qr code as it is not maintained.



",scarlett2018,39592018,open,True,0,2024-07-03T10:55:09+00:00,2024-07-03T10:57:17+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2387932270,5796,"model space ""num_labels"" parameter inconsistent among various classes ","when using the premade model spaces e.g. mobile net , there is a parameter to change the classification layer to align with our own dataset e.g. 

<img width=""196"" alt=""image"" src=""https://github.com/microsoft/nni/assets/55270549/3c709971-2e13-438e-9d8f-4796332b9d70"">


Why is this not a parameter for other model spaces e.g. dart space : 
<img width=""214"" alt=""image"" src=""https://github.com/microsoft/nni/assets/55270549/4b4b2c6c-3205-43d5-952f-5ab3b5d7a930"">

for the Dart model space, there is only the option to use CIFAR or IMAGENET model spaces. But what if I want to change the number of labels to 2 for my own dataset? ",btebbutt,55270549,open,True,0,2024-07-03T07:50:21+00:00,2024-07-03T07:50:21+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2371566194,5795,How to load model in test dataset,"In the DARTS tutorial, I replaced Cifar10 with my own dataset and the val results were good, but the Acc performance on my custom test.py file was very poor. I think there is a problem with model loading. Could you tell me how to load the model correctly and run it on the test dataset?
Retrain result:
![image](https://github.com/microsoft/nni/assets/101927546/044b90b9-28ed-451d-bf37-28fe56e3d4c9)
test result
![image](https://github.com/microsoft/nni/assets/101927546/41f57aff-e147-4eea-9893-65dffc360717)
Help me!!!!",Marks00996,101927546,open,True,0,2024-06-25T03:37:38+00:00,2024-06-25T03:37:38+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2362317294,5794,Model speedup fails due to Attribute Error,"**Describe the bug**:
I am attempting to run this pruned mirror detection model [https://github.com/memgonzales/mirror-segmentation], but when I attempt to  perform speedup_model(), I get the following error:
```Traceback (most recent call last):
  File ""C:\Users\pillai.k\AppData\Local\Programs\Python\Python39\lib\runpy.py"", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""C:\Users\pillai.k\AppData\Local\Programs\Python\Python39\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\pillai.k\PMDLite\mirror-segmentation-main\prune.py"", line 178, in <module>
    main()
  File ""C:\Users\pillai.k\PMDLite\mirror-segmentation-main\prune.py"", line 127, in main
    ModelSpeedup(net, dummy, masks).speedup_model()
  File ""C:\Users\pillai.k\MirrorEnv\lib\site-packages\nni\compression\speedup\model_speedup.py"", line 429, in speedup_model
    self.fix_mask_conflict()
  File ""C:\Users\pillai.k\MirrorEnv\lib\site-packages\nni\compression\speedup\model_speedup.py"", line 243, in fix_mask_conflict
    fix_channel_mask_conflict(self.graph_module, self.masks)
  File ""C:\Users\pillai.k\MirrorEnv\lib\site-packages\nni\compression\speedup\mask_conflict.py"", line 229, in fix_channel_mask_conflict
    prune_axis = detect_mask_prune_dim(graph_module, masks)
  File ""C:\Users\pillai.k\MirrorEnv\lib\site-packages\nni\compression\speedup\mask_conflict.py"", line 400, in detect_mask_prune_dim
    sub_module = graph_module.get_submodule(layer_name)
  File ""C:\Users\pillai.k\MirrorEnv\lib\site-packages\torch\nn\modules\module.py"", line 686, in get_submodule
    raise AttributeError(mod._get_name() + "" has no ""
AttributeError: BFE_Module has no attribute `cbam`
```
This error occurs for the mask element `edge_extract.cbam.ChannelGate.mlp.1`. Given below is the model description for this module:
```
  (edge_extract): BFE_Module(
    (cbam): CBAM(
      (ChannelGate): ChannelGate(
        (mlp): Sequential(
          (0): Flatten()
          (1): Linear(in_features=56, out_features=3, bias=True)
          (2): ReLU(inplace=True)
          (3): Linear(in_features=3, out_features=56, bias=True)
        )
      )
      (SpatialGate): SpatialGate(
        (compress): ChannelPool()
        (spatial): BasicConv(
          (conv): Conv2d(1, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)
          (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)
        )
      )
    )
    (edge_layer1): Sequential(
      (0): Conv2d(56, 28, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (edge_layer2): Sequential(
      (0): Conv2d(56, 28, kernel_size=(3, 3), stride=(1, 1))
      (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (edge_layer3): Sequential(
      (0): Conv2d(56, 28, kernel_size=(3, 3), stride=(1, 1), dilation=(2, 2))
      (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (edge_layer4): Sequential(
      (0): Conv2d(56, 28, kernel_size=(3, 3), stride=(1, 1), dilation=(4, 4))
      (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
  ) 
```



**Environment**:
- NNI version:3.0
- Training service (local|remote|pai|aml|etc):local
- Python version: Python 3.9.4
- PyTorch version: 2.1.2
- Cpu or cuda version: cpu


**Reproduce the problem**
- Code|Example:
Model weights can be downloaded at https://drive.google.com/file/d/18zsqjK1aHVC4D8Ky530C--fwxdQylQ37/view?usp=sharing
clone the above mentioned github and add the following to the main function of `prune.py` :
```
def main():
    net = model.to(device)
    
    # Load model weights and biases. Change the device ordinal as needed.
    old_state_dict=torch.load(pruned_weights_path, map_location=device)
    new_state_dict={}
    for key in old_state_dict.keys():
        new_key=key.replace('module.','_nni_wrapper.')
        if('_mask' in key):
            tmp=new_key.split('.')
            tmp.insert(-1,""_nni_wrapper"")
            new_key=(""."".join(tmp))
        new_state_dict[new_key]=old_state_dict[key]
    net.load_state_dict(new_state_dict)

    pruner.unwrap_model()
    fp=open('model_desc.txt','w')
    print(net,file=fp)
    fp.close()
    from nni.compression.speedup import ModelSpeedup
    ModelSpeedup(net, dummy, masks).speedup_model()
    ...rest of main()

```

- How to reproduce:
- set the path to pruned weights file and run `prune.py`",krteyu,102747037,open,True,1,2024-06-19T12:54:14+00:00,2024-06-20T05:18:55+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2356185877,5793,Trial details page empty,"**Describe the issue**:
Trial details page is empty while overview page and others seems okay. I am using Edge browser. Here is a screenshot of total blank details page below, after clicking the button:

<img width=""1459"" alt=""image"" src=""https://github.com/microsoft/nni/assets/20085697/1fca0dc1-fb14-43aa-ad31-807f5e0adba4"">


Overview page is okay:

<img width=""1432"" alt=""image"" src=""https://github.com/microsoft/nni/assets/20085697/01a0328f-6341-4ab5-86b1-6fe808e4be5a"">


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: Ubuntu 22.04 docker container
- Server OS (for remote mode only): NaN
- Python version:  3.10.12
- PyTorch/TensorFlow version: PyTorch 2.3.1+cu121
- Is conda/virtualenv/venv used?:No
- Is running in Docker?: Yes


**Configuration**:
 - Experiment config (remember to remove secrets!):
```
{
  ""params"": {
    ""experimentType"": ""hpo"",
    ""trialCommand"": ""python dl_run.py --use_nni --config /life_changer/experiments/ws_related/train/nni/v25/default.yaml"",
    ""trialCodeDirectory"": ""/life_changer/experiments/ws_related/train"",
    ""trialConcurrency"": 1,
    ""maxTrialDuration"": ""1h"",
    ""useAnnotation"": false,
    ""debug"": false,
    ""logLevel"": ""info"",
    ""experimentWorkingDirectory"": ""/life_changer/experiments/ws_related/train/nni/experiments"",
    ""tuner"": {
      ""name"": ""GridSearch""
    },
    ""trainingService"": {
      ""platform"": ""local"",
      ""trialCommand"": ""python dl_run.py --use_nni --config /life_changer/experiments/ws_related/train/nni/v25/default.yaml"",
      ""trialCodeDirectory"": ""/life_changer/experiments/ws_related/train"",
      ""debug"": false,
      ""maxTrialNumberPerGpu"": 1,
      ""reuseMode"": false
    }
  },
  ""execDuration"": ""1m 40s"",
  ""nextSequenceId"": 2,
  ""revision"": 14
}
```
 - Search space:
```
{
  ""model.embedding.mark_unknown_as_padding"": {
    ""_type"": ""choice"",
    ""_value"": [true, false]
  },
  ""model.embedding.init_weight_by_numerical_intensity"": {
    ""_type"": ""choice"",
    ""_value"": [true, false]
  },
  ""model.cross_layer.sub_net_dims"": {
    ""_type"": ""choice"",
    ""_value"": [4, 16]
  },
  ""model.cross_layer.score_fn"": {
    ""_type"": ""choice"",
    ""_value"": [
      {
        ""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}
      },
      {
        ""softmax"": {""use"": true}
      },
      {
        ""silu"": {""use"": true}
      }
    ]
  },
  ""model.cross_layer.use_lhuc"": {
    ""_type"": ""choice"",
    ""_value"": [true, false]
  },
  ""model.cross_layer.global_score_fn"": {
    ""_type"": ""choice"",
    ""_value"": [true, false]
  },
  ""model.output_layer.moe.topk"": {
    ""_type"": ""choice"",
    ""_value"": [
      null,
      2
    ]
  },
  ""model.output_layer.moe.num_experts"": {
    ""_type"": ""choice"",
    ""_value"": [8, 32]
  },
  ""model.output_layer.moe.experts_share_input"": {
    ""_type"": ""choice"",
    ""_value"": [true, false]
  },
  ""model.output_layer.moe.gating_input"": {
    ""_type"": ""choice"",
    ""_value"": [""odds_team_ha_emb"", ""same_as_expert""]
  },
  ""model.loss"": {
    ""_type"": ""choice"",
    ""_value"": [
      {
        ""focal"": {""use"": true, ""gamma"": 2}
      },
      {
        ""ghmc"": {""use"": true, ""bins"": 10}
      },
      {
        ""wdl_prob_rank"": {""use"": true, ""top_k_frac"": 0.1, ""top_k_weight"""": 10}
      },
      {
        ""odds_weighted"": {""use"": true, ""pow_of_n"": 2}
      },
      {
        ""pred_odds_topk"": {
          ""use"": true,
          ""top_k_frac"": 0.1,
          ""top_k_weight"": 10,
          ""weight_by_odds"": false
        }
      },
      {
        ""pred_odds_topk"": {
          ""use"": true,
          ""top_k_frac"": 0.1,
          ""top_k_weight"": null,
          ""weight_by_odds"": true
        }
      },
      {
        ""pred_odds_topk"": {
          ""use"": true,
          ""top_k_frac"": 0.2,
          ""top_k_weight"": 5,
          ""weight_by_odds"": false
        }
      },
      {
        ""preds_correct_odds"": {
          ""use"": true,
          ""threshold"": 3,
          ""weight"": 10,
          ""weight_by_odds"": false
        }
      },
      {
        ""preds_correct_odds"": {
          ""use"": true,
          ""threshold"": 3,
          ""weight"": null,
          ""weight_by_odds"": true
        }
      }
    ]
  },
  ""model.degree_1.last_n"": {
    ""_type"": ""choice"",
    ""_value"": [1, 3]
  },
  ""model.degree_1.use_lhuc"": {
    ""_type"": ""choice"",
    ""_value"": [true, false]
  },
  ""optimizer"": {
    ""_type"": ""choice"",
    ""_value"": [
      {
        ""_name"": ""RAdam"",
        ""weight_decay"": {
          ""_type"": ""choice"",
          ""_value"": [0.0001, 0.001]
        }
      },
      {
        ""_name"": ""Ranger"",
        ""weight_decay"": {
          ""_type"": ""choice"",
          ""_value"": [0.0001, 0.001]
        }
      }
    ]
  }
}
```


**Log message**:
 - nnimanager.log:
```
[2024-06-17 01:58:52] INFO (main) Start NNI manager
[2024-06-17 01:58:52] INFO (RestServer) Starting REST server at port 8888, URL prefix: ""/""
[2024-06-17 01:58:52] INFO (RestServer) REST server started.
[2024-06-17 01:58:53] INFO (NNIDataStore) Datastore initialization done
[2024-06-17 01:58:54] INFO (NNIManager) Starting experiment: l2yv4dn1
[2024-06-17 01:58:54] INFO (NNIManager) Setup training service...
[2024-06-17 01:58:54] INFO (NNIManager) Setup tuner...
[2024-06-17 01:58:54] INFO (NNIManager) Change NNIManager status from: INITIALIZED to: RUNNING
[2024-06-17 01:58:54] INFO (NNIManager) Add event listeners
[2024-06-17 01:58:54] INFO (LocalV3.local) Start
[2024-06-17 01:58:54] INFO (NNIManager) NNIManager received command from dispatcher: ID, 
[2024-06-17 01:58:54] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""RAdam"", ""weight_decay"": 0.0001}}, ""parameter_index"": 0}
[2024-06-17 01:58:55] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 0,
  hyperParameters: {
    value: '{""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""RAdam"", ""weight_decay"": 0.0001}}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-06-17 01:58:55] INFO (LocalV3.local) Register directory trial_code = /life_changer/experiments/ws_related/train
[2024-06-17 01:58:55] INFO (LocalV3.local) Created trial FOUTA
[2024-06-17 01:58:58] INFO (LocalV3.local) Trial parameter: FOUTA {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""RAdam"", ""weight_decay"": 0.0001}}, ""parameter_index"": 0}
[2024-06-17 02:00:08] INFO (NNIManager) Trial job FOUTA status changed from RUNNING to SUCCEEDED
[2024-06-17 02:00:09] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 1, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""RAdam"", ""weight_decay"": 0.001}}, ""parameter_index"": 0}
[2024-06-17 02:00:09] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 1,
  hyperParameters: {
    value: '{""parameter_id"": 1, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""RAdam"", ""weight_decay"": 0.001}}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-06-17 02:00:09] INFO (LocalV3.local) Created trial sCcur
[2024-06-17 02:00:12] INFO (LocalV3.local) Trial parameter: sCcur {""parameter_id"": 1, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""RAdam"", ""weight_decay"": 0.001}}, ""parameter_index"": 0}
[2024-06-17 02:01:20] INFO (NNIManager) Trial job sCcur status changed from RUNNING to SUCCEEDED
[2024-06-17 02:01:20] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 2, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""Ranger"", ""weight_decay"": 0.0001}}, ""parameter_index"": 0}
[2024-06-17 02:01:20] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 2,
  hyperParameters: {
    value: '{""parameter_id"": 2, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""Ranger"", ""weight_decay"": 0.0001}}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-06-17 02:01:20] INFO (LocalV3.local) Created trial y553t
[2024-06-17 02:01:23] INFO (LocalV3.local) Trial parameter: y553t {""parameter_id"": 2, ""parameter_source"": ""algorithm"", ""parameters"": {""model.embedding.mark_unknown_as_padding"": true, ""model.embedding.init_weight_by_numerical_intensity"": true, ""model.cross_layer.sub_net_dims"": 4, ""model.cross_layer.score_fn"": {""softplus"": {""use"": true, ""beta"": 5, ""threshold"": 100}}, ""model.cross_layer.use_lhuc"": true, ""model.cross_layer.global_score_fn"": true, ""model.output_layer.moe.topk"": null, ""model.output_layer.moe.num_experts"": 8, ""model.output_layer.moe.experts_share_input"": true, ""model.output_layer.moe.gating_input"": ""odds_team_ha_emb"", ""model.loss"": {""focal"": {""use"": true, ""gamma"": 2}}, ""model.degree_1.last_n"": 1, ""model.degree_1.use_lhuc"": true, ""optimizer"": {""_name"": ""Ranger"", ""weight_decay"": 0.0001}}, ""parameter_index"": 0}
```
 - dispatcher.log:
```
[2024-06-17 01:58:54] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher started
[2024-06-17 01:58:54] INFO (nni.runtime.msg_dispatcher/Thread-1 (command_queue_worker)) Initial search space: {'model.embedding.mark_unknown_as_padding': {'_type': 'choice', '_value': [True, False]}, 'model.embedding.init_weight_by_numerical_intensity': {'_type': 'choice', '_value': [True, False]}, 'model.cross_layer.sub_net_dims': {'_type': 'choice', '_value': [4, 16]}, 'model.cross_layer.score_fn': {'_type': 'choice', '_value': [{'softplus': {'use': True, 'beta': 5, 'threshold': 100}}, {'softmax': {'use': True}}, {'silu': {'use': True}}]}, 'model.cross_layer.use_lhuc': {'_type': 'choice', '_value': [True, False]}, 'model.cross_layer.global_score_fn': {'_type': 'choice', '_value': [True, False]}, 'model.output_layer.moe.topk': {'_type': 'choice', '_value': [None, 2]}, 'model.output_layer.moe.num_experts': {'_type': 'choice', '_value': [8, 32]}, 'model.output_layer.moe.experts_share_input': {'_type': 'choice', '_value': [True, False]}, 'model.output_layer.moe.gating_input': {'_type': 'choice', '_value': ['odds_team_ha_emb', 'same_as_expert']}, 'model.loss': {'_type': 'choice', '_value': [{'focal': {'use': True, 'gamma': 2}}, {'ghmc': {'use': True, 'bins': 10}}, {'wdl_prob_rank': {'use': True, 'top_k_frac': 0.1, 'top_k_weight""': 10}}, {'odds_weighted': {'use': True, 'pow_of_n': 2}}, {'pred_odds_topk': {'use': True, 'top_k_frac': 0.1, 'top_k_weight': 10, 'weight_by_odds': False}}, {'pred_odds_topk': {'use': True, 'top_k_frac': 0.1, 'top_k_weight': None, 'weight_by_odds': True}}, {'pred_odds_topk': {'use': True, 'top_k_frac': 0.2, 'top_k_weight': 5, 'weight_by_odds': False}}, {'preds_correct_odds': {'use': True, 'threshold': 3, 'weight': 10, 'weight_by_odds': False}}, {'preds_correct_odds': {'use': True, 'threshold': 3, 'weight': None, 'weight_by_odds': True}}]}, 'model.degree_1.last_n': {'_type': 'choice', '_value': [1, 3]}, 'model.degree_1.use_lhuc': {'_type': 'choice', '_value': [True, False]}, 'optimizer': {'_type': 'choice', '_value': [{'_name': 'RAdam', 'weight_decay': {'_type': 'choice', '_value': [0.0001, 0.001]}}, {'_name': 'Ranger', 'weight_decay': {'_type': 'choice', '_value': [0.0001, 0.001]}}]}}
[2024-06-17 01:58:54] INFO (nni.tuner.gridsearch/Thread-1 (command_queue_worker)) Grid initialized, size: (2×2×2×3×2×2×2×2×2×2×9×2×2×2×2×2) = 442368
```
 - nnictl stdout and stderr:
 ```
--------------------------------------------------------------------------------
Experiment l2yv4dn1 start: 2024-06-17 01:58:52.009267
--------------------------------------------------------------------------------
```

<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:
Maybe hard to reproduce because the exact env is complicated on my machine.",Interfish,20085697,open,True,1,2024-06-17T02:06:31+00:00,2024-06-17T15:32:24+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2337357339,5792,MY Trial keeps on failing.,"I am trying to run a NAS implementation and all of the trial (trial jobs) are seen as failed. The dispatcher sends hyperparameter configurations for different neural network architectures to the NNI manager one by one. The manager creates trials (M0YoL, gNQ3c, etc.) with these configurations and submits them to the LocalV3.local service for execution. Each trial ends with a ""FAILED"" status. Unfortunately, the logs don't reveal the specific reason for the failures. New hyperparameter sets are received: Even though previous trials failed, the dispatcher keeps sending new hyperparameter configurations for the NNI manager to try. 

Here is a snippet from the NNI manager log:
[2024-06-06 12:31:17] INFO (main) Start NNI manager
[2024-06-06 12:31:17] INFO (RestServer) Starting REST server at port 8080, URL prefix: ""/""
[2024-06-06 12:31:17] INFO (RestServer) REST server started.
[2024-06-06 12:31:17] INFO (NNIDataStore) Datastore initialization done
[2024-06-06 12:31:17] INFO (NNIManager) Starting experiment: ldkwaep5
[2024-06-06 12:31:17] INFO (NNIManager) Setup training service...
[2024-06-06 12:31:17] INFO (NNIManager) Setup tuner...
[2024-06-06 12:31:17] INFO (NNIManager) Change NNIManager status from: INITIALIZED to: RUNNING
[2024-06-06 12:31:18] INFO (NNIManager) Add event listeners
[2024-06-06 12:31:18] INFO (LocalV3.local) Start
[2024-06-06 12:31:18] INFO (NNIManager) NNIManager received command from dispatcher: ID, 
[2024-06-06 12:31:18] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""LSTM"", ""layer1_units"": 128, ""layer2_type"": ""GRU"", ""layer2_units"": 128, ""layer3_type"": ""LSTM"", ""layer3_units"": 32, ""dropout_rate"": 0.5}, ""parameter_index"": 0}
[2024-06-06 12:31:19] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 0,
  hyperParameters: {
    value: '{""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""LSTM"", ""layer1_units"": 128, ""layer2_type"": ""GRU"", ""layer2_units"": 128, ""layer3_type"": ""LSTM"", ""layer3_units"": 32, ""dropout_rate"": 0.5}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-06-06 12:31:19] INFO (LocalV3.local) Register directory trial_code = /home/kd/nni/second_nas
[2024-06-06 12:31:19] INFO (LocalV3.local) Created trial M0YoL
[2024-06-06 12:31:20] INFO (LocalV3.local) Trial parameter: M0YoL {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""LSTM"", ""layer1_units"": 128, ""layer2_type"": ""GRU"", ""layer2_units"": 128, ""layer3_type"": ""LSTM"", ""layer3_units"": 32, ""dropout_rate"": 0.5}, ""parameter_index"": 0}
[2024-06-06 12:31:21] INFO (NNIManager) Trial job M0YoL status changed from RUNNING to FAILED
[2024-06-06 12:31:21] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 1, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""GRU"", ""layer1_units"": 64, ""layer2_type"": ""GRU"", ""layer2_units"": 64, ""layer3_type"": ""LSTM"", ""layer3_units"": 32, ""dropout_rate"": 0.5}, ""parameter_index"": 0}
[2024-06-06 12:31:21] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 1,
  hyperParameters: {
    value: '{""parameter_id"": 1, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""GRU"", ""layer1_units"": 64, ""layer2_type"": ""GRU"", ""layer2_units"": 64, ""layer3_type"": ""LSTM"", ""layer3_units"": 32, ""dropout_rate"": 0.5}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-06-06 12:31:21] INFO (LocalV3.local) Created trial gNQ3c
[2024-06-06 12:31:22] INFO (LocalV3.local) Trial parameter: gNQ3c {""parameter_id"": 1, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""GRU"", ""layer1_units"": 64, ""layer2_type"": ""GRU"", ""layer2_units"": 64, ""layer3_type"": ""LSTM"", ""layer3_units"": 32, ""dropout_rate"": 0.5}, ""parameter_index"": 0}
[2024-06-06 12:31:23] INFO (NNIManager) Trial job gNQ3c status changed from RUNNING to FAILED
[2024-06-06 12:31:23] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 2, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""LSTM"", ""layer1_units"": 128, ""layer2_type"": ""LSTM"", ""layer2_units"": 128, ""layer3_type"": ""GRU"", ""layer3_units"": 128, ""dropout_rate"": 0.5}, ""parameter_index"": 0}
[2024-06-06 12:31:24] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 2,
  hyperParameters: {
    value: '{""parameter_id"": 2, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""LSTM"", ""layer1_units"": 128, ""layer2_type"": ""LSTM"", ""layer2_units"": 128, ""layer3_type"": ""GRU"", ""layer3_units"": 128, ""dropout_rate"": 0.5}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-06-06 12:31:24] INFO (LocalV3.local) Created trial MBlBa
[2024-06-06 12:31:25] INFO (LocalV3.local) Trial parameter: MBlBa {""parameter_id"": 2, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 3, ""layer1_type"": ""LSTM"", ""layer1_units"": 128, ""layer2_type"": ""LSTM"", ""layer2_units"": 128, ""layer3_type"": ""GRU"", ""layer3_units"": 128, ""dropout_rate"": 0.5}, ""parameter_index"": 0}
[2024-06-06 12:31:26] INFO (NNIManager) Trial job MBlBa status changed from RUNNING to FAILED
[2024-06-06 12:31:26] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 3, ""parameter_source"": ""algorithm"", ""parameters"": {""num_layers"": 2, ""layer1_type"": ""GRU"", ""layer1_units"": 64, ""layer2_type"": ""LSTM"", ""layer2_units"": 32, ""layer3_type"": ""LSTM"", ""layer3_units"": 128, ""dropout_rate"": 0.8}, ""parameter_index"": 0}

And the contents of the Dispatcher log:
[2024-06-06 12:31:18] INFO (numexpr.utils/MainThread) Note: NumExpr detected 20 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-06-06 12:31:18] INFO (numexpr.utils/MainThread) NumExpr defaulting to 8 threads.
[2024-06-06 12:31:18] INFO (nni.tuner.tpe/MainThread) Using random seed 1280300711
[2024-06-06 12:31:18] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher started
[2024-06-06 12:31:18] INFO (nni.runtime.msg_dispatcher/Thread-1 (command_queue_worker)) Initial search space: {'num_layers': {'_type': 'choice', '_value': [1, 2, 3]}, 'layer1_type': {'_type': 'choice', '_value': ['LSTM', 'GRU']}, 'layer1_units': {'_type': 'choice', '_value': [32, 64, 128]}, 'layer2_type': {'_type': 'choice', '_value': ['LSTM', 'GRU']}, 'layer2_units': {'_type': 'choice', '_value': [32, 64, 128]}, 'layer3_type': {'_type': 'choice', '_value': ['LSTM', 'GRU']}, 'layer3_units': {'_type': 'choice', '_value': [32, 64, 128]}, 'dropout_rate': {'_type': 'choice', '_value': [0.2, 0.5, 0.8]}}
",karmad84,8774641,closed,True,0,2024-06-06T05:43:00+00:00,2024-06-06T07:02:30+00:00,2024-06-06T07:02:30+00:00,,0,0,0,0,0,0,0
microsoft/nni,2333025399,5791,Transitioning from classic HPO to Retiarii experiments: file layout and setup advice needed,"I have successfully used NNI for classic HPO experiments, leveraging configurations such as those detailed below. My setup includes a search_space.json, config.yml, and a trial.py script:

- search_space.json:
```json
{
  ""x"": {
    ""_type"": ""choice"",
    ""_value"": [0, 1, 2]
  },
  ""y"": {
    ""_type"": ""choice"",
    ""_value"": [0, 1, 2]
  }
}
```

- config.yml:
```yml
experimentName: ""Hello, NNI!""
experimentWorkingDirectory: ./runs
searchSpaceFile: search_space.json
trainingService:
  platform: local
trialCodeDirectory: .
trialCommand: python3 trial.py
trialConcurrency: 3
tuner:
  name: GridSearch
```

- trial.py:
```python
import nni

def f(x, y):
    return x - y

def main(params):
    x, y = params['x'], params['y']
    result = f(x, y)
    nni.report_final_result(result)

if __name__ == ""__main__"":
    params = nni.get_next_parameter()
    main(params)
```

Now, I am interested in exploring the Retiarii block to conduct experiments that involve evolving neural network architectures. Could anyone provide guidance or share examples on how to set up the necessary files and configurations for a Retiarii experiment, especially with a config file aside? ",dtamienER,158458048,open,True,0,2024-06-04T09:27:41+00:00,2024-06-04T09:27:41+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2325278321,5790,Error in model speedup when using a single logit output layer,"**Describe the issue**:



**Environment**: 
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: Ubuntu 18.04.4 LTS
- Server OS (for remote mode only):
- Python version: 3.9
- PyTorch/TensorFlow version: 1.12.0
- Is conda/virtualenv/venv used?: conda
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
```
AttributeError                            Traceback (most recent call last)
Input In [19], in <cell line: 9>()
     22 _, masks = pruner.compress()
     23 pruner.unwrap_model()
---> 25 model = ModelSpeedup(model, dummy_input, masks).speedup_model()

File ~/miniconda3/envs/optha/lib/python3.9/site-packages/nni/compression/speedup/model_speedup.py:435, in ModelSpeedup.speedup_model(self)
    433 self.initialize_update_sparsity()
    434 self.update_direct_sparsity()
--> 435 self.update_indirect_sparsity()
    436 self.logger.info('Resolve the mask conflict after mask propagate...')
    437 # fix_mask_conflict(self.masks, self.graph_module, self.dummy_input)

File ~/miniconda3/envs/optha/lib/python3.9/site-packages/nni/compression/speedup/model_speedup.py:306, in ModelSpeedup.update_indirect_sparsity(self)
    304 for node in reversed(self.graph_module.graph.nodes):
    305     node: Node
--> 306     self.node_infos[node].mask_updater.indirect_update_process(self, node)
    307     sp = f', {sparsity_stats(self.masks.get(node.target, {}))}' if node.op == 'call_module' else ''
    308     sp += f', {sparsity_stats({""output mask"": self.node_infos[node].output_masks})}'

File ~/miniconda3/envs/optha/lib/python3.9/site-packages/nni/compression/speedup/mask_updater.py:229, in LeafModuleMaskUpdater.indirect_update_process(self, model_speedup, node)
    227 for k, v in node_info.module.named_parameters():
    228     if isinstance(v, torch.Tensor) and model_speedup.tensor_propagate_check(v) and v.dtype in torch_float_dtype:
--> 229         grad_zero = v.grad.data == 0
    230         node_info.param_masks[k][grad_zero] = 0

AttributeError: 'NoneType' object has no attribute 'data'
```
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:

```
import torch
import torch.nn as nn
import torchvision.models as tvmodels

from nni.compression.pruning import L1NormPruner
from nni.compression.utils import auto_set_denpendency_group_ids
from nni.compression.speedup import ModelSpeedup

if __name__ == '__main__':
    model = tvmodels.resnet18()
    model.fc = nn.Linear(in_features=512, out_features=1, bias=True)

    config_list = [{
        'op_types': ['Conv2d'],
        'sparse_ratio': 0.1
    }]
    dummy_input = torch.rand(1, 3, 224, 224)
    config_list = auto_set_denpendency_group_ids(model, config_list, dummy_input)

    pruner = L1NormPruner(model, config_list)

    _, masks = pruner.compress()
    pruner.unwrap_model()

    model = ModelSpeedup(model, dummy_input, masks).speedup_model()
```",rishabh-WIAI,109497666,open,True,0,2024-05-30T10:10:29+00:00,2024-05-30T10:10:29+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2322186853,5788,I wanna know how to use python to output the best hyperparameters generating by NNI,Does NNI has API or method to output hte best hyperparameters?I wanna use the best params to run a model ,liljiang999,131887878,open,True,0,2024-05-29T01:51:59+00:00,2024-05-29T01:51:59+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2300601610,5783,WARNING: GPU found but will not be used. Please set `experiment.config.trial_gpu_number` to the number of GPUs you want to use for each trial.,"Hello, NAS! was found the problem:WARNING: GPU found but will not be used. Please set `experiment.config.trial_gpu_number` to the number of GPUs you want to use for each trial.
```[tasklist]
### Tasks
```
",xutongpure,61935352,open,True,1,2024-05-16T14:40:12+00:00,2024-05-29T02:27:43+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2295022670,5782,Pytorch HPO tutorials stucked in strating server,"**Describe the issue**:
The pytorch tutorials code stucked in Starting web server... for long time(more than 20mins), without setting up logged in console 




**Environment**:
- NNI version:3.0
- Training service (local|remote|pai|aml|etc):local
- Client OS:windows 11
- Server OS (for remote mode only): local
- Python version:3.9.11
- PyTorch/TensorFlow version:2.10
- Is conda/virtualenv/venv used?:yes
- Is running in Docker?:No


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - 
![image](https://github.com/microsoft/nni/assets/68151037/cf780746-f5c2-48c8-aee1-9b7f823bf1e1)

 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->
![image](https://github.com/microsoft/nni/assets/68151037/2eff21d0-afb8-4b08-9690-06e24d97afea)
![image](https://github.com/microsoft/nni/assets/68151037/d6004b85-dbb4-4038-a2a6-93a49c1ed0b7)


**How to reproduce it?**:",rrc693,68151037,closed,True,1,2024-05-14T10:10:23+00:00,2024-05-24T16:05:38+00:00,2024-05-15T03:03:25+00:00,,0,0,0,0,0,0,0
microsoft/nni,2286135664,5780,Training Services all URL fixed,"The URL's of mentioned training services in support section and Compression section are fixed which were showing **error 404 not found**. From this reference  
https://nni.readthedocs.io/en/latest/experiment/training_service/overview.html
",Onkarpatil7,142410764,open,True,2,2024-05-08T17:53:26+00:00,2024-05-11T08:40:15+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2285572355,5779,Fix 2 issues,"## Description ##
HPO with GPTuner is not working with current versions of SciPy

### Issue 1:
Starting from SciPy v1.11.0 the shape of argument x0 for scipy.optimize.minimize() should be of shape (n,). The current NNI implementation assumes shape (1, n).
**Related issues:**
 - https://github.com/aristoteleo/dynamo-release/issues/578

### Issue 2:
scipy.optimize.minimize() returns a scipy.optimize.OptimizeResult object. The current NNI implementation assumes the ""fun"" item to be an array, when the type should actually be a float.
**Related issues:**
 - [#4978](https://github.com/microsoft/nni/issues/4978) (closed but not fixed)

#### Test Options ####
  - [x] fast test
  - [ ] full test - HPO
  - [ ] full test - NAS
  - [ ] full test - compression

### Checklist ###
  - [ ] test case
  - [ ] doc

### How to test ###


",TimSchim,75132378,open,True,0,2024-05-08T13:14:03+00:00,2024-05-08T13:14:03+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2283200609,5778,Supporting string choices for GPTuner,"**What would you like to be added**:
Currently the search space for GPTuner only supports numerical values for type choice.
A possiblity to provide strings would be a convenient option.

**Why is this needed**:
While it is possible to replace strings with numerical encodings, this reduces readability of code and intuitive interpretability of the hyperparameter plots.

**Brief description of your proposal if any**:
Internal encoding to numerical values if strings are provided.

Duplicate of [#1546](https://github.com/microsoft/nni/issues/1546)",TimSchim,75132378,open,True,0,2024-05-07T12:45:54+00:00,2024-05-07T12:45:54+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2282329876,5777,Is there any way to run the ptq-quantization without training?,"**Describe the bug**:
As far as I know, most ptq-quantization methods needn't train again but it seems that the current ptq-quantization of nni must run with the training process. It would cost too many time to train again. Is there any way to run the ptq-quantization of nni without training?


**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Python version:
- PyTorch version:
- Cpu or cuda version:


**Reproduce the problem**
- Code|Example:


- How to reproduce:",powk2,169126233,open,True,0,2024-05-07T05:43:05+00:00,2024-05-07T05:43:05+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2277845717,5776,I cannot make it work on GPU for training,"## Description of the issue

I cannot run any experiment on GPU.

I have tried both with a Tesla P4, a P100 and a GTX 1060. I can only make it work using CPU only. 

I have tried many configs with setting useActiveGpu to True or False, trialGpuNumber to 1, gpuIndices: '0'. However it always couldn't complete a single architecture training.

I have tried both outside and inside a Docker container.

## Configuration
 - Experiment config: `nni/examples/trials/mnist-pytorch/config.yml`

## Outside a Docker container
### Environment
- NNI version: 3.0
- Training service: local
- Client OS: Debian 10
- Python version: 3.10.13
- PyTorch/TensorFlow version: 2.3.0+cu121
- Is conda/virtualenv/venv used?: yes

### Log message
#### nnimanager.log

```
[2024-05-03 10:54:56] WARNING (pythonScript) Python command [nni.tools.nni_manager_scripts.collect_gpu_info] has stderr: Traceback (most recent call last):
  File ""/opt/conda/lib/python3.10/runpy.py"", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/opt/conda/lib/python3.10/runpy.py"", line 86, in _run_code
    exec(code, run_globals)
  File ""/opt/conda/lib/python3.10/site-packages/nni/tools/nni_manager_scripts/collect_gpu_info.py"", line 174, in <module>
    main()
  File ""/opt/conda/lib/python3.10/site-packages/nni/tools/nni_manager_scripts/collect_gpu_info.py"", line 34, in main
    print(json.dumps(data), flush=True)
  File ""/opt/conda/lib/python3.10/json/__init__.py"", line 231, in dumps
    return _default_encoder.encode(obj)
  File ""/opt/conda/lib/python3.10/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/opt/conda/lib/python3.10/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/opt/conda/lib/python3.10/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type bytes is not JSON serializable
 
[2024-05-03 10:54:56] INFO (ShutdownManager) Initiate shutdown: training service initialize failed
[2024-05-03 10:54:56] ERROR (GpuInfoCollector) Failed to collect GPU info, collector output: 
[2024-05-03 10:54:56] ERROR (TrainingServiceCompat) Training srevice initialize failed: Error: TaskScheduler: Failed to collect GPU info
    at TaskScheduler.init (/opt/conda/lib/python3.10/site-packages/nni_node/common/trial_keeper/task_scheduler/scheduler.js:16:19)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async TaskSchedulerClient.start (/opt/conda/lib/python3.10/site-packages/nni_node/common/trial_keeper/task_scheduler_client.js:20:13)
    at async Promise.all (index 0)
    at async TrialKeeper.start (/opt/conda/lib/python3.10/site-packages/nni_node/common/trial_keeper/keeper.js:48:9)
    at async LocalTrainingServiceV3.start (/opt/conda/lib/python3.10/site-packages/nni_node/training_service/local_v3/local.js:28:9)
    at async V3asV1.start (/opt/conda/lib/python3.10/site-packages/nni_node/training_service/v3/compat.js:235:29
```

There, the GPU's infos cannot be retreived.

#### experiment.log

```
[2024-05-03 13:52:31] INFO (nni.experiment) Starting web server...
[2024-05-03 13:52:32] INFO (nni.experiment) Setting up...
[2024-05-03 13:52:33] INFO (nni.experiment) Web portal URLs: http://127.0.0.1:8081 http://10.164.0.8:8081 http://172.17.0.1:8081
[2024-05-03 13:53:03] INFO (nni.experiment) Stopping experiment, please wait...
[2024-05-03 13:53:03] INFO (nni.experiment) Saving experiment checkpoint...
[2024-05-03 13:53:03] INFO (nni.experiment) Stopping NNI manager, if any...
[2024-05-03 13:53:23] ERROR (nni.experiment) HTTPConnectionPool(host='localhost', port=8081): Read timed out. (read timeout=20)
Traceback (most recent call last):
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/urllib3/connectionpool.py"", line 537, in _make_request
    response = conn.getresponse()
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/urllib3/connection.py"", line 466, in getresponse
    httplib_response = super().getresponse()
  File ""/opt/conda/envs/nni/lib/python3.9/http/client.py"", line 1377, in getresponse
    response.begin()
  File ""/opt/conda/envs/nni/lib/python3.9/http/client.py"", line 320, in begin
    version, status, reason = self._read_status()
  File ""/opt/conda/envs/nni/lib/python3.9/http/client.py"", line 281, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""/opt/conda/envs/nni/lib/python3.9/socket.py"", line 704, in readinto
    return self._sock.recv_into(b)
socket.timeout: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/requests/adapters.py"", line 486, in send
    resp = conn.urlopen(
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/urllib3/connectionpool.py"", line 847, in urlopen
    retries = retries.increment(
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/urllib3/util/retry.py"", line 470, in increment
    raise reraise(type(error), error, _stacktrace)
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/urllib3/util/util.py"", line 39, in reraise
    raise value
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/urllib3/connectionpool.py"", line 793, in urlopen
    response = self._make_request(
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/urllib3/connectionpool.py"", line 539, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/urllib3/connectionpool.py"", line 370, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=8081): Read timed out. (read timeout=20)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/nni/experiment/experiment.py"", line 171, in _stop_nni_manager
    rest.delete(self.port, '/experiment', self.url_prefix)
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/nni/experiment/rest.py"", line 52, in delete
    request('delete', port, api, prefix=prefix)
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/nni/experiment/rest.py"", line 31, in request
    resp = requests.request(method, url, timeout=timeout)
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/requests/api.py"", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/requests/sessions.py"", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/requests/sessions.py"", line 703, in send
    r = adapter.send(request, **kwargs)
  File ""/opt/conda/envs/nni/lib/python3.9/site-packages/requests/adapters.py"", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=8081): Read timed out. (read timeout=20)
[2024-05-03 13:53:23] WARNING (nni.experiment) Cannot gracefully stop experiment, killing NNI process...
```

There is a timeout since data cannot be retreived.

## Inside a Docker container
### Dockerfile

```
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

FROM nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04

ARG NNI_RELEASE

LABEL maintainer='Microsoft NNI Team<nni@microsoft.com>'

ENV DEBIAN_FRONTEND=noninteractive 

RUN apt-get -y update
RUN apt-get -y install \
    automake \
    build-essential \
    cmake \
    curl \
    git \
    openssh-server \
    python3 \
    python3-dev \
    python3-pip \
    sudo \
    unzip \
    wget \
    zip
RUN apt-get clean
RUN rm -rf /var/lib/apt/lists/*

RUN ln -s python3 /usr/bin/python

RUN python3 -m pip --no-cache-dir install pip==22.0.3 setuptools==60.9.1 wheel==0.37.1

RUN python3 -m pip --no-cache-dir install \
    lightgbm==3.3.2 \
    numpy==1.22.2 \
    pandas==1.4.1 \
    scikit-learn==1.0.2 \
    scipy==1.8.0

RUN python3 -m pip --no-cache-dir install \
    torch==1.10.2+cu113 \
    torchvision==0.11.3+cu113 \
    torchaudio==0.10.2+cu113 \
    -f https://download.pytorch.org/whl/cu113/torch_stable.html
RUN python3 -m pip --no-cache-dir install pytorch-lightning==1.6.1

RUN python3 -m pip --no-cache-dir install tensorflow==2.9.1

RUN python3 -m pip --no-cache-dir install azureml==0.2.7 azureml-sdk==1.38.0

# COPY dist/nni-${NNI_RELEASE}-py3-none-manylinux1_x86_64.whl .
# RUN python3 -m pip install nni-${NNI_RELEASE}-py3-none-manylinux1_x86_64.whl
# RUN rm nni-${NNI_RELEASE}-py3-none-manylinux1_x86_64.whl

ENV PATH=/root/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/bin:/usr/bin:/usr/sbin

WORKDIR /root

RUN pip install nni
RUN git clone https://github.com/microsoft/nni.git

RUN apt-get -y update
RUN apt-get -y install nano
```

### Log message
#### nnimanager.log

```
root@1b02414e6d3e:~/nni-experiments/_latest/log# cat nnimanager.log 
[2024-05-03 14:46:11] DEBUG (WsChannelServer.tuner) Start listening tuner/:channel
[2024-05-03 14:46:11] INFO (main) Start NNI manager
[2024-05-03 14:46:11] INFO (RestServer) Starting REST server at port 8080, URL prefix: ""/""
[2024-05-03 14:46:11] INFO (RestServer) REST server started.
[2024-05-03 14:46:11] DEBUG (SqlDB) Database directory: /root/nni-experiments/o21hdgqs/db
[2024-05-03 14:46:11] INFO (NNIDataStore) Datastore initialization done
[2024-05-03 14:46:11] DEBUG (main) start() returned.
[2024-05-03 14:46:12] DEBUG (NNIRestHandler) GET: /check-status: body: {}
[2024-05-03 14:46:12] DEBUG (NNIRestHandler) POST: /experiment: body: {
  experimentType: 'hpo',
  searchSpaceFile: '/root/nni/examples/trials/mnist-pytorch/search_space.json',
  searchSpace: {
    batch_size: { _type: 'choice', _value: [Array] },
    hidden_size: { _type: 'choice', _value: [Array] },
    lr: { _type: 'choice', _value: [Array] },
    momentum: { _type: 'uniform', _value: [Array] }
  },
  trialCommand: 'python3 mnist.py',
  trialCodeDirectory: '/root/nni/examples/trials/mnist-pytorch',
  trialConcurrency: 1,
  trialGpuNumber: 1,
  useAnnotation: false,
  debug: false,
  logLevel: 'info',
  experimentWorkingDirectory: '/root/nni-experiments',
  tuner: { name: 'TPE', classArgs: { optimize_mode: 'maximize' } },
  trainingService: {
    platform: 'local',
    trialCommand: 'python3 mnist.py',
    trialCodeDirectory: '/root/nni/examples/trials/mnist-pytorch',
    trialGpuNumber: 1,
    debug: false,
    useActiveGpu: true,
    maxTrialNumberPerGpu: 1,
    reuseMode: false
  }
}
[2024-05-03 14:46:12] INFO (NNIManager) Starting experiment: o21hdgqs
[2024-05-03 14:46:12] INFO (NNIManager) Setup training service...
[2024-05-03 14:46:12] DEBUG (LocalV3.local) Training sevice config: {
  platform: 'local',
  trialCommand: 'python3 mnist.py',
  trialCodeDirectory: '/root/nni/examples/trials/mnist-pytorch',
  trialGpuNumber: 1,
  debug: false,
  useActiveGpu: true,
  maxTrialNumberPerGpu: 1,
  reuseMode: false
}
[2024-05-03 14:46:12] INFO (NNIManager) Setup tuner...
[2024-05-03 14:46:12] DEBUG (NNIManager) dispatcher command: /usr/bin/python3,-m,nni,--exp_params,eyJleHBlcmltZW50VHlwZSI6ImhwbyIsInNlYXJjaFNwYWNlRmlsZSI6Ii9yb290L25uaS9leGFtcGxlcy90cmlhbHMvbW5pc3QtcHl0b3JjaC9zZWFyY2hfc3BhY2UuanNvbiIsInRyaWFsQ29tbWFuZCI6InB5dGhvbjMgbW5pc3QucHkiLCJ0cmlhbENvZGVEaXJlY3RvcnkiOiIvcm9vdC9ubmkvZXhhbXBsZXMvdHJpYWxzL21uaXN0LXB5dG9yY2giLCJ0cmlhbENvbmN1cnJlbmN5IjoxLCJ0cmlhbEdwdU51bWJlciI6MSwidXNlQW5ub3RhdGlvbiI6ZmFsc2UsImRlYnVnIjpmYWxzZSwibG9nTGV2ZWwiOiJpbmZvIiwiZXhwZXJpbWVudFdvcmtpbmdEaXJlY3RvcnkiOiIvcm9vdC9ubmktZXhwZXJpbWVudHMiLCJ0dW5lciI6eyJuYW1lIjoiVFBFIiwiY2xhc3NBcmdzIjp7Im9wdGltaXplX21vZGUiOiJtYXhpbWl6ZSJ9fSwidHJhaW5pbmdTZXJ2aWNlIjp7InBsYXRmb3JtIjoibG9jYWwiLCJ0cmlhbENvbW1hbmQiOiJweXRob24zIG1uaXN0LnB5IiwidHJpYWxDb2RlRGlyZWN0b3J5IjoiL3Jvb3Qvbm5pL2V4YW1wbGVzL3RyaWFscy9tbmlzdC1weXRvcmNoIiwidHJpYWxHcHVOdW1iZXIiOjEsImRlYnVnIjpmYWxzZSwidXNlQWN0aXZlR3B1Ijp0cnVlLCJtYXhUcmlhbE51bWJlclBlckdwdSI6MSwicmV1c2VNb2RlIjpmYWxzZX19
[2024-05-03 14:46:12] INFO (NNIManager) Change NNIManager status from: INITIALIZED to: RUNNING
[2024-05-03 14:46:12] DEBUG (tuner_command_channel) Waiting connection...
[2024-05-03 14:46:12] DEBUG (NNIRestHandler) GET: /check-status: body: {}
[2024-05-03 14:46:13] DEBUG (WsChannelServer.tuner) Incoming connection __default__
[2024-05-03 14:46:13] DEBUG (WsChannel.__default__) Epoch 0 start
[2024-05-03 14:46:13] INFO (NNIManager) Add event listeners
[2024-05-03 14:46:13] DEBUG (NNIManager) Send tuner command: INITIALIZE: [object Object]
[2024-05-03 14:46:13] INFO (LocalV3.local) Start
[2024-05-03 14:46:13] INFO (NNIManager) NNIManager received command from dispatcher: ID, 
[2024-05-03 14:46:13] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""batch_size"": 32, ""hidden_size"": 128, ""lr"": 0.001, ""momentum"": 0.47523697672790355}, ""parameter_index"": 0}
[2024-05-03 14:46:14] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 0,
  hyperParameters: {
    value: '{""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""batch_size"": 32, ""hidden_size"": 128, ""lr"": 0.001, ""momentum"": 0.47523697672790355}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-05-03 14:46:15] INFO (GpuInfoCollector) Forced update: {
  gpuNumber: 1,
  driverVersion: '550.54.15',
  cudaVersion: 12040,
  gpus: [
    {
      index: 0,
      model: 'Tesla T4',
      cudaCores: 2560,
      gpuMemory: 16106127360,
      freeGpuMemory: 15642263552,
      gpuCoreUtilization: 0,
      gpuMemoryUtilization: 0
    }
  ],
  processes: [],
  success: true
}
[2024-05-03 14:46:17] INFO (LocalV3.local) Register directory trial_code = /root/nni/examples/trials/mnist-pytorch
```

#### experiment.log

```
root@1b02414e6d3e:~/nni-experiments/_latest/log# cat experiment.log 
[2024-05-03 14:46:11] INFO (nni.experiment) Creating experiment, Experiment ID: o21hdgqs
[2024-05-03 14:46:11] INFO (nni.experiment) Starting web server...
[2024-05-03 14:46:12] INFO (nni.experiment) Setting up...
[2024-05-03 14:46:12] INFO (nni.experiment) Web portal URLs: http://127.0.0.1:8080 http://172.17.0.2:8080
[2024-05-03 14:46:42] INFO (nni.experiment) Stopping experiment, please wait...
[2024-05-03 14:46:42] INFO (nni.experiment) Saving experiment checkpoint...
[2024-05-03 14:46:42] INFO (nni.experiment) Stopping NNI manager, if any...
```


## When I'm using CPU only:
I obtain what I want using the GPU, the WebUI, the experiments trials, and so on...

```
root@6dcd2267cf44:~# nnictl create --config nni/examples/trials/mnist-pytorch/config.yml --foreground --debug
[2024-05-03 14:37:54] Creating experiment, Experiment ID: tcq192jf
[2024-05-03 14:37:54] Starting web server...
[2024-05-03 14:37:55] DEBUG (WsChannelServer.tuner) Start listening tuner/:channel
[2024-05-03 14:37:55] INFO (main) Start NNI manager
[2024-05-03 14:37:55] INFO (RestServer) Starting REST server at port 8080, URL prefix: ""/""
[2024-05-03 14:37:55] INFO (RestServer) REST server started.
[2024-05-03 14:37:55] DEBUG (SqlDB) Database directory: /root/nni-experiments/tcq192jf/db
[2024-05-03 14:37:55] INFO (NNIDataStore) Datastore initialization done
[2024-05-03 14:37:55] DEBUG (main) start() returned.
[2024-05-03 14:37:55] DEBUG (NNIRestHandler) GET: /check-status: body: {}
[2024-05-03 14:37:55] Setting up...
[2024-05-03 14:37:55] DEBUG (NNIRestHandler) POST: /experiment: body: {
  experimentType: 'hpo',
  searchSpaceFile: '/root/nni/examples/trials/mnist-pytorch/search_space.json',
  searchSpace: {
    batch_size: { _type: 'choice', _value: [Array] },
    hidden_size: { _type: 'choice', _value: [Array] },
    lr: { _type: 'choice', _value: [Array] },
    momentum: { _type: 'uniform', _value: [Array] }
  },
  trialCommand: 'python3 mnist.py',
  trialCodeDirectory: '/root/nni/examples/trials/mnist-pytorch',
  trialConcurrency: 1,
  trialGpuNumber: 0,
  useAnnotation: false,
  debug: false,
  logLevel: 'info',
  experimentWorkingDirectory: '/root/nni-experiments',
  tuner: { name: 'TPE', classArgs: { optimize_mode: 'maximize' } },
  trainingService: {
    platform: 'local',
    trialCommand: 'python3 mnist.py',
    trialCodeDirectory: '/root/nni/examples/trials/mnist-pytorch',
    trialGpuNumber: 0,
    debug: false,
    maxTrialNumberPerGpu: 1,
    reuseMode: false
  }
}
[2024-05-03 14:37:55] INFO (NNIManager) Starting experiment: tcq192jf
[2024-05-03 14:37:55] INFO (NNIManager) Setup training service...
[2024-05-03 14:37:55] DEBUG (LocalV3.local) Training sevice config: {
  platform: 'local',
  trialCommand: 'python3 mnist.py',
  trialCodeDirectory: '/root/nni/examples/trials/mnist-pytorch',
  trialGpuNumber: 0,
  debug: false,
  maxTrialNumberPerGpu: 1,
  reuseMode: false
}
[2024-05-03 14:37:55] INFO (NNIManager) Setup tuner...
[2024-05-03 14:37:55] DEBUG (NNIManager) dispatcher command: /usr/bin/python3,-m,nni,--exp_params,eyJleHBlcmltZW50VHlwZSI6ImhwbyIsInNlYXJjaFNwYWNlRmlsZSI6Ii9yb290L25uaS9leGFtcGxlcy90cmlhbHMvbW5pc3QtcHl0b3JjaC9zZWFyY2hfc3BhY2UuanNvbiIsInRyaWFsQ29tbWFuZCI6InB5dGhvbjMgbW5pc3QucHkiLCJ0cmlhbENvZGVEaXJlY3RvcnkiOiIvcm9vdC9ubmkvZXhhbXBsZXMvdHJpYWxzL21uaXN0LXB5dG9yY2giLCJ0cmlhbENvbmN1cnJlbmN5IjoxLCJ0cmlhbEdwdU51bWJlciI6MCwidXNlQW5ub3RhdGlvbiI6ZmFsc2UsImRlYnVnIjpmYWxzZSwibG9nTGV2ZWwiOiJpbmZvIiwiZXhwZXJpbWVudFdvcmtpbmdEaXJlY3RvcnkiOiIvcm9vdC9ubmktZXhwZXJpbWVudHMiLCJ0dW5lciI6eyJuYW1lIjoiVFBFIiwiY2xhc3NBcmdzIjp7Im9wdGltaXplX21vZGUiOiJtYXhpbWl6ZSJ9fSwidHJhaW5pbmdTZXJ2aWNlIjp7InBsYXRmb3JtIjoibG9jYWwiLCJ0cmlhbENvbW1hbmQiOiJweXRob24zIG1uaXN0LnB5IiwidHJpYWxDb2RlRGlyZWN0b3J5IjoiL3Jvb3Qvbm5pL2V4YW1wbGVzL3RyaWFscy9tbmlzdC1weXRvcmNoIiwidHJpYWxHcHVOdW1iZXIiOjAsImRlYnVnIjpmYWxzZSwibWF4VHJpYWxOdW1iZXJQZXJHcHUiOjEsInJldXNlTW9kZSI6ZmFsc2V9fQ==
[2024-05-03 14:37:55] INFO (NNIManager) Change NNIManager status from: INITIALIZED to: RUNNING
[2024-05-03 14:37:55] DEBUG (tuner_command_channel) Waiting connection...
[2024-05-03 14:37:55] Web portal URLs: http://127.0.0.1:8080 http://172.17.0.2:8080
[2024-05-03 14:37:55] DEBUG (NNIRestHandler) GET: /check-status: body: {}
[2024-05-03 14:37:57] DEBUG (WsChannelServer.tuner) Incoming connection __default__
[2024-05-03 14:37:57] DEBUG (WsChannel.__default__) Epoch 0 start
[2024-05-03 14:37:57] INFO (NNIManager) Add event listeners
[2024-05-03 14:37:57] DEBUG (NNIManager) Send tuner command: INITIALIZE: [object Object]
[2024-05-03 14:37:57] INFO (LocalV3.local) Start
[2024-05-03 14:37:57] INFO (NNIManager) NNIManager received command from dispatcher: ID, 
[2024-05-03 14:37:57] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""batch_size"": 128, ""hidden_size"": 1024, ""lr"": 0.001, ""momentum"": 0.6039114358987745}, ""parameter_index"": 0}
[2024-05-03 14:37:57] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 0,
  hyperParameters: {
    value: '{""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""batch_size"": 128, ""hidden_size"": 1024, ""lr"": 0.001, ""momentum"": 0.6039114358987745}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-05-03 14:37:58] INFO (LocalV3.local) Register directory trial_code = /root/nni/examples/trials/mnist-pytorch
[2024-05-03 14:37:58] INFO (LocalV3.local) Created trial wcvTY
[2024-05-03 14:38:00] INFO (LocalV3.local) Trial parameter: wcvTY {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""batch_size"": 128, ""hidden_size"": 1024, ""lr"": 0.001, ""momentum"": 0.6039114358987745}, ""parameter_index"": 0}
[2024-05-03 14:38:05] DEBUG (NNIRestHandler) GET: /check-status: body: {}
...
```

## How to reproduce it?

If from a Docker container:
```
docker build -t ""nas-experiment"" .
nvidia-docker run -it -p 8081:8081 nas-101-experiment
```

Then in both cases:
1. I would both outside and inside a Docker container modify the file from /nni/example/trials/mnist-pytorch/config.yml in order to set the process on GPU.
2. Then I would run the following command so I could see the logs in direct.
```
nnictl create --config /nni/example/trials/mnist-pytorch/config.yml --port 8081 --debug --foreground
```

As a result, the WebUI wouldn't start due to a timeout trying to retrive data, since the experiment won't load on GPU.

## Notes

- I very available to answer and get helped on the subject as I currently work on NAS.
- I'm going to see what is ArchAI and how it differs from nii util I can use GPU for training there.
- I'm using GCP Instances to do this search",dtamienER,158458048,open,True,3,2024-05-03T14:50:54+00:00,2024-05-23T15:01:40+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2274480521,5775,NNI not running anymore without error messages when CPU reached 100% once,"**Describe the issue**:
Once the CPU utilization reached 100% once, even though NNI will finish the running trials but will not run the remaining trials.


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: ubuntu 22.04, 20.04
- Server OS (for remote mode only):
- Python version: 3.8
- PyTorch/TensorFlow version: 2.1.0
- Is conda/virtualenv/venv used?: conda
- Is running in Docker?: no

**How to reproduce it?**: 
You could run a task that consumes CPU resources across multiple trials simultaneously, and you will observe this issue.

I think this issue is as the same as this one #965 .",BitCalSaul,142638691,open,True,0,2024-05-02T02:35:02+00:00,2024-05-02T02:35:02+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2265793249,5774, ERROR: Strategy failed to execute.,"**Describe the issue**:
I’m trying to learn how to implement NAS using NNI. However, I'm getting the ‘ImportError: Cannot use a path to identify something from __main__.’ and ‘TypeError: cannot pickle 'CudnnModule' object’ errors listed below.


my code: https://github.com/ktunlab/nas-resnet-demo


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: Windows 10
- Server OS (for remote mode only):
- Python version: 3.8
- PyTorch/TensorFlow version: pyTorch==2.3.0
- Is conda/virtualenv/venv used?: Yes 
- Is running in Docker?: No


**Configuration**:
 - Experiment config (remember to remove secrets!): https://github.com/ktunlab/nas-resnet-demo/blob/main/train.py
 - Search space: https://github.com/ktunlab/nas-resnet-demo/blob/main/resnet18.py


**Log message**:
 - nnictl stdout and stderr:
 

log:
`[2024-04-26 15:45:28] Config is not provided. Will try to infer.
[2024-04-26 15:45:28] Using execution engine based on training service. Trial concurrency is set to 1.
[2024-04-26 15:45:28] Using simplified model format.
[2024-04-26 15:45:28] Using local training service.
[2024-04-26 15:45:28] WARNING: GPU found but will not be used. Please set `experiment.config.trial_gpu_number` to the number of GPUs you want to use for each trial.
[2024-04-26 15:45:30] Creating experiment, Experiment ID: lyjc7okv
[2024-04-26 15:45:30] Starting web server...
[2024-04-26 15:45:30] Setting up...
[2024-04-26 15:45:30] Web portal URLs: http://172.22.9.46:8081 http://127.0.0.1:8081
[2024-04-26 15:45:30] Successfully update searchSpace.
[2024-04-26 15:45:30] Checkpoint saved to C:\Users\Lab-d\nni-experiments\lyjc7okv\checkpoint.
[2024-04-26 15:45:30] Experiment initialized successfully. Starting exploration strategy...
[2024-04-26 15:45:30] ERROR: Strategy failed to execute.
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\common\serializer.py"", line 831, in get_hybrid_cls_or_func_name
    name = _get_cls_or_func_name(cls_or_func)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\common\serializer.py"", line 810, in _get_cls_or_func_name
    raise ImportError('Cannot use a path to identify something from __main__.')
ImportError: Cannot use a path to identify something from __main__.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train.py"", line 103, in <module>
    exp.run(port=8081)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\experiment\experiment.py"", line 236, in run
    return self._run_impl(port, wait_completion, debug)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\experiment\experiment.py"", line 205, in _run_impl
    self.start(port, debug)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\nas\experiment\experiment.py"", line 270, in start
    self._start_engine_and_strategy()
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\nas\experiment\experiment.py"", line 230, in _start_engine_and_strategy
    self.strategy.run()
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\nas\strategy\base.py"", line 170, in run
    self._run()
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\nas\strategy\bruteforce.py"", line 223, in _run
    self.engine.submit_models(model)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\nas\execution\training_service.py"", line 172, in submit_models
    self._channel.send_trial(
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\runtime\tuner_command_channel\channel.py"", line 144, in send_trial
    send_payload = dump(trial_dict, pickle_size_limit=int(os.getenv('PICKLE_SIZE_LIMIT', 64 * 1024)))
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\common\serializer.py"", line 372, in dump
    result = _dump(
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\common\serializer.py"", line 424, in _dump
    return json_tricks.dumps(obj, obj_encoders=encoders, **json_tricks_kwargs)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\json_tricks\nonp.py"", line 125, in dumps
    txt = combined_encoder.encode(obj)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\json\encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\json\encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\json_tricks\encoders.py"", line 76, in default
    obj = encoder(obj, primitives=self.primitives, is_changed=id(obj) != prev_id, properties=self.properties)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\json_tricks\utils.py"", line 66, in wrapper
    return encoder(*args, **{k: v for k, v in kwargs.items() if k in names})
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\common\serializer.py"", line 858, in _json_tricks_func_or_cls_encode
    '__nni_type__': get_hybrid_cls_or_func_name(cls_or_func, pickle_size_limit)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\nni\common\serializer.py"", line 835, in get_hybrid_cls_or_func_name
    b = cloudpickle.dumps(cls_or_func)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\cloudpickle\cloudpickle.py"", line 1479, in dumps
    cp.dump(obj)
  File ""C:\ProgramData\Anaconda3\envs\proje\lib\site-packages\cloudpickle\cloudpickle.py"", line 1245, in dump
    return super().dump(obj)
TypeError: cannot pickle 'CudnnModule' object
[2024-04-26 15:45:30] Stopping experiment, please wait...
[2024-04-26 15:45:30] Checkpoint saved to C:\Users\Lab-d\nni-experiments\lyjc7okv\checkpoint.
[2024-04-26 15:45:30] Experiment stopped`



**How to reproduce it?**: python train.py",ktunlab,168195738,open,True,0,2024-04-26T13:28:20+00:00,2024-04-26T13:28:20+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2259147772,5773,Which framework to use for Neural Architecture Search: NNI or Archai?,"Hello,

I have been exploring the open source framework for NAS and came across NNI and Archai. How these two frameworks are different? As both are part of Microsoft research group so a clarification would be really helpful. Additionally, I would appreciate guidance on which framework would be more suitable for me as a user / researcher.

Archai Github: https://github.com/microsoft/archai
Documentation: https://microsoft.github.io/archai/index.html#

Any feedback would be highly appreciated. Thank you!",mkumar73,14772927,open,True,0,2024-04-23T15:15:10+00:00,2024-04-23T15:15:10+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2256626025,5772,使用QAT进行量化时，config中'granularity':使用'per_channel'出现报错,"
## NNI 学生项目问题概述 / General Question of Student Program
使用QAT进行量化时，config中'granularity':使用'per_channel'出现报错

**请简要概述您的问题 / 观点 ：
使用的是Doc里面的quick start 历程，将量化粒度由default改为per_channel无法完成量化

**请提供 NNI 环境信息 :**
**nni Environment :**
- nni 3.0
- python version:
conda 
python 3.10

## 其他建议 / Other Advice

**是否需要更新文档（是 / 否）:**
是


**报错信息**
{
	""name"": ""AssertionError"",
	""message"": """",
	""stack"": ""---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
Cell In[214], line 53
     10 config_list = [{
     11     'op_names': ['conv1', 'conv2', 'fc1', 'fc2'],
     12     'target_names': ['_input_', 'bias'],
   (...)
     31     'granularity': 'per_channel',
     32 }]
     34 # config_list = [{
     35 #     'op_names': ['conv1', 'conv2', 'fc1', 'fc2'],
     36 #     'target_names': ['weight', 'bias'],
   (...)
     50 
     51 # ]
---> 53 quantizer = QATQuantizer(model, config_list, evaluator, len(train_dataloader))
     54 real_input = next(iter(train_dataloader))[0].to(device)
     55 quantizer.track_forward(real_input)

File ~/anaconda3/envs/pt/lib/python3.10/site-packages/nni/compression/quantization/qat_quantizer.py:75, in QATQuantizer.__init__(self, model, config_list, evaluator, quant_start_step, existed_wrappers)
     73 def __init__(self, model: torch.nn.Module, config_list: List[Dict], evaluator: Evaluator,
     74              quant_start_step: int = 0, existed_wrappers: Dict[str, ModuleWrapper] | None = None):
---> 75     super().__init__(model, config_list, evaluator, existed_wrappers)
     76     self.evaluator: Evaluator
     77     self.quant_start_step = max(quant_start_step, 0)

File ~/anaconda3/envs/pt/lib/python3.10/site-packages/nni/compression/base/compressor.py:295, in Quantizer.__init__(self, model, config_list, evaluator, existed_wrappers)
    293 super().__init__(model=model, config_list=config_list, evaluator=evaluator, existed_wrappers=existed_wrappers)
    294 self._target_spaces: _QUANTIZATION_TARGET_SPACES
--> 295 self._register_scalers()

File ~/anaconda3/envs/pt/lib/python3.10/site-packages/nni/compression/base/compressor.py:299, in Quantizer._register_scalers(self)
    297 def _register_scalers(self):
    298     # scalers are used to support different sparse/quant granularity
--> 299     register_scalers(self._target_spaces, self._set_default_sparse_granularity)

File ~/anaconda3/envs/pt/lib/python3.10/site-packages/nni/compression/base/compressor.py:433, in register_scalers(target_spaces, set_default_granularity)
    430     target_space._scaler = Scaling([-1, 1], kernel_padding_mode='back', kernel_padding_val=-1)
    431 elif target_space.granularity == 'per_channel':
    432     # NOTE: here assume dim 0 is batch, dim 1 is channel
--> 433     assert target_space._target_type in [TargetType.INPUT, TargetType.OUTPUT]
    434     target_space._scaler = Scaling([-1, 1], kernel_padding_mode='back', kernel_padding_val=-1)
    435 else:

AssertionError: ""
}






























",Count1ngStar,155871799,open,True,0,2024-04-22T14:03:22+00:00,2024-04-22T14:03:22+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2255124109,5771,Using export_data() not working with DartsStrategy(),"Hi,

I want to use export_data() after experiment.run() in the DARTS tutorial (https://nni.readthedocs.io/en/stable/tutorials/darts.html). However, I get a runtime error:

> RuntimeError: Experiment is not running

For the ""Hello NAS!"" (https://nni.readthedocs.io/en/stable/tutorials/hello_nas.html) example, this function works fine.

Do you have any workaround/idea on how to fix this?

Best regards,
Felix",felix011235,167696629,open,True,0,2024-04-21T15:07:52+00:00,2024-04-21T15:07:52+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2249797085,5770,Anyone have some idea of time series forecasting problem using DARTS strategy over the search space of Recurrent neural networks?,"**Describe the issue**:
I go through different 3 types of Neural Networks for the forecasting problem. All of them have the similar structure: Recurrent layer, and two dense layer. However when I tried with just modify the Recurrent layer to Layerchoice, there always have some problem like 

File [~/Documents/GitHub/NAS/nni/nni/nas/experiment/experiment.py:270](https://file+.vscode-resource.vscode-cdn.net/Users/franciszhang/Documents/GitHub/NAS/notebooks/~/Documents/GitHub/NAS/nni/nni/nas/experiment/experiment.py:270), in NasExperiment.start(self, port, debug, run_mode)
...
   [1079](https://file+.vscode-resource.vscode-cdn.net/opt/anaconda3/envs/NAS/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1079)                 f""For unbatched 2-D input, hx should also be 2-D but got {hx.dim()}-D tensor"")
   [1080](https://file+.vscode-resource.vscode-cdn.net/opt/anaconda3/envs/NAS/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1080)         hx = hx.unsqueeze(1)
   [1081](https://file+.vscode-resource.vscode-cdn.net/opt/anaconda3/envs/NAS/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1081) else:

RuntimeError: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor. 

Where 

X_train, X_test, y_train, y_test = train_test_split(X_tensor, Y_tensor, random_state = 0)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

is

torch.Size([9656, 300]) torch.Size([3219, 300]) torch.Size([9656]) torch.Size([3219])
and the Dataloader is set as 
train_dataset = TensorDataset(X_train, y_train.unsqueeze(1))
test_dataset = TensorDataset(X_test, y_test.unsqueeze(1))
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

I'm not sure what is the exact issue happening here.


**Environment**:
- NNI version: 3.0rc1
- Training service (local|remote|pai|aml|etc): local
- Client OS: macOS Sonoma 14.4 (23E214)
- Server OS (for remote mode only): N/A
- Python version: 3.11.8
- PyTorch/TensorFlow version: 2.2.1
- Is conda/virtualenv/venv used?: conda has been used
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space: RNN, GRU, and LSTM if the output problem can be solved


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",franciszh0716,43384851,open,True,0,2024-04-18T05:40:50+00:00,2024-04-18T05:40:50+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2239946493,5769,ERROR (nni.runtime.msg_dispatcher_base/Thread-2),"**Describe the issue**:
I created the trial by `nnictl create --config xx --p xxxx`
For a while I use `nnictl experiment --all` to check it, and find it stopped. The dispatcher.log shows the error below.
But the corresponding process is still running in gpu.
btw in the last time I use nni, this error didn't occur. I don't know what caused it.

**Environment**:
- NNI version: 2.10.1
- Training service (local|remote|pai|aml|etc): local
- Client OS: linux
- Server OS (for remote mode only):
- Python version: 3.8.13
- PyTorch/TensorFlow version: pytorch 1.10.1
- Is conda/virtualenv/venv used?: conda 
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!): 
trialCommand: CUDA_VISIBLE_DEVICES=0 python k+1_gan.py
trialConcurrency: 2
maxTrialNumber: 1000
maxExperimentDuration: 200h
experimentWorkingDirectory: ""/home/yiran/codes/Knowledge-Enriched-DMI/nni-experiment""
tuner:
  name: TPE
  classArgs:
    optimize_mode: maximize
trainingService:
  platform: local

 - Search space:
{
    ""lr"":{""_type"":""choice"",""_value"":[0.00005, 0.0001,0.0002, 0.0005, 0.001]},
    ""beta1"":{""_type"":""choice"",""_value"":[0.001, 0.0001, 0.00001]},
    ""beta2"": {""_type"":""choice"",""_value"":[0.9,0.999]},
    ""lambda_e"": {""_type"":""choice"",""_value"":[0.00005]}
}

**Log message**:
 - nnimanager.log:
 [2024-04-12 18:48:34] INFO (main) Start NNI manager
[2024-04-12 18:48:34] INFO (NNIDataStore) Datastore initialization done
[2024-04-12 18:48:34] INFO (RestServer) Starting REST server at port 8080, URL prefix: ""/""
[2024-04-12 18:48:34] INFO (RestServer) REST server started.
[2024-04-12 18:48:35] INFO (NNIManager) Starting experiment: b7edpl94
[2024-04-12 18:48:35] INFO (NNIManager) Setup training service...
[2024-04-12 18:48:35] INFO (LocalTrainingService) Construct local machine training service.
[2024-04-12 18:48:35] INFO (NNIManager) Setup tuner...
[2024-04-12 18:48:35] INFO (NNIManager) Change NNIManager status from: INITIALIZED to: RUNNING
[2024-04-12 18:48:36] INFO (NNIManager) Add event listeners
[2024-04-12 18:48:36] INFO (LocalTrainingService) Run local machine training service.
[2024-04-12 18:48:36] INFO (NNIManager) NNIManager received command from dispatcher: ID, 
[2024-04-12 18:48:36] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""lr"": 0.0002, ""beta1"": 0.0001, ""beta2"": 0.999, ""lambda_e"": 5e-05}, ""parameter_index"": 0}
[2024-04-12 18:48:36] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 1, ""parameter_source"": ""algorithm"", ""parameters"": {""lr"": 0.001, ""beta1"": 1e-05, ""beta2"": 0.9, ""lambda_e"": 5e-05}, ""parameter_index"": 0}
[2024-04-12 18:48:41] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 0,
  hyperParameters: {
    value: '{""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""lr"": 0.0002, ""beta1"": 0.0001, ""beta2"": 0.999, ""lambda_e"": 5e-05}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-04-12 18:48:41] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 1,
  hyperParameters: {
    value: '{""parameter_id"": 1, ""parameter_source"": ""algorithm"", ""parameters"": {""lr"": 0.001, ""beta1"": 1e-05, ""beta2"": 0.9, ""lambda_e"": 5e-05}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-04-12 18:48:51] INFO (NNIManager) Trial job ZlXeN status changed from WAITING to RUNNING
[2024-04-12 18:48:51] INFO (NNIManager) Trial job Rh0Pn status changed from WAITING to RUNNING
[2024-04-12 18:49:42] ERROR (tuner_command_channel.WebSocketChannel) Error: Error: tuner_command_channel: Tuner closed connection
    at WebSocket.handleWsClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/core/tuner_command_channel/websocket_channel.js:83:26)
    at WebSocket.emit (node:events:538:35)
    at WebSocket.emitClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:246:10)
    at Socket.socketOnClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:1127:15)
    at Socket.emit (node:events:526:28)
    at TCP.<anonymous> (node:net:687:12)


 - dispatcher.log:
 [2024-04-12 18:48:35] INFO (numexpr.utils/MainThread) Note: NumExpr detected 64 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2024-04-12 18:48:35] INFO (numexpr.utils/MainThread) NumExpr defaulting to 8 threads.
[2024-04-12 18:48:36] INFO (nni.tuner.tpe/MainThread) Using random seed 1314744945
[2024-04-12 18:48:36] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher started
[2024-04-12 18:49:19] ERROR (nni.runtime.msg_dispatcher_base/Thread-2) Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
Traceback (most recent call last):
  File ""/home/yiran/.local/lib/python3.8/site-packages/nni/runtime/msg_dispatcher_base.py"", line 108, in command_queue_worker
    self.process_command(command, data)
  File ""/home/yiran/.local/lib/python3.8/site-packages/nni/runtime/msg_dispatcher_base.py"", line 154, in process_command
    command_handlers[command](data)
  File ""/home/yiran/.local/lib/python3.8/site-packages/nni/runtime/msg_dispatcher.py"", line 144, in handle_report_metric_data
    data['value'] = load(data['value'])
  File ""/home/yiran/.local/lib/python3.8/site-packages/nni/common/serializer.py"", line 443, in load
    return json_tricks.loads(string, obj_pairs_hooks=hooks, **json_tricks_kwargs)
  File ""/home/yiran/.local/lib/python3.8/site-packages/json_tricks/nonp.py"", line 259, in loads
    return _strip_loads(string, hook, True, **jsonkwargs)
  File ""/home/yiran/.local/lib/python3.8/site-packages/json_tricks/nonp.py"", line 266, in _strip_loads
    return json_loads(string, object_pairs_hook=object_pairs_hook, **jsonkwargs)
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/json/__init__.py"", line 370, in loads
    return cls(**kw).decode(s)
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/json/decoder.py"", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
  File ""/home/yiran/.local/lib/python3.8/site-packages/json_tricks/decoders.py"", line 46, in __call__
    map = hook(map, properties=self.properties)
  File ""/home/yiran/.local/lib/python3.8/site-packages/json_tricks/utils.py"", line 66, in wrapper
    return encoder(*args, **{k: v for k, v in kwargs.items() if k in names})
  File ""/home/yiran/.local/lib/python3.8/site-packages/nni/common/serializer.py"", line 877, in _json_tricks_any_object_decode
    return _wrapped_cloudpickle_loads(b)
  File ""/home/yiran/.local/lib/python3.8/site-packages/nni/common/serializer.py"", line 883, in _wrapped_cloudpickle_loads
    return cloudpickle.loads(b)
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/storage.py"", line 161, in _load_from_bytes
    return torch.load(io.BytesIO(b))
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py"", line 608, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py"", line 787, in _legacy_load
    result = unpickler.load()
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py"", line 743, in persistent_load
    deserialized_objects[root_key] = restore_location(obj, location)
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py"", line 175, in default_restore_location
    result = fn(storage, location)
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py"", line 151, in _cuda_deserialize
    device = validate_cuda_device(location)
  File ""/home/yiran/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/serialization.py"", line 135, in validate_cuda_device
    raise RuntimeError('Attempting to deserialize object on a CUDA '
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
[2024-04-12 18:49:40] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher exiting...
[2024-04-12 18:49:42] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher terminiated



 - nnictl stdout and stderr:
 --------------------------------------------------------------------------------
Experiment b7edpl94 start: 2024-04-12 18:48:34.614673
--------------------------------------------------------------------------------
node:events:504
      throw er; // Unhandled 'error' event
      ^

Error: tuner_command_channel: Tuner closed connection
    at WebSocket.handleWsClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/core/tuner_command_channel/websocket_channel.js:83:26)
    at WebSocket.emit (node:events:538:35)
    at WebSocket.emitClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:246:10)
    at Socket.socketOnClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:1127:15)
    at Socket.emit (node:events:526:28)
    at TCP.<anonymous> (node:net:687:12)
Emitted 'error' event at:
    at WebSocketChannelImpl.handleError (/home/yiran/.local/lib/python3.8/site-packages/nni_node/core/tuner_command_channel/websocket_channel.js:135:22)
    at WebSocket.handleWsClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/core/tuner_command_channel/websocket_channel.js:83:14)
    at WebSocket.emit (node:events:538:35)
    [... lines matching original stack trace ...]
    at TCP.<anonymous> (node:net:687:12)
Thrown at:
    at handleWsClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/core/tuner_command_channel/websocket_channel.js:83:26)
    at emit (node:events:538:35)
    at emitClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:246:10)
    at socketOnClose (/home/yiran/.local/lib/python3.8/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:1127:15)
    at emit (node:events:526:28)
    at node:net:687:12

 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",C-Comfundo,61780940,open,True,0,2024-04-12T11:39:58+00:00,2024-04-12T11:39:58+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2221680040,5768,QuickStart example for pruning is not working properly !!,"**Describe the bug**:
QuickStart Example provides Error at model SpeedUp.  AttributeError: 'NoneType' object has no attribute 'startswith'. I tried running the pruning example quick start code and all worked fine until the section of model speedup.



**Environment**:
- NNI version: 3
- Training service (local|remote|pai|aml|etc):
- Python version: 3.10
- PyTorch version: 2.2
- Cpu or cuda version: CPU


**Reproduce the problem**
- Code|Example:
from nni.compression.speedup import ModelSpeedup

ModelSpeedup(model, torch.rand(3, 1, 28, 28).to(device), masks).speedup_model()

- How to reproduce:
AttributeError                            Traceback (most recent call last)
[<ipython-input-8-3de7984fa1ba>](https://localhost:8080/#) in <cell line: 7>()
      5 from nni.compression.speedup import ModelSpeedup
      6 
----> 7 ModelSpeedup(model, torch.rand(3, 1, 28, 28).to(device), masks).speedup_model()

2 frames
[/usr/local/lib/python3.10/dist-packages/nni/common/concrete_trace_utils/concrete_tracer.py](https://localhost:8080/#) in trace(self, root, autowrap_modules, autowrap_leaf_function, autowrap_leaf_class, leaf_module, fake_middle_class, concrete_args, use_operator_patch, operator_patch_backlist, forward_function_name)
    956                 elif func.__name__ != func.__qualname__ and func.__qualname__ != 'boolean_dispatch.<locals>.fn':
    957                     # method
--> 958                     if func.__module__.startswith('_') and func.__module__ != '__main__':
    959                         path = sys.modules[func.__module__[1:]]
    960                     else:

AttributeError: 'NoneType' object has no attribute 'startswith'",hamdy-cryptic,105237590,open,True,4,2024-04-03T00:42:21+00:00,2024-07-18T00:10:37+00:00,,,2,2,0,0,0,0,0
microsoft/nni,2221523516,5767,NNI console won't show after trialGpuNumber was set to 1,"**Describe the issue**:

NNI console won't show after trialGpuNumber was set to 1

**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version: 2.1
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?: N/A


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**: N/A
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",yiqiaoc11,30539007,closed,True,0,2024-04-02T21:54:41+00:00,2024-04-03T01:56:05+00:00,2024-04-03T01:56:05+00:00,,0,0,0,0,0,0,0
microsoft/nni,2204644767,5766,TypeError: 'Tensor' object is not a mapping,"**Describe the issue**:

Upon running the following code  to compress and speed up the YOLOv5 model using NNI's pruning and speedup techniques.:
```
import torch

from nni.common.concrete_trace_utils import concrete_trace
from nni.compression.pruning import L1NormPruner
from nni.compression.utils import auto_set_denpendency_group_ids
from nni.compression.speedup import ModelSpeedup

model = torch.hub.load('ultralytics/yolov5', 'yolov5s', device='cpu')

model(torch.rand([1, 3, 640, 640]))

config_list = [{
    'sparsity': 0.5,
    'op_types': ['Conv2d'],
    'exclude_op_names_re': ['model.model.model.24.*'],  # this layer is detector head
}]

config_list = auto_set_denpendency_group_ids(model, config_list, torch.rand([1, 3, 640, 640]))

pruner = L1NormPruner(model, config_list)
masked_model, masks = pruner.compress()
pruner.unwrap_model()

graph_module = concrete_trace(model, (torch.rand([1, 3, 640, 640])))
ModelSpeedup(model, torch.rand([1, 3, 640, 640]), masks, graph_module=graph_module).speedup_model()

model(torch.rand([1, 3, 640, 640]))
```
The observed error:

```
/usr/local/lib/python3.10/dist-packages/nni/compression/speedup/utils.py:32: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(immutable_dict, _idict_flatten, _idict_unflatten)
/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'torch.fx.immutable_collections.immutable_dict'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/nni/compression/speedup/utils.py:33: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _register_pytree_node(immutable_list, _ilist_flatten, _ilist_unflatten)
/usr/local/lib/python3.10/dist-packages/torch/utils/_pytree.py:254: UserWarning: <class 'torch.fx.immutable_collections.immutable_list'> is already registered as pytree node. Overwriting the previous registration.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/torch/hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour
  warnings.warn(
Downloading: ""https://github.com/ultralytics/yolov5/zipball/master"" to /root/.cache/torch/hub/master.zip
requirements: Ultralytics requirement ['gitpython>=3.1.30'] not found, attempting AutoUpdate...
Collecting gitpython>=3.1.30
  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 195.4/195.4 kB 2.1 MB/s eta 0:00:00
Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)
  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 20.4 MB/s eta 0:00:00
Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)
  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)
Installing collected packages: smmap, gitdb, gitpython
Successfully installed gitdb-4.0.11 gitpython-3.1.42 smmap-5.0.1

requirements: AutoUpdate success ✅ 6.3s, installed 1 package: ['gitpython>=3.1.30']
requirements: ⚠️ Restart runtime or rerun command for updates to take effect

YOLOv5 🚀 2024-3-25 Python-3.10.12 torch-2.2.1+cu121 CPU

Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...
100%|██████████| 14.1M/14.1M [00:00<00:00, 134MB/s]

Fusing layers... 
YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs
Adding AutoShape... 
/root/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:100: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
/root/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:100: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
/root/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:100: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
/root/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:100: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if self.dynamic or self.grid[i].shape[2:4] != x[i].shape[2:4]:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
[<ipython-input-3-667814b03cd6>](https://localhost:8080/#) in <cell line: 24>()
     22 pruner.unwrap_model()
     23 
---> 24 graph_module = concrete_trace(model, (torch.rand([1, 3, 640, 640])))
     25 ModelSpeedup(model, torch.rand([1, 3, 640, 640]), masks, graph_module=graph_module).speedup_model()
     26 

1 frames
[/usr/local/lib/python3.10/dist-packages/nni/common/concrete_trace_utils/concrete_tracer.py](https://localhost:8080/#) in trace(self, root, autowrap_modules, autowrap_leaf_function, autowrap_leaf_class, leaf_module, fake_middle_class, concrete_args, use_operator_patch, operator_patch_backlist, forward_function_name)
    677         else:
    678             kv_default = {k: v for k, v in zip(args[-len(defaults):], defaults)}
--> 679             concrete_args = {
    680                 **concrete_args,
    681                 **{n: kv_default[n] for n in args if n not in concrete_args}

TypeError: 'Tensor' object is not a mapping
```


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): remote
- Client OS:
- Server OS (for remote mode only): Google Colab
- Python version:  3.10.12
- PyTorch/TensorFlow version: 2.2.1+cu121
- Is conda/virtualenv/venv used?: No
- Is running in Docker?: No


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",AKSHILMY,77114086,open,True,0,2024-03-25T00:29:17+00:00,2024-03-25T00:29:17+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2202447387,5765,Anybody can run the NAS exp ON GPU?,"**Describe the issue**:

I try to run the NAS demo on GPU, but it never works. Every time I run it, the filesystem memory will be filled. 

**Environment**:
- NNI version: 3.0rc1
- Training service (local|remote|pai|aml|etc): local
- Client OS: windows11/ubuntu
- Server OS (for remote mode only):
- Python version: 3.8
- PyTorch/TensorFlow version: 2.0
- Is conda/virtualenv/venv used?: yes
- Is running in Docker?: no 


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",534145232,30302418,open,True,2,2024-03-22T13:00:17+00:00,2024-04-29T20:24:19+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2201681759,5764,ModuleNotFoundError: No module named 'torch.fx._compatibility',"**Describe the issue**:

torch version:1.9

**Environment**:
- NNI version:3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: ubuntu
- Server OS (for remote mode only):
- Python version:3.8
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?: yes 
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",99999-LQL,61398141,open,True,0,2024-03-22T04:35:55+00:00,2024-03-22T04:56:37+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2201662392,5763,gpunum,,fantasy0905,100355131,closed,True,0,2024-03-22T04:13:46+00:00,2024-03-22T04:14:37+00:00,2024-03-22T04:14:37+00:00,,0,0,0,0,0,0,0
microsoft/nni,2201649395,5762,ModuleNotFoundError: No module named 'torch.fx._compatibility',"**Describe the issue**:
torch:1.9


**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",99999-LQL,61398141,closed,True,0,2024-03-22T03:58:19+00:00,2024-03-22T04:31:14+00:00,2024-03-22T04:31:13+00:00,,0,0,0,0,0,0,0
microsoft/nni,2199363738,5761,The process is stuck in the waiting status,"**Describe the issue**:
![image](https://github.com/microsoft/nni/assets/30302418/1e87f652-663d-4f87-8e87-766916e91754)

Anybody know how to solve it?

**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",534145232,30302418,open,True,1,2024-03-21T07:12:35+00:00,2024-05-15T12:06:34+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2196770173,5760,ChildProcessError: Command failed due to the wrong path?,"**Describe the issue**:

Hi, here is part of the error log:

ChildProcessError: Command failed: powershell.exe New-Item -Path ""C:\Users\DAYINC~1\AppData\Local\Temp/Dayin Chen/nni/script"" -ItemType ""directory"" -Force
New-Item : A positional parameter cannot be found that accepts argument 'Chen/nni/script'.
At line:1 char:1
+ New-Item -Path C:\Users\DAYINC~1\AppData\Local\Temp/Dayin Chen/nni/sc ...
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

I think maybe it is due to there is a space between the user name? But I don know how to revise the path parameter.


**Environment**:
- NNI version: 2.10
- Training service (local|remote|pai|aml|etc): local
- Client OS: Windows11
- Server OS (for remote mode only):
- Python version: 3.8
- PyTorch/TensorFlow version: 2.0.0
- Is conda/virtualenv/venv used?: yes
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",534145232,30302418,open,True,0,2024-03-20T06:42:47+00:00,2024-03-20T06:42:47+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2194930366,5759,Shivam,"**Describe the bug**:



**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Python version:
- PyTorch version:
- Cpu or cuda version:


**Reproduce the problem**
- Code|Example:


- How to reproduce:",shivaay9673,96260910,open,True,0,2024-03-19T13:07:51+00:00,2024-03-19T13:07:51+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2194702266,5758,USER_CANCELED,"![issue](https://github.com/microsoft/nni/assets/100355131/f41ea6b1-420e-41ed-a6fb-50fbabd8970d)
When I submit the code to run on the server, without performing any operations, the status changes to ""USER_CANCELED"". Even the NNI code that used to run successfully before is now encountering this issue when I try to run it. Could anyone please advise on how to solve this problem?",fantasy0905,100355131,closed,True,0,2024-03-19T11:25:33+00:00,2024-03-28T02:32:51+00:00,2024-03-28T02:32:51+00:00,,1,1,0,0,0,0,0
microsoft/nni,2183440768,5757," There is an if else statement in the model I built myself, and an error will appear when the model is accelerated during model pruning. How should I solve it?","**Describe the issue**:



**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",xxiayy,94794763,open,True,0,2024-03-13T08:53:49+00:00,2024-03-13T08:53:49+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2176129059,5755,please support some operator.,"**Describe the bug**:

When I use ""torch.view_as_complex"", the error information is:
[2024-03-08 21:48:32] ERROR (nni.compression.pytorch.speedup.jit_translate/MainThread) aten::view_as_complex is not Supported! Please report an issue at https://github.com/microsoft/nni. Thanks~

The ""torch.fft.rfft2"" and ""torch.fft.irfft2"" maybe need to be fixed too.
Thank you for considering to fix them.

**Environment**:
- NNI version:2.6
- Training service (local|remote|pai|aml|etc):local
- Python version:3.9.7
- PyTorch version:2.0.1
- Cpu or cuda version:cuda_11.7.r11.7/compiler.31294372_0


**Reproduce the problem**
- Code|Example:
",Sahala08,54014960,open,True,0,2024-03-08T14:16:34+00:00,2024-03-08T14:29:35+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2175205164,5754,RuntimeError: you can only change requires_grad flags of leaf variables. ,"**Describe the bug**:
RuntimeError: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().
when I use NNI to prune my customed transformer ,it first looks great with these 
**[2024-03-08 11:11:12] Update indirect mask for call_function: truediv, 
[2024-03-08 11:11:12] Update indirect mask for call_function: sqrt, 
[2024-03-08 11:11:12] Update indirect mask for call_function: getitem_13, 
[2024-03-08 11:11:12] Update indirect mask for call_function: getattr_3, 
[2024-03-08 11:11:12] Update indirect mask for call_method: transpose_2, output mask:  0.0000 
[2024-03-08 11:11:12] Update indirect mask for call_method: view_2, output mask:  0.0000 
[2024-03-08 11:11:12] Update indirect mask for call_module: encoder_encoder_layers_0_attention_value_projection, weight:  0.0000 bias:  0.0000 , output mask:  0.0000 **
until it throw an issue
Traceback (most recent call last):
  File ""F:\研究生学习文件\研二\时序预测算法\transformer\pythonProject2\0305\4.py"", line 219, in <module>
    ModelSpeedup(model, dummy_input, masks).speedup_model()
  File ""E:\ANACONDA\Anaconda\envs\torch\lib\site-packages\nni\compression\speedup\model_speedup.py"", line 435, in speedup_model
    self.update_indirect_sparsity()
  File ""E:\ANACONDA\Anaconda\envs\torch\lib\site-packages\nni\compression\speedup\model_speedup.py"", line 306, in update_indirect_sparsity
    self.node_infos[node].mask_updater.indirect_update_process(self, node)
  File ""E:\ANACONDA\Anaconda\envs\torch\lib\site-packages\nni\compression\speedup\mask_updater.py"", line 160, in indirect_update_process
    output = getattr(model_speedup, node.op)(node.target, args_cloned, kwargs_cloned)
  File ""E:\ANACONDA\Anaconda\envs\torch\lib\site-packages\torch\fx\interpreter.py"", line 289, in call_method
    return getattr(self_obj, target)(*args_tail, **kwargs)
RuntimeError: you can only change requires_grad flags of leaf variables. If you want to use a computed variable in a subgraph that doesn't require differentiation use var_no_grad = var.detach().

我在使用NNI对自己定义的transformer模型进行剪枝的时候报出这个错误,我尝试用L1NormPruner和MovementPruner进行,并且参考了NNI官方对transformer模型的剪枝案例(没有使用案例中的知识蒸馏),都尝试无果,会在speedup的过程中报出以上错误,我无法判断是我对NNI的设置有问题还是我自己定义的transformer模型不符合NNI的标准,故而寻求帮助

**Environment**:
- NNI version:3.0
- Training service (local|remote|pai|aml|etc):local
- Python version:3.8.0
- PyTorch version:2.1.2
- Cpu or cuda version:12.2(cuda)


**Reproduce the problem**
- Code|Example:
- 
# 模型剪枝
from nni.compression.pruning import MovementPruner
from nni.compression.speedup import ModelSpeedup
from nni.compression.utils.external.external_replacer import TransformersAttentionReplacer

print(model)
config_list = [{
        'op_types': ['Linear'],
        'op_names_re': ['encoder\.encoder_layers\.0\.attention\.*'],
        'sparse_threshold': 0.1,
        'granularity': [4, 4]
    }]
pruner = MovementPruner(model, config_list, evaluator, warmup_step=10, cooldown_begin_step=20, regular_scale=20)
pruner.compress(40, 4)
print(model)
pruner.unwrap_model()
masks = pruner.get_masks()
dummy_input = (torch.randint(0, 1, (32, 16, 1)).to(device).float(), torch.randint(0, 1, (32, 16, 1)).to(device).float())
# replacer = TransformersAttentionReplacer(model)
ModelSpeedup(model, dummy_input, masks).speedup_model()

- How to reproduce:
- this is my customed Transformer stucture I use it to predict time series

CustomTransformer(
  (embedding): Linear(in_features=1, out_features=64, bias=True)
  (positional_encoding): PositionalEncoding(
    (dropout): Dropout(p=0, inplace=False)
  )
  (encoder): Encoder(
    (encoder_layers): ModuleList(
      (0): Encoderlayer(
        (attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=64, out_features=64, bias=True)
          (key_projection): Linear(in_features=64, out_features=64, bias=True)
          (value_projection): Linear(in_features=64, out_features=64, bias=True)
          (out_projection): Linear(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (linear): Linear(in_features=64, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (linear_layers): ModuleList(
      (0): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (decoder): Decoder(
    (decoder_layers): ModuleList(
      (0): Decoderlayer(
        (self_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=64, out_features=64, bias=True)
          (key_projection): Linear(in_features=64, out_features=64, bias=True)
          (value_projection): Linear(in_features=64, out_features=64, bias=True)
          (out_projection): Linear(in_features=64, out_features=64, bias=True)
        )
        (cross_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=64, out_features=64, bias=True)
          (key_projection): Linear(in_features=64, out_features=64, bias=True)
          (value_projection): Linear(in_features=64, out_features=64, bias=True)
          (out_projection): Linear(in_features=64, out_features=64, bias=True)
        )
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (linear1): Linear(in_features=64, out_features=256, bias=True)
        (linear2): Linear(in_features=256, out_features=64, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (fc_in): Linear(in_features=64, out_features=64, bias=True)
  (relu): ReLU()
  (dropout): Dropout(p=0.1, inplace=False)
  (fc_out): Linear(in_features=64, out_features=1, bias=True)
)",Cindytjj,66307396,open,True,0,2024-03-08T03:26:00+00:00,2024-03-08T03:26:00+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2173672332,5752,Speedup bugfix: _output_randomize error,"### Description ###

When `NoMaskUpdater` `direct_update_process` is run with `collect_garbage_values` set to `True`, it leads to an error on the line containing `_output_randomize` which should be referring to the attribute `output_randomize`.

Fixes #5631


",saravanabalagi,8567893,open,True,0,2024-03-07T11:44:47+00:00,2024-03-07T11:44:47+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2173638291,5751,Pruning Bugfix: 1 out ch conv layer being treated as depthwise conv layer,"### Description ###

When the output channels of the last layer is 1, even when there are 2 successive conv blocks, pruning is met with the error `assert len(set(num_channels_list)) == 1`. This is due to this 1 out channel conv layer is being treated as depthwise convolution and ends up wrongly in dependency group.

Fixes #5736 




",saravanabalagi,8567893,open,True,0,2024-03-07T11:26:39+00:00,2024-03-07T11:26:39+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2169660042,5750,ModuleNotFoundError: No module named 'nni.algorithms.compression',"**Describe the issue**:



**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version: 3.12
- PyTorch/TensorFlow version: Pytorch=2.2.1
- Is conda/virtualenv/venv used?: yes
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",mo-shahab,98043363,open,True,1,2024-03-05T16:30:04+00:00,2024-03-05T16:43:10+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2154381768,5748,"it only ran 8 combinations each time, and then no code was running. But nni didn’t stop","**Describe the issue**:
I ran it with the settings below, but it only ran 8 combinations each time, and then no code was running. But nni didn’t stop


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): remote
- Client OS: mac
- Server OS (for remote mode only): linux
- Python version: 3.8
- PyTorch/TensorFlow version: 1.13
- Is conda/virtualenv/venv used?: conda


**Configuration**:
 - Experiment config (remember to remove secrets!):
experimentName: sgd
searchSpaceFile: search_space.json
trialGpuNumber: 1
trialConcurrency: 8
max_trial_number: 10000
tuner:
name: TPE
classArgs:
optimize_mode: maximize
trainingService:
platform: local
useActiveGpu: True

 - Search space:
{
""lr"": {""_type"": ""uniform"", ""_value"": [0.0001, 1.0]},
""batch_size"":{""_type"":""choice"",""_value"": [8, 16, 32, 64, 128]}
}



**How to reproduce it?**:Why and How to deal with it?",CCJing14,111256909,open,True,0,2024-02-26T14:55:29+00:00,2024-02-26T14:55:29+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2153656993,5747, ERROR (nni.runtime.command_channel.websocket.channel/MainThread) Failed to receive command. Retry in 0s,"**Describe the issue**:
I get the following error in logger:
[2024-02-26 16:53:12] ERROR (nni.runtime.command_channel.websocket.channel/MainThread) Failed to receive command. Retry in 0s
Traceback (most recent call last):
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/websockets/legacy/protocol.py"", line 963, in transfer_data
    message = await self.read_message()
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/websockets/legacy/protocol.py"", line 1033, in read_message
    frame = await self.read_data_frame(max_size=self.max_size)
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/websockets/legacy/protocol.py"", line 1108, in read_data_frame
    frame = await self.read_frame(max_size)
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/websockets/legacy/protocol.py"", line 1165, in read_frame
    frame = await Frame.read(
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/websockets/legacy/framing.py"", line 68, in read
    data = await reader(2)
  File ""/miniconda3/envs/yolo/lib/python3.8/asyncio/streams.py"", line 723, in readexactly
    await self._wait_for_data('readexactly')
  File ""miniconda3/envs/yolo/lib/python3.8/asyncio/streams.py"", line 517, in _wait_for_data
    await self._waiter
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/nni/runtime/command_channel/websocket/channel.py"", line 99, in _receive_command
    command = conn.receive()
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/nni/runtime/command_channel/websocket/connection.py"", line 103, in receive
    msg = _wait(self._ws.recv())
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/nni/runtime/command_channel/websocket/connection.py"", line 121, in _wait
    return future.result()
  File ""miniconda3/envs/yolo/lib/python3.8/concurrent/futures/_base.py"", line 444, in result
    return self.__get_result()
  File ""miniconda3/envs/yolo/lib/python3.8/concurrent/futures/_base.py"", line 389, in __get_result
    raise self._exception
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/websockets/legacy/protocol.py"", line 568, in recv
    await self.ensure_open()
  File ""miniconda3/envs/yolo/lib/python3.8/site-packages/websockets/legacy/protocol.py"", line 948, in ensure_open
    raise self.connection_closed_exc()
websockets.exceptions.ConnectionClosedError: sent 1011 (internal error) keepalive ping timeout; no close frame received


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): remote
- Client OS: mac
- Server OS (for remote mode only):  linux
- Python version: 3.8
- PyTorch/TensorFlow version: 1.13
- Is conda/virtualenv/venv used?: conda


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - experimentName: sgd_yolov7
searchSpaceFile: search_space_sgd_yolov7.json


trialGpuNumber: 1
trialConcurrency: 8
max_trial_number: 10000
tuner:
  name: TPE
  classArgs:
    optimize_mode: maximize
trainingService:
  platform: local
  useActiveGpu: True


 - Search space:
{
    ""lr"": {""_type"": ""uniform"", ""_value"": [0.0001, 1.0]},
    ""batch_size"":{""_type"":""choice"",""_value"": [8, 16, 32, 64, 128]}
}",CCJing14,111256909,open,True,6,2024-02-26T09:17:08+00:00,2024-06-06T12:37:30+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2146948435,5746,The system crashes when running the “Hello NAS” example code with GPU,"The system crashes when running the “Hello NAS” example code with GPU

- The crash occurs in three environments: colab, Windows11 system using conda, and Windows11 system without using conda. 
- In all three environments, I tried to downgrade pytorch to version 13.0, but it still crashes. 
- And it crashes when running in both .py and .ipynb modes.

### My steps
- colab: use `pip` to install `nni` and `lightning`, then run the “Hello NAS” example code. 
- Windows11: install `pytorch` (using the official installation instructions, which also installs `torchvision`), `nni`, `lightning`, `ipykernel`, `jupyterlab`

I cleared my environment beforehand, there are no extra package conflicts

### The details of one of the errors
 (Env: Windows11, using conda, pytorch2.2.0)
Before the crash, I saw a lot of python.exe in the task manager
I recorded the error at that time: 



```
[2024-02-20 22:24:17] Creating experiment, Experiment ID: 5p9fhwgt
[2024-02-20 22:24:17] Starting web server...
[2024-02-20 22:24:20] Setting up...
[2024-02-20 22:24:20] Web portal URLs: http://26.26.26.1:8084 http://169.254.77.17:8084 http://169.254.202.152:8084 http://169.254.67.238:8084 http://192.168.101.15:8084 http://127.0.0.1:8084
[2024-02-20 22:24:21] Successfully update searchSpace.
[2024-02-20 22:24:21] Checkpoint saved to C:\Users\DELL\nni-experiments\5p9fhwgt\checkpoint.
[2024-02-20 22:24:21] Experiment initialized successfully. Starting exploration strategy...
[2024-02-20 22:24:59] ERROR: Strategy failed to execute.
Traceback (most recent call last):
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 537, in _make_request
    response = conn.getresponse()
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connection.py"", line 466, in getresponse
    httplib_response = super().getresponse()
  File ""E:\conda\envs\pytorch_nni\lib\http\client.py"", line 1375, in getresponse
    response.begin()
  File ""E:\conda\envs\pytorch_nni\lib\http\client.py"", line 318, in begin
    version, status, reason = self._read_status()
  File ""E:\conda\envs\pytorch_nni\lib\http\client.py"", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""E:\conda\envs\pytorch_nni\lib\socket.py"", line 705, in readinto
    return self._sock.recv_into(b)
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\adapters.py"", line 486, in send
    resp = conn.urlopen(
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 847, in urlopen
    retries = retries.increment(
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\util\retry.py"", line 470, in increment
    raise reraise(type(error), error, _stacktrace)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\util\util.py"", line 39, in reraise
    raise value
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 793, in urlopen
    response = self._make_request(
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 539, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 370, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=8084): Read timed out. (read timeout=20)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""f:\today\nni\try_nni.py"", line 144, in <module>
    exp3.run(port=8084)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\experiment\experiment.py"", line 236, in run
    return self._run_impl(port, wait_completion, debug)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\experiment\experiment.py"", line 205, in _run_impl
    self.start(port, debug)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\nas\experiment\experiment.py"", line 270, in start
    self._start_engine_and_strategy()
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\nas\experiment\experiment.py"", line 230, in _start_engine_and_strategy
    self.strategy.run()
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\nas\strategy\base.py"", line 170, in run
    self._run()
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\nas\strategy\bruteforce.py"", line 220, in _run
    if not self.wait_for_resource():
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\nas\strategy\base.py"", line 100, in wait_for_resource
    if not self.engine.budget_available():
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\nas\execution\training_service.py"", line 271, in budget_available
    return self.nodejs_binding.get_status() in ['INITIALIZED', 'RUNNING', 'TUNER_NO_MORE_TRIAL']
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\experiment\experiment.py"", line 413, in get_status
    resp = rest.get(self.port, '/check-status', self.url_prefix)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\experiment\rest.py"", line 43, in get
    return request('get', port, api, prefix=prefix)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\experiment\rest.py"", line 31, in request
    resp = requests.request(method, url, timeout=timeout)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\api.py"", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\sessions.py"", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\sessions.py"", line 703, in send
    r = adapter.send(request, **kwargs)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\adapters.py"", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=8084): Read timed out. (read timeout=20)
[2024-02-20 22:24:59] Stopping experiment, please wait...
[2024-02-20 22:25:00] Checkpoint saved to C:\Users\DELL\nni-experiments\5p9fhwgt\checkpoint.
[2024-02-20 22:25:20] ERROR: HTTPConnectionPool(host='localhost', port=8084): Read timed out. (read timeout=20)
Traceback (most recent call last):
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 537, in _make_request
    response = conn.getresponse()
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connection.py"", line 466, in getresponse
    httplib_response = super().getresponse()
  File ""E:\conda\envs\pytorch_nni\lib\http\client.py"", line 1375, in getresponse
    response.begin()
  File ""E:\conda\envs\pytorch_nni\lib\http\client.py"", line 318, in begin
    version, status, reason = self._read_status()
  File ""E:\conda\envs\pytorch_nni\lib\http\client.py"", line 279, in _read_status
    line = str(self.fp.readline(_MAXLINE + 1), ""iso-8859-1"")
  File ""E:\conda\envs\pytorch_nni\lib\socket.py"", line 705, in readinto
    return self._sock.recv_into(b)
TimeoutError: timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\adapters.py"", line 486, in send
    resp = conn.urlopen(
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 847, in urlopen
    retries = retries.increment(
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\util\retry.py"", line 470, in increment
    raise reraise(type(error), error, _stacktrace)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\util\util.py"", line 39, in reraise
    raise value
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 793, in urlopen
    response = self._make_request(
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 539, in _make_request
    self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\urllib3\connectionpool.py"", line 370, in _raise_timeout
    raise ReadTimeoutError(
urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='localhost', port=8084): Read timed out. (read timeout=20)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\experiment\experiment.py"", line 171, in _stop_nni_manager
    rest.delete(self.port, '/experiment', self.url_prefix)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\experiment\rest.py"", line 52, in delete
    request('delete', port, api, prefix=prefix)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\nni\experiment\rest.py"", line 31, in request
    resp = requests.request(method, url, timeout=timeout)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\api.py"", line 59, in request
    return session.request(method=method, url=url, **kwargs)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\sessions.py"", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\sessions.py"", line 703, in send
    r = adapter.send(request, **kwargs)
  File ""E:\conda\envs\pytorch_nni\lib\site-packages\requests\adapters.py"", line 532, in send
    raise ReadTimeout(e, request=request)
requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=8084): Read timed out. (read timeout=20)
[2024-02-20 22:25:20] WARNING: Cannot gracefully stop experiment, killing NNI process...
[2024-02-20 22:25:21] ERROR: Failed to receive command. Retry in 0s
```",miwimisawi,79499876,open,True,7,2024-02-21T14:51:26+00:00,2024-07-30T10:09:05+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2140722576,5745,有人用NNI剪枝过wenet吗？,有人用NNI剪枝过[wenet](https://github.com/wenet-e2e/wenet)吗？或者类似包含各种cache的流式transformer模型？我用起来好像问题很多，各种,a122760,22231161,open,True,0,2024-02-18T03:53:33+00:00,2024-02-18T03:53:33+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2133511139,5744,Not training enough times,"Stopped training after only running 6 times, although it displayed as RUING, there was no issue with memory usage.

Ubuntu 22.04.3 LTS
python 3.8.18
nni 3.0b2

![image](https://github.com/microsoft/nni/assets/73021377/22598583-744b-4175-89dd-b04bcab3d570)

![image](https://github.com/microsoft/nni/assets/73021377/f87cf53b-e3c8-4185-aec8-39116635ceac)

![image](https://github.com/microsoft/nni/assets/73021377/a19aa29e-1748-45d4-a397-cfe645942fed)

![image](https://github.com/microsoft/nni/assets/73021377/50c84b37-695d-4602-8a04-2647006f9530)

![image](https://github.com/microsoft/nni/assets/73021377/59681b77-b583-4f59-b06a-cc9ba2073723)

[nnimanager.log](https://github.com/microsoft/nni/files/14275177/nnimanager.log)
![image](https://github.com/microsoft/nni/assets/73021377/ff717a2c-6941-404e-87ab-eb6316a621d4)
",Fly-Pluche,73021377,open,True,1,2024-02-14T04:18:00+00:00,2024-02-15T09:36:40+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2130564831,5743,   # get parameters from tuner         RECEIVED_PARAMS = nni.get_next_parameter() ,"**Describe the issue**:

   # get parameters from tuner
        RECEIVED_PARAMS = nni.get_next_parameter() 
return {}，is nan;
How to obtain the best parameters from the training results
example:https://github.com/microsoft/nni/tree/master/examples/trials/sklearn/classification
Thanks!
**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",whao159107,47801921,open,True,0,2024-02-12T17:00:32+00:00,2024-02-12T17:00:32+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2127474667,5742,Pruning Yolov8m model,"**Describe the issue**:



**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:

```
import torch

from nni.common.concrete_trace_utils import concrete_trace
from nni.compression.pruning import L1NormPruner
from nni.compression.utils import auto_set_denpendency_group_ids
from nni.compression.speedup import ModelSpeedup

from ultralytics import YOLO

model = YOLO(""/content/yolov8n.pt"")

model(torch.rand([1, 3, 640, 640]))

config_list = [{
    'sparsity': 0.5,
    'op_types': ['Conv2d'],
    'exclude_op_names_re': ['model.model.model.24.*'],  # this layer is detector head
}]

config_list = auto_set_denpendency_group_ids(model, config_list, torch.rand([1, 3, 640, 640]))

pruner = L1NormPruner(model, config_list)
masked_model, masks = pruner.compress()
pruner.unwrap_model()

graph_module = concrete_trace(model, (torch.rand([1, 3, 640, 640]), None, None, None))
ModelSpeedup(model, torch.rand([1, 3, 640, 640]), masks, graph_module=graph_module).speedup_model()

model(torch.rand([1, 3, 640, 640])
```


i tried to prune yolov8m model using the same code which is used to prune yolov5s model. But it is retraining in ""config_list = auto_set_denpendency_group_ids(model, config_list, torch.rand([1, 3, 640, 640]))"" this step and giving error like

```
/usr/local/lib/python3.10/dist-packages/ultralytics/data/loaders.py:456: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if im.shape[2] % stride or im.shape[3] % stride:
/usr/local/lib/python3.10/dist-packages/ultralytics/data/loaders.py:458: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if im.max() > 1.0 + torch.finfo(im.dtype).eps:  # torch.float32 eps is 1.2e-07
/usr/local/lib/python3.10/dist-packages/ultralytics/data/loaders.py:442: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
  self.paths = [getattr(im, ""filename"", f""image{i}.jpg"") for i, im in enumerate(im0)]
/usr/local/lib/python3.10/dist-packages/ultralytics/nn/modules/head.py:53: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if self.dynamic or self.shape != shape:
/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py:408: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  assert x.shape[-1] == 4, f""input shape last dimension expected 4 but input shape is {x.shape}""
/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py:231: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
  for xi, x in enumerate(prediction):  # image index, image inference
/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py:245: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if not x.shape[0]:
/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py:829: TracerWarning: Converting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return (batch.permute(0, 2, 3, 1).contiguous() * 255).clamp(0, 255).to(torch.uint8).cpu().numpy()
/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py:108: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
[<ipython-input-8-a46380595ef0>](https://localhost:8080/#) in <cell line: 20>()
     18 }]
     19 
---> 20 config_list = auto_set_denpendency_group_ids(model, config_list, torch.rand([1, 3, 640, 640]))
     21 
     22 pruner = L1NormPruner(model, config_list)

14 frames
[/usr/local/lib/python3.10/dist-packages/ultralytics/utils/ops.py](https://localhost:8080/#) in scale_boxes(img1_shape, boxes, img0_shape, ratio_pad, padding, xywh)
    108         gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new
    109         pad = (
--> 110             round((img1_shape[1] - img0_shape[1] * gain) / 2 - 0.1),
    111             round((img1_shape[0] - img0_shape[0] * gain) / 2 - 0.1),
    112         )  # wh padding

TypeError: type Tensor doesn't define __round__ method
```",hafeelnm19,77114909,open,True,1,2024-02-09T16:28:22+00:00,2024-02-22T01:17:23+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2112419346,5741,"Comparison exception:   The values for attribute 'shape' do not match: torch.Size([]) != torch.Size([1, 8400]).","使用nni对yolov8进行剪枝，报了如下错误
Comparison exception:   The values for attribute 'shape' do not match: torch.Size([]) != torch.Size([1, 8400]).",Gooddz1,77923234,open,True,0,2024-02-01T12:37:04+00:00,2024-02-01T12:37:04+00:00,,,1,1,0,0,0,0,0
microsoft/nni,2099506178,5740,"dispather comand：globals.args.pythonInterpreter, '-m', 'nni', '--exp_params', 占用内存太多","**Describe the issue**:

![image](https://github.com/microsoft/nni/assets/106226695/6d53fe36-5930-455d-96a6-97ce605b6b4b)


**Environment**:
- NNI version: 2.9
- Training service (local|remote|pai|aml|etc): local
- Client OS:
- Server OS (for remote mode only):
- Python version:
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 ```
assessor:
  classArgs:
    earlystop: true
    optimize_mode: maximize
    start_step: 5
  name: PAIAssessor
experimentName: ""\u65B0\u589E-deepfm-\u6FC0\u6D3B\u6A21\u578B\u4ED8\u8D39\u7387\u9884\
  \u4F30_0117_copy_copy_copy_copy_copy_copy_copy""
experimentWorkingDirectory: ../expdir
maxTrialNumber: 200
searchSpaceFile: search_space.json
trainingService:
  platform: local
trialCommand: python3 -m hpo_tools.core.utils.run --config=./config.ini
trialConcurrency: 2
tuner:
  classArgs:
    optimize_mode: maximize
  name: TPE
```
 - Search space:
```
{""${batch_size}"":{""_type"":""choice"",""_value"":[""64"",""256"",""512"",""1024"",""5000"",""1500"",""2500"",""7500"",""10000""]},""${learning_rate}"":{""_type"":""choice"",""_value"":[""1e-5"",""1e-4"",""1e-3"",""5e-5"",""5e-4"",""5e-3""]},""${deep_dnn1_units}"":{""_type"":""randint"",""_value"":[10,1000]},""${deep_dnn2_units}"":{""_type"":""randint"",""_value"":[10,1000]},""${fm_dnn_units}"":{""_type"":""randint"",""_value"":[10,1000]},""${combine_dnn_units}"":{""_type"":""randint"",""_value"":[10,100]},""${seed}"":{""_type"":""randint"",""_value"":[1,4294967290]},""${epochs}"":{""_type"":""randint"",""_value"":[3,50]}}
```

**Log message**:
 - nnimanager.log:
 ```
  placementConstraint: { type: 'None', gpus: [] }
}
[2024-01-24 15:37:33] INFO (NNIManager) Trial job zIKH1 status changed from WAITING to RUNNING
[2024-01-24 15:49:24] INFO (NNIManager) Trial job ndLXi status changed from RUNNING to SUCCEEDED
[2024-01-24 15:50:01] ERROR (tuner_command_channel.WebSocketChannel) Error: Error: tuner_command_channel: Tuner closed connection
    at WebSocket.<anonymous> (/usr/lib/python3.7/site-packages/nni_node/core/tuner_command_channel/websocket_channel.js:41:49)
    at WebSocket.emit (node:events:538:35)
    at WebSocket.emitClose (/usr/lib/python3.7/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:246:10)
    at Socket.socketOnClose (/usr/lib/python3.7/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:1127:15)
    at Socket.emit (node:events:526:28)
    at TCP.<anonymous> (node:net:687:12)
[2024-01-24 15:50:01] ERROR (NNIManager) Dispatcher error: tuner_command_channel: Tuner closed connection
[2024-01-24 15:50:01] ERROR (NNIManager) Error: Dispatcher stream error, tuner may have crashed.
    at EventEmitter.<anonymous> (/usr/lib/python3.7/site-packages/nni_node/core/nnimanager.js:647:32)
    at EventEmitter.emit (node:events:526:28)
    at WebSocketChannelImpl.handleError (/usr/lib/python3.7/site-packages/nni_node/core/tuner_command_channel/websocket_channel.js:107:22)
    at WebSocket.<anonymous> (/usr/lib/python3.7/site-packages/nni_node/core/tuner_command_channel/websocket_channel.js:41:37)
    at WebSocket.emit (node:events:538:35)
    at WebSocket.emitClose (/usr/lib/python3.7/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:246:10)
    at Socket.socketOnClose (/usr/lib/python3.7/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:1127:15)
    at Socket.emit (node:events:526:28)
    at TCP.<anonymous> (node:net:687:12)
[2024-01-24 15:50:01] INFO (NNIManager) Change NNIManager status from: RUNNING to: ERROR
[2024-01-24 16:05:49] INFO (NNIManager) User cancelTrialJob: zIKH1
[2024-01-24 16:05:49] INFO (ShutdownManager) Initiate shutdown: REST request
[2024-01-24 16:05:49] INFO (RestServer) Stopping REST server.
[2024-01-24 16:05:49] INFO (NNIManager) Change NNIManager status from: ERROR to: STOPPING
[2024-01-24 16:05:49] INFO (NNIManager) Stopping experiment, cleaning up ...
[2024-01-24 16:05:49] INFO (RestServer) REST server stopped.
[2024-01-24 16:05:49] INFO (LocalTrainingService) Stopping local machine training service...
[2024-01-24 16:05:49] INFO (NNIManager) Change NNIManager status from: STOPPING to: STOPPED
[2024-01-24 16:05:49] INFO (NNIManager) Experiment stopped.
[2024-01-24 16:05:49] INFO (NNITensorboardManager) Forced stopping all tensorboard task.
[2024-01-24 16:05:49] INFO (NNITensorboardManager) All tensorboard task stopped.
[2024-01-24 16:05:49] INFO (NNITensorboardManager) Tensorboard manager stopped.
[2024-01-24 16:05:49] INFO (ShutdownManager) Shutdown complete.
```
 - dispatcher.log:
没有报错
 - nnictl stdout and stderr:
没有报错
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",yjjinjie,106226695,closed,True,1,2024-01-25T03:42:39+00:00,2024-01-26T01:51:23+00:00,2024-01-26T01:51:23+00:00,,0,0,0,0,0,0,0
microsoft/nni,2099479102,5739,ERROR (nni.runtime.msg_dispatcher_base/Thread-1),"dispacher log:

ERROR (nni.runtime.msg_dispatcher_base/Thread-1) 68
Traceback (most recent call last):
  File ""/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni/runtime/msg_dispatcher_base.py"", line 108, in command_queue_worker
    self.process_command(command, data)
  File ""/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni/runtime/msg_dispatcher_base.py"", line 154, in process_command
    command_handlers[command](data)
  File ""/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni/runtime/msg_dispatcher.py"", line 148, in handle_report_metric_data
    self._handle_final_metric_data(data)
  File ""/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni/runtime/msg_dispatcher.py"", line 201, in _handle_final_metric_data
    self.tuner.receive_trial_result(id_, _trial_params[id_], value, customized=customized,
  File ""/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni/algorithms/hpo/tpe_tuner.py"", line 197, in receive_trial_result
    params = self._running_params.pop(parameter_id)
KeyError: 68

NNImanager log:

ERROR (WsChannel.__default__) Channel closed. Ignored command { type: 'GE', content: '1' }
[2024-01-25 11:08:42] WARNING (WsConnection.__default__) Missing pong
[2024-01-25 11:08:47] WARNING (WsConnection.__default__) Missing pong
[2024-01-25 11:08:47] ERROR (WsConnection.__default__) Failed sending command. Drop connection: Error: WebSocket is not open: readyState 3 (CLOSED)
    at sendAfterClose (/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:988:17)
    at WebSocket.send (/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni_node/node_modules/express-ws/node_modules/ws/lib/websocket.js:405:7)
    at node:internal/util:375:7
    at new Promise (<anonymous>)
    at bound send (node:internal/util:361:12)
    at WsConnection.sendAsync (/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni_node/common/command_channel/websocket/connection.js:92:16)
    at WsConnection.heartbeat (/home/mm/anaconda3/envs/CLNode_env/lib/python3.8/site-packages/nni_node/common/command_channel/websocket/connection.js:144:18)
    at listOnTimeout (node:internal/timers:569:17)
    at process.processTimers (node:internal/timers:512:7)",Moreau14,57832075,open,True,2,2024-01-25T03:12:30+00:00,2024-07-03T12:07:23+00:00,,,1,0,0,0,0,0,1
microsoft/nni,2091273604,5738,ModelSpeedup() error,"**Environment**:
- NNI version: 3.0

I have implemented FPGM pruninig to an object detector with FPN and skip connections. ModelSpeedup() doesn't work with my model’s architecture 

Here is the code I used:
```
import torch
from modelsimport build_model
from data.config import cfg
from nni.compression.pruning import FPGMPruner
from nni.common.concrete_trace_utils import concrete_trace
from nni.compression.speedup import ModelSpeedup

device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model = build_model(""test"", cfg.NUM_CLASSES, width_mult=0.0625).to(device)

config_list = [{
        'sparsity_per_layer' : 0.2,
        'op_types' : ['Conv2d'],
    }, {
        'exclude' : True,
        'op_names' : [
                    'loc.0', 'loc.1', 'loc.2', 'loc.3', 'loc.4', 'loc.5',
                    'conf.0', 'conf.1', 'conf.2', 'conf.3', 'conf.4', 'conf.5'
                    ]
    }]

dummy_input = torch.rand(2, 3, 640, 640).to(device)


pruner = FPGMPruner(model, config_list)
_, masks = pruner.compress()
pruner.unwrap_model()

model = ModelSpeedup(model, dummy_input, masks, graph_module=graph_module).speedup_model()
```

And the error code is this:
```
IndexError                                Traceback (most recent call last)
Cell In[8], line 32
     29 pruner.unwrap_model()
     31 dummy_input_for_trace = torch.rand([1, 3, 640, 640]).to(device)
---> 32 graph_module = concrete_trace(model, {'x': dummy_input_for_trace})
     33 #print(masks)
     34 model = ModelSpeedup(model, dummy_input, masks, graph_module=graph_module).speedup_model()

File [~/anaconda3/envs/gpu/lib/python3.11/site-packages/nni/common/concrete_trace_utils/concrete_tracer.py:1606](https://vscode-remote+ssh-002dremote-002b160-002e40-002e54-002e160.vscode-resource.vscode-cdn.net/home/gkrispanis/Projects/EResFD-main/~/anaconda3/envs/gpu/lib/python3.11/site-packages/nni/common/concrete_trace_utils/concrete_tracer.py:1606), in concrete_trace(root, concrete_args, use_operator_patch, operator_patch_backlist, forward_function_name, check_args, autowrap_leaf_function, autowrap_leaf_class, leaf_module, fake_middle_class, dce, cpu_offload, trace_twice)
   1603 is_training = root.training
   1604 root.eval()
-> 1606 graph = tracer.trace(root,
   1607     autowrap_leaf_function = autowrap_leaf_function,
   1608     autowrap_leaf_class = autowrap_leaf_class,
   1609     leaf_module = leaf_module,
   1610     fake_middle_class = fake_middle_class,
   1611     concrete_args = concrete_args,
   1612     use_operator_patch = use_operator_patch,
   1613     operator_patch_backlist = operator_patch_backlist,
   1614     forward_function_name = forward_function_name,
   1615 )
   1617 if trace_twice:
   1618     graph_check = tracer.trace(root,
   1619         autowrap_leaf_function = autowrap_leaf_function,
...
    165 if insts[cur].opcode in self.jump_opcodes or (
    166     insts[cur].opcode in self.jump_before_opcodes and insts[cur + 1].opcode in self.jump_opcodes):
    167     # in executing branch condition

IndexError: list index out of range
```",gkrisp98,57015579,open,True,0,2024-01-19T19:52:16+00:00,2024-01-19T19:52:16+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2086640876,5736,"ModelSpeedup error: assert len(set(num_channels_list)) == 1, possible incorrect layers in dependency set","ModelSpeedup does not alter the model successfully for a model with 3 successive conv blocks.

**Environment**:
- NNI version: 3.0
- Python version: 3.8.16
- PyTorch version: 1.13.0
- Cpu or cuda version: CUDA 11.6


**Reproduce the problem**

- create a model and config with desired `sparsity_ratio`
- obtain pruning masks using `L1NormPruner`
- call `ModelSpeedup` with `batch_size` parameter

<details>
<summary>Minimal Code</summary>

```python
# %%
import torch
import torch.nn as nn

from nni.compression.pruning import L1NormPruner
from nni.compression.utils import auto_set_denpendency_group_ids
from nni.compression.speedup import ModelSpeedup

# %%
class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 40, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(40)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(40, 80, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(80)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv3 = nn.Conv2d(80, 1, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(1)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x)
        x = self.conv3(x)
        x = self.bn3(x)
        return x
    
model = ConvNet()
num_params_unpruned = sum(p.numel() for p in model.parameters())
dummy_input = torch.randn(1, 3, 32, 32)
dummy_output = model(dummy_input)
print(dummy_output.shape)

# %%
sparsity_ratio = 0.5
config_list = [{
    'op_types': ['Conv2d'],
    'sparse_ratio': sparsity_ratio,
}]
config_list = auto_set_denpendency_group_ids(model, config_list, [dummy_input])
pruner = L1NormPruner(model, config_list)
_, masks = pruner.compress()
pruner.unwrap_model()
model = ModelSpeedup(model, [dummy_input], masks, garbage_collect_values=False).speedup_model()

# %%
num_params_pruned = sum(p.numel() for p in model.parameters())
print(f'Number of parameters before pruning: {num_params_unpruned}')
print(f'Number of parameters after pruning: {num_params_pruned}')

num_params_diff = num_params_unpruned - num_params_pruned
prune_ratio = num_params_diff / num_params_unpruned
print(f'Number of parameters pruned: {num_params_diff}')
print(f'Parameter ratio: {(1-prune_ratio)*100:.2f}%')
```
</details>

Error:

Assertion error: number of channels in same set should be identical

<details>
<summary>Error Trace</summary>

```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
Cell In[108], line 1
----> 1 model = ModelSpeedup(model, [dummy_input], masks, garbage_collect_values=False).speedup_model()

File /usr/local/lib/python3.8/dist-packages/nni/compression/speedup/model_speedup.py:429, in ModelSpeedup.speedup_model(self)
    427 self.logger.info('Resolve the mask conflict before mask propagate...')
    428 # fix_mask_conflict(self.masks, self.graph_module, self.dummy_input)
--> 429 self.fix_mask_conflict()
    430 self.logger.info('Infer module masks...')
    431 self.initialize_propagate(self.dummy_input)

File /usr/local/lib/python3.8/dist-packages/nni/compression/speedup/model_speedup.py:243, in ModelSpeedup.fix_mask_conflict(self)
    241 def fix_mask_conflict(self):
    242     fix_group_mask_conflict(self.graph_module, self.masks)
--> 243     fix_channel_mask_conflict(self.graph_module, self.masks)
    244     fix_weight_sharing_mask_conflict(self.graph_module, self.masks)

File /usr/local/lib/python3.8/dist-packages/nni/compression/speedup/mask_conflict.py:296, in fix_channel_mask_conflict(graph_module, masks)
    294 num_channels_list = [len(x) for x in channel_masks if x is not None]
    295 # number of channels in same set should be identical
--> 296 assert len(set(num_channels_list)) == 1
    297 num_channels = num_channels_list[0]
    299 for i, dim_mask in enumerate(channel_masks):

AssertionError: 
```
</details>

The same code works fine without `self.conv3` and `self.bn3`.",saravanabalagi,8567893,open,True,1,2024-01-17T17:24:04+00:00,2024-01-24T11:55:03+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2086623587,5735,Bug: batch_size parameter in ModelSpeedup does not alter the model when set to 1,"`batch_size` parameter in ModelSpeedup does not prune or alter the model when set to 1.

**Environment**:
- NNI version: 3.0
- Python version: 3.8.16
- PyTorch version: 1.13.0
- Cpu or cuda version: CUDA 11.6


**Reproduce the problem**

- create a model and config with desired `sparsity_ratio`
- obtain pruning masks using `L1NormPruner`
- call `ModelSpeedup` with `batch_size` parameter

<details>
<summary>Minimal Code</summary>

```python
# %%
import torch
import torch.nn as nn

from nni.compression.pruning import L1NormPruner
from nni.compression.utils import auto_set_denpendency_group_ids
from nni.compression.speedup import ModelSpeedup

# %%
class ConvNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 40, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(40)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(40, 80, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(80)
        self.relu2 = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu2(x)
        return x
    
model = ConvNet()
num_params_unpruned = sum(p.numel() for p in model.parameters())
dummy_input = torch.randn(1, 3, 32, 32)
dummy_output = model(dummy_input)
print(dummy_output.shape)

# %%
sparsity_ratio = 0.5
config_list = [{
    'op_types': ['Conv2d'],
    'sparse_ratio': sparsity_ratio,
}]
config_list = auto_set_denpendency_group_ids(model, config_list, [dummy_input])
pruner = L1NormPruner(model, config_list)
_, masks = pruner.compress()
pruner.unwrap_model()
model = ModelSpeedup(model, [dummy_input], masks, batch_size=1, garbage_collect_values=False).speedup_model()

# %%
num_params_pruned = sum(p.numel() for p in model.parameters())
print(f'Number of parameters before pruning: {num_params_unpruned}')
print(f'Number of parameters after pruning: {num_params_pruned}')

num_params_diff = num_params_unpruned - num_params_pruned
prune_ratio = num_params_diff / num_params_unpruned
print(f'Number of parameters pruned: {num_params_diff}')
print(f'Parameter ratio: {(1-prune_ratio)*100:.2f}%')
```
</details>

Output:

Regardless of the sparsity ratio, the model is not altered

```
Number of parameters before pruning: 30240
Number of parameters after pruning: 30240
Number of parameters pruned: 0
Parameter ratio: 100.00%
```

<details>
<summary>Full Log</summary>

```
[2024-01-17 16:55:43] Start to speedup the model...
[2024-01-17 16:55:43] Resolve the mask conflict before mask propagate...
[2024-01-17 16:55:43] dim0 sparsity: 0.500000
[2024-01-17 16:55:43] dim1 sparsity: 0.000000
0 Filter
[2024-01-17 16:55:43] dim0 sparsity: 0.500000
[2024-01-17 16:55:43] dim1 sparsity: 0.000000
[2024-01-17 16:55:43] Infer module masks...
[2024-01-17 16:55:43] Propagate original variables
[2024-01-17 16:55:43] Propagate variables for placeholder: x, output mask:  0.0000 
[2024-01-17 16:55:43] Propagate variables for call_module: conv1, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 
[2024-01-17 16:55:44] Propagate variables for call_module: bn1, , output mask:  0.0000 
[2024-01-17 16:55:44] Propagate variables for call_module: relu1, , output mask:  0.0000 
[2024-01-17 16:55:44] Propagate variables for call_module: conv2, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 
[2024-01-17 16:55:44] Propagate variables for call_module: bn2, , output mask:  0.0000 
[2024-01-17 16:55:44] Propagate variables for call_module: relu2, , output mask:  0.0000 
[2024-01-17 16:55:44] Propagate variables for output: output, output mask:  0.0000 
[2024-01-17 16:55:44] Update direct sparsity...
[2024-01-17 16:55:45] Update direct mask for placeholder: x, output mask:  0.0000 
[2024-01-17 16:55:45] Update direct mask for call_module: conv1, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 
[2024-01-17 16:55:45] Update direct mask for call_module: bn1, , output mask:  0.0000 
[2024-01-17 16:55:45] Update direct mask for call_module: relu1, , output mask:  0.0000 
[2024-01-17 16:55:45] Update direct mask for call_module: conv2, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 
[2024-01-17 16:55:45] Update direct mask for call_module: bn2, , output mask:  0.0000 
[2024-01-17 16:55:45] Update direct mask for call_module: relu2, , output mask:  0.0000 
[2024-01-17 16:55:46] Update direct mask for output: output, output mask:  0.0000 
[2024-01-17 16:55:46] Update indirect sparsity...
[2024-01-17 16:55:46] Update indirect mask for output: output, output mask:  0.0000 
[2024-01-17 16:55:46] Update indirect mask for call_module: relu2, , output mask:  0.0000 
[2024-01-17 16:55:46] Update indirect mask for call_module: bn2, , output mask:  0.0000 
[2024-01-17 16:55:46] Update indirect mask for call_module: conv2, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 
[2024-01-17 16:55:47] Update indirect mask for call_module: relu1, , output mask:  0.0000 
[2024-01-17 16:55:47] Update indirect mask for call_module: bn1, , output mask:  0.0000 
[2024-01-17 16:55:47] Update indirect mask for call_module: conv1, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 
[2024-01-17 16:55:47] Update indirect mask for placeholder: x, output mask:  0.0000 
[2024-01-17 16:55:47] Resolve the mask conflict after mask propagate...
[2024-01-17 16:55:47] dim0 sparsity: 0.500000
[2024-01-17 16:55:47] dim1 sparsity: 0.000000
0 Filter
[2024-01-17 16:55:47] dim0 sparsity: 0.500000
[2024-01-17 16:55:47] dim1 sparsity: 0.000000
[2024-01-17 16:55:47] Replace compressed modules...
[2024-01-17 16:55:47] replace module (name: conv1, op_type: Conv2d)
[2024-01-17 16:55:47] replace conv2d with in_channels: 3, out_channels: 40
[2024-01-17 16:55:47] replace module (name: bn1, op_type: BatchNorm2d)
[2024-01-17 16:55:47] replace batchnorm2d with num_features: 40
[2024-01-17 16:55:47] replace module (name: relu1, op_type: ReLU)
[2024-01-17 16:55:47] replace module (name: conv2, op_type: Conv2d)
[2024-01-17 16:55:47] replace conv2d with in_channels: 40, out_channels: 80
[2024-01-17 16:55:47] replace module (name: bn2, op_type: BatchNorm2d)
[2024-01-17 16:55:47] replace batchnorm2d with num_features: 80
[2024-01-17 16:55:47] replace module (name: relu2, op_type: ReLU)
[2024-01-17 16:55:47] Speedup done.
```
</details>

Expected Output:

If any `batch_size` above 1 is specified, or if the argument is not specified, then model is altered as expected

```
Number of parameters before pruning: 30240
Number of parameters after pruning: 7920
Number of parameters pruned: 22320
Parameter ratio: 26.19%
```

<details>
<summary>Full Log</summary>

```
[2024-01-17 17:07:12] Start to speedup the model...
[2024-01-17 17:07:12] Resolve the mask conflict before mask propagate...
[2024-01-17 17:07:12] dim0 sparsity: 0.500000
[2024-01-17 17:07:12] dim1 sparsity: 0.000000
0 Filter
[2024-01-17 17:07:12] dim0 sparsity: 0.500000
[2024-01-17 17:07:12] dim1 sparsity: 0.000000
[2024-01-17 17:07:12] Infer module masks...
[2024-01-17 17:07:12] Propagate original variables
[2024-01-17 17:07:12] Propagate variables for placeholder: x, output mask:  0.0000 
[2024-01-17 17:07:12] Propagate variables for call_module: conv1, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 
[2024-01-17 17:07:12] Propagate variables for call_module: bn1, , output mask:  0.0000 
[2024-01-17 17:07:12] Propagate variables for call_module: relu1, , output mask:  0.0000 
[2024-01-17 17:07:12] Propagate variables for call_module: conv2, weight:  0.5000 bias:  0.5000 , output mask:  0.0000 
[2024-01-17 17:07:13] Propagate variables for call_module: bn2, , output mask:  0.0000 
[2024-01-17 17:07:13] Propagate variables for call_module: relu2, , output mask:  0.0000 
[2024-01-17 17:07:13] Propagate variables for output: output, output mask:  0.0000 
[2024-01-17 17:07:13] Update direct sparsity...
[2024-01-17 17:07:13] Update direct mask for placeholder: x, output mask:  0.0000 
[2024-01-17 17:07:13] Update direct mask for call_module: conv1, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 
[2024-01-17 17:07:13] Update direct mask for call_module: bn1, , output mask:  0.5000 
[2024-01-17 17:07:13] Update direct mask for call_module: relu1, , output mask:  0.5000 
[2024-01-17 17:07:14] Update direct mask for call_module: conv2, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 
[2024-01-17 17:07:14] Update direct mask for call_module: bn2, , output mask:  0.5000 
[2024-01-17 17:07:14] Update direct mask for call_module: relu2, , output mask:  0.5000 
[2024-01-17 17:07:14] Update direct mask for output: output, output mask:  0.5000 
[2024-01-17 17:07:14] Update indirect sparsity...
[2024-01-17 17:07:14] Update indirect mask for output: output, output mask:  0.5000 
[2024-01-17 17:07:14] Update indirect mask for call_module: relu2, , output mask:  0.5000 
[2024-01-17 17:07:15] Update indirect mask for call_module: bn2, , output mask:  0.5000 
[2024-01-17 17:07:15] Update indirect mask for call_module: conv2, weight:  0.7500 bias:  0.5000 , output mask:  0.5000 
[2024-01-17 17:07:15] Update indirect mask for call_module: relu1, , output mask:  0.5000 
[2024-01-17 17:07:16] Update indirect mask for call_module: bn1, , output mask:  0.5000 
[2024-01-17 17:07:16] Update indirect mask for call_module: conv1, weight:  0.5000 bias:  0.5000 , output mask:  0.5000 
[2024-01-17 17:07:16] Update indirect mask for placeholder: x, output mask:  0.0000 
[2024-01-17 17:07:16] Resolve the mask conflict after mask propagate...
[2024-01-17 17:07:16] dim0 sparsity: 0.500000
[2024-01-17 17:07:16] dim1 sparsity: 0.465116
[2024-01-17 17:07:16] WARNING: both dim0 and dim1 masks found.
0 Filter
[2024-01-17 17:07:16] dim0 sparsity: 0.500000
[2024-01-17 17:07:16] dim1 sparsity: 0.465116
[2024-01-17 17:07:16] WARNING: both dim0 and dim1 masks found.
[2024-01-17 17:07:16] Replace compressed modules...
[2024-01-17 17:07:16] replace module (name: conv1, op_type: Conv2d)
[2024-01-17 17:07:16] replace conv2d with in_channels: 3, out_channels: 20
[2024-01-17 17:07:16] replace module (name: bn1, op_type: BatchNorm2d)
[2024-01-17 17:07:16] replace batchnorm2d with num_features: 20
[2024-01-17 17:07:16] replace module (name: relu1, op_type: ReLU)
[2024-01-17 17:07:16] replace module (name: conv2, op_type: Conv2d)
[2024-01-17 17:07:16] replace conv2d with in_channels: 20, out_channels: 40
[2024-01-17 17:07:16] replace module (name: bn2, op_type: BatchNorm2d)
[2024-01-17 17:07:16] replace batchnorm2d with num_features: 40
[2024-01-17 17:07:16] replace module (name: relu2, op_type: ReLU)
[2024-01-17 17:07:16] Speedup done.
```
</details>",saravanabalagi,8567893,open,True,2,2024-01-17T17:13:08+00:00,2024-03-07T12:06:35+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2083941626,5734,"Cannot get the correct value,Can you tell me how to get the correct value? thank you very much","**Describe the issue**:
    when i use Detectron2 and adet model， i want to prune model, and i got error, how can i solve it;
    
![image](https://github.com/microsoft/nni/assets/71381036/66e39505-2e9c-4b66-9fae-bafe73304081)
![image](https://github.com/microsoft/nni/assets/71381036/66e47088-bfb8-4e5a-be8b-40e585ec7f52)
![image](https://github.com/microsoft/nni/assets/71381036/a44b1a30-16f5-45fb-b6d7-88fac56494f0)



**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:3.8.5
- PyTorch/TensorFlow version:PyTorch:1.13.1
- Is conda/virtualenv/venv used?:conda
- Is running in Docker?:no


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->
![image](https://github.com/microsoft/nni/assets/71381036/7b41257f-b7fd-4796-a1b4-13ef02b99c30)



**How to reproduce it?**:",tianlan6767,71381036,open,True,0,2024-01-16T13:13:53+00:00,2024-01-16T13:16:00+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2079541009,5733,require a missing '.experiment' even for the first time running,"**Describe the issue**: 
When I tried HPO Quickstart with PyTorch, I stuck in
![image](https://github.com/microsoft/nni/assets/47420453/18f32c4d-b68b-4e01-9c62-6648890ed34d)
![image](https://github.com/microsoft/nni/assets/47420453/924ddd64-56be-4ee0-ad96-58f9dda33e23)
Error: ENOENT: no such file or directory, open 'C:\Users\14048\nni-experiments\.experiment'
in this snapshot, you can see that it keeps asking for ""user/nni-experiments/.experiment"" repeatedly (I understand the repeating should be the refreshing request to the backend data) after I tried to put a blank .experiment manually, I found out that it(from browser log as the screenshot above) requires a json file, and some information is required, without which I cannot run.  It requires that file even for the first time running.

what nni-experiment looks like, I tried to delete the whole folder but not work
![image](https://github.com/microsoft/nni/assets/47420453/becf6bcb-b834-4607-850b-4c23be34ba90)


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: windows anaconda
- Server OS (for remote mode only):
- Python version: 3.10.13
- PyTorch/TensorFlow version: 2.1.0
- Is conda/virtualenv/venv used?: conda
- Is running in Docker?: Windows Anaconda


**Configuration**:
everything default


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",FrankYJY,47420453,open,True,2,2024-01-12T19:20:21+00:00,2024-03-25T11:56:19+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2071928323,5732,How nni performs qat quantization on leaky relu,"I used nni-2.10.1 and pytorch2.0
How nni performs qat quantization on leaky relu?",11923303233,49989322,open,True,0,2024-01-09T09:06:18+00:00,2024-01-09T09:06:18+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2063648487,5731,Cannot install nni on Jetson Nano Developer Kit,"I am trying to install nni on jetson nano developer kit using pip but I get : 

ERROR: could not find a version that satisfies the requirement (from versions : none)
ERROR: No matching distribution found for nni

My environement:
OS: Ubuntu 18.04
Python : 3.8
pip : 23.3.2
",yasmineLalabouali,37512440,open,True,0,2024-01-03T09:53:22+00:00,2024-01-03T09:53:22+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2060833750,5730,mint,"{  
  ""p"": ""GRC20"", 
  ""op"": ""mint"", 
  ""tick"": ""GitHub"", 
  ""amt"": ""2000""
}",monirulnaim897,58027376,open,True,0,2023-12-30T19:22:09+00:00,2023-12-30T19:22:09+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2060657501,5729,Mint,"{  
  ""p"": ""GRC20"", 
  ""op"": ""mint"", 
  ""tick"": ""GitHub"", 
  ""amt"": ""2000""
}",shubhamjainbittu,91271374,open,True,0,2023-12-30T06:52:42+00:00,2023-12-30T06:52:42+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2060256023,5728,mint,"1. Copy code :
{  
  ""p"": ""GRC20"", 
  ""op"": ""mint"", 
  ""tick"": ""GitHub"", 
  ""amt"": ""2000""
}",vitotbontoy,127694345,open,True,0,2023-12-29T16:52:37+00:00,2023-12-29T16:52:37+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2058046356,5727,Dispatcher terminiated reporting cuda unavailable issue,"**Describe the issue**:
When I start an experiment, it stops after one trail and gives the error below. But my code can run on GPU on its own without error, and GPU is available at the time.


**Environment**:
- NNI version: 2.7
- Training service (local|remote|pai|aml|etc): local
- Client OS:
- Server OS (for remote mode only):
- Python version: 3.8
- PyTorch/TensorFlow version: pytorch 1.10.0+cu113
- Is conda/virtualenv/venv used?: no
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!):
trialConcurrency: 1
maxExecDuration: 7d
trainingServicePlatform: local
localConfig:
  maxTrialNumPerGpu: 1
  useActiveGpu: true
useAnnotation: false
tuner:
  builtinTunerName: GridSearch
 - Search space:
{
    ""w_adv"":{""_type"":""choice"",""_value"":[0.25,0.5,0.75,1]},
    ""N_steps_D"":{""_type"":""choice"",""_value"":[5,10,15]}
}

**Log message**:
 - nnimanager.log:
 -[2023-12-28 02:31:12] INFO (main) Start NNI manager
[2023-12-28 02:31:12] INFO (NNIDataStore) Datastore initialization done
[2023-12-28 02:31:12] INFO (RestServer) Starting REST server at port 8088, URL prefix: ""/""
[2023-12-28 02:31:12] INFO (RestServer) REST server started.
[2023-12-28 02:31:13] INFO (NNIManager) Starting experiment: wsjofmed
[2023-12-28 02:31:13] INFO (NNIManager) Setup training service...
[2023-12-28 02:31:13] INFO (LocalTrainingService) Construct local machine training service.
[2023-12-28 02:31:13] INFO (NNIManager) Setup tuner...
[2023-12-28 02:31:13] INFO (NNIManager) Change NNIManager status from: INITIALIZED to: RUNNING
[2023-12-28 02:31:13] INFO (NNIManager) Add event listeners
[2023-12-28 02:31:13] INFO (LocalTrainingService) Run local machine training service.
[2023-12-28 02:31:13] WARNING (GPUScheduler) gpu_metrics file does not exist!
[2023-12-28 02:31:13] INFO (NNIManager) NNIManager received command from dispatcher: ID, 
[2023-12-28 02:31:13] INFO (NNIManager) NNIManager received command from dispatcher: TR, {""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""w_adv"": 0.25, ""N_steps_D"": 5}, ""parameter_index"": 0}
[2023-12-28 02:31:18] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 0,
  hyperParameters: {
    value: '{""parameter_id"": 0, ""parameter_source"": ""algorithm"", ""parameters"": {""w_adv"": 0.25, ""N_steps_D"": 5}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2023-12-28 02:31:28] INFO (NNIManager) Trial job IzTYm status changed from WAITING to RUNNING
[2023-12-28 02:52:33] WARNING (IpcInterface) Commands jammed in buffer!
[2023-12-28 02:52:38] WARNING (IpcInterface) Commands jammed in buffer!
[2023-12-28 02:52:43] WARNING (IpcInterface) Commands jammed in buffer!

This same warning repeats for many times

 - dispatcher.log:
 -[2023-12-28 02:31:12] INFO (nni.experiment/MainThread) Creating experiment, Experiment ID: [36mwsjofmed[0m
[2023-12-28 02:31:12] INFO (nni.experiment/MainThread) Starting web server...
[2023-12-28 02:31:13] INFO (nni.experiment/MainThread) Setting up...
[2023-12-28 02:31:13] INFO (nni.experiment/MainThread) Web portal URLs: [36mhttp://127.0.0.1:8088 http://10.214.163.164:8088[0m
[2023-12-28 02:31:13] INFO (nni.tools.nnictl.launcher/MainThread) To stop experiment run ""nnictl stop wsjofmed"" or ""nnictl stop --all""
[2023-12-28 02:31:13] INFO (nni.tools.nnictl.launcher/MainThread) Reference: https://nni.readthedocs.io/en/stable/Tutorial/Nnictl.html
[2023-12-28 02:31:13] INFO (numexpr.utils/MainThread) Note: NumExpr detected 24 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2023-12-28 02:31:13] INFO (numexpr.utils/MainThread) NumExpr defaulting to 8 threads.
[2023-12-28 02:31:13] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher started
[2023-12-28 02:31:13] INFO (nni.tuner.gridsearch/Thread-1) Grid initialized, size: (4×3) = 12
[2023-12-28 02:52:27] ERROR (nni.runtime.msg_dispatcher_base/Thread-2) Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
Traceback (most recent call last):
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/nni/runtime/msg_dispatcher_base.py"", line 88, in command_queue_worker
    self.process_command(command, data)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/nni/runtime/msg_dispatcher_base.py"", line 147, in process_command
    command_handlers[command](data)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/nni/runtime/msg_dispatcher.py"", line 136, in handle_report_metric_data
    data['value'] = load(data['value'])
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/nni/common/serializer.py"", line 401, in load
    return json_tricks.loads(string, obj_pairs_hooks=hooks, **json_tricks_kwargs)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/json_tricks/nonp.py"", line 236, in loads
    return json_loads(string, object_pairs_hook=hook, **jsonkwargs)
  File ""/home/rox/anaconda3/lib/python3.8/json/__init__.py"", line 370, in loads
    return cls(**kw).decode(s)
  File ""/home/rox/anaconda3/lib/python3.8/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/home/rox/anaconda3/lib/python3.8/json/decoder.py"", line 353, in raw_decode
    obj, end = self.scan_once(s, idx)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/json_tricks/decoders.py"", line 44, in __call__
    map = hook(map, properties=self.properties)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/json_tricks/utils.py"", line 66, in wrapper
    return encoder(*args, **{k: v for k, v in kwargs.items() if k in names})
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/nni/common/serializer.py"", line 820, in _json_tricks_any_object_decode
    return _wrapped_cloudpickle_loads(b)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/nni/common/serializer.py"", line 826, in _wrapped_cloudpickle_loads
    return cloudpickle.loads(b)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/torch/storage.py"", line 161, in _load_from_bytes
    return torch.load(io.BytesIO(b))
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/torch/serialization.py"", line 608, in load
    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/torch/serialization.py"", line 787, in _legacy_load
    result = unpickler.load()
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/torch/serialization.py"", line 743, in persistent_load
    deserialized_objects[root_key] = restore_location(obj, location)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/torch/serialization.py"", line 175, in default_restore_location
    result = fn(storage, location)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/torch/serialization.py"", line 151, in _cuda_deserialize
    device = validate_cuda_device(location)
  File ""/home/rox/anaconda3/lib/python3.8/site-packages/torch/serialization.py"", line 135, in validate_cuda_device
    raise RuntimeError('Attempting to deserialize object on a CUDA '
RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.
[2023-12-28 02:52:28] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher exiting...
[2023-12-28 02:52:31] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher terminiated

 - nnictl stdout and stderr:
 these two shows no error",Rxannro,105044070,closed,True,1,2023-12-28T06:28:35+00:00,2023-12-28T07:57:54+00:00,2023-12-28T07:57:54+00:00,,0,0,0,0,0,0,0
microsoft/nni,2057138202,5726,Mismatched hyperparameters between web server display and their actual values,"**Describe the issue**:

**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: Ubuntu 20.04.4 LTS (GNU/Linux 5.13.0-30-generic x86_64)
- Server OS (for remote mode only):
- Python version: 3.11
- PyTorch/TensorFlow version: 2.1.2
- Is conda/virtualenv/venv used?: Conda
- Is running in Docker?: No


**Configuration**:
 - Experiment config (remember to remove secrets!):
```yaml
experimentName: MRNN hyper-param searching
authorName: WenjieDu
trialConcurrency: 1
trainingServicePlatform: local
searchSpacePath: MRNN_ETTm1_tuning_space.json
multiThread: true
useAnnotation: false
tuner:
    builtinTunerName: Random

trial:
    command: enable_tuning=1 pypots-cli tuning --model pypots.imputation.MRNN --train_set ../../data/ettm1/train.h5 --val_set ../../data/ettm1/val.h5
    codeDir: .
    gpuNum: 1

localConfig:
    useActiveGpu: true
    maxTrialNumPerGpu: 20
    gpuIndices: 3
```
 - Search space: 
```json
{
  ""n_steps"":  {""_type"":""choice"",""_value"":[60]},
  ""n_features"":  {""_type"":""choice"",""_value"":[7]},
  ""patience"":  {""_type"":""choice"",""_value"":[10]},
  ""epochs"":  {""_type"":""choice"",""_value"":[200]},
  ""rnn_hidden_size"":  {""_type"":""choice"",""_value"":[16,32,64,128,256,512]},
  ""lr"":{""_type"":""loguniform"",""_value"":[0.0001,0.01]}
}
```



**Log message**:
 - nnimanager.log:
```
[2023-12-27 16:16:42] INFO (NNIManager) submitTrialJob: form: {
  sequenceId: 7,
  hyperParameters: {
    value: '{""parameter_id"": 7, ""parameter_source"": ""algorithm"", ""parameters"": {""n_steps"": 60, ""n_features"": 7, ""patience"": 10, ""epochs"": 200, ""rnn_hidden_size"": 32, ""lr"": 0.0008698020401037771}, ""parameter_index"": 0}',
    index: 0
  },
  placementConstraint: { type: 'None', gpus: [] }
}
[2023-12-27 16:16:42] INFO (LocalV3.local) Created trial XsB6F
```

 - dispatcher.log:
```
[2023-12-27 16:15:06] INFO (numexpr.utils/MainThread) Note: detected 128 virtual cores but NumExpr set to maximum of 64, check ""NUMEXPR_MAX_THREADS"" environment variable.
[2023-12-27 16:15:06] INFO (numexpr.utils/MainThread) Note: NumExpr detected 128 cores but ""NUMEXPR_MAX_THREADS"" not set, so enforcing safe limit of 8.
[2023-12-27 16:15:06] INFO (numexpr.utils/MainThread) NumExpr defaulting to 8 threads.
[2023-12-27 16:15:06] INFO (nni.tuner.random/MainThread) Using random seed 220808582
[2023-12-27 16:15:06] INFO (nni.runtime.msg_dispatcher_base/MainThread) Dispatcher started
[2023-12-27 16:15:06] INFO (nni.runtime.msg_dispatcher/Thread-1 (command_queue_worker)) Initial search space: {'n_steps': {'_type': 'choice', '_value': [60]}, 'n_features': {'_type': 'choice', '_value': [7]}, 'patience': {'_type': 'choice', '_value': [10]}, 'epochs': {'_type': 'choice', '_value': [200]}, 'rnn_hidden_size': {'_type': 'choice', '_value': [16, 32, 64, 128, 256, 512]}, 'lr': {'_type': 'loguniform', '_value': [0.0001, 0.01]}}
```

 - nnictl stdout and stderr:
```
2023-12-27 16:16:44 [INFO]: Have set the random seed as 2204 for numpy and pytorch.
2023-12-27 16:16:44 [INFO]: The tunner assigns a new group of params: {'n_steps': 60, 'n_features': 7, 'patience': 10, 'epochs': 200, 'rnn_hidden_size': 256, 'lr': 0.0054442307300676335}
2023-12-27 16:16:45 [INFO]: No given device, using default device: cuda
2023-12-27 16:16:45 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.
2023-12-27 16:16:48 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 401,619
2023-12-27 16:16:48 [INFO]: Option lazy_load is set as False, hence loading all data from file...
2023-12-27 16:16:52 [INFO]: Epoch 001 - training loss: 1.3847, validating loss: 1.3214
``` 

<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:

Note that in the nnimanager.log: `lr` of trial XsB6F is `0.0008698020401037771` and this is also the value displayed on the local web page, but in the nnictl stdout log, the actual `lr` received by the model is `0.0054442307300676335`, and they're mismatched. This is not a single case, I notice that hyperparameters of some trials are mismatched between the nnimanager tells and their actual values, while some of them are matched and fine.",WenjieDu,17807970,open,True,4,2023-12-27T09:33:25+00:00,2024-07-16T03:02:25+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2041612951,5724,"Dispatcher stream error, tuner may have crashed.","**Describe the issue**:  
I have been trying to apply GridSearch strategy to do some NAS experiments, but the tuner seems to crash constantly when starting an experiment. The python script works fine when I run it. When I look into the NNI manager log, it states that the Websocket is not open (readystate 3). So there is probably a problem with the connection, because it is closed by a peer. I already turned off my firewall and the problem still occurs.  


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: Windows 11
- Server OS (for remote mode only):
- Python version: 3.10.11
- PyTorch version: 2.1.0
- Is conda/virtualenv/venv used?: venv
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!): 
 `experimentName: FNN_NAS
searchSpaceFile: search_space.json
trialCommand: python FNN_NAS_training.py
trialCodeDirectory: .
trialGpuNumber: 0
trialConcurrency: 1
maxExperimentDuration: 5h
maxTrialNumber: 10
tuner:
  name: GridSearch
trainingService:
  platform: local
  useActiveGpu: False`
 - Search space: 
[search_space.json](https://github.com/microsoft/nni/files/13673229/search_space.json)



**Log message**:
 - nnimanager.log: 
[nnimanager.log](https://github.com/microsoft/nni/files/13673244/nnimanager.log)

 - dispatcher.log: 
[dispatcher.log](https://github.com/microsoft/nni/files/13673245/dispatcher.log)
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->
",QuintenDanneels,102586692,open,True,0,2023-12-14T12:36:10+00:00,2023-12-14T15:46:34+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2038606954,5723,Unsupported Layer nn.TransformerEncoderLayer,"**Describe the bug**:



**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Python version: 3.10
- PyTorch version: 1.13
- Cpu or cuda version: Cuda/Cpu


**Reproduce the problem**
- How to reproduce: It seems like the class[`TransformerEncoderLayer`](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html) is not currently supported by NNI. I am attempting to prune the network [RepNet](https://github.com/materight/RepNet-pytorch/tree/main/repnet) . Can someone help adding the implementation for that?",saeedashrraf,48128381,open,True,0,2023-12-12T21:56:24+00:00,2023-12-12T21:57:13+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2036847661,5722,reload data make insufficient memory space,"I'm new to NNI. I learn that we have to load same data for every trail,  if the data is very large, this framework is waste time and memory space,  can’t execute trails by parallel. 
",HouJian2020,39508506,open,True,0,2023-12-12T02:15:43+00:00,2023-12-12T02:15:43+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2034328931,5720,"NNI is starting, it's time to run an epoch but there's no value in the page?","**Describe the issue**:

it's time to run an epoch but there's no value in the page?

**Environment**:
- NNI version:2.5
- Training service (local|remote|pai|aml|etc):local
- Client OS:Win10
- Server OS (for remote mode only):
- Python version: 3.7
- PyTorch/TensorFlow version:PyTorch
- Is conda/virtualenv/venv used?:conda
- Is running in Docker?: no


**Configuration**:
searchSpaceFile: search_space.json
trialCommand: python train_nni.py
trialGpuNumber: 0
trialConcurrency: 1
tuner:
  name: TPE
  classArgs:
    optimize_mode: maximize
trainingService:
  platform: local

![image](https://github.com/microsoft/nni/assets/58765840/d6ad9705-574c-4e88-bc24-a1b031215e7e)

**How to reproduce it?**:",yao-ao,58765840,open,True,0,2023-12-10T11:22:42+00:00,2023-12-10T11:22:42+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2022943118,5719,Enhancement of GBDTSelector inherited from FeatureSelector,"<!-- Please only use this template for submitting enhancement requests -->


**What would you like to be added**:
The FeatureSelector class was written in a preliminary form, like the following referenced code snippet:
https://github.com/microsoft/nni/blob/767ed7f22e1e588ce76cbbecb6c6a4a76a309805/nni/feature_engineering/feature_selector.py#L26-L34
I think GBDTSelector is not necessary to inherit FeatureSelector while it rewrite all class methods, though it doesn't inherit the class properties of FeatureSelector.
https://github.com/microsoft/nni/blob/767ed7f22e1e588ce76cbbecb6c6a4a76a309805/nni/algorithms/feature_engineering/gbdt_selector/gbdt_selector.py#L35
and GBDTSelector class adopts train_test_split function from scikit-learn, I was wondering if the validation datasets will be needed to enhance the effect of fit module ([like what I implemented for training cycle while evaluate validation to early stop](https://github.com/linjing-lab/easy-pytorch/blob/9651774dcc4581104f914980baf2ebc05f96fd85/released_box/perming/_utils.py#L382)):
https://github.com/microsoft/nni/blob/767ed7f22e1e588ce76cbbecb6c6a4a76a309805/nni/algorithms/feature_engineering/gbdt_selector/gbdt_selector.py#L86-L89


**Why is this needed**:
The subclass that inherits from FeatureSelector is GBDTSelector, and it is not a valid inheritance because all subclass properties and methods are overridden.

**Without this feature, how does current nni work**：
GBDTSelector works in nni that powered by `fit` module and [`get_selected_features` module](https://github.com/microsoft/nni/blob/767ed7f22e1e588ce76cbbecb6c6a4a76a309805/nni/algorithms/feature_engineering/gbdt_selector/gbdt_selector.py#L102), not obtained by the methods from FeatureSelector class.

**Components that may involve changes**:
Add `super(GBDTSelector, self).__init__` to [initial part of GBDTSelector](https://github.com/microsoft/nni/blob/767ed7f22e1e588ce76cbbecb6c6a4a76a309805/nni/algorithms/feature_engineering/gbdt_selector/gbdt_selector.py#L37), or drop FeatureSelector class if inherited properties wasn't important when subclass rewrited all the methods instead of giving some changes.

**Brief description of your proposal if any**:
The properties and methods contained in FeatureSelector may not contribute to the code logic of any module in GBDTSelector.

",linjing-lab,82669431,open,True,0,2023-12-04T03:25:52+00:00,2023-12-04T03:27:24+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2021676133,5718,How to use cell to create a  Model Space ?,"I'm trying to use cell to create my Model Space but when I run the experiment, appears this  warning

 Cannot convert CategoricalMultiple([0, 1], n_chosen=1, label='[2, 3, 4, 5, 6]/input_2_0') to legacy format.",AlondraMM,152126153,open,True,0,2023-12-01T22:33:53+00:00,2023-12-02T04:59:28+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2019227074,5717,after speed up the number of output dimention change,"**Describe the issue**:
i used nni.compression.speedup and after the speed uo the number of output featuears change from 10 to 5 featurears only!!!!

GoogLeNet(
  (conv1): BasicConv2d(
    (conv): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
  (conv2): BasicConv2d(
    (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (conv3): BasicConv2d(
    (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
  )
  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
  (inception3a): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(96, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception3b): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (maxpool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)
  (inception4a): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(240, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(48, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(104, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(240, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(240, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception4b): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(256, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(56, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception4c): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(12, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(12, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception4d): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(256, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(56, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(72, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception4e): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(264, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(264, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(264, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(264, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)
  (inception5a): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(416, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(416, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(416, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (inception5b): Inception(
    (branch1): BasicConv2d(
      (conv): Conv2d(416, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
    )
    (branch2): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(416, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch3): Sequential(
      (0): BasicConv2d(
        (conv): Conv2d(416, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicConv2d(
        (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (branch4): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)
      (1): BasicConv2d(
        (conv): Conv2d(416, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (aux1): None
  (aux2): None
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (dropout): Dropout(p=0.2, inplace=False)
  (fc): Linear(in_features=512, out_features=5, bias=True)
)



**Environment**:
- NNI version:
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:
- PyTorch/TensorFlow version:
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",m3bbass,46040260,open,True,0,2023-11-30T18:01:26+00:00,2023-11-30T18:05:54+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2011822935,5715,How can I review the experiment that has been stopped? It seems that the Web can only show the detail of 'RUNNING' experiment.,,lightup666,117833552,closed,True,1,2023-11-27T09:09:32+00:00,2023-11-28T12:16:30+00:00,2023-11-28T12:14:46+00:00,,0,0,0,0,0,0,0
microsoft/nni,2007483331,5714,Quantization with NAS,Does NNI support using NAS to quantify neural networks? To find the optimal bit width for each layer,lightup666,117833552,open,True,0,2023-11-23T04:50:21+00:00,2023-11-23T04:50:21+00:00,,,0,0,0,0,0,0,0
microsoft/nni,2001497042,5713,Does the current pruning tool not support Channel Split operation?,"I uesd the **Partial_conv** in my model, which includes the **Channel Split(torch.split)**.  In the inference(after **ModelSpeedup**), i got the error in `x1, x2 = torch.split(x, [self.dim_conv3, self.dim_untouched], dim=1)`:

RuntimeError: start (36) + length (108) exceeds dimension size (139).",YHaooo-4508,80744649,open,True,0,2023-11-20T06:56:58+00:00,2023-11-20T06:56:58+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1992293524,5712,pip install nni[SMAC] reports error in version requirements,"**Describe the issue**:
While installing nni with either 'pip install nni[SMAC]' or 'pip install nni[all]' an error related to minimum version requirement appears:
See output at 'log message' section.

**Environment**:
- NNI version: current (3.0) online installation using PIP:
- Training service (local|remote|pai|aml|etc): not relevant
- Client OS: Ubuntu 18.04LTE
- Server OS (for remote mode only): not relevant
- Python version: 3.10
- PyTorch/TensorFlow version: none
- Is conda/virtualenv/venv used?: virtualenv
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!): not relevant
 - Search space: not relevant


**Log message**:
Collecting ConfigSpaceNNI>=0.4.7.3 (from nni[SMAC])
  Downloading ConfigSpaceNNI-0.4.7.3.tar.gz (108 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 2.2 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [28 lines of output]
      /mnt/workdata/envs/nnioptim/lib/python3.10/site-packages/setuptools/_distutils/extension.py:134: UserWarning: Unknown Extension options: 'compiler_directives'
        warnings.warn(msg)
      /mnt/workdata/envs/nnioptim/lib/python3.10/site-packages/setuptools/dist.py:498: SetuptoolsDeprecationWarning: Invalid dash-separated options
      !!

              ********************************************************************************
              Usage of dash-separated 'description-file' will not be supported in future
              versions. Please use the underscore name 'description_file' instead.

              This deprecation is overdue, please update your project and remove deprecated
              calls to avoid build errors in the future.

              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.
              ********************************************************************************

      !!
        opt = self.warn_dash_deprecation(opt, section)
      /mnt/workdata/envs/nnioptim/lib/python3.10/site-packages/setuptools/__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.
      !!

              ********************************************************************************
              Requirements should be satisfied by a PEP 517 installer.
              If you are using pip, you can try `pip install --use-pep517`.
              ********************************************************************************

      !!
        dist.fetch_build_eggs(dist.setup_requires)
      error in ConfigSpaceNNI setup command: 'python_requires' must be a string containing valid version specifiers; Invalid specifier: '>=3.4.*'
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:
Try to install pip install nni[SMAC]",marcinchomicz,34246339,open,True,3,2023-11-14T09:04:18+00:00,2024-09-07T15:12:19+00:00,,,4,4,0,0,0,0,0
microsoft/nni,1987219163,5711,Error about the web port,"**Describe the issue**:
I’m using the nni toolbox for neural architecture searh. However, I find that I must provide an avalible web port to experiment.run() when I select the multi-trial exploration strategy, The code is runing using Remote-SSH of VSCode, connecting to the AutoDL server. How can I find or create the available web port?  


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version: 3.7
- PyTorch/TensorFlow version: 1.13.1
- Is conda/virtualenv/venv used?:
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",wang1239435478,11307044,open,True,0,2023-11-10T09:12:04+00:00,2023-11-10T09:12:04+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1983808614,5709,Adding back the sensitivity analysis tool to v3.0 ,"<!-- Please only use this template for submitting enhancement requests -->


**What would you like to be added**:
Are there any plans to add back the sensitivty analysis tool to v3.0 ? 

**Why is this needed**:
It would be great to have this tool back as a debug tool, to target better the layers that are not sensible to pruning. This tool was available in the previous versions (until [v2.6](https://nni.readthedocs.io/en/v2.6/Compression/CompressionUtils.html#sensitivity-analysis)) but was removed later on.

**Without this feature, how does current nni work**：

Without this tool, it's more difficult to identify which layers can be pruned further instead of using a uniform sparsity rate for the whole network without goint into the tedious process of trial & error. Which can be a big overhead for big models with long evaluation processes.

**Components that may involve changes**:
the utils package

**Brief description of your proposal if any**:
",mehdi-nait,45018669,open,True,0,2023-11-08T15:01:19+00:00,2023-11-08T15:01:19+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1978691092,5708,WARNING: GPU found but will not be used. Please set `experiment.config.trial_gpu_number` to the number of GPUs you want to use for each trial.,"''Hello,NAS'':GPU can't be used but i set all:
and if i use CPU,there is a error :策略执行失败



**Environment**:
- NNI version:3.0
- Training service (local|remote|pai|aml|etc):；local
- Client OS:Windows 10
- Server OS (for remote mode only):No
- Python version:3.10
- PyTorch/TensorFlow version:1.12.1+cu113
- Is conda/virtualenv/venv used?:Pycharm Edit
- Is running in Docker?:No


**Configuration**:
 - Experiment config (remember to remove secrets!):{
  ""experimentType"": ""nas"",
  ""searchSpace"": {},
  ""trialCommand"": ""D:\\learnsaoftware\\anaconda3\\python.exe -m nni.nas.execution.training_service trial"",
  ""trialCodeDirectory"": ""C:\\Users\\Administrator\\Desktop\\yyh"",
  ""trialConcurrency"": 100,
  ""trialGpuNumber"": 1,
  ""maxTrialNumber"": 100,
  ""useAnnotation"": false,
  ""debug"": false,
  ""logLevel"": ""info"",
  ""experimentWorkingDirectory"": ""C:\\Users\\Administrator\\nni-experiments"",
  ""trainingService"": {
    ""platform"": ""local"",
    ""trialCommand"": ""D:\\learnsaoftware\\anaconda3\\python.exe -m nni.nas.execution.training_service trial"",
    ""trialCodeDirectory"": ""C:\\Users\\Administrator\\Desktop\\yyh"",
    ""trialGpuNumber"": 1,
    ""debug"": false,
    ""useActiveGpu"": true,
    ""maxTrialNumberPerGpu"": 1,
    ""reuseMode"": false
  },
  ""executionEngine"": {
    ""name"": ""ts""
  },
  ""modelFormat"": {
    ""name"": ""simplified""
  }
}
 - Search space:class MyModelSpace(ModelSpace):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        # LayerChoice is used to select a layer between Conv2d and DwConv.
        self.conv2 = LayerChoice([
            nn.Conv2d(32, 64, 3, 1),
            DepthwiseSeparableConv(32, 64)
        ], label='conv2')
        # nni.choice is used to select a dropout rate.
        # The result can be used as parameters of `MutableXXX`.
        self.dropout1 = MutableDropout(nni.choice('dropout', [0.25, 0.5, 0.75]))  # choose dropout rate from 0.25, 0.5 and 0.75
        self.dropout2 = nn.Dropout(0.5)
        feature = nni.choice('feature', [64, 128, 256])
        self.fc1 = MutableLinear(9216, feature)
        self.fc2 = MutableLinear(feature, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(self.conv2(x), 2)
        x = torch.flatten(self.dropout1(x), 1)
        x = self.fc2(self.dropout2(F.relu(self.fc1(x))))
        output = F.log_softmax(x, dim=1)
        return output


model_space = MyModelSpace()

上述是我的设置情况，请问应该如何解决？",yyh7773,37353928,open,True,2,2023-11-06T09:33:58+00:00,2023-12-28T07:13:33+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1978215608,5706,pruning_bert_glue example pruning error,"**Describe the issue**:



**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): ubuntu deep learning ami
- Client OS: ubuntu
- Server OS (for remote mode only):
- Python version: 3.10.9
- PyTorch/TensorFlow version: Pytorch 2.1.0+cu121
- Is conda/virtualenv/venv used?: no
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:
just run the pruning_bert_glue tutorial 


from nni.compression.pruning import MovementPruner
from nni.compression.speedup import ModelSpeedup
from nni.compression.utils.external.external_replacer import TransformersAttentionReplacer


def pruning_attn():
    Path('./output/bert_finetuned/').mkdir(parents=True, exist_ok=True)
    model = build_finetuning_model(task_name, f'./output/bert_finetuned/{task_name}.bin')
    trainer = prepare_traced_trainer(model, task_name)
    evaluator = TransformersEvaluator(trainer)

    config_list = [{
        'op_types': ['Linear'],
        'op_names_re': ['bert\.encoder\.layer\.[0-9]*\.attention\.*'],
        'sparse_threshold': 0.1,
        'granularity': [64, 64]
    }]

    pruner = MovementPruner(model, config_list, evaluator, warmup_step=9000, cooldown_begin_step=36000, regular_scale=10)
    pruner.compress(None, 4)
    pruner.unwrap_model()

    masks = pruner.get_masks()
    Path('./output/pruning/').mkdir(parents=True, exist_ok=True)
    torch.save(masks, './output/pruning/attn_masks.pth')
    torch.save(model, './output/pruning/attn_masked_model.pth')


if not skip_exec:
    pruning_attn()
```

Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[/tmp/ipykernel_56471/3847997197.py:3](https://vscode-remote+ssh-002dremote-002b7b22686f73744e616d65223a2243617073746f6e65227d.vscode-resource.vscode-cdn.net/tmp/ipykernel_56471/3847997197.py:3): FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric('glue', task_name)
[2023-11-06 02:54:46] WARNING: trainer.optimzer is not wrapped by nni.trace, or trainer.optimzer is None, will using huggingface default optimizer.
[2023-11-06 02:54:46] WARNING: trainer.lr_scheduler is not wrapped by nni.trace, or trainer.lr_scheduler is None, will using huggingface default lr_scheduler.
[2023-11-06 02:54:46] WARNING: Using epochs number as training duration, please make sure the total training steps larger than `cooldown_begin_step`.
You are adding a <class 'nni.compression.utils.evaluator.PatchCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is
:DefaultFlowCallback
PrinterCallback
PatchCallback
[/home/ubuntu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557](https://vscode-remote+ssh-002dremote-002b7b22686f73744e616d65223a2243617073746f6e65227d.vscode-resource.vscode-cdn.net/home/ubuntu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557): UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
[/home/ubuntu/NAS_exps/new_pruning_bert_glue.ipynb](https://vscode-remote+ssh-002dremote-002b7b22686f73744e616d65223a2243617073746f6e65227d.vscode-resource.vscode-cdn.net/home/ubuntu/NAS_exps/new_pruning_bert_glue.ipynb) Cell 23 line 3
     26     torch.save(model, '[./output/pruning/attn_masked_model.pth](https://vscode-remote+ssh-002dremote-002b7b22686f73744e616d65223a2243617073746f6e65227d.vscode-resource.vscode-cdn.net/home/ubuntu/NAS_exps/output/pruning/attn_masked_model.pth)')
     29 if not skip_exec:
---> 30     pruning_attn()

[/home/ubuntu/NAS_exps/new_pruning_bert_glue.ipynb](https://vscode-remote+ssh-002dremote-002b7b22686f73744e616d65223a2243617073746f6e65227d.vscode-resource.vscode-cdn.net/home/ubuntu/NAS_exps/new_pruning_bert_glue.ipynb) Cell 23 line 2
     12 config_list = [{
     13     'op_types': ['Linear'],
     14     'op_names_re': ['bert\.encoder\.layer\.[0-9]*\.attention\.*'],
     15     'sparse_threshold': 0.1,
     16     'granularity': [64, 64]
     17 }]
     19 pruner = MovementPruner(model, config_list, evaluator, warmup_step=9000, cooldown_begin_step=36000, regular_scale=10)
---> 20 pruner.compress(None, 4)
     21 pruner.unwrap_model()
     23 masks = pruner.get_masks()

File [~/.local/lib/python3.8/site-packages/nni/compression/pruning/movement_pruner.py:228](https://vscode-remote+ssh-002dremote-002b7b22686f73744e616d65223a2243617073746f6e65227d.vscode-resource.vscode-cdn.net/home/ubuntu/NAS_exps/~/.local/lib/python3.8/site-packages/nni/compression/pruning/movement_pruner.py:228), in MovementPruner.compress(self, max_steps, max_epochs)
    225     warn_msg = \
    226         f'Using epochs number as training duration, please make sure the total training steps larger than `cooldown_begin_step`.'
    227     _logger.warning(warn_msg)
--> 228 return super().compress(max_steps, max_epochs)
...
-> 1080     assert optimizer is not None
   1081     old_step = optimizer.step
   1083     def patched_step(_, *args, **kwargs):

AssertionError: 
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
```",sukritij29,48414723,open,True,3,2023-11-06T03:30:30+00:00,2024-07-13T07:32:45+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1975639490,5705,Do we have a plan for Stable Diffusion compression?,,moonlightian,49636631,open,True,0,2023-11-03T08:10:36+00:00,2023-11-03T08:10:36+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1967871389,5704,About BatchNorm QAT 's problems,"**Describe the bug**:
when i use QAT_Quantizer() to qat my model，i got a wrap_model.pt，then i test with this model，encounter this bug：
![image](https://github.com/microsoft/nni/assets/43179227/5db45961-4e4e-45f4-8dcf-3102f0ec0c70)



**Environment**:  linux
- NNI version:   __version__ = '999.dev0'
- Training service (local|remote|pai|aml|etc):   remote
- Python version:   3.7.10
- PyTorch version:  1.12.0 + cu113
- Cpu or cuda version:  GPU version GeForce RTX 3090


**Reproduce the problem**
- Code|Example:
![image](https://github.com/microsoft/nni/assets/43179227/2a318a81-b51f-4233-b0c3-fb83089f9a8f)
![image](https://github.com/microsoft/nni/assets/43179227/36b2f958-f90a-485f-b27f-9ada7421123a)

and my model arch is :
![image](https://github.com/microsoft/nni/assets/43179227/c512391c-aa74-46bb-a42a-831497ac41d5)
![image](https://github.com/microsoft/nni/assets/43179227/09b7eb6c-401c-43b7-aced-dbc04418f2f9)


- How to reproduce:",typhoonlee,43179227,open,True,0,2023-10-30T09:13:15+00:00,2023-10-30T09:43:33+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1964885376,5703,How many epoches for each trail?,"**Describe the issue**: I followed the examples/tutorials/hpo_quickstart_pytorch and modify it to optimize for some regression model. In each trail, I set up an epoch loop to 1000 epoches and report the itermidiaate losses in every iteration. Does the tuner wait till the end of each trail and evaluate its metric or does it stop it earlier if I believes it had enough for that trail? Which document describe this? 



**Environment**:
- NNI version: 2.7
- Training service (local|remote|pai|aml|etc): local
- Client OS:ubuntu 2204
- Server OS (for remote mode only):
- Python version: 3.10
- PyTorch/TensorFlow version: pytorch 2.1
- Is conda/virtualenv/venv used?: conda
- Is running in Docker?:no 


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:",0i0i0i,43296293,open,True,0,2023-10-27T06:54:40+00:00,2023-10-27T06:54:40+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1964877797,5702,How to use loss instead of accuray in HPO,"**Describe the issue**: I just started using nni and followed the  /hpo_quickstart_pytorch/ example to inplement.
But, instead of accuracy, I need to thave the main monitor and optimize for the loss, and the lower loss, the better, obviously. 

I used  nni.report_intermediate_result(valLossSingle) to report the loss. I think I need to let the tuner know that this is actually loss, rather than accuracy. How to do that? 


**Environment**:
- NNI version: 2.7
- Training service (local|remote|pai|aml|etc): local
- Client OS: ubuntu
- Server OS (for remote mode only):
- Python version: 3.10
- PyTorch/TensorFlow version: 2
- Is conda/virtualenv/venv used?: conda
- Is running in Docker?: no


**Configuration**:
 - Experiment config (remember to remove secrets!): 
 - Search space: 


**Log message**: 
 - nnimanager.log: 
 - dispatcher.log:
 - nnictl stdout and stderr: 
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**: ",0i0i0i,43296293,open,True,0,2023-10-27T06:48:29+00:00,2023-10-27T06:48:29+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1964866905,5701,微信群2维码过期了,"<!-- Please only use this template for submitting enhancement requests -->
可否提供新的维码，或者给一的群主的微信，让大家先加Ta。避免再次过期？
有微信群还是很好的。有些小疑问可以不必在github上开issue

**What would you like to be added**:

**Why is this needed**:

**Without this feature, how does current nni work**：

**Components that may involve changes**:

**Brief description of your proposal if any**:
",0i0i0i,43296293,open,True,0,2023-10-27T06:39:44+00:00,2023-10-27T06:39:44+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1962694030,5700,update draft note (#5668),"### Description ###

#### Test Options ####
  - [ ] fast test
  - [ ] full test - HPO
  - [ ] full test - NAS
  - [ ] full test - compression

### Checklist ###
  - [ ] test case
  - [ ] doc

### How to test ###


",Bonytu,47250017,closed,True,0,2023-10-26T04:45:00+00:00,2023-10-26T05:31:53+00:00,2023-10-26T05:31:53+00:00,,0,0,0,0,0,0,0
microsoft/nni,1959242066,5699,NotImplementedError: argument of type: <class 'generator'>,"**Describe the issue**:
Helllo！I prune yolov8 model with L1Pruner. I used yolov8s.pt.
But I got this error.

```
Traceback (most recent call last):
 File ""test_v8.py"", line 36, in <module>
   graph_module = concrete_trace(model, (torch.rand([1, 3, 640, 640]),))
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/concrete_tracer.py"", line 1606, in concrete_trace
    graph = tracer.trace(root,
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/concrete_tracer.py"", line 1085, in trace
    (self.create_arg(OperatorPatcherContext.patch_run(fn, *args, *more_args, **kwargs)),),
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/operator_patcher.py"", line 291, in patch_run
    return new_func(*args, **kwargs)
 File ""/usr/local/lib/python3.8/dist-packages/ultralytics/nn/tasks.py"", line 42, in new_func
    return self.predict(x, *args, **kwargs)
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/operator_patcher.py"", line 291, in patch_run
    return new_func(*args, **kwargs)
 File ""/usr/local/lib/python3.8/dist-packages/ultralytics/nn/tasks.py"", line 59, in new_func
    return self._predict_once(x, profile, visualize)
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/operator_patcher.py"", line 291, in patch_run
    return new_func(*args, **kwargs)
 File ""/usr/local/lib/python3.8/dist-packages/ultralytics/nn/tasks.py"", line 79, in new_func
    x = m(x)  # run
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/operator_patcher.py"", line 291, in patch_run
    return new_func(*args, **kwargs)
 File ""/usr/local/lib/python3.8/dist-packages/ultralytics/nn/modules/block.py"", line 203, in new_func
    y.extend(m(y[-1]) for m in self.m)
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/operator_patcher.py"", line 291, in patch_run
    return new_func(*args, **kwargs)
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/concrete_proxy.py"", line 279, in __call__
    return self.tracer.create_proxy('call_method', self.attr, (self.root,) + args, kwargs)
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/concrete_tracer.py"", line 432, in create_proxy
    args_ = self.create_arg(args)
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/concrete_tracer.py"", line 524, in create_arg
    return super().create_arg(a)
 File ""/usr/local/lib/python3.8/dist-packages/torch/fx/proxy.py"", line 125, in create_arg
    return type(a)(self.create_arg(elem) for elem in a)
 File ""/usr/local/lib/python3.8/dist-packages/torch/fx/proxy.py"", line 125, in <genexpr>
    return type(a)(self.create_arg(elem) for elem in a)
 File ""/usr/local/lib/python3.8/dist-packages/nni/common/concrete_trace_utils/concrete_tracer.py"", line 524, in create_arg
    return super().create_arg(a)
 File ""/usr/local/lib/python3.8/dist-packages/torch/fx/proxy.py"", line 151, in create_arg
    raise NotImplementedError(f""argument of type: {type(a)}"")

NotImplementedError: argument of type: <class 'generator'>
```





**Environment**:
- NNI version:3.0
- Training service (local|remote|pai|aml|etc):
- Client OS:
- Server OS (for remote mode only):
- Python version:3.8.10
- PyTorch/TensorFlow version:1.11.0+cu113
- Is conda/virtualenv/venv used?:no
- Is running in Docker?:yes


",zx214,55902473,open,True,0,2023-10-24T13:20:11+00:00,2023-10-25T00:54:48+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1958942354,5698,What is the correct order to use DistributedDataParallel and QAT Quantizer?,"**Describe the issue**:



**Environment**:
- NNI version: Master(3.0?)
- Training service (local|remote|pai|aml|etc): local
- Client OS: Arch Linux
- Server OS (for remote mode only): N/A
- Python version: 3.11
- PyTorch/TensorFlow version: PyTorch 1.13
- Is conda/virtualenv/venv used?: No
- Is running in Docker?: No


**Configuration**:
 - Experiment config (remember to remove secrets!): N/A
 - Search space: N/A


**Log message**:
 - nnimanager.log:
 - dispatcher.log:
 - nnictl stdout and stderr:
 
<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:
I'm trying to do QAT with DDP, but I'm confused with the order of initializing optimizer.
According to Pytorch official code, definition of optimizer should happen after wrapping model in DDP.
But in NNI, https://github.com/microsoft/nni/blob/master/nni/compression/quantization/qat_quantizer.py
this example shows that we should have optimizer first, pass it into evaluator, then let QAT Quantizer wrap the model.
I can't find any example code for DPP+QAT, could anyone help?",neesetifa,36785948,open,True,0,2023-10-24T10:20:21+00:00,2023-10-24T10:20:21+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1952803840,5697,I have made some minor changes in the comments of the code and some modifications in documentation. It is very useful when we have a crystal clear comments to explain the code which makes it well documented and clean code.,"### Description ###

#### Test Options ####
  - [x] fast test
  - [x] full test - HPO
  - [x] full test - NAS
  - [x] full test - compression

### Checklist ###
  - [x] test case
  - [x] doc

### How to test ###


",saikiran76,80874246,open,True,2,2023-10-19T18:08:06+00:00,2023-10-30T04:23:19+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1942284502,5695,ValueError: num_classes must be specified for torchmetrics >= 0.11. Please either specify it or use an older version of torchmetrics.,"<!--
    Here is an issue template for NNI student program China. You are encouraged to raise concerns about any issue and share your ideas of NNI or our student program. Both Chinese and English are acceptable.

    If it is a general question / idea of NNI, you could just make a short summary.

    If it is a operational issue, please fill nni environment and provide as many details as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

    下面是 NNI 学生项目问题表单，我们鼓励您提出问题或分享观点，期待同学们的思维碰撞和灵感火花!（中英文均可）
    如果是概念性问题，您可以进行简单概述；
    如果是操作性问题，您需要尽可能详细地提供 NNI 环境信息。
-->
 
## NNI 学生项目问题概述 / General Question of Student Program

**请简要概述您的问题 / 观点 :**
**Short summary about the question / idea :**

**请提供 NNI 环境信息 :**
**nni Environment :**
- nni version:
- nni mode(local|pai|remote):
- OS:
- python version:
- is conda or virtualenv used?:
- is running in docker?:

## 其他建议 / Other Advice

**是否需要更新文档（是 / 否）:**
**Need to update document ( yes / no ) :**

**其他分享内容 :**
**Anything else we need to know :**

**Log message / 日志信息 :**
 - [nnimanager.log and dispatcher.log](https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-directory) : 

 - [nnictl stdout and stderr](https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout) : 

",Kevin11Kaikai,48342531,open,True,1,2023-10-13T16:53:01+00:00,2023-10-13T16:55:13+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1934740009,5694,NNI v3.0 - NAS search doesn't start at all,"**Describe the issue**:
Using version 3.0, the tutorial HelloNAS doesn't work at all. Once the experiment is launched an error is raised regarding the HTTP connection with the rest server. Also there is a problem with the gpu info collection process, which is spawned multiple times slowing down the pc.


**Environment**:
- NNI version: 3.0
- Training service (local|remote|pai|aml|etc): local
- Client OS: Ubuntu
- Server OS (for remote mode only):
- Python version: 3.9
- PyTorch/TensorFlow version: 1.12
- Is conda/virtualenv/venv used?: yes
- Is running in Docker?:


**Configuration**:
 - Experiment config (remember to remove secrets!):
 - Search space:


**Log message**:
 - [nnimanager.log](https://github.com/microsoft/nni/files/12854536/nnimanager.log)
- [experiment.log](https://github.com/microsoft/nni/files/12854545/experiment.log)


<!--
Where can you find the log files:
LOG: https://github.com/microsoft/nni/blob/master/docs/en_US/Tutorial/HowToDebug.md#experiment-root-director
STDOUT/STDERR: https://nni.readthedocs.io/en/stable/reference/nnictl.html#nnictl-log-stdout
-->


**How to reproduce it?**:
Run the HelloNAS tutorial using NNIv3.0",chachus,45073899,open,True,5,2023-10-10T08:20:32+00:00,2024-01-31T16:07:02+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1933858101,5693,NNI Tuner system parameters for trial is different from nni.get_next_parameter() in every trial in tuner example code,"**Describe the issue**:



**Environment**:
- NNI version: 3.0
- Training service local:
- Client OS: Ubuntu 22.04
- Python version:  3.10
- PyTorch version: 1.11.0
- Is conda/virtualenv/venv used?: No
- Is running in Docker?: Yes


**Configuration**:
 - Experiment config:
 - experiment.config.max_trial_number = 100
 - experiment.config.trial_concurrency = 8
 - Search space: Predefined search space 
    - for each layer, the block can be different type


**Log message**:
- Web GUI:
- 
![image](https://github.com/microsoft/nni/assets/24198258/a1617be7-e7f3-467a-8bb2-315d370b1438)


 - nnictl stdout and stderr:
 - stdout:
```bash
[2023-10-11 23:59:01] INFO (nni.runtime.trial_command_channel.v3/MainThread) Connect to trial command channel http://localhost:9090/env/local-env/AUNKt
[2023-10-11 23:59:01] PRINT {'features': 1024, 'lr': 0.0005644677962695111, 'momentum': 0.9973347380805804}
[2023-10-11 23:59:01] PRINT Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
[2023-10-11 23:59:02] PRINT Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /ssd/data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz
[2023-10-11 23:59:05] PRINT Extracting /ssd/data/FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to /ssd/data/FashionMNIST/FashionMNIST/raw
[2023-10-11 23:59:05] PRINT
[2023-10-11 23:59:05] PRINT Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
[2023-10-11 23:59:05] PRINT Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /ssd/data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz
[2023-10-11 23:59:06] PRINT Extracting /ssd/data/FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /ssd/data/FashionMNIST/FashionMNIST/raw
[2023-10-11 23:59:06] PRINT
[2023-10-11 23:59:06] PRINT Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
[2023-10-11 23:59:06] PRINT Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /ssd/data/FashionMNIST/Fashio
```


**How to reproduce it?**: Please run https://github.com/microsoft/nni/blob/v3.0/examples/tutorials/hpo_quickstart_pytorch/main.py",dzk9528,24198258,open,True,0,2023-10-09T21:38:18+00:00,2023-10-12T00:05:18+00:00,,,0,0,0,0,0,0,0
microsoft/nni,1931363621,5692,Bump postcss from 8.4.21 to 8.4.31 in /ts/webui,"Bumps [postcss](https://github.com/postcss/postcss) from 8.4.21 to 8.4.31.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/postcss/postcss/releases"">postcss's releases</a>.</em></p>
<blockquote>
<h2>8.4.31</h2>
<ul>
<li>Fixed <code>\r</code> parsing to fix CVE-2023-44270.</li>
</ul>
<h2>8.4.30</h2>
<ul>
<li>Improved source map performance (by <a href=""https://github.com/romainmenke""><code>@​romainmenke</code></a>).</li>
</ul>
<h2>8.4.29</h2>
<ul>
<li>Fixed <code>Node#source.offset</code> (by <a href=""https://github.com/idoros""><code>@​idoros</code></a>).</li>
<li>Fixed docs (by <a href=""https://github.com/coliff""><code>@​coliff</code></a>).</li>
</ul>
<h2>8.4.28</h2>
<ul>
<li>Fixed <code>Root.source.end</code> for better source map (by <a href=""https://github.com/romainmenke""><code>@​romainmenke</code></a>).</li>
<li>Fixed <code>Result.root</code> types when <code>process()</code> has no parser.</li>
</ul>
<h2>8.4.27</h2>
<ul>
<li>Fixed <code>Container</code> clone methods types.</li>
</ul>
<h2>8.4.26</h2>
<ul>
<li>Fixed clone methods types.</li>
</ul>
<h2>8.4.25</h2>
<ul>
<li>Improve stringify performance (by <a href=""https://github.com/romainmenke""><code>@​romainmenke</code></a>).</li>
<li>Fixed docs (by <a href=""https://github.com/vikaskaliramna07""><code>@​vikaskaliramna07</code></a>).</li>
</ul>
<h2>8.4.24</h2>
<ul>
<li>Fixed <code>Plugin</code> types.</li>
</ul>
<h2>8.4.23</h2>
<ul>
<li>Fixed warnings in TypeDoc.</li>
</ul>
<h2>8.4.22</h2>
<ul>
<li>Fixed TypeScript support with <code>node16</code> (by <a href=""https://github.com/remcohaszing""><code>@​remcohaszing</code></a>).</li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/postcss/postcss/blob/main/CHANGELOG.md"">postcss's changelog</a>.</em></p>
<blockquote>
<h2>8.4.31</h2>
<ul>
<li>Fixed <code>\r</code> parsing to fix CVE-2023-44270.</li>
</ul>
<h2>8.4.30</h2>
<ul>
<li>Improved source map performance (by Romain Menke).</li>
</ul>
<h2>8.4.29</h2>
<ul>
<li>Fixed <code>Node#source.offset</code> (by Ido Rosenthal).</li>
<li>Fixed docs (by Christian Oliff).</li>
</ul>
<h2>8.4.28</h2>
<ul>
<li>Fixed <code>Root.source.end</code> for better source map (by Romain Menke).</li>
<li>Fixed <code>Result.root</code> types when <code>process()</code> has no parser.</li>
</ul>
<h2>8.4.27</h2>
<ul>
<li>Fixed <code>Container</code> clone methods types.</li>
</ul>
<h2>8.4.26</h2>
<ul>
<li>Fixed clone methods types.</li>
</ul>
<h2>8.4.25</h2>
<ul>
<li>Improve stringify performance (by Romain Menke).</li>
<li>Fixed docs (by <a href=""https://github.com/vikaskaliramna07""><code>@​vikaskaliramna07</code></a>).</li>
</ul>
<h2>8.4.24</h2>
<ul>
<li>Fixed <code>Plugin</code> types.</li>
</ul>
<h2>8.4.23</h2>
<ul>
<li>Fixed warnings in TypeDoc.</li>
</ul>
<h2>8.4.22</h2>
<ul>
<li>Fixed TypeScript support with <code>node16</code> (by Remco Haszing).</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/postcss/postcss/commit/90208de8805dd762596c0028b8637ffbed23e371""><code>90208de</code></a> Release 8.4.31 version</li>
<li><a href=""https://github.com/postcss/postcss/commit/58cc860b4c1707510c9cd1bc1fa30b423a9ad6c5""><code>58cc860</code></a> Fix carrier return parsing</li>
<li><a href=""https://github.com/postcss/postcss/commit/4fff8e4cdc237619df1d73a444c0a8329701c1e2""><code>4fff8e4</code></a> Improve pnpm test output</li>
<li><a href=""https://github.com/postcss/postcss/commit/cd43ed123274a92ebc13a1e8cccf1d65b8198f84""><code>cd43ed1</code></a> Update dependencies</li>
<li><a href=""https://github.com/postcss/postcss/commit/caa916bdcbf66c51321574e2dde112ab13e8b306""><code>caa916b</code></a> Update dependencies</li>
<li><a href=""https://github.com/postcss/postcss/commit/8972f76923e921a3c9655822382039b31b1c8e1a""><code>8972f76</code></a> Typo</li>
<li><a href=""https://github.com/postcss/postcss/commit/11a5286f781d2a637f2c545c5e9cd661055acaab""><code>11a5286</code></a> Typo</li>
<li><a href=""https://github.com/postcss/postcss/commit/45c55017776fc61f7815d1ea8e92d5291ca5d6c8""><code>45c5501</code></a> Release 8.4.30 version</li>
<li><a href=""https://github.com/postcss/postcss/commit/bc3c341f589f9c15f1b56838a33d908374e537e0""><code>bc3c341</code></a> Update linter</li>
<li><a href=""https://github.com/postcss/postcss/commit/b2be58a2eb788d12474ee1335f8ecdb9fa6225aa""><code>b2be58a</code></a> Merge pull request <a href=""https://redirect.github.com/postcss/postcss/issues/1881"">#1881</a> from romainmenke/improve-sourcemap-performance--phil...</li>
<li>Additional commits viewable in <a href=""https://github.com/postcss/postcss/compare/8.4.21...8.4.31"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=postcss&package-manager=npm_and_yarn&previous-version=8.4.21&new-version=8.4.31)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/microsoft/nni/network/alerts).

</details>",dependabot[bot],49699333,open,True,0,2023-10-07T14:01:46+00:00,2023-10-07T14:01:48+00:00,,dependencies;javascript,0,0,0,0,0,0,0
microsoft/nni,1930489851,5691,Bump postcss from 8.4.12 to 8.4.31 in /ts/jupyter_extension,"Bumps [postcss](https://github.com/postcss/postcss) from 8.4.12 to 8.4.31.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/postcss/postcss/releases"">postcss's releases</a>.</em></p>
<blockquote>
<h2>8.4.31</h2>
<ul>
<li>Fixed <code>\r</code> parsing to fix CVE-2023-44270.</li>
</ul>
<h2>8.4.30</h2>
<ul>
<li>Improved source map performance (by <a href=""https://github.com/romainmenke""><code>@​romainmenke</code></a>).</li>
</ul>
<h2>8.4.29</h2>
<ul>
<li>Fixed <code>Node#source.offset</code> (by <a href=""https://github.com/idoros""><code>@​idoros</code></a>).</li>
<li>Fixed docs (by <a href=""https://github.com/coliff""><code>@​coliff</code></a>).</li>
</ul>
<h2>8.4.28</h2>
<ul>
<li>Fixed <code>Root.source.end</code> for better source map (by <a href=""https://github.com/romainmenke""><code>@​romainmenke</code></a>).</li>
<li>Fixed <code>Result.root</code> types when <code>process()</code> has no parser.</li>
</ul>
<h2>8.4.27</h2>
<ul>
<li>Fixed <code>Container</code> clone methods types.</li>
</ul>
<h2>8.4.26</h2>
<ul>
<li>Fixed clone methods types.</li>
</ul>
<h2>8.4.25</h2>
<ul>
<li>Improve stringify performance (by <a href=""https://github.com/romainmenke""><code>@​romainmenke</code></a>).</li>
<li>Fixed docs (by <a href=""https://github.com/vikaskaliramna07""><code>@​vikaskaliramna07</code></a>).</li>
</ul>
<h2>8.4.24</h2>
<ul>
<li>Fixed <code>Plugin</code> types.</li>
</ul>
<h2>8.4.23</h2>
<ul>
<li>Fixed warnings in TypeDoc.</li>
</ul>
<h2>8.4.22</h2>
<ul>
<li>Fixed TypeScript support with <code>node16</code> (by <a href=""https://github.com/remcohaszing""><code>@​remcohaszing</code></a>).</li>
</ul>
<h2>8.4.21</h2>
<ul>
<li>Fixed <code>Input#error</code> types (by <a href=""https://github.com/hudochenkov""><code>@​hudochenkov</code></a>).</li>
</ul>
<h2>8.4.20</h2>
<ul>
<li>Fixed source map generation for childless at-rules like <code>@layer</code>.</li>
</ul>
<h2>8.4.19</h2>
<ul>
<li>Fixed whitespace preserving after AST transformations (by <a href=""https://github.com/romainmenke""><code>@​romainmenke</code></a>).</li>
</ul>
<h2>8.4.18</h2>
<ul>
<li>Fixed an error on <code>absolute: true</code> with empty <code>sourceContent</code> (by <a href=""https://github.com/KingSora""><code>@​KingSora</code></a>).</li>
</ul>
<h2>8.4.17</h2>
<ul>
<li>Fixed <code>Node.before()</code> unexpected behavior (by <a href=""https://github.com/romainmenke""><code>@​romainmenke</code></a>).</li>
<li>Added TOC to docs (by <a href=""https://github.com/muddv""><code>@​muddv</code></a>).</li>
</ul>
<h2>8.4.16</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/postcss/postcss/blob/main/CHANGELOG.md"">postcss's changelog</a>.</em></p>
<blockquote>
<h2>8.4.31</h2>
<ul>
<li>Fixed <code>\r</code> parsing to fix CVE-2023-44270.</li>
</ul>
<h2>8.4.30</h2>
<ul>
<li>Improved source map performance (by Romain Menke).</li>
</ul>
<h2>8.4.29</h2>
<ul>
<li>Fixed <code>Node#source.offset</code> (by Ido Rosenthal).</li>
<li>Fixed docs (by Christian Oliff).</li>
</ul>
<h2>8.4.28</h2>
<ul>
<li>Fixed <code>Root.source.end</code> for better source map (by Romain Menke).</li>
<li>Fixed <code>Result.root</code> types when <code>process()</code> has no parser.</li>
</ul>
<h2>8.4.27</h2>
<ul>
<li>Fixed <code>Container</code> clone methods types.</li>
</ul>
<h2>8.4.26</h2>
<ul>
<li>Fixed clone methods types.</li>
</ul>
<h2>8.4.25</h2>
<ul>
<li>Improve stringify performance (by Romain Menke).</li>
<li>Fixed docs (by <a href=""https://github.com/vikaskaliramna07""><code>@​vikaskaliramna07</code></a>).</li>
</ul>
<h2>8.4.24</h2>
<ul>
<li>Fixed <code>Plugin</code> types.</li>
</ul>
<h2>8.4.23</h2>
<ul>
<li>Fixed warnings in TypeDoc.</li>
</ul>
<h2>8.4.22</h2>
<ul>
<li>Fixed TypeScript support with <code>node16</code> (by Remco Haszing).</li>
</ul>
<h2>8.4.21</h2>
<ul>
<li>Fixed <code>Input#error</code> types (by Aleks Hudochenkov).</li>
</ul>
<h2>8.4.20</h2>
<ul>
<li>Fixed source map generation for childless at-rules like <code>@layer</code>.</li>
</ul>
<h2>8.4.19</h2>
<ul>
<li>Fixed whitespace preserving after AST transformations (by Romain Menke).</li>
</ul>
<h2>8.4.18</h2>
<ul>
<li>Fixed an error on <code>absolute: true</code> with empty <code>sourceContent</code> (by Rene Haas).</li>
</ul>
<h2>8.4.17</h2>
<ul>
<li>Fixed <code>Node.before()</code> unexpected behavior (by Romain Menke).</li>
<li>Added TOC to docs (by Mikhail Dedov).</li>
</ul>
<h2>8.4.16</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/postcss/postcss/commit/90208de8805dd762596c0028b8637ffbed23e371""><code>90208de</code></a> Release 8.4.31 version</li>
<li><a href=""https://github.com/postcss/postcss/commit/58cc860b4c1707510c9cd1bc1fa30b423a9ad6c5""><code>58cc860</code></a> Fix carrier return parsing</li>
<li><a href=""https://github.com/postcss/postcss/commit/4fff8e4cdc237619df1d73a444c0a8329701c1e2""><code>4fff8e4</code></a> Improve pnpm test output</li>
<li><a href=""https://github.com/postcss/postcss/commit/cd43ed123274a92ebc13a1e8cccf1d65b8198f84""><code>cd43ed1</code></a> Update dependencies</li>
<li><a href=""https://github.com/postcss/postcss/commit/caa916bdcbf66c51321574e2dde112ab13e8b306""><code>caa916b</code></a> Update dependencies</li>
<li><a href=""https://github.com/postcss/postcss/commit/8972f76923e921a3c9655822382039b31b1c8e1a""><code>8972f76</code></a> Typo</li>
<li><a href=""https://github.com/postcss/postcss/commit/11a5286f781d2a637f2c545c5e9cd661055acaab""><code>11a5286</code></a> Typo</li>
<li><a href=""https://github.com/postcss/postcss/commit/45c55017776fc61f7815d1ea8e92d5291ca5d6c8""><code>45c5501</code></a> Release 8.4.30 version</li>
<li><a href=""https://github.com/postcss/postcss/commit/bc3c341f589f9c15f1b56838a33d908374e537e0""><code>bc3c341</code></a> Update linter</li>
<li><a href=""https://github.com/postcss/postcss/commit/b2be58a2eb788d12474ee1335f8ecdb9fa6225aa""><code>b2be58a</code></a> Merge pull request <a href=""https://redirect.github.com/postcss/postcss/issues/1881"">#1881</a> from romainmenke/improve-sourcemap-performance--phil...</li>
<li>Additional commits viewable in <a href=""https://github.com/postcss/postcss/compare/8.4.12...8.4.31"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=postcss&package-manager=npm_and_yarn&previous-version=8.4.12&new-version=8.4.31)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/microsoft/nni/network/alerts).

</details>",dependabot[bot],49699333,open,True,0,2023-10-06T15:52:00+00:00,2023-10-06T15:52:02+00:00,,dependencies;javascript,0,0,0,0,0,0,0
