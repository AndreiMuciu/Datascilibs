repo_full_name,issue_id,number,title,body,user_login,user_id,state,locked,comments_count,created_at,updated_at,closed_at,labels,reactions_total,reactions_plus1,reactions_minus1,reactions_laugh,reactions_hooray,reactions_confused,reactions_heart
scikit-learn/scikit-learn,3056929431,31357,CI Use Cython 3.1 for free-threaded build,"Extracted from https://github.com/scikit-learn/scikit-learn/pull/31342.

This uses cython 3.1 rather than cython rc, and scipy from conda-forge rather than PyPI.",lesteve,1680079,closed,False,2,2025-05-12T13:10:54+00:00,2025-05-12T15:35:40+00:00,2025-05-12T14:23:07+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3056104977,31356,Benchmark Function,"### Describe the workflow you want to enable

I would like to define multiple pipelines and compare them against each other on multiple datasets.

### Describe your proposed solution

A single helper function that executes this benchmark fully in parallel. This would allow 

### Describe alternatives you've considered, if relevant

There is an [MLR3 function](https://mlr3.mlr-org.com/reference/benchmark.html) that inspired this issue. 

### Additional context

Reasoning: I'm currently co-teaching a course where students can do the exercises in R using MLR3 or Python using scikit-learn. Doing the exercises in R appears to be less repetitive overall, as for example, there is a simple function for benchmarking. Also, it would require less time to actually wait for the results to finish as one could make more use of parallelism.",mfeurer,5320498,closed,False,1,2025-05-12T08:32:05+00:00,2025-05-12T10:32:47+00:00,2025-05-12T10:32:46+00:00,New Feature;Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3055644093,31355,:lock: :robot: CI Update lock files for main CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-05-12T05:05:18+00:00,2025-05-12T09:25:40+00:00,2025-05-12T09:25:40+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3055640079,31354,:lock: :robot: CI Update lock files for array-api CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-05-12T05:03:10+00:00,2025-05-12T08:36:59+00:00,2025-05-12T08:36:59+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3055639981,31353,:lock: :robot: CI Update lock files for free-threaded CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-05-12T05:03:05+00:00,2025-05-12T08:33:02+00:00,2025-05-12T08:33:02+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3055639339,31352,:lock: :robot: CI Update lock files for scipy-dev CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-05-12T05:02:49+00:00,2025-05-12T09:24:43+00:00,2025-05-12T09:24:43+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3055527748,31351,DOC Fix typo in `DetCurveDisplay` docstring,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
Reference both `visualization.rst` and the section of the user guide explaining det curves, as we have done with other displays (e.g., roc curve)
Also fixes a typo

#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",lucyleeow,23182829,open,False,3,2025-05-12T03:31:21+00:00,2025-05-12T12:09:41+00:00,,Documentation;module:metrics;No Changelog Needed;Quick Review,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3054688147,31350,"SimpleImputer casts `category` into `object` when using ""most_frequent"" strategy","### Describe the bug

The column `dtype` changes from `category` to `object` when I transform it using `SimpleImputer`.

Here is a list of related Issues and PRs that I found while trying to solve this problem:
#29381 
#18860
#17625 
#17526
#17525

If this is truly a bug, I would like to work on a fix.

### Steps/Code to Reproduce

```python
import pandas as pd
from sklearn.impute import SimpleImputer

df = pd.DataFrame(data=['A', 'B', 'C', 'A', pd.NA], columns=['column_1'], dtype='category')

df.info()

imputer = SimpleImputer(missing_values=pd.NA, strategy=""most_frequent"").set_output(transform='pandas')

output = imputer.fit_transform(df)

output.info()
```

### Expected Results

This is the output I expected to see on the terminal
```
> > > df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5 entries, 0 to 4
Data columns (total 1 columns):
 #   Column    Non-Null Count  Dtype   
---  ------    --------------  -----   
 0   column_1  4 non-null      category
dtypes: category(1)
memory usage: 269.0 bytes

>>> output.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5 entries, 0 to 4
Data columns (total 1 columns):
 #   Column    Non-Null Count  Dtype 
---  ------    --------------  ----- 
 0   column_1  5 non-null      category
dtypes: object(1)
memory usage: 172.0+ bytes
```

I expected `output` to keep the same `dtype` as the original `pd.DataFrame`.

### Actual Results

The actual results for when `output.info()` is called is:
```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5 entries, 0 to 4
Data columns (total 1 columns):
 #   Column    Non-Null Count  Dtype 
---  ------    --------------  ----- 
 0   column_1  5 non-null      object
dtypes: object(1)
memory usage: 172.0+ bytes
```
Observe that the `Dtype` for `column_1` is now object instead of category.

### Versions

```shell
System:
    python: 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:46:43) [GCC 11.2.0]
executable: /home/user/miniconda3/envs/prod/bin/python
   machine: Linux-6.8.0-59-lowlatency-x86_64-with-glibc2.39

Python dependencies:
      sklearn: 1.5.2
          pip: 25.0
   setuptools: 75.8.0
        numpy: 2.1.1
        scipy: 1.14.1
       Cython: None
       pandas: 2.2.3
   matplotlib: 3.9.2
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/user/miniconda3/envs/prod/lib/python3.12/site-packages/numpy.libs/libscipy_openblas64_-ff651d7f.so
        version: 0.3.27
threading_layer: pthreads
   architecture: SkylakeX

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/user/miniconda3/envs/prod/lib/python3.12/site-packages/scipy.libs/libscipy_openblas-c128ec02.so
        version: 0.3.27.dev
threading_layer: pthreads
   architecture: SkylakeX

       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libgomp
       filepath: /home/user/miniconda3/envs/prod/lib/python3.12/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None
```",jschubnell,17047494,open,False,3,2025-05-11T03:55:11+00:00,2025-05-12T10:07:50+00:00,,Bug;Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3053902770,31349,Add Multiple Kernel Learning (MKL) for Support Vector Machines (SVM),"### Describe the workflow you want to enable

I propose adding a [Multiple Kernel Learning (MKL)](https://en.wikipedia.org/wiki/Multiple_kernel_learning) module for kernel optimization in kernel-based methods (such as SVM) to scikit-learn. MKL is a more advanced approach compared to GridSearchCV, offering a way to combine multiple kernels into a single, optimal kernel. In the worst case, MKL will behave like GridSearchCV by assigning a weight of 1 to the best kernel, but in the other cases, it will provide a weighted combination of kernels for better generalization.

### Describe your proposed solution

I have already implemented a complete MKL solution for regression, binary and multi-class classification, and clustering (One-Class). This implementation includes the [SimpleMKL algorithm](https://www.jmlr.org/papers/volume9/rakotomamonjy08a/rakotomamonjy08a.pdf), which optimizes the weights of the kernels, as well as the AverageMKL (simply averages the kernels) and SumMKL (simply sums the kernels) algorithms. This implementation is available on a [previously closed pull request](https://github.com/scikit-learn/scikit-learn/pull/31166).

### Describe alternatives you've considered, if relevant

An alternative would be to continue relying on GridSearchCV for kernel selection. However, GridSearchCV is limited to selecting only one kernel and does not consider the possibility of combining multiple kernels, which can result in suboptimal performance. MKL provides a more sophisticated approach by optimizing kernel weights, leading to better performance in many machine learning tasks.",thomasbauer76,33568196,open,False,2,2025-05-10T08:24:28+00:00,2025-05-12T10:30:56+00:00,,New Feature;Needs Decision,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3053663703,31348,"⚠️ CI failed on Wheel builder (last failure: May 10, 2025) ⚠️","**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14941597365)** (May 10, 2025)
",scikit-learn-bot,8699527,closed,False,1,2025-05-10T04:32:09+00:00,2025-05-11T04:38:40+00:00,2025-05-11T04:38:40+00:00,Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3053371745,31347,MAINT: mutual information using upstream KDTree,"* This patch aims to test the waters for reducing community duplication of effort/maintenance with `KDTree`. In particular, a few select use cases of the `sklearn` in-house `KDTree` by the mutual information infrastructure are replaced with upstream `KDTree` from SciPy. The full test suite appears to pass locally.

If there is an appetite for this, we could spend some time on it (https://github.com/scientific-python/summit-2025/issues/31). I've placed some more detailed discussion points below. This may also interest @sturlamolden.

---------------

More detailed Analysis of the Situation

**Potential Advantages of Doing This**:

1. The SciPy version of `KDTree` has been heavily optimized and supports concurrency in many places that the `sklearn` version does not, as far as I can tell.
2. Reduced duplication of effort--community folks who are experts on `KDTree` could focus their efforts on a single (or at least, less) implementation(s). This is the main one I had in mind.

**Considerations, Drawbacks, Points that are not clear**:
1. An obvious question is what ""replacing"" would even mean and how far it would go--we could just replace a few obvious cases to start, like here, but leave the in-house `KDTree` sources and offerings alone for quite some time to see how that goes first, progressively performing replacements in cases where there's an obvious benefit, or at least no loss of performance or functionality.
2. The switchover would require reviewer bandwidth, and `KDTree` code is quite complex. The features offered by both libraries are not identical, and if you have a code base using two types of `KDTree` it may temporarily make things even more complex.
3. (minor) you don't have `asv` benchmarks for your `KDTree` implementation I don't think? We'd want to add that to avoid performance regressions I suspect, and/or to demonstrate improvements where SciPy adds the `workers`/concurrency option.
4. A design challenge is that `sklearn` uses a ""base"" `BinaryTree` that is ""specialized"" to `KDTree` and `BallTree` in an object-oriented style. SciPy doesn't have `BallTree` (should it?)--how would this be coordinated if there was an appetite for upstreaming?
5. There appears to be some `sklearn` activity surrounding 32- and 64-bit ""variations"" or type preservation with the binary tree infrastructure? I'm not so sure how this would fly/work for SciPy.
6. Compatibility with pipelines may require quite a bit more thought when upstreaming.",tylerjereddy,7903078,open,False,2,2025-05-09T23:16:18+00:00,2025-05-12T00:17:54+00:00,,module:feature_selection,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3052578440,31346,BLD Add missing cython generator for a few extensions,"Follow-up of #31212 noticed in #31342.

I found all of them through the following `git grep`. The remaining matches are `custom_target`:
```
❯ git grep '\.pyx' -- **/meson.build | grep -v process | grep -P '\w+\.pyx'
sklearn/_loss/meson.build:  output: '_loss.pyx',
sklearn/_loss/meson.build:  input: '_loss.pyx.tp',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  output: '_datasets_pair.pyx',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  input: '_datasets_pair.pyx.tp',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  output: '_base.pyx',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  input: '_base.pyx.tp',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  output: '_middle_term_computer.pyx',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  input: '_middle_term_computer.pyx.tp',
sklearn/metrics/_pairwise_distances_reduction/meson.build:    output: '_argkmin.pyx',
sklearn/metrics/_pairwise_distances_reduction/meson.build:    input: '_argkmin.pyx.tp',
sklearn/metrics/_pairwise_distances_reduction/meson.build:    output: '_radius_neighbors.pyx',
sklearn/metrics/_pairwise_distances_reduction/meson.build:    input: '_radius_neighbors.pyx.tp',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  output: '_argkmin_classmode.pyx',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  input: '_argkmin_classmode.pyx.tp',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  output: '_radius_neighbors_classmode.pyx',
sklearn/metrics/_pairwise_distances_reduction/meson.build:  input: '_radius_neighbors_classmode.pyx.tp',
sklearn/metrics/meson.build:  output: '_dist_metrics.pyx',
sklearn/metrics/meson.build:  input: '_dist_metrics.pyx.tp',
```

The impact of this is that `cython_args` are not used for these extensions. Probably worth back-porting for 1.7.",lesteve,1680079,closed,False,1,2025-05-09T15:54:41+00:00,2025-05-12T13:22:32+00:00,2025-05-09T19:40:11+00:00,module:cluster;module:neighbors;module:utils;To backport;Quick Review,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3052567363,31345,DOC Add policy to upper bound build deps in maintainers page,"closes #31183 

I only added this to the list of tasks to do for the RC, unless we want to reconsider the upper bounds at each bug-fix release as well ?",jeremiedbb,34657725,open,False,2,2025-05-09T15:49:31+00:00,2025-05-12T09:51:50+00:00,,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3052373340,31344,Add MultiHorizonTimeSeriesSplit for Multi-Horizon Time Series Cross-Validation,"### Describe the workflow you want to enable

The current `TimeSeriesSplit` in scikit-learn supports cross-validation for time series data with a single prediction horizon per split, which limits its use for scenarios requiring forecasts over multiple future steps (e.g., predicting 1, 3, and 5 days ahead). I propose adding a new class, `MultiHorizonTimeSeriesSplit`, to enable cross-validation with multiple prediction horizons in a single split.

This would allow users to:
- Specify a list of horizons (e.g., `[1, 3, 5]`) to generate train-test splits where the test set includes indices for multiple future steps.
- Evaluate time series models for short, medium, and long-term forecasts simultaneously.
- Simplify workflows for applications like demand forecasting, financial modeling, or weather prediction, avoiding manual splitting.

Example usage with daily temperatures:
```
from sklearn.model_selection import MultiHorizonTimeSeriesSplit
import numpy as np

# Daily temperatures for 10 days (in °C)
X = np.array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])
cv = MultiHorizonTimeSeriesSplit(n_splits=2, horizons=[1, 2])
for train_idx, test_idx in cv.split(X):
    print(f""Train indices: {train_idx}, Test indices: {test_idx}"")
```
Expected output:
```
Train indices: [0 1 2 3 4], Test indices: [5 6]
Train indices: [0 1 2 3 4 5 6], Test indices: [7 8]
```

### Describe your proposed solution

I propose implementing a new class, `MultiHorizonTimeSeriesSplit`, inheriting from `TimeSeriesSplit`. The class will:
- Add a `horizons` parameter (list of integers) to specify prediction steps.
- Modify the `split` method to generate test indices for each horizon while preserving temporal order.
- Include input validation to ensure valid horizons and splits.

To ensure the correctness of MultiHorizonTimeSeriesSplit, we will develop unit tests covering various configurations and edge cases. For benchmarking, we will assess the computational efficiency and correctness of the new class compared to manual splitting. We will use synthetic time series to evaluate scalability and measure split generation time and memory usage, running tests on a personal laptop.


### Describe alternatives you've considered, if relevant

_No response_

### Additional context

_No response_",andrelrodriguess,134166739,open,False,2,2025-05-09T14:36:35+00:00,2025-05-09T15:34:29+00:00,,New Feature;Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3052359957,31343,Automatically generate a SBOM file when vendoring `array-api-compat` and `array-api-extra`,"This is an experimental branch to explore a way to adopt [PEP 770](https://peps.python.org/pep-0770/) for our vendored runtime dependencies. The goal is to make software composition analysis (SCA) tools such as [syft](https://github.com/anchore/syft) able to properly report metadata about vendored components for vulnerability detection or licensing compliance concerns.

This involves using the development branch of [vendoring](https://github.com/pradyunsg/vendoring) as documented in the `sklearn/_vendored/README.md` file instead of custom bash scripts. `vendoring` allows ignoring a list of files in the destination folder but not folders. Hence, I decided to move `array-api-extra` and `array-api-compat` to `sklearn/_vendored` instead of `sklearn/externals` to avoid deleting our `sklearn/externals/_scipy` compat folder structure. Note that `sklearn/externals` should better be renamed to `sklearn/_compat` because the code there cannot be automatically managed by a vendoring tool.

Also note that the current state of this PR is not enough to implement PEP 770 for our vendored runtime dependencies, as we would also need to move the generated SBOM file under `.dist-info/sboms` when building the wheel: https://github.com/psf/sboms-for-python-packages/issues/5#issuecomment-2866380466

Note that a SBOM file for vendored build-time injected native runtime dependencies (such as `libgomp` on Linux) will be generated by auditwheel repair once https://github.com/pypa/auditwheel/pull/577 is merged and released. Similar efforts in wheel post-processing tools will be necessary for macOS and Windows.

This PR is loosely related to #28151, in the sense that #28151 might also involve adopting SBOM files, although for another purpose: explicitly recording metadata of build-time dependencies (including compilers) for the sake of making the build bitwise reproducible.

### TODO:

- [ ] Find a way to move or copy the BOM file into the `.dist-info/sboms` folder when building the wheel.
- [ ] Wait for a stable release of `vendoring`? Instruct maintainainers to use install a specific commit hash of `vendoring` if no planned release?",ogrisel,89061,open,False,2,2025-05-09T14:31:55+00:00,2025-05-12T12:39:09+00:00,,module:utils;No Changelog Needed,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3051914207,31342,MNT Mark cython extensions as free-threaded compatible,"Cython 3.1 has been released on May 8 2025.

Following scipy PR https://github.com/scipy/scipy/pull/22658 to use `-Xfreethreading_compatible=True` cython argument if cython >= 3.1

This cleans up the lock-file and install.sh script as well because cython and scipy are available through conda-forge.

I realized I forgot a `cython_gen.process` in #31212 for murmurhash and I was getting a warning about it locally ...

",lesteve,1680079,open,False,3,2025-05-09T11:51:07+00:00,2025-05-12T15:33:32+00:00,,module:utils;free-threading,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3051266634,31341,MNT Use PYTHON_GIL=0 only at test time to avoid interference with conda,This does not seem an issue anymore (probably runners image update?) but this avoids the interference between `PYTHON_GIL=0` and conda as noted in https://github.com/scikit-learn/scikit-learn/pull/31335#issuecomment-2865331291.,lesteve,1680079,closed,False,1,2025-05-09T07:26:40+00:00,2025-05-09T08:10:28+00:00,2025-05-09T08:10:28+00:00,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3051223594,31340,MNT Fix doctest dict value,"Currently scipy-doctest has a bug in dict comparison reported in https://github.com/scipy/scipy_doctest/issues/195 and will be fixed in https://github.com/scipy/scipy_doctest/pull/196.

We may as well fix this doctest now.",lesteve,1680079,closed,False,1,2025-05-09T07:13:14+00:00,2025-05-12T06:50:29+00:00,2025-05-12T03:10:19+00:00,module:model_selection,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3048517015,31339,DOC Remove old section `_fit_and_score_over_thresholds`,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs

As discussed with @glemaitre this part was dropped during implementation, so removing this from the docs.

#### What does this implement/fix? Explain your changes.


#### Any other comments?


",lucyleeow,23182829,closed,False,2,2025-05-08T10:12:48+00:00,2025-05-09T22:11:39+00:00,2025-05-09T19:42:14+00:00,Documentation;module:model_selection;Quick Review,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3048279501,31338,FEA Adds `decision_threshold_curve` function,"#### Reference Issues/PRs
closes  #25639 (supercedes)
closes https://github.com/scikit-learn/scikit-learn/issues/21391

Thought it would be easier to open a new PR and there were no long discussions on the old PR (#25639).

#### What does this implement/fix? Explain your changes.
Adds a function that takes a scoring function, `y_true`, `y_score`, `thresholds` and outputs score per threshold.

The intention is to later add a new display class using this function, allowing us to plot metric score per threshold e.g.

![image](https://github.com/user-attachments/assets/33486b80-5c00-48e3-b7f8-de8d4ee0f514)

Uses the `_CurveScorer` as suggested by @glemaitre . Refactors out a new `_scores_from_predictions` static method, that takes the predictions. The old `_score` takes estimator, calculates `y_score` and passes to the new `_scores_from_prediction`.

Notes:

* `_scores_from_predictions` - name is inspired by the display class methods 'from_predictions' , but happy to change
* it did not make sense to use `from_scorer` because we are not using a `scorer` (which has the signature `callable(estimator, X, y)`) we are using a 'scoring_function` with signature `score_func(y_true, y_pred, **kwargs)`
   * we instantiate `_CurveScorer` directly instead, and then call `_scores_from_predictions` in `decision_threshold_curve`
   * ~decided to make  `_scores_from_predictions` a static method, but I also could have made it a method, and in `decision_threshold_curve` instantiate `_CurveScorer` directly first (not via `from_scorer`). Only went with staticmethod path because I initially didn't register `from_scorer` instantiates differently than directly via `_CurveScorer`. Not 100% on what is best here.~ [realised that method is nicer to avoid having too many params in `decision_threshold_curve` and avoids some lines of code (use self.xx directly, instead of passing self.xx to `_scores_from_predictions`)]



#### Any other comments?
cc @glemaitre 

Lots more to do, but should get implementation right first.

To do:

- [ ] Add tests
- [ ] Add example
- [ ] Review `_decision_threshold.py` module docstring",lucyleeow,23182829,open,False,1,2025-05-08T08:50:50+00:00,2025-05-12T03:26:11+00:00,,module:metrics;module:model_selection,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3047568139,31337,FIX Stop EfficiencyWarnings in DBSCAN,"closes #31030

#### What does this implement/fix? Explain your changes.

As per #31030, DBSCAN consistently triggers efficiency warnings due to explicitly setting the diagonal of the X matrix in fitting neighborhoods. This stems from not sorting the precomputed sparse matrix by row values. Here, we instead update the neighborhoods variable after the initial fitting to avoid this.



#### Any other comments?

It may also be possible to simply add `X = sort_graph_by_row_values(X, warn_when_not_sorted=False)` after the original code's `X.setdiag(X.diagonal())`, but (1) this way seems more efficient and (2) I am not sure if this reordering would potentially affect the data in an undesired manner.",Luis-Varona,166072235,open,False,4,2025-05-08T01:46:13+00:00,2025-05-12T07:51:14+00:00,,module:cluster,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3047099727,31336,MNT Bump version to 1.8.dev0 on main,ref https://github.com/scikit-learn/scikit-learn/pull/31335,jeremiedbb,34657725,closed,False,2,2025-05-07T20:29:06+00:00,2025-05-09T15:29:47+00:00,2025-05-09T15:29:47+00:00,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3047072514,31335,Release 1.7.0rc1,"* [x] Update the sklearn dev0 version in main branch  (https://github.com/scikit-learn/scikit-learn/pull/31336)
* [x] Set the version number in the release branch
* [x] Generate the changelog in the release branch
* [x] Check that the wheels for the release can be built successfully
* [x] Merge the PR with `[cd build]` commit message to upload wheels to the staging repo
* [x] Upload the wheels and source tarball to https://test.pypi.org
* [x] Create tag on the main repo
* [x] Confirm bot detected at https://github.com/conda-forge/scikit-learn-feedstock
      and wait for merge
* [x] Upload the wheels and source tarball to PyPI
* [x] Announce on mailing list and on social media platforms (LinkedIn, Bluesky, etc.)",jeremiedbb,34657725,closed,False,4,2025-05-07T20:14:32+00:00,2025-05-10T11:00:09+00:00,2025-05-09T12:34:56+00:00,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3046228225,31334,"Title: Clarify misleading threshold implication in ""ROC with Cross-Validation"" example","### Describe the issue linked to the documentation

Location of the issue:
The example titled ""Receiver Operating Characteristic (ROC) with cross validation"" [(link)](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html) can lead to misunderstanding regarding decision threshold selection.

🔍 Description of the problem
The example uses RocCurveDisplay.from_estimator() to plot ROC curves for each test fold in cross-validation,

fig, ax = plt.subplots(figsize=(6, 6))
for fold, (train, test) in enumerate(cv.split(X, y)):
    classifier.fit(X[train], y[train])
    viz = RocCurveDisplay.from_estimator(
        classifier,
        X[test],                           the test set is used here instead of train 
        y[test],
        name=f""ROC fold {fold}"",
        alpha=0.3,
        lw=1,
        ax=ax,
        plot_chance_level=(fold == n_splits - 1),
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

**here is no warning or clarification that:**

1)Users should not select thresholds based on predictions from these test folds.

2)Even for ROC visualization, using predictions from training folds (via cross_val_predict) avoids potential bias and better simulates threshold tuning workflows.

Without this guidance, users may mistakenly tune thresholds by inspecting ROC curves on test sets — leading to data leakage and over-optimistic results.

✅ Proposed solution
replace the test set with the train set in this code

fig, ax = plt.subplots(figsize=(6, 6))
for fold, (train, test) in enumerate(cv.split(X, y)):
    classifier.fit(X[train], y[train])
    viz = RocCurveDisplay.from_estimator(
        classifier,
        X[train],                           train set is used here
        y[train],
        name=f""ROC fold {fold}"",
        alpha=0.3,
        lw=1,
        ax=ax,
        plot_chance_level=(fold == n_splits - 1),
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)

add point: 
use predictions from the training folds (e.g., via cross_val_predict) or apply nested cross-validation.
This prevents data leakage and ensures realistic model evaluation.




### Suggest a potential alternative/fix

make changes in this example   [link](https://scikitlearn.org/stable/auto_examples/model_selection/plot_roc_crossval.html)

use predictions from the training folds (e.g., via cross_val_predict) or apply nested cross-validation.
This prevents data leakage and ensures realistic model evaluation.
here is revised code
and also add short docs that for roc we should use the validation train data for the using threshold value. and not the test this will not result in over optimistic model and will not add data leakage

fig, ax = plt.subplots(figsize=(6, 6))
for fold, (train, test) in enumerate(cv.split(X, y)):
    classifier.fit(X[train], y[train])
    viz = RocCurveDisplay.from_estimator(
        classifier,
        X[train],
        y[train],
        name=f""ROC fold {fold}"",
        alpha=0.3,
        lw=1,
        ax=ax,
        plot_chance_level=(fold == n_splits - 1),
    )
    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)
    interp_tpr[0] = 0.0
    tprs.append(interp_tpr)
    aucs.append(viz.roc_auc)",rajgurubhosale,124778651,closed,False,0,2025-05-07T14:46:24+00:00,2025-05-07T15:31:40+00:00,2025-05-07T15:31:39+00:00,Documentation;Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3046150745,31333,MNT Update conda-lock to 3.0.1,"conda-lock 3.0.1 has been released in May 6 2025.

This updates all the lock-files.",lesteve,1680079,closed,False,4,2025-05-07T14:23:33+00:00,2025-05-09T08:06:33+00:00,2025-05-09T07:40:50+00:00,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3045777598,31332,MNT Remove ellipsis from doctests,"Since we are using scipy-doctest #30496, most ellipsis (aka `...`) can be removed from doctests. scipy-doctest uses floating point comparison with a default `rtol=1e-3`.

A few cases where it is not so convenient to remove the ellipsis:
- `print` statements seen in `sklearn/linear_model/_huber.py`
- only showing the first few values of a very long array seen in `doc/modules/partial_dependence.rst`
- brittle doctest see comment below",lesteve,1680079,closed,False,2,2025-05-07T12:18:45+00:00,2025-05-07T16:06:23+00:00,2025-05-07T16:06:23+00:00,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3045627226,31331,MNT remove default behaviour deprecation from class_likelihood_ratios,"#### Reference Issues/PRs
towards #29048
(It's a partial reversal of #29288.)

#### What does this implement/fix? Explain your changes.
This removes the deprecation of the default behaviour in case of a zero division from `class_likelihood_ratios`.

The default behaviour in case of a division by zero was `np.nan` before, as (indirectly) defined by `raise_warning=True`, which was the default before..

CC @adrinjalali, @jeremiedbb, @virchan ",StefanieSenger,91849487,closed,False,5,2025-05-07T11:18:24+00:00,2025-05-08T05:22:31+00:00,2025-05-07T19:55:24+00:00,module:metrics;No Changelog Needed,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3045173189,31330,DOC Improve Contributer guide for writting docstrings,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.

During the Unaite x :probabl. sprint I realized this piece of documentation can be improved to avoid common mistakes.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",ArturoAmorQ,86408019,closed,False,1,2025-05-07T08:36:57+00:00,2025-05-08T11:24:39+00:00,2025-05-08T10:48:49+00:00,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3044721813,31328,"ENH: Add train_validation_test_split function to split data into train, test and validation datasets","## Description
This PR adds a new function `train_validation_test_split` to split arrays or matrices into random train, validation, and test subsets. This is a common requirement in machine learning workflows where a validation set is needed for hyperparameter tuning.

## Changes
- Added `train_validation_test_split` function to `sklearn/model_selection/_split.py`
- Function supports:
  - Relative or absolute sizes for train, validation, and test sets
  - Stratification
  - Shuffling
  - Multiple arrays
  - Proper error handling

## Examples
```python
from sklearn.model_selection import train_validation_test_split

# Split with default parameters (0.6/0.2/0.2 split)
X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(
    X, y, random_state=42
)

# Split with custom sizes
X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(
    X, y, train_size=0.7, val_size=0.15, test_size=0.15
)

# Stratified split
X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(
    X, y, stratify=y
)
```


## Checklist
- [x] I have read the [Contributing Guidelines](https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md)
- [x] I have added tests that prove my fix is effective or that my feature works
- [x] I have added necessary documentation (if applicable)
- [x] I have updated the user guide (if applicable)
- [x] I have added type hints (if applicable)",abhigyan631,101783480,closed,False,2,2025-05-07T05:20:10+00:00,2025-05-07T16:36:24+00:00,2025-05-07T16:36:23+00:00,module:model_selection,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3044670168,31327,"⚠️ CI failed on Wheel builder (last failure: May 07, 2025) ⚠️","**CI failed on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14874735765)** (May 07, 2025)
",scikit-learn-bot,8699527,closed,False,1,2025-05-07T04:44:27+00:00,2025-05-07T10:13:26+00:00,2025-05-07T10:13:26+00:00,Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3044495612,31326,"⚠️ CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: May 07, 2025) ⚠️","**CI failed on [Linux_free_threaded.pylatest_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76323&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd)** (May 07, 2025)
Unable to find junit file. Please see link for details.",scikit-learn-bot,8699527,closed,False,1,2025-05-07T02:33:31+00:00,2025-05-08T08:15:43+00:00,2025-05-08T08:15:43+00:00,Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3044147506,31325,DOC Math/code formatting in docs,"
#### What does this implement/fix? Explain your changes.

Many of the documentation pages were missing the proper formatting to display as inline LaTeX or code. I did not do a comprehensive check, or verify the accuracy of any of the assertions, but I fixed the formatting issues on some of the most important pages. Much more work needs to be done to clean up the documentation. 
",joshhilton,25337478,open,False,1,2025-05-06T22:17:16+00:00,2025-05-12T02:38:43+00:00,,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3043714412,31324,feat: Add support for sample_weights in TargetEncoder,"This PR introduces the ability for TargetEncoder to respect sample_weight during fitting, addressing [#28881](https://github.com/scikit-learn/scikit-learn/issues/28881)",DuarteSJ,106409172,open,False,1,2025-05-06T18:48:33+00:00,2025-05-06T22:01:24+00:00,,module:preprocessing;cython,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3043156255,31323,Add train_validation_test_split for three-way dataset splits,"### Describe the workflow you want to enable

Enable the user to divide the dataset into 3 parts (train, validation and test) instead of only two (train and test) using only one method. This would present a more elegant solution than using the method train_test_split twice.

```python 
from sklearn.model_selection import train_val_test_split
X_train, X_val, X_test, y_train, y_val, y_test = train_validation_test_split(
    X, y,
    train_size=0.6,
    val_size=0.2,
    test_size=0.2,
    random_state=42,
    shuffle=True,
    stratify=y
)
```

### Describe your proposed solution

Add a new method called train_test_validation_split where the dataset is divided into train, validation and test set. The arguments would be the same as the train_test_split method with the additional  val_size, similar to test_size and train_size but for the validation set.

### Describe alternatives you've considered, if relevant

Using train_test_split twice works, but having a dedicated train_validation_test_split function would be cleaner and more concise.

### Additional context

Using a validation set helps avoiding both overfitting aswell as underfitting.",vasco-s-pereira,102482382,closed,False,1,2025-05-06T15:07:03+00:00,2025-05-07T08:56:30+00:00,2025-05-07T08:56:29+00:00,New Feature;Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3042926134,31322,Implement classical MDS,"Fixes #15272. Supersedes #22330.

This PR implements classical MDS, also known as principal coordinates analysis (PCoA) or Torgerson's scaling, see https://en.wikipedia.org/wiki/Multidimensional_scaling#Classical_multidimensional_scaling. As discussed in #22330, it is implemented as new class `ClassicalMDS`.

Simple demonstration:
```Python
import pylab as plt
import numpy as np

from sklearn.datasets import load_iris
from sklearn.manifold import ClassicalMDS, MDS
from sklearn.decomposition import PCA

X, y = load_iris(return_X_y=True)

Z1 = PCA(n_components=2).fit_transform(X)
Z2 = ClassicalMDS(n_components=2, dissimilarity=""euclidean"").fit_transform(X)
Z3 = ClassicalMDS(n_components=2, dissimilarity=""cosine"").fit_transform(X)
Z4 = ClassicalMDS(n_components=2, dissimilarity=""manhattan"").fit_transform(X)

fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(6, 6), layout=""constrained"")

axs.flat[0].scatter(Z1[:,0], Z1[:,1], c=y)
axs.flat[0].set_title(""PCA"")

axs.flat[1].scatter(Z2[:,0], Z2[:,1], c=y)
axs.flat[1].set_title(""Classical MDS, Euclidean dist."")

axs.flat[2].scatter(-Z3[:,0], Z3[:,1], c=y)
axs.flat[2].set_title(""Classical MDS, cosine dist."")

axs.flat[3].scatter(Z4[:,0], Z4[:,1], c=y)
axs.flat[3].set_title(""Classical MDS, Manhattan dist."")
```
![cmds](https://github.com/user-attachments/assets/d521c970-4b69-4339-8b7e-de48383854f4)

<s>Classical MDS is also set as default initialization for metric/non-metric MDS in the `MDS()` class.</s>

<s>For consistency, this PR also adds support for non-Euclidean metrics to the `MDS` class.</s>",dkobak,8970231,open,False,4,2025-05-06T13:50:42+00:00,2025-05-12T08:52:06+00:00,,module:manifold,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3042681449,31321,"DOC Add ""see also"" section to learning_curve docstring","<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Somewhat related to #31302

#### What does this implement/fix? Explain your changes.

This PR adds a cross-reference in the `learning_curve` docstring to `LearningCurveDisplay.from_estimator`

#### Any other comments?
done during the probabl unaite workshop

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",chemousesi,43786170,closed,False,2,2025-05-06T12:32:31+00:00,2025-05-12T12:18:52+00:00,2025-05-12T12:18:52+00:00,Documentation;module:model_selection,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3042625505,31320,DOC add versionadded directive to new method in GPC,"Forgot to note lack of `.. versionadded` in https://github.com/scikit-learn/scikit-learn/pull/22227

This PR adds that.

cc @jeremiedbb for the release.",adrinjalali,1663864,closed,False,1,2025-05-06T12:12:19+00:00,2025-05-06T12:59:42+00:00,2025-05-06T12:57:23+00:00,Documentation;module:gaussian_process,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3042584642,31319,Argument order in haversine_distances for latitude/longitude,"### Describe the issue linked to the documentation

Hello! I frequently use [sklearn.metrics.pairwise.haversine_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.haversine_distances.html) to estimate distances on the globe. The example on the linked page uses a geographic example, but it does not specify whether geographic coordinates are in (latitude, longitude) or (longitude, latitude) form. From context, one can infer that the correct order is (latitude,longitude); however, it would be useful to explicitly state the order.

This is my first issue submission; please let me know if there is something more that might be useful for resolution!

### Suggest a potential alternative/fix

There are two ways that this could be resolved:
1. Include a short note stating that geographic coordinates should be input as (latitude,longitude)
2. Include a comment in the example code describing the coordinate order.",wlush,31411540,closed,False,2,2025-05-06T11:56:26+00:00,2025-05-07T11:39:33+00:00,2025-05-07T11:39:32+00:00,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3042553298,31318,Bug in scikit-learn version 1.6,"### Describe the bug

Hello,

I'm currently working with scikit-learn version 1.6, and I encountered a regression that wasn't present in version 1.4.

The following minimal code computes two features — the cumulative mean of age and weight grouped by id. Each transformation function returns a pandas.Series:



When I run this code with scikit-learn 1.6, I get the following error:

After investigation, I found that the issue occurs because each transformer returns a Series, not a DataFrame. If I update the functions to return DataFrame objects instead, the error disappears.

Interestingly, in scikit-learn 1.4, the same code works correctly even when the functions return Series.


Do you have any explanation for why this changed between version 1.4 and 1.6 ?

Thanks in advance for your help!

### Steps/Code to Reproduce


```python
import pandas as pd
from sklearn.pipeline import FunctionTransformer, FeatureUnion
import numpy as np

def compute_cumulative_mean_age(df: pd.DataFrame) -> pd.Series:
    return (
        df[""age""]
        .astype(float)
        .groupby(df[""id""])
        .expanding()
        .mean()
        .droplevel(level=""id"")
        .reindex(df.index)
        .rename(""cumulative_mean_age"")
    )

def compute_cumulative_mean_weight(df: pd.DataFrame) -> pd.Series:
    return (
        df[""poids""]
        .astype(float)
        .groupby(df[""id""])
        .expanding()
        .mean()
        .droplevel(level=""id"")
        .reindex(df.index)
        .rename(""cumulative_mean_weight"")
    )

def compute_features(df: pd.DataFrame) -> pd.DataFrame:
    feature_union = FeatureUnion(
        [
            (""cumulative_mean_age"", FunctionTransformer(compute_cumulative_mean_age)),
            (""cumulative_mean_weight"", FunctionTransformer(compute_cumulative_mean_weight))
        ]
    ).set_output(transform=""pandas"")

    return feature_union.fit_transform(X=df).astype(float)

def transform(df: pd.DataFrame) -> pd.DataFrame:
    return compute_features(df)

if __name__ == ""__main__"":
    np.random.seed(42)
    df = pd.DataFrame({
        'id': [1, 2, 3, 1, 4, 5, 6, 6, 7, 8],
        'age': np.random.randint(18, 70, size=10),
        'poids': np.random.randint(50, 100, size=10)
    })

    print(transform(df))
```

### Expected Results

```pytb
Traceback (most recent call last):
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\tmp.py"", line 73, in <module>
    print(transform(df=df))
          ^^^^^^^^^^^^^^^^
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\tmp.py"", line 55, in transform
    return compute_features(
           ^^^^^^^^^^^^^^^^^
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\tmp.py"", line 45, in compute_features
    .fit_transform(
     ^^^^^^^^^^^^^^
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\.venv\Lib\site-packages\sklearn\utils\_set_output.py"", line 332, in wrapped
    return _wrap_data_with_container(method, data_to_wrap, X, self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\.venv\Lib\site-packages\sklearn\utils\_set_output.py"", line 307, in _wrap_data_with_container
    return adapter.create_container(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\.venv\Lib\site-packages\sklearn\utils\_set_output.py"", line 135, in create_container
    X_output = _create_pandas_dataframe_from_non_pandas_container(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\.venv\Lib\site-packages\sklearn\utils\fixes.py"", line 428, in _create_pandas_dataframe_from_non_pandas_container
    return pd.DataFrame(X, index=index, copy=copy)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\.venv\Lib\site-packages\pandas\core\frame.py"", line 722, in __init__
    mgr = ndarray_to_mgr(
          ^^^^^^^^^^^^^^^
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\.venv\Lib\site-packages\pandas\core\internals\construction.py"", line 349, in ndarray_to_mgr
    _check_values_indices_shape_match(values, index, columns)
  File ""C:\Users\XXX\PycharmProjects\fraude_detection_pec_audio\.venv\Lib\site-packages\pandas\core\internals\construction.py"", line 420, in _check_values_indices_shape_match
    raise ValueError(f""Shape of passed values is {passed}, indices imply {implied}"")
ValueError: Shape of passed values is (20, 1), indices imply (10, 1)
```

### Actual Results

```pytb
    raise ValueError(f""Shape of passed values is {passed}, indices imply {implied}"")
ValueError: Shape of passed values is (20, 1), indices imply (10, 1)
```

### Versions

```shell
sklearn: 1.6.0
numpy: 1.26.4
pandas: 1.5.3
```",Bilalbrahimi,73801822,open,False,2,2025-05-06T11:44:35+00:00,2025-05-07T10:38:51+00:00,,Bug,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3042146513,31317,TST Added global_random_seed to test functions,"I picked up on the Issue#22827 and added global_random_seed to functions that use RandomState.

All my changes were in the sklearn/ensemble/_hist_gradient_boosting/tests/ directory/
Most of the functions were changed to add the global_random_seed, but some produced errors when changing so I kept them as they were.
All the tests passed after the changes and now the following python files are cleared from the issue :
- sklearn/ensemble/_hist_gradient_boosting/tests/test_binning.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_bitset.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_compare_lightgbm.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_gradient_boosting.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_grower.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_histogram.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_monotonic_contraints.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_predictor.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_splitting.py
- sklearn/ensemble/_hist_gradient_boosting/tests/test_warm_start.py",HakimTaoufik,142173482,open,False,1,2025-05-06T09:26:44+00:00,2025-05-07T15:17:09+00:00,,module:ensemble,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3042128779,31316,Fix do not recommend to increase `max_iter` in `ConvergenceWarning` when not appropriate,"While investigating #31164 I found out that `_check_optimize_result` can sometimes lead to misleading error messages.

This fix should help avoid pointing people in a misleading direction when investigating such convergence problems.",ogrisel,89061,closed,False,2,2025-05-06T09:21:02+00:00,2025-05-12T12:46:13+00:00,2025-05-12T12:46:07+00:00,module:linear_model;module:utils,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3042079715,31315,SGDRegressor is not inheriting from LinearModel,"### Describe the bug

I wanted to rely on the base class [LinearModel](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_base.py#L267) to identify linear models, but I found out that [SGDRegressor](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_stochastic_gradient.py#L1757) (nor any of its sub classes) is not inheriting this class. However, SGDClassifier is (through LinearClassifierMixin).

Is there any reason for [BaseSGDRegressor](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_stochastic_gradient.py#L1383) to not inherit [LinearModel](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/linear_model/_base.py#L267)? Is it because it overloads all of LinearModel's methods?

### Steps/Code to Reproduce

from sklearn.linear_model import SGDRegressor
from sklearn.linear_model._base import LinearModel

issubclass(SGDRegressor, LinearModel)

### Expected Results

from sklearn.linear_model import SGDRegressor
from sklearn.linear_model._base import LinearModel

issubclass(SGDRegressor, LinearModel)
# True

### Actual Results

from sklearn.linear_model import SGDRegressor
from sklearn.linear_model._base import LinearModel

issubclass(SGDRegressor, LinearModel)
# False

### Versions

```shell
System:
    python: 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)]
executable: /usr/local/bin/python3.10
   machine: macOS-14.4.1-arm64-arm-64bit

Python dependencies:
      sklearn: 1.5.0
          pip: 24.2
   setuptools: 74.0.0
        numpy: 1.26.4
        scipy: 1.13.1
       Cython: 3.0.12
       pandas: 1.5.3
   matplotlib: 3.8.4
       joblib: 1.2.0
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/.dylibs/libopenblas64_.0.dylib
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: armv8

       user_api: blas
   internal_api: openblas
    num_threads: 8
         prefix: libopenblas
       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/.dylibs/libopenblas.0.dylib
        version: 0.3.27
threading_layer: pthreads
   architecture: neoversen1

       user_api: openmp
   internal_api: openmp
    num_threads: 8
         prefix: libomp
       filepath: /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/.dylibs/libomp.dylib
        version: None
```",MarcBresson,50196352,open,False,4,2025-05-06T09:05:13+00:00,2025-05-11T05:17:58+00:00,,Enhancement,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040733861,31314,"DOC Add ""See Also"" reference to ValidationCurveDisplay in validation_curve docstring","<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Aims to solve PR https://github.com/scikit-learn/scikit-learn/issues/31302.

#### What does this implement/fix? Explain your changes.
Updated the docstring of validation_curve, added ""See Also"" references to ValidationCurveDisplay.

#### Any other comments?
N/A.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",TheAyos,34219939,open,False,2,2025-05-05T20:19:10+00:00,2025-05-06T16:20:20+00:00,,Documentation;module:feature_extraction;module:model_selection,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040717376,31313,"DOC add reference to ""Visualizations"" in user doc guide from ""PartialDependenceDisplay"" docstring.","<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Addresses #31304

#### What does this implement/fix? Explain your changes.
Refer to ""Visualizations"" in user doc guide from ""PartialDependenceDisplay"" docstring.

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",mounirLbath,100532921,closed,False,1,2025-05-05T20:10:48+00:00,2025-05-07T15:23:11+00:00,2025-05-07T15:23:11+00:00,Documentation;module:inspection,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040641992,31312,DOC Add reference to CalibrationDisplay from calibration_curve's docstring,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
Fixes #31311. See also #31302 
Reference CalibrationDisplay from calibration_curve's docstring in a ""See also section""

<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
Modify Doc of CalibrationDisplay

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",metlouf,107807424,closed,False,1,2025-05-05T19:39:19+00:00,2025-05-06T15:32:01+00:00,2025-05-06T15:31:49+00:00,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040630630,31311,"Reference CalibrationDisplay from calibration_curve's docstring in a ""See also section""","### Describe the issue linked to the documentation

Enrich documentation like proposed in #31302 for calibration_curve's

### Suggest a potential alternative/fix

_No response_",metlouf,107807424,closed,False,3,2025-05-05T19:34:17+00:00,2025-05-06T15:31:50+00:00,2025-05-06T15:31:50+00:00,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040588845,31310,TST use global_random_seed in sklearn/feature_extraction/tests/test_image.py,"Hello :)

<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
This aims to solve the PR https://github.com/scikit-learn/scikit-learn/issues/22827 with regard to the mentioned test file.


#### What does this implement/fix? Explain your changes.
I added some additional global_random_seed's to replace the previously hardcoded random_state's and RandomState()'s.

#### Any other comments?
Running pytest on this test file before and after the edit yields similar results, in a similar runtime.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",TheAyos,34219939,open,False,1,2025-05-05T19:19:41+00:00,2025-05-05T20:08:41+00:00,,module:feature_extraction,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040552743,31309,DOC Update docstring in partial_dependence.py,"### Summary

This PR updates the documentation of the `PartialDependenceDisplay` class in `sklearn/inspection/_plot/partial_dependence.py` to mirror the style of other Display objects by linking both to the general Visualization API guide and to the interpretation section for partial dependence and ICE plots (issue #31304).

### Changes

* Changed the one-line summary to mention both PDP and ICE visualization.
* Added a “For general information…” link to the `:ref:\`Visualization Guide <visualizations>\`\`.
* Added a “For guidance on interpreting…” link to the `:ref:\`Partial Dependence and ICE plots section \<partial\_dependence>\`\`.
* Removed the old “Read more in … and the User Guide” phrasing and replaced with the dual-link format.

### Motivation

Some Display classes in scikit-learn inconsistently point only to their low-level plotting function or only to the Visualizations guide. To give users both “how do I use this API?” and “what does this plot mean?” contexts, we standardize on dual linking—just like `RocCurveDisplay`.
Closes #31304.

### How to test (if relevant)

1. Build the HTML docs (`make html`) and open the Inspection → Partial Dependence page.
2. Confirm that the `PartialDependenceDisplay` class doc now shows two guide links at the top:

   * “Visualization Guide” (general API)
   * “Partial Dependence and ICE plots section” (interpretation)
3. Repeat for the `from_estimator` method doc.
4. Ensure no rendering errors and that links resolve correctly.

### Risks & considerations

* **Rendering**: Verify the added RST references exist and render without broken links.
* **Backwards compatibility**: Documentation only; no API behavior changes.
* **Consistency**: Other Display docs will need similar updates (e.g. ConfusionMatrixDisplay, DetCurveDisplay, PrecisionRecallDisplay).

### Additional information

This is one of several PRs addressing #31304. See related issues/PRs for metric Display classes.
",Azzedde,81826283,open,False,3,2025-05-05T19:04:39+00:00,2025-05-12T12:05:06+00:00,,Documentation;module:inspection,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040523777,31308,DOC Link PrecisionRecallDisplay to visualization and evaluation guides,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
adresses #31304 

#### What does this implement/fix? Explain your changes.
This PR updates the PrecisionRecallDisplay docstring to include a link to the Model Evaluation Guide (precision-recall-f-measure-metrics), in addition to the existing link to the Visualization Guide. This ensures consistency with other display objects like RocCurveDisplay.

#### Any other comments?

part of the probabl x unaite sprint

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",AchrafTasfaout,78175662,closed,False,2,2025-05-05T18:52:54+00:00,2025-05-09T18:13:49+00:00,2025-05-07T15:40:57+00:00,Documentation;module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040506156,31307,DOC Add references to DetCurveDisplay docstring,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Addresses #31304

#### What does this implement/fix? Explain your changes.

Adds cross-references in the DetCurveDisplay docstrings (class, from_estimator, and from_predictions) to the Visualization Guide and the Model Evaluation Guide for improved documentation discoverability.

#### Any other comments?

Done during the unaite x probabl sprint.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Meddhif13,40263854,closed,False,1,2025-05-05T18:44:53+00:00,2025-05-12T08:41:11+00:00,2025-05-12T08:41:10+00:00,Documentation;module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040464250,31306,DOC Link Visualization tools to their respective interpretation,"
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Addresses #31304 

#### What does this implement/fix? Explain your changes.
This PR updates the docstring of the ConfusionMatrixDisplay class to correctly reference the intended documentation.

#### Any other comments?

Done as part of the Unaite x :probabl. sprint.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",ahmedmokeddem,63117505,closed,False,2,2025-05-05T18:26:25+00:00,2025-05-08T09:59:48+00:00,2025-05-08T09:59:48+00:00,Documentation;module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040375540,31305,DOC Add reference to PrecisionRecallDisplay in average_precision_score docstring,"
<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->

Somewhat related to #31302

#### What does this implement/fix? Explain your changes.

Adds reference to PrecisionRecallDisplay in average_precision_score docstring

#### Any other comments?

done during the probabl x unaite workshop

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Superbowy,23294574,closed,False,1,2025-05-05T17:49:31+00:00,2025-05-12T12:14:25+00:00,2025-05-12T12:14:25+00:00,Documentation;module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3040092989,31304,DOC Link Visualization tools to their respective interpretation in the User Guide,"### Describe the issue linked to the documentation

As of today, some of our [Display objects](https://scikit-learn.org/dev/visualizations.html#display-objects) point towards the [`Visualizations`](https://scikit-learn.org/dev/visualizations.html) section of the User Guide, some of them point toward the respective plotted function, some of them do both.

As sometimes users want to know how to interpret the plot and sometimes they want to understand the plot API, we've resorted to linking both, e.g. for the [RocCurveDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.RocCurveDisplay.html) we have:

```
    For general information regarding `scikit-learn` visualization tools, see
    the :ref:`Visualization Guide <visualizations>`.
    For guidance on interpreting these plots, refer to the :ref:`Model
    Evaluation Guide <roc_metrics>`.
```

Contributors willing to address this issue, please fix **one** of the following listed Display Objects **per pull request**.

- [x] [inspection.PartialDependenceDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.inspection.PartialDependenceDisplay.html) points to [`partial-dependence`](https://scikit-learn.org/dev/modules/partial_dependence.html#partial-dependence). It should point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html) as well. #31313

- [x] [metrics.ConfusionMatrixDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html) points only to point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html). It should point to [`confusion-matrix`](https://scikit-learn.org/dev/modules/model_evaluation.html#confusion-matrix) as well. #31306

- [x] [metrics.DetCurveDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.DetCurveDisplay.html) points to [`det-curve`](https://scikit-learn.org/dev/modules/model_evaluation.html#det-curve). It should point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html) as well. #31307


- [x] [metrics.PrecisionRecallDisplay](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html) points only to point [`Visualizations`](https://scikit-learn.org/dev/visualizations.html). It should point to [`precision-recall-f-measure-metrics`](https://scikit-learn.org/dev/modules/model_evaluation.html#precision-recall-f-measure-metrics) as well. #31308

Thanks for your help!

### Suggest a potential alternative/fix

_No response_",ArturoAmorQ,86408019,closed,False,5,2025-05-05T15:51:12+00:00,2025-05-12T08:42:08+00:00,2025-05-12T08:42:07+00:00,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3039885784,31303,MNT Remove pr directives from towncrier fragments,"PR number is automatically added by towncrier, no need to add `:pr:` directive.

See for example https://scikit-learn.org/dev/whats_new/v1.7.html#sklearn-metrics

![image](https://github.com/user-attachments/assets/5a22bd0c-b1e9-4615-860d-66995eedc4e9)

In case this is useful, the fragment instructions are in [`doc/whats_new/upcoming_changes/README.md`](https://github.com/scikit-learn/scikit-learn/blob/main/doc/whats_new/upcoming_changes/README.md)",lesteve,1680079,closed,False,2,2025-05-05T14:32:36+00:00,2025-05-05T16:09:37+00:00,2025-05-05T16:04:17+00:00,Quick Review,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3039570397,31302,"Reference `ValidationCurveDisplay` from `validation_curve`'s docstring in a ""See also section""","### Describe the issue linked to the documentation

The docstring of `validation_curve` should point to the `ValidationCurveDisplay.from_estimator` factory method as a complementary tool that both computes the curves points and display them using matplotlib.

If you want to open a PR for this, please review some ""See also"" sections in other sections using `git grep` or the search feature of your IDE or github code search:

https://github.com/search?q=repo%3Ascikit-learn%2Fscikit-learn+%22See+also%22+language%3APython+path%3A%2F%5Esklearn%5C%2F%2F&type=code
",ogrisel,89061,open,False,1,2025-05-05T12:50:38+00:00,2025-05-05T18:49:21+00:00,,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3039181756,31301,Fix BLAS_Order.RowMajor import and similar in test_cython_blas with Cython 3.1,"Follow-up on #31263.

Fixes #31296, #31284, #31283, #31269.",ogrisel,89061,closed,False,3,2025-05-05T10:00:50+00:00,2025-05-05T12:01:59+00:00,2025-05-05T12:01:39+00:00,module:utils;No Changelog Needed,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3038849569,31300,FIX solve issue with Cython in pylatest build,Investigate https://github.com/scikit-learn/scikit-learn/issues/31284,glemaitre,7454015,closed,False,3,2025-05-05T07:37:59+00:00,2025-05-05T12:41:05+00:00,2025-05-05T12:41:04+00:00,module:utils;cython,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3038615898,31299,:lock: :robot: CI Update lock files for main CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-05-05T05:09:30+00:00,2025-05-05T08:21:20+00:00,2025-05-05T08:21:20+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3038609249,31298,:lock: :robot: CI Update lock files for array-api CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-05-05T05:03:23+00:00,2025-05-05T08:20:00+00:00,2025-05-05T08:20:00+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3038609077,31297,:lock: :robot: CI Update lock files for free-threaded CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-05-05T05:03:13+00:00,2025-05-05T12:55:12+00:00,2025-05-05T12:55:12+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3038608921,31296,:lock: :robot: CI Update lock files for scipy-dev CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,4,2025-05-05T05:03:05+00:00,2025-05-05T15:17:10+00:00,2025-05-05T12:57:27+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3035206250,31295,Use OS-specific cache directories instead of home directory,"Resolves #31267

The get_data_home function now uses standard OS cache directories:
- Linux/Unix: $XDG_CACHE_HOME/scikit-learn (~/.cache/scikit-learn)
- macOS: ~/Library/Caches/scikit-learn
- Windows: %LOCALAPPDATA%/scikit-learn (~/AppData/Local/scikit-learn)

Previously, data was stored in ~/scikit_learn_data by default.
This change follows OS conventions for cache storage and improves
maintainability.

Implemented deprecation protocol and added tests in test_base.py",norgera,95401214,open,False,7,2025-05-02T05:21:24+00:00,2025-05-12T09:32:02+00:00,,module:datasets,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3034962831,31294,MNT Move `entropy` to private function,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
Related to #31282

#### What does this implement/fix? Explain your changes.
`entropy` is not a API documented function (on purpose) so removing reference to it.

#### Any other comments?
cc @thomasjpfan 

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",lucyleeow,23182829,open,False,3,2025-05-02T00:39:04+00:00,2025-05-07T11:37:00+00:00,,Documentation;Needs Decision;No Changelog Needed,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3034578226,31293,Fix internal kwargs leakage in scorers (metrics._scorer),"I believe this could resolve the problem described in [#31288](https://github.com/scikit-learn/scikit-learn/issues/31288). By filtering out internal control parameters like `needs_sample_weight` before passing arguments to user-defined scoring functions, this change prevents unexpected TypeErrors while preserving backward compatibility. The implementation maintains the intended functionality of sample weight handling without exposing internal implementation details to end users. 🛠️",NEREUScode,174478950,closed,False,4,2025-05-01T19:48:49+00:00,2025-05-05T08:33:45+00:00,2025-05-05T08:33:44+00:00,module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3034549209,31292,Fix internal kwargs leakage in scorers (metrics._scorers),"I believe this could resolve the problem described in [#31288](https://github.com/scikit-learn/scikit-learn/issues/31288). By filtering out internal control parameters like `needs_sample_weight` before passing arguments to user-defined scoring functions, this change prevents unexpected TypeErrors while preserving backward compatibility. The implementation maintains the intended functionality of sample weight handling without exposing internal implementation details to end users. 🛠️",NEREUScode,174478950,closed,False,1,2025-05-01T19:30:38+00:00,2025-05-01T19:42:29+00:00,2025-05-01T19:42:29+00:00,module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3034032650,31291,Bump pypa/cibuildwheel from 2.23.2 to 2.23.3 in the actions group,"Bumps the actions group with 1 update: [pypa/cibuildwheel](https://github.com/pypa/cibuildwheel).

Updates `pypa/cibuildwheel` from 2.23.2 to 2.23.3
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/pypa/cibuildwheel/releases"">pypa/cibuildwheel's releases</a>.</em></p>
<blockquote>
<h2>v2.23.3</h2>
<ul>
<li>🛠 Dependency updates, including Python 3.13.3 (<a href=""https://redirect.github.com/pypa/cibuildwheel/issues/2371"">#2371</a>)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/pypa/cibuildwheel/commit/faf86a6ed7efa889faf6996aa23820831055001a""><code>faf86a6</code></a> Bump version: v2.23.3</li>
<li><a href=""https://github.com/pypa/cibuildwheel/commit/4241f37b2c5be7f7ed96214b83f8cfbe1496cc28""><code>4241f37</code></a> [2.x] Update dependencies (<a href=""https://redirect.github.com/pypa/cibuildwheel/issues/2371"">#2371</a>)</li>
<li>See full diff in <a href=""https://github.com/pypa/cibuildwheel/compare/v2.23.2...v2.23.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=pypa/cibuildwheel&package-manager=github_actions&previous-version=2.23.2&new-version=2.23.3)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency
- `@dependabot ignore <dependency name> major version` will close this group update PR and stop Dependabot creating any more for the specific dependency's major version (unless you unignore this specific dependency's major version or upgrade to it yourself)
- `@dependabot ignore <dependency name> minor version` will close this group update PR and stop Dependabot creating any more for the specific dependency's minor version (unless you unignore this specific dependency's minor version or upgrade to it yourself)
- `@dependabot ignore <dependency name>` will close this group update PR and stop Dependabot creating any more for the specific dependency (unless you unignore this specific dependency or upgrade to it yourself)
- `@dependabot unignore <dependency name>` will remove all of the ignore conditions of the specified dependency
- `@dependabot unignore <dependency name> <ignore condition>` will remove the ignore condition of the specified dependency and ignore conditions


</details>",dependabot[bot],49699333,closed,False,1,2025-05-01T14:51:57+00:00,2025-05-05T09:06:58+00:00,2025-05-05T09:06:49+00:00,Build / CI;dependencies,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033955290,31290,`_safe_indexing` triggers `SettingWithCopyWarning` when used with `slice`,"### Describe the bug

Here's something I noticed while looking into https://github.com/scikit-learn/scikit-learn/pull/31127

The test
```
pytest sklearn/utils/tests/test_indexing.py::test_safe_indexing_pandas_no_settingwithcopy_warning
```
checks that a copy is produced, and that no `SettingWithCopyWarning` is produced

Indeed, no copy is raised, but why is using `_safe_indexing` with a slice allowed to not make a copy? Is this intentional?

Based on responses, I can suggest what to do instead in https://github.com/scikit-learn/scikit-learn/pull/31127

(I am a little surprised that this always makes copies, given that a lot of the discussion in https://github.com/scikit-learn/scikit-learn/issues/28341 centered around wanting to avoid copies)

### Steps/Code to Reproduce

```python
import numpy as np

from sklearn.utils import _safe_indexing
import pandas as pd

X = pd.DataFrame({""a"": [1, 2, 3], ""b"": [3, 4, 5]})
subset = _safe_indexing(X, slice(0, 2), axis=0)
subset.iloc[0, 0] = 10
```

### Expected Results

No `SettingWithCopyWarning`

### Actual Results

```
/home/marcogorelli/scikit-learn-dev/t.py:13: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  subset.iloc[0, 0] = 10
```

### Versions

```shell
System:
    python: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]
executable: /home/marcogorelli/scikit-learn-dev/.venv/bin/python
   machine: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.7.dev0
          pip: 24.2
   setuptools: None
        numpy: 2.1.0
        scipy: 1.14.0
       Cython: 3.0.11
       pandas: 2.2.2
   matplotlib: None
       joblib: 1.4.2
threadpoolctl: 3.5.0

Built with OpenMP: True

threadpoolctl info:
       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/marcogorelli/scikit-learn-dev/.venv/lib/python3.11/site-packages/numpy.libs/libscipy_openblas64_-ff651d7f.so
        version: 0.3.27
threading_layer: pthreads
   architecture: SkylakeX

       user_api: blas
   internal_api: openblas
    num_threads: 16
         prefix: libscipy_openblas
       filepath: /home/marcogorelli/scikit-learn-dev/.venv/lib/python3.11/site-packages/scipy.libs/libscipy_openblas-c128ec02.so
        version: 0.3.27.dev
threading_layer: pthreads
   architecture: SkylakeX

       user_api: openmp
   internal_api: openmp
    num_threads: 16
         prefix: libgomp
       filepath: /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0
        version: None
```",MarcoGorelli,33491632,open,False,5,2025-05-01T14:17:02+00:00,2025-05-06T09:27:49+00:00,,Bug,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033888932,31289,FIX ConvergenceWarning in plot_gpr_on_structured_data (#31164),"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Closes #31164

#### What does this implement/fix? Explain your changes.
This PR fixes the `ConvergenceWarning` and subsequent L-BFGS abort in the structured-sequence Gaussian Process example by freezing `baseline_similarity_bounds`, exactly as core tests already do in [test_gpr.py](https://github.com/scikit-learn/scikit-learn/blob/1527b1fe98d129f85f9a3c5cd0358214247d236b/sklearn/gaussian_process/tests/test_gpr.py#L69). 
No API change.

#### Any other comments?
Added a one-word typo correction (“operate”) in the example narrative.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",EngineerDanny,47421661,open,False,1,2025-05-01T13:40:47+00:00,2025-05-09T09:38:30+00:00,,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033368276,31288,`make_scorer(needs_sample_weight=True)` wrongly injects `needs_sample_weight` into the scoring function,"### Describe the bug


When using `make_scorer(..., needs_sample_weight=True)`, the generated scorer unexpectedly passes `needs_sample_weight=True` as a keyword argument to the scoring function itself, leading to `TypeError` unless **kwargs is manually added.


### Steps/Code to Reproduce


Minimal example:
```
import numpy as np
from sklearn.datasets import make_regression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

def weighted_mape(y_true, y_pred, sample_weight=None):
    return np.average(np.abs((y_true - y_pred) / (y_true + 1e-8)), weights=sample_weight)

scoring = make_scorer(weighted_mape, greater_is_better=False, needs_sample_weight=True)

X, y = make_regression(n_samples=100, n_features=5, random_state=0)
weights = np.random.rand(100)

model = GradientBoostingRegressor()
grid = GridSearchCV(model, param_grid={""n_estimators"": [10]}, scoring=scoring, cv=3)
grid.fit(X, y, sample_weight=weights)
```



### Expected Results

**Expected behavior:**

`make_scorer(..., needs_sample_weight=True)` should cause `sample_weight` to be passed during cross-validation scoring.

The scoring function should not receive `needs_sample_weight=True` as a kwarg.




### Actual Results

**Actual behavior:**

The scoring function raises:

>TypeError: weighted_mape() got an unexpected keyword argument 'needs_sample_weight'
unless manually patched with **kwargs.

### Versions

```shell
System:
    python: 3.10.12 (main, Feb  4 2025, 14:57:36) [GCC 11.4.0]
executable: /home/X/XX/pax_env/bin/python
   machine: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35

Python dependencies:
      sklearn: 1.2.2
          pip: 22.0.2
   setuptools: 59.6.0
        numpy: 1.26.0
        scipy: 1.15.2
       Cython: None
       pandas: 1.5.3
   matplotlib: 3.7.1
       joblib: 1.2.0
threadpoolctl: 3.6.0

Built with OpenMP: True

threadpoolctl info:
       user_api: openmp
   internal_api: openmp
    num_threads: 14
         prefix: libgomp
       filepath: /home/X/XX/pax_env/lib/python3.10/site-packages/scikit_learn.libs/libgomp-a34b3233.so.1.0.0
        version: None

       user_api: blas
   internal_api: openblas
    num_threads: 14
         prefix: libopenblas
       filepath: /home/X/XX/pax_env/lib/python3.10/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so
        version: 0.3.23.dev
threading_layer: pthreads
   architecture: Haswell

       user_api: blas
   internal_api: openblas
    num_threads: 14
         prefix: libscipy_openblas
       filepath: /home/X/XX/pax_env/lib/python3.10/site-packages/scipy.libs/libscipy_openblas-68440149.so
        version: 0.3.28
threading_layer: pthreads
   architecture: Haswell
```",seralouk,25902670,closed,False,3,2025-05-01T07:52:49+00:00,2025-05-05T08:33:27+00:00,2025-05-05T08:33:25+00:00,Bug;Needs Triage,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033205865,31287,COSMIT Use `get_namespace_and_device` in `multilabel_confusion_matrix`,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.


#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",lucyleeow,23182829,closed,False,1,2025-05-01T05:30:43+00:00,2025-05-05T12:26:40+00:00,2025-05-05T12:25:46+00:00,module:metrics;Quick Review;Array API,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033203552,31286,Clarification of output array type when metrics accept multiclass/multioutput,"Clarification of how we should handle array output type when a metric outputs several values (i.e. accepts multiclass or multioutput input).

The issue was summarised succinctly in https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2532238196:

> Not sure what should be the output namespace / device in case we output an array, e.g. roc_auc_score with average=None on multiclass problems...

Currently all regression/classification metrics that support array API and multiclass or multioutput, all output an array in the same namespace and device as the input (checked code and manually). Summary of these metrics :

### Regression metrics

Returns array in same namespace/device:
* [explained_variance_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.explained_variance_score.html#sklearn.metrics.explained_variance_score)
* [r2_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) 
* [mean_absolute_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error)
* [mean_absolute_percentage_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html#sklearn.metrics.mean_absolute_percentage_error)
* [mean_pinball_loss](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_pinball_loss.html#sklearn.metrics.mean_pinball_loss)
* [mean_squared_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)
* [mean_squared_log_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.mean_squared_log_error.html#sklearn.metrics.mean_squared_log_error)
* [root_mean_squared_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.root_mean_squared_error.html#sklearn.metrics.root_mean_squared_error)
* [root_mean_squared_log_error](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.root_mean_squared_log_error.html#sklearn.metrics.root_mean_squared_log_error)

### Classification metrics
Returns array in same namespace/device:

* [precision_recall_fscore_support](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support) and family ([f1_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score), [fbeta_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.fbeta_score.html#sklearn.metrics.fbeta_score), [precision_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score), [recall_score](https://scikit-learn.org/dev/modules/generated/sklearn.metrics.recall_score.html#sklearn.metrics.recall_score))

Looking at the metrics code, if we wanted to support a list of scalars, we'd generally have to do extra processing to convert an array (often output of an xp.** function) to a list of scalars.

Once we arrive at a consensus we should update the array API documentation and update the `check_array_api_metric` in tests such that when the output is array/list - we check that the output type etc is correct.

cc @ogrisel @betatim ",lucyleeow,23182829,open,False,9,2025-05-01T05:28:13+00:00,2025-05-07T11:49:13+00:00,,Needs Decision;Array API,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033123720,31285,DOC Fix return type for `d2_tweedie_score`,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs



#### What does this implement/fix? Explain your changes.
AFAICT `d2_tweedie_score` does not accept multi-output regression data, so I don't think it ever returns a ""ndarray of floats"" ?
Note this has been in docstring since initial PR that added this function (#17036)

#### Any other comments?


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",lucyleeow,23182829,closed,False,1,2025-05-01T04:15:13+00:00,2025-05-05T12:30:49+00:00,2025-05-05T12:28:23+00:00,Documentation;Quick Review,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033043552,31284,"⚠️ CI failed on Linux_Nightly.pylatest_pip_scipy_dev (last failure: May 05, 2025) ⚠️","**CI is still failing on [Linux_Nightly.pylatest_pip_scipy_dev](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76198&view=logs&j=dfe99b15-50db-5d7b-b1e9-4105c42527cf)** (May 05, 2025)
- Test Collection Failure",scikit-learn-bot,8699527,closed,False,1,2025-05-01T02:52:32+00:00,2025-05-05T12:54:29+00:00,2025-05-05T12:54:29+00:00,Bug;cython,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033042840,31283,"⚠️ CI failed on Linux_free_threaded.pylatest_free_threaded (last failure: May 05, 2025) ⚠️","**CI is still failing on [Linux_free_threaded.pylatest_free_threaded](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=76198&view=logs&j=c10228e9-6cf7-5c29-593f-d74f893ca1bd)** (May 05, 2025)
- Test Collection Failure",scikit-learn-bot,8699527,closed,False,1,2025-05-01T02:51:48+00:00,2025-05-05T12:55:15+00:00,2025-05-05T12:55:15+00:00,Build / CI;cython,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3033025837,31282,Add API documentation entry for `entropy`,"
#### Reference Issues/PRs
Noticed that `entropy` does not have a API entry in the docs (see link https://scikit-learn.org/dev/modules/array_api.html#metrics)

Added it to `api_reference.py` - hopefully this is all that is required?

Thought about adding it to `metrics` - so you can use it with `sklearn.metrics.entropy` and not `sklearn.metrics.cluster.entropy`, as we have things like ""completeness_score"" (which is not symmetric) under `metrics` but am not sure about this.

#### What does this implement/fix? Explain your changes.


#### Any other comments?



",lucyleeow,23182829,closed,False,2,2025-05-01T02:33:51+00:00,2025-05-02T00:24:54+00:00,2025-05-02T00:24:48+00:00,module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3032172885,31280,DOC: Add link to plot_nnls example,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
Towards #30621.

#### What does this implement/fix? Explain your changes.
This is intended to add a link to the Non-negative least squares example in the LimearRegression API page.
The following example is used: `plot_nnls.py`
This example is linked in the User Guide for Linear Regression, but not anywhere on the API page.
<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",AidenFrank,100732015,closed,False,2,2025-04-30T18:44:57+00:00,2025-05-06T10:02:24+00:00,2025-05-06T10:02:24+00:00,Documentation;module:linear_model,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3031748016,31279,Unbiased MDI-like feature importance measure for random forests,"#### Reference Issues/PRs
Fixes #20059 


#### What does this implement/fix? Explain your changes.
This implements two methods that correct the cardinality bias of the `feature_importances_` attribute of random forest estimators by leveraging out-of-bag (oob) samples.
The first method is derived from [Unbiased Measurement of Feature Importance in Tree-Based Methods, Zhengze Zhou & Giles Hooker](https://arxiv.org/pdf/1903.05179). The corresponding attribute is named `ufi_feature_importances_`.
The second method is derived from [A Debiased MDI Feature Importance Measure for Random Forests, Xiao Li et al.](https://arxiv.org/pdf/1906.10845). The corresponding attribute is named `mdi_oob_feature_importances_`.
The names are temporary, we are still seeking a way of favoring one method over the other (currently investigating whether one of the two reaches asymptotic behavior faster than the other).

These attributes are set by the `fit` method after training, if the parameter `oob_score` is set to `True`. In this case we send the oob samples to a Cython method at tree level that propagates them through the tree and returns the corresponding oob prediction function and feature importance measure.

This new feature importance measure has a similar behavior to regular Mean Decrease Impurity but mixes the in-bag and out-of-bag values of each node instead of using the in-bag impurity. The two proposed method differ in the way they mix in-bag and oob samples.

This PR also includes these two new feature importance measures to the test suite, specifically in test_forest.py. Existing tests are widened to test these two measures and new tests are added to make sure they behave correctly (e.g. they coincide with values given by the code of the cited papers, they recover traditional MDI when used on in-bag samples).

#### Any other comments?
The papers only suggest fixes for trees built with the Gini (classification) and Mean Squared Error (regression) criteria, but we would like the new methods to support the other available criteria in scikit-learn. `log_loss` support was added for classification with the ufi method by generalizing the idea of mixing in-bag and oob samples.

Some CPU and memory profiling was done to ensure that the computational overhead was controlled enough compared to the cost of model fitting for large enough datasets.

Support for sparse matrix input should be added soon.

This work is done in close colaboration with @ogrisel. 

#### TODO:

- [x] Fix the tests related to `oob_score_`
Done in d198f20a24496fef08205ba570c94827d994ff50
- [x] Can the `""mdi_oob""` method be naturally expanded to support `criterion=""log_loss""` as seems to be the case for the `""ufi""` method?
Doesn't seem to be the case, see: https://www.overleaf.com/read/wqsvxqqrffyw#090a1b
- [ ] Add support for sparse input data (scipy sparse matrix and scipy sparse array containers).
- [ ] Add support and tests for `sample_weight`
Support added in f10721e39e1eba919901eab7dac1e6e19b2f3f51. Tests to come.
- [x] Expose the feature for `GradientBoostingClassifier` and `GradientBoostintRegressor` when row-wise (sub)sampling is enabled at training time.
Done in ce5215932ecfb2b31b8752710db9f106b2ceb1b8
- [ ] Shall we expose some public method to allow the user to pass held-out data instead of just computing the importance using OOB samples identified at training time?
",GaetandeCast,115986055,open,False,1,2025-04-30T16:01:48+00:00,2025-05-12T12:30:16+00:00,,module:ensemble;module:tree;cython,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3030592874,31278,DOC Add link to plot_sparse_cov example,"#### Reference Issues
Towards #30621

#### What does this implement / change?
Adds a sphinx-gallery reference to `plot_sparse_cov.py` in the
`GraphicalLassoCV` docstring (and GraphicalLasso for symmetry).  
No runtime code touched.",EngineerDanny,47421661,closed,False,1,2025-04-30T08:50:27+00:00,2025-05-06T10:17:11+00:00,2025-05-06T10:17:11+00:00,Documentation;module:covariance,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3030508124,31277,DOC Fix typos found by codespell,"#### Reference Issues/PRs

#### What does this implement/fix? Explain your changes.

Fix typos and update codespell configuration.

#### Any other comments?",DimitriPapadopoulos,3234522,closed,False,1,2025-04-30T08:16:06+00:00,2025-04-30T14:05:25+00:00,2025-04-30T10:58:42+00:00,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3030403639,31276,MNT Avoid pre-commit failures,"#### Reference Issues/PRs

See also #31047 and #31273.

#### What does this implement/fix? Explain your changes.

* Bump linter versions.
* Fix pre-commit failures, that may or may not result from the above change.

#### Any other comments?

Note that `cython-lint` does not seem to be run outside of `pre-commit`, and the required version is not defined anywhere outside `.pre-commit-config.yaml`.",DimitriPapadopoulos,3234522,closed,False,1,2025-04-30T07:31:12+00:00,2025-04-30T14:05:08+00:00,2025-04-30T10:58:03+00:00,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3030352066,31275,DOC Minor update to CalibratedClassifierCV docstring,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
None.
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
This implementation updates the doc string for CalibratedClassifierCV to remediate an inaccurate version of the doc string. 

#### Any other comments?
Minor change, but important to be clear on default parameters for the given function signature.

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",BPDanek,24256419,closed,False,1,2025-04-30T07:07:50+00:00,2025-04-30T08:37:34+00:00,2025-04-30T08:37:34+00:00,Documentation,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3030193886,31274,Automatically move `y_true` to the same device and namespace as `y_pred` for metrics,"This is closely linked to #28668 but separate enough to warrant it's own issue (https://github.com/scikit-learn/scikit-learn/issues/28668#issuecomment-2814771519). This is mostly a summary of discussions so far. If we are happy with a decision, we can move to updating the documentation.

---

For classification metrics to support array API, there is a problem in the case where `y_pred` is not in the same namespace/device as `y_true`.

`y_pred` is likely to be the output of `predict_proba` or `decision_function` and would be in the same namespace/device as `X` (if we decide in #28668 that ""everything should follow X"").
`y_true` could be an integer array or a numpy array or pandas series (this is pertinent as `y_true` may be string labels)

Motivating use case:

Using e.g., `GridSearchCV` or `cross_validate` with a pipeline that moves `X` to GPU.
Consider a pipeline like below (copied from https://github.com/scikit-learn/scikit-learn/issues/28668#issuecomment-2154958666): 

```python
pipeline = make_pipeline(
   SomeDataFrameAwareFeatureExtractor(),
   MoveFeaturesToPyTorch(device=""cuda""),
   SomeArrayAPICapableClassifier(),
)
```

Pipelines do not ever touch `y` so we are not able to alter `y` within the pipeline.
We would need to pass a metric to `GridSearchCV` or `cross_validate`, which would be passed `y_true` and `y_pred` on different namespace / devices.

Thus the motivation to automatically move `y_true` to the same namespace / device as `y_pred`, in metrics functions.

(Note another example is discussed in https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2531072292)

As it is more likely that `y_pred` is on GPU, `y_true` follow `y_pred` was slightly preferred over `y_pred` follows `y_true`. Computation wise, CPU vs GPU is probably similar for metrics like log-loss, but for metrics that require sorting (e.g., ROC AUC) GPU may be faster? (see https://github.com/scikit-learn/scikit-learn/pull/30439#issuecomment-2532238196 for more discussion on this point)


Question for my own clarification, the main motivation is for usability, so the user does not have to manually convert `y_true` ? Would a helper function to help the user convert `y_true` to the correct namespace/device be interesting?


cc @ogrisel @betatim ",lucyleeow,23182829,open,False,0,2025-04-30T05:41:14+00:00,2025-04-30T10:57:17+00:00,,API;Needs Decision;Array API,1,1,0,0,0,0,0
scikit-learn/scikit-learn,3028524943,31273,MNT Avoid pre-commit failure,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->


#### What does this implement/fix? Explain your changes.
`mypy` in Pre-commit was failing locally:
```
sklearn/cluster/_agglomerative.py:39: error: Module ""sklearn.cluster"" has no attribute ""_hierarchical_fast""
```
 The skip was removed here: https://github.com/scikit-learn/scikit-learn/pull/31226/files#diff-4cd0e4b7b1063f3f70c05f3d299765b1533d922cdc7d209ae86a331e7d668447L39:~:text=import%20_hierarchical_fast%20as-,_hierarchical,-%23%20type%3A%20ignore

#### Any other comments?
cc @glemaitre 


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",DeaMariaLeon,11835246,closed,False,4,2025-04-29T14:14:06+00:00,2025-04-30T07:38:53+00:00,2025-04-29T15:17:25+00:00,module:cluster,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3028135604,31272,MNT Fix the formatting of the what's new entries for 1.7,Multiple what's new entries for 1.7 have wrong formatting and render incorrectly. This PR fixes all of them.,dkobak,8970231,closed,False,1,2025-04-29T12:27:07+00:00,2025-04-29T16:05:32+00:00,2025-04-29T16:05:32+00:00,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3027809598,31271,Unbiased MDI-like feature importance measure for random forests,"#### Reference Issues/PRs
Fixes #20059 


#### What does this implement/fix? Explain your changes.
This implements two methods that correct the cardinality bias of the `feature_importances_` attribute of random forest estimators by leveraging out-of-bag (oob) samples.
The first method is derived from [Unbiased Measurement of Feature Importance in Tree-Based Methods, Zhengze Zhou & Giles Hooker](https://arxiv.org/pdf/1903.05179). The corresponding attribute is named `ufi_feature_importances_`.
The second method is derived from [A Debiased MDI Feature Importance Measure for Random Forests, Xiao Li et al.](https://arxiv.org/pdf/1906.10845). The corresponding attribute is named `mdi_oob_feature_importances_`.
The names are temporary, we are still seeking a way of favoring one method over the other (currently investigating whether one of the two reaches asymptotic behavior faster than the other).

These attributes are set by the `fit` method after training, if the parameter `oob_score` is set to `True`. In this case we send the oob samples to a Cython method at tree level that propagates them through the tree and returns the corresponding oob prediction function and feature importance measure.

This new feature importance measure has a similar behavior to regular Mean Decrease Impurity but mixes the in-bag and out-of-bag values of each node instead of using the in-bag impurity. The two proposed method differ in the way they mix in-bag and oob samples.

This PR also includes these two new feature importance measures to the test suite, specifically in test_forest.py. Existing tests are widened to test these two measures and new tests are added to make sure they behave correctly (e.g. they coincide with values given by the code of the cited papers, they recover traditional MDI when used on in-bag samples).

#### Any other comments?
The papers only suggest fixes for trees built with the Gini (classification) and Mean Squared Error (regression) criteria, but we would like the new methods to support the other available criteria in scikit-learn. `log_loss` support was added for classification with the ufi method by generalizing the idea of mixing in-bag and oob samples.

Some CPU and memory profiling was done to ensure that the computational overhead was controlled enough compared to the cost of model fitting for large enough datasets.

Support for sparse matrix input and for sample weights should be added soon.

Tests on `oob_score_` currently fail, this is under investigation.

This work is done in close colaboration with @ogrisel. 

#### TODO:

- [x] Fix the tests related to `oob_score_`
done in d198f20a24496fef08205ba570c94827d994ff50
- [ ] Can the `""mdi_oob""` method be naturally expanded to support `criterion=""log_loss""` as seems to be the case for the `""ufi""` method?
- [ ] Add support for sparse input data (scipy sparse matrix and scipy sparse array containers).
- [ ] Add support and tests for `sample_weight`
- [ ] Expose the feature for `GradientBoostingClassifier` and `GradientBoostintRegressor` when row-wise (sub)sampling is enabled at training time.
- [ ] Shall we expose some public method to allow the user to pass held-out data instead of just computing the importance using OOB samples identified at training time?
",GaetandeCast,115986055,closed,False,3,2025-04-29T10:22:20+00:00,2025-04-30T16:02:26+00:00,2025-04-30T16:02:25+00:00,module:ensemble;module:tree;cython,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3027201544,31270,Fix default value of average in precision_recall_fscore_support,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
None. This is a very trivial issue

#### What does this implement/fix? Explain your changes.

`precision_recall_fscore_support` has a parameter called `average`, and it is documented to be `binary` by default. However, it is actually coded to be `None`, and this mismatch causes an unexpected behaviour.

#### Any other comments?

The original aim might be the opposite to this commit, leaving it as it was(`None`), and changing the documentation. However, the fact that the original documentation explicitly mentions `'binary'` for that value led me to make this decision

<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Foundsheep,92705171,open,False,8,2025-04-29T06:23:23+00:00,2025-05-12T10:35:03+00:00,,module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3027015904,31269,"⚠️ CI failed on Wheel builder (last failure: May 05, 2025) ⚠️","**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14828681637)** (May 05, 2025)
",scikit-learn-bot,8699527,closed,False,3,2025-04-29T04:32:02+00:00,2025-05-05T12:55:34+00:00,2025-05-05T12:55:34+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3026326526,31268,Enable using 2SD scaling in StandardScaler,"<!--
Thanks for contributing a pull request! Please ensure you have taken a look at
the contribution guidelines: https://github.com/scikit-learn/scikit-learn/blob/main/CONTRIBUTING.md
-->

#### Reference Issues/PRs
<!--
Example: Fixes #1234. See also #3456.
Please use keywords (e.g., Fixes) to create link to the issues or pull requests
you resolved, so that they will automatically be closed when your pull request
is merged. See https://github.com/blog/1506-closing-issues-via-pull-requests
-->
This pull request finishes #10875 and Closes #10864.

#### What does this implement/fix? Explain your changes.

Enables using 2SD scaling in StandardScaler.

#### Any other comments?

From what I've read in [Gelman2008], we scale continuous variables by 2 times their standard deviation so that the interpretation of the model coefficients (weights) will be similar to dichotomic variables.

> For these reasons, we recommend the general practice of scaling numeric inputs by dividing
by two standard deviations, which allows the coefficients to be interpreted in the same way as
with binary inputs

[Gelman2008]: https://sites.stat.columbia.edu/gelman/research/published/standardizing7.pdf


<!--
Please be aware that we are a loose team of volunteers so patience is
necessary; assistance handling other issues is very welcome. We value
all user contributions, no matter how minor they are. If we are slow to
review, either the pull request needs some benchmarking, tinkering,
convincing, etc. or more likely the reviewers are simply busy. In either
case, we ask for your understanding during the review process.
For more information, see our FAQ on this topic:
https://scikit-learn.org/dev/faq.html#why-is-my-pull-request-not-getting-any-attention.

Thanks for contributing!
-->
",Alvaro-Kothe,60141605,open,False,1,2025-04-28T21:52:30+00:00,2025-04-29T17:59:52+00:00,,module:preprocessing,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3026266050,31267,Change the default data directory,"### Describe the workflow you want to enable

It's not a good practice to put files directly into the home directory.

### Describe your proposed solution

A more common way is to put them into the standard cache directories recommended by operating systems:

| OS | Path |
| -- | ---- |
| Linux | `$XDG_CACHE_HOME` (if the env var presents) or `~/.cache` |
| macOS | `~/Library/Caches` |
| Windows | `%LOCALAPPDATA%` (`~/AppData/Local`) |

### Describe alternatives you've considered, if relevant

Put into `~/.cache/scikit-learn` for all operating systems. Though not being standard, it's still better than the home dir.

### Additional context

https://scikit-learn.org/stable/modules/generated/sklearn.datasets.get_data_home.html",balthild,2662758,open,False,2,2025-04-28T21:22:54+00:00,2025-05-03T20:20:52+00:00,,New Feature,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3024693682,31266,Enhance ROC Curve Display Tests for Improved Clarity and Maintainability,"### PR Description:

#### Summary of Changes:
This PR refactors the `data_binary` fixture in the `test_roc_curve_display.py` file. The previous fixture filtered a multiclass dataset (Iris) to create a binary classification task. However, this approach resulted in AUC values consistently reaching 1.0, which does not reflect real-world challenges.

The new fixture utilizes `make_classification` from `sklearn.datasets` to generate a synthetic binary classification dataset with the following characteristics:
- 200 samples and 20 features.
- 5 informative features and 2 redundant features.
- 10% label noise (`flip_y=0.1`) to simulate real-world imperfections in the data.
- Class separation (`class_sep=0.8`) set to avoid perfect separation.

These changes provide a more complex and representative dataset for testing the `roc_curve_display` function and other related metrics, thereby improving the robustness of tests.

#### Reference Issues/PRs:
- Fixes #31243
- See also #30399 (comment)

---

#### For Reviewers:
- This change ensures that the dataset used for testing is more reflective of real-world data, particularly in classification tasks that may involve noise and less clear separation between classes.
",NEREUScode,174478950,closed,False,6,2025-04-28T11:54:35+00:00,2025-04-30T10:55:14+00:00,2025-04-30T09:00:53+00:00,module:metrics;No Changelog Needed;Waiting for Second Reviewer,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3024507315,31265,MNT git ignore application of ruff PGH rules (#31226),"#### Reference Issues/PRs

* #31226

#### What does this implement/fix? Explain your changes.

Ignore these commits:
* b98dc79

#### Any other comments?
",DimitriPapadopoulos,3234522,closed,False,1,2025-04-28T10:38:24+00:00,2025-04-30T09:05:58+00:00,2025-04-30T08:33:34+00:00,,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3024355128,31264,Enhance ROC Curve Display Tests for Improved Clarity and Maintainability,"### PR Description:

#### Summary of Changes:
This PR refactors the `data_binary` fixture in the `test_roc_curve_display.py` file. The previous fixture filtered a multiclass dataset (Iris) to create a binary classification task. However, this approach resulted in AUC values consistently reaching 1.0, which does not reflect real-world challenges.

The new fixture utilizes `make_classification` from `sklearn.datasets` to generate a synthetic binary classification dataset with the following characteristics:
- 200 samples and 20 features.
- 5 informative features and 2 redundant features.
- 10% label noise (`flip_y=0.1`) to simulate real-world imperfections in the data.
- Class separation (`class_sep=0.8`) set to avoid perfect separation.

These changes provide a more complex and representative dataset for testing the `roc_curve_display` function and other related metrics, thereby improving the robustness of tests.

#### Reference Issues/PRs:
- Fixes #31243
- See also #30399 (comment)

---

#### For Reviewers:
- This change ensures that the dataset used for testing is more reflective of real-world data, particularly in classification tasks that may involve noise and less clear separation between classes.
",NEREUScode,174478950,closed,False,2,2025-04-28T09:44:20+00:00,2025-04-28T11:51:04+00:00,2025-04-28T11:51:04+00:00,module:metrics,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3024177717,31263,Use BLAS_Order.ColMajor sklearn/utils/_cython_blas.pyx,"Attempt to fix #31257.

I also had a linter warning when editing this file in my IDE prior to making the changes in this PR.

I wonder why this problem would be free-threading specific though.",ogrisel,89061,closed,False,3,2025-04-28T08:41:26+00:00,2025-04-29T08:02:17+00:00,2025-04-28T15:05:42+00:00,module:utils;cython;Quick Review,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3024128726,31262,Investigate `test_precomputed_nearest_neighbors_filtering[60]` failure on CI,Trying to reproduce #31256 on the CI.,ogrisel,89061,closed,False,4,2025-04-28T08:24:05+00:00,2025-04-30T08:45:00+00:00,2025-04-30T08:45:00+00:00,Build / CI;module:test-suite;No Changelog Needed;Quick Review,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3023721324,31261,:lock: :robot: CI Update lock files for main CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-04-28T05:10:50+00:00,2025-04-28T08:32:53+00:00,2025-04-28T08:32:53+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3023710998,31260,:lock: :robot: CI Update lock files for array-api CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-04-28T05:04:04+00:00,2025-04-28T08:56:15+00:00,2025-04-28T08:56:15+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3023710901,31259,:lock: :robot: CI Update lock files for free-threaded CI build(s) :lock: :robot:,"Update lock files.

### Note
If the CI tasks fail, create a new branch based on this PR and add the required fixes to that branch.",scikit-learn-bot,8699527,closed,False,1,2025-04-28T05:03:59+00:00,2025-04-28T08:33:40+00:00,2025-04-28T08:33:40+00:00,Build / CI,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3023266776,31258,DOC: Add reference to digits classification example in LogisticRegression docstring,"Adds a reference to the digits classification example (plot_digits_classification.py) in the LogisticRegression class docstring, following issue #30621.",yanamis,72974057,closed,False,3,2025-04-27T19:07:00+00:00,2025-04-30T08:22:51+00:00,2025-04-29T10:22:23+00:00,Documentation;module:linear_model,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3022708145,31257,"⚠️ CI failed on Wheel builder (last failure: Apr 28, 2025) ⚠️","**CI is still failing on [Wheel builder](https://github.com/scikit-learn/scikit-learn/actions/runs/14699848568)** (Apr 28, 2025)
",scikit-learn-bot,8699527,closed,False,4,2025-04-27T04:31:01+00:00,2025-04-28T15:05:43+00:00,2025-04-28T15:05:43+00:00,Bug;free-threading,0,0,0,0,0,0,0
scikit-learn/scikit-learn,3021406161,31256,"⚠️ CI failed on Linux_Runs.pylatest_conda_forge_mkl (last failure: Apr 26, 2025) ⚠️","**CI failed on [Linux_Runs.pylatest_conda_forge_mkl](https://dev.azure.com/scikit-learn/scikit-learn/_build/results?buildId=75987&view=logs&j=dde5042c-7464-5d47-9507-31bdd2ee0a3a)** (Apr 26, 2025)
- test_precomputed_nearest_neighbors_filtering[60]",scikit-learn-bot,8699527,closed,False,4,2025-04-26T02:50:37+00:00,2025-04-30T08:45:23+00:00,2025-04-30T08:45:21+00:00,module:test-suite,0,0,0,0,0,0,0
