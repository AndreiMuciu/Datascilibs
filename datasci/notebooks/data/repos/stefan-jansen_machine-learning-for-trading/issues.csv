repo_full_name,issue_id,number,title,body,user_login,user_id,state,locked,comments_count,created_at,updated_at,closed_at,labels,reactions_total,reactions_plus1,reactions_minus1,reactions_laugh,reactions_hooray,reactions_confused,reactions_heart
stefan-jansen/machine-learning-for-trading,2954908322,332,How to Ingesting Data from .csv Files,"My csv file data meets the requirements of the help document
date,open,high,low,close,volume,dividend,split
This is my extension.py code
import pandas as pd
from zipline.data.bundles import register
from zipline.data.bundles.csvdir import csvdir_equities

# Define data time range
start_session = pd.Timestamp('2020-1-1', tz='UTC')
end_session = pd.Timestamp('2025-1-1', tz='UTC')

# Register Bundle
register(
'my-csv-bundle',
csvdir_equities(
['daily'], # Data frequency (must be consistent with the subfolder name)
'C:/Users/86137/.zipline/data/mycsvs/daily',
),
calendar_name='SSE', # Exchange Calendar
start_session=start_session,
end_session=end_session
)
My program runs normally, but I don't have the corresponding bundle",OodiOodiOodi,172836581,open,False,0,2025-03-28T03:17:43+00:00,2025-04-08T10:25:27+00:00,,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2769322551,331,pyfolio demo notebook issue,"I'm getting absolutely different results in 03_pyfolio_demo.ipynb

![pyfolio](https://github.com/user-attachments/assets/edcc855d-e44f-46f6-bf5c-575eac316e8a)

It looks like some changes was done in pyfolio library. I am trying to follow notebooks and before that line everything was inline I also  double checked data and it seems to be ok. I tried different versions of pandas and another libs same result. Please help

My requirements.txt

anyio==4.6.2.post1
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==2.4.1
async-lru==2.0.4
attrs==24.2.0
babel==2.16.0
beautifulsoup4==4.12.3
bleach==6.2.0
blosc2==2.5.1
certifi==2024.8.30
cffi==1.17.1
charset-normalizer==3.4.0
comm==0.2.2
contourpy==1.3.0
cycler==0.12.1
debugpy==1.8.7
decorator==5.1.1
defusedxml==0.7.1
exceptiongroup==1.2.2
executing==2.1.0
fastjsonschema==2.20.0
fonttools==4.54.1
fqdn==1.5.1
h11==0.14.0
h5py==3.12.1
httpcore==1.0.6
httpx==0.27.2
idna==3.10
importlib-metadata==8.5.0
importlib-resources==6.4.5
ipykernel==6.29.5
ipython==8.18.1
isoduration==20.11.0
jedi==0.19.1
jinja2==3.1.4
joblib==1.4.2
json5==0.9.25
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
jupyter-client==8.6.3
jupyter-core==5.7.2
jupyter-events==0.10.0
jupyter-lsp==2.2.5
jupyter-server==2.14.2
jupyter-server-terminals==0.5.3
jupyterlab==4.3.0
jupyterlab-pygments==0.3.0
jupyterlab-server==2.27.3
kiwisolver==1.4.7
lxml==5.3.0
markupsafe==3.0.2
matplotlib==3.9.2
matplotlib-inline==0.1.7
mistune==3.0.2
msgpack==1.1.0
nbclient==0.10.0
nbconvert==7.16.4
nbformat==5.10.4
ndindex==1.9.2
nest-asyncio==1.6.0
notebook-shim==0.2.4
numexpr==2.10.1
numpy==1.26.4
overrides==7.7.0
packaging==24.1
pandas==1.4.4
pandas-datareader==0.10.0
pandocfilters==1.5.1
parso==0.8.4
patsy==1.0.1
pexpect==4.9.0
pillow==11.0.0
platformdirs==4.3.6
prometheus-client==0.21.0
prompt-toolkit==3.0.48
psutil==6.1.0
ptyprocess==0.7.0
pure-eval==0.2.3
py-cpuinfo==9.0.0
pycparser==2.22
pygments==2.18.0
pyparsing==3.2.0
python-dateutil==2.9.0.post0
python-json-logger==2.0.7
pytz==2024.2
pyyaml==6.0.2
pyzmq==26.2.0
referencing==0.35.1
requests==2.32.3
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.20.1
scikit-learn==1.5.2
scipy==1.13.1
seaborn==0.13.2
send2trash==1.8.3
setuptools==75.3.0
six==1.16.0
sniffio==1.3.1
soupsieve==2.6
stack-data==0.6.3
statsmodels==0.14.4
ta-lib==0.4.32
terminado==0.18.1
threadpoolctl==3.5.0
tinycss2==1.4.0
tomli==2.0.2
tornado==6.4.1
tqdm==4.66.6
traitlets==5.14.3
types-python-dateutil==2.9.0.20241003
typing-extensions==4.12.2
tzdata==2024.2
tables==3.7.0
uri-template==1.3.0
urllib3==2.2.3
wcwidth==0.2.13
webcolors==24.8.0
webencodings==0.5.1
websocket-client==1.8.0
zipp==3.20.2
logbook==1.8.0
zipline-reloaded==3.1
pyfolio-reloaded==0.9.4
pyportfolioopt==1.5.6

python version 3.9
Linux [Nix OS]
",Leo777,8978065,closed,False,0,2025-01-05T15:23:09+00:00,2025-01-05T21:11:13+00:00,2025-01-05T21:11:13+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2646888214,330,Error in the last line of code 01_parse_itch_order_flow_messages.ipynb,"**Describe the bug**
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
Cell In[32], line 3
      1 with pd.HDFStore(itch_store) as store:
      2     stocks = store['R'].loc[:, ['stock_locate', 'stock']]
----> 3     trades = store['P'].append(store['Q'].rename(columns={'cross_price': 'price'}), sort=False).merge(stocks)
      5 trades['value'] = trades.shares.mul(trades.price)
      6 trades['value_share'] = trades.value.div(trades.value.sum())

File [/opt/anaconda3/envs/stocks-env/lib/python3.12/site-packages/pandas/io/pytables.py:602](http://localhost:8888/opt/anaconda3/envs/stocks-env/lib/python3.12/site-packages/pandas/io/pytables.py#line=601), in HDFStore.__getitem__(self, key)
    601 def __getitem__(self, key: str):
--> 602     return self.get(key)

File [/opt/anaconda3/envs/stocks-env/lib/python3.12/site-packages/pandas/io/pytables.py:812](http://localhost:8888/opt/anaconda3/envs/stocks-env/lib/python3.12/site-packages/pandas/io/pytables.py#line=811), in HDFStore.get(self, key)
    810 group = self.get_node(key)
    811 if group is None:
--> 812     raise KeyError(f""No object named {key} in the file"")
    813 return self._read_group(group)

KeyError: 'No object named P in the file'

**To Reproduce**
- In case you are not running the current version of the notebook/script as found on GitHub.

Steps to reproduce the behavior:
1. Go to 01_parse_itch_order_flow_messages.ipynb'
2. Scroll down to '6.2. Top Equities by Traded Value'
3. Run the code bellow
4. See error

**Expected behavior**

Must plot 

Add any other context about the problem here.

The error message “No object named ‘P’ in the file” indicates that the key 'P' is not present in the HDF5 file. This key represents a dataset or a group in the file that is expected by your script but does not exist in the file structure.",adanog,96137621,open,False,2,2024-11-10T06:34:22+00:00,2025-03-28T16:43:41+00:00,,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2568902687,329,Update 00_data_prep.ipynb,,JosueAjavon,168900729,open,False,0,2024-10-06T21:09:51+00:00,2024-10-06T21:09:51+00:00,,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2529717510,328,"chapter 02 _ 01 jupyter file , key error : No Object named P in the file .","**Describe the bug**
in the last cell , there is an error : No object found P in the file .

Steps to reproduce the behavior:
1. Go to chapter 2 in jupyter notebook 1
2. Click on run all cells
3. Scroll down to last cell
4. See error : no object named P in the file .

**Environment**
vscode jupyter notebook
 
- OS: ubuntu 24.04 LTS",mahdic200,92747023,closed,False,1,2024-09-17T00:25:03+00:00,2024-09-18T23:23:14+00:00,2024-09-18T23:23:14+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2526878527,327,"For the alpha101 factor, how do you consider the stock's suspension period in your calculation process?","**Describe the bug**
A brief description of the bug and in which notebook/script it lives.

**To Reproduce**
- In case you are not running the current version of the notebook/script as found on GitHub.

Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment**
If you are not using the latest version of the Docker imag:
 
- OS: [e.g. MacOSX, Windows, Ubuntu]
 - Version [e.g. Big Sur, 10, 20.04]

**Additional context**
Add any other context about the problem here.
",berisfu,32442747,closed,False,1,2024-09-15T11:54:54+00:00,2024-09-23T10:57:44+00:00,2024-09-23T10:57:44+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2498712234,326,Zipline Issues. Which serogate to backtest strategies?,"Hi, 

This bug is not specific to this repository. I am facing great issues when installing zipline in order to backtest strategies. I have tried numerous ways (with python 3.5, by downgrading its dependancies, building a conda environement (env_zipline), etc.) but nothing works.

Could you please indicate me which library would be the closest to zipline, in order to use it instead? Backtrader, QuantConnect, Backtesting.py?

Thank you!",gabdesta,60288969,closed,False,2,2024-08-31T07:42:28+00:00,2024-09-23T09:47:31+00:00,2024-09-23T09:47:30+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2467399390,325,Versioning updates,"- Added dockerfile
- Added pyproject.toml, since poetry has better dependency resolution than pip or conda
- Added dockerfile for x86 as well as m1 mac
- Tested on notebooks till ch7, no issues",0xrushi,6279035,open,False,1,2024-08-15T04:53:28+00:00,2024-09-23T11:07:41+00:00,,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2459295970,324,Ch 11-5 LightGBM  Random Forest & Ch 12-5/8 LightGBM and Catboost Prediction Probability Scores,"I have reproduced the notebooks of chapter 11 and partly the notebooks of chapter 12 and I am interested in the probability score of the predictions made:

I would like to calculate the probability score for each lgb - random forest regression prediction. I know, normally you should fit the model via the LGBMClassifier to get the predict_proba variable and hereby the following scores. However, I cannot reproduce this in (for example) the training and test data in notebook ch11-5 random forest return signals. Does anyone know how to implement such probability scores in (for example) the notebook ch11-5 random forest return signals or ch12-5 or ch12-8?",hbtholen,105981849,closed,False,1,2024-08-10T21:30:31+00:00,2024-09-23T09:50:10+00:00,2024-09-23T09:50:09+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2423090080,323,Reconstructed order book data seems incorrect,"fixed
",dylanprins,30610302,closed,False,0,2024-07-22T14:54:58+00:00,2024-07-23T10:39:12+00:00,2024-07-23T10:35:38+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2416488331,322,Chapter 02 : 'UnImplemented' object has no attribute 'description' ,"**Describe the bug**
Everytime I'm trying to access `store['P']` I get the same error : 
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-48-7ec50b54eb29> in <module>
      6     # The append() method is deprecated on new versions of Pandas and no longer works. Use the _append() method.
      7     # The concat method cannot be used because it changes the format of Q, from ""table"" to ""fixed""
----> 8     trades = store['P']._append(store['Q'].rename(columns={'cross_price': 'price'}), sort=False).merge(stocks)
      9     trades = pd.concat([store['P'], store['Q']]).merge(stocks)
     10 

/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/pytables.py in __getitem__(self, key)
    576 
    577     def __getitem__(self, key: str):
--> 578         return self.get(key)
    579 
    580     def __setitem__(self, key: str, value):

/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/pytables.py in get(self, key)
    768             if group is None:
    769                 raise KeyError(f""No object named {key} in the file"")
--> 770             return self._read_group(group)
    771 
    772     def select(

/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/pytables.py in _read_group(self, group)
   1761     def _read_group(self, group: ""Node""):
...
-> 3352         return self.table.description
   3353 
   3354     @property

AttributeError: 'UnImplemented' object has no attribute 'description'
```


**To Reproduce**
I'm running the 'newer' versions with the Impesud Notes, for example this cell :

```python
with pd.HDFStore(itch_store) as store:
    stocks = store['R'].loc[:, ['stock_locate', 'stock']]
    # trades = store['P'].append(store['Q'].rename(columns={'cross_price': 'price'}), sort=False).merge(stocks)
    # Impesud Note:
    # The append() method is deprecated on new versions of Pandas and no longer works. Use the _append() method.
    # The concat method cannot be used because it changes the format of Q, from ""table"" to ""fixed""
    trades = store['P']._append(store['Q'].rename(columns={'cross_price': 'price'}), sort=False).merge(stocks)
    trades = pd.concat([store['P'], store['Q']]).merge(stocks)

trades['value'] = trades.shares.mul(trades.price)
trades['value_share'] = trades.value.div(trades.value.sum())

trade_summary = trades.groupby('stock').value_share.sum().sort_values(ascending=False)
trade_summary.iloc[:50].plot.bar(figsize=(14, 6), color='darkblue', title='Share of Traded Value')

plt.gca().yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))
sns.despine()
plt.tight_layout()
```

Steps to reproduce the behavior:
As seen in the picture, this behavior is systematic when executing some of the first cells of the second chapter of the repo. P is indeed a key to the data frame, but the type isn't recognized because it 'doesn't have a description' ...

<img width=""1316"" alt=""Capture d’écran 2024-07-18 à 14 18 48"" src=""https://github.com/user-attachments/assets/2d60ba24-5c20-4665-8d7f-b8a2f2b21d47"">


And I systematically get this error :



**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment**
If you are not using the latest version of the Docker imag:
 
- OS: MacOSX 
 - Version Sonoma 14.5
",EarltShirt,112868917,closed,False,1,2024-07-18T14:03:18+00:00,2024-09-23T10:57:32+00:00,2024-09-23T10:57:32+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2364145903,321,no data.h5: '../12_gradient_boosting_machines/data.h5',"**Describe the bug**
A brief description of the bug and in which notebook/script it lives.

**To Reproduce**
- In case you are not running the current version of the notebook/script as found on GitHub.

Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment**
If you are not using the latest version of the Docker imag:
 
- OS: [e.g. MacOSX, Windows, Ubuntu]
 - Version [e.g. Big Sur, 10, 20.04]

**Additional context**
Add any other context about the problem here.
",672819398,45328290,closed,False,5,2024-06-20T10:56:03+00:00,2025-01-27T12:29:20+00:00,2024-09-23T09:54:36+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2317013496,320,update,,mtrshirazi21,84249699,closed,False,2,2024-05-25T13:36:43+00:00,2024-08-06T20:25:53+00:00,2024-08-06T20:25:53+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2271952746,319,ML pipeline on orderflow data,"Hey everyone, I've been trying out FutureAnalytica over the last month, specifically on NinjaTrader 8, and it's been a game-changer. This tool is specially made for NT8, and it’s super straightforward to use. It comes with a strategy that really helps you outperform the market. If you’re looking to make more informed, effective trading with realtime learning, and ongoing training of the models.
I would say its the latest tech for ML and trading - basically its ML pipeline on orderflow...I have been looking for such products for long time, if anyone knows other products like this let me know! 

Their strategy is based on this article - http://tesi.luiss.it/27169/1/701851_PECCHIARI_MATTEO.pdf


![image](https://github.com/txu2014/binance_orderflow/assets/5313475/dc1b7502-48ed-4e05-a61c-6a10ac3354f7)

### 
[Learn more](https://futuresanalytica.com/products/polarityati?sca_ref=4815454.jaD0bckx57)
",TheSnowGuru,5313475,closed,False,0,2024-04-30T16:18:06+00:00,2024-09-23T09:55:30+00:00,2024-09-23T09:55:29+00:00,,1,0,0,0,1,0,0
stefan-jansen/machine-learning-for-trading,2103505693,318,Chapter 22_deep_reinforcement_learning Google Colab Python 3.10 03_lunar_lander,"**Describe the bug**
A brief description of the bug and in which notebook/script it lives.
03_lunar_lander_deep_q_learning.ipynb
env = gym.make('LunarLander-v2')
ModuleNotFoundError: No module named 'Box2D'
DependencyNotInstalled: box2D is not installed, run `pip install gym[box2d]`

so... 

create cell
!pip install gym[box2d]
fails to install

**To Reproduce**
- In case you are not running the current version of the notebook/script as found on GitHub.
notebook unmodified, except for command to attempt install 
!pip install gym[box2d]


Steps to reproduce the behavior:
Open notebook in Google Colab
run it
see first error
take corrective action (run pip install gym[box2d])
see second error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.
Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)
Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.23.5)
Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)
Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)
Collecting box2d-py==2.3.5 (from gym[box2d])
  Downloading box2d-py-2.3.5.tar.gz (374 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 374.4/374.4 kB 8.6 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Collecting pygame==2.1.0 (from gym[box2d])
  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.3/18.3 MB 52.5 MB/s eta 0:00:00
Collecting swig==4.* (from gym[box2d])
  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 87.8 MB/s eta 0:00:00
Building wheels for collected packages: box2d-py
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Building wheel for box2d-py (setup.py) ... error
  ERROR: Failed building wheel for box2d-py
  Running setup.py clean for box2d-py
Failed to build box2d-py
ERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects

**Environment**
If you are not using the latest version of the Docker imag:
 Google Colab running python 3.10
pip list as per the attached file
[piplist.txt](https://github.com/stefan-jansen/machine-learning-for-trading/files/14072013/piplist.txt)

- OS: [e.g. MacOSX, Windows, Ubuntu]
 - Version [e.g. Big Sur, 10, 20.04]

**Additional context**
Add any other context about the problem here.
",martin0,2607781,closed,False,1,2024-01-27T11:00:15+00:00,2024-09-23T09:57:12+00:00,2024-09-23T09:57:12+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2103264467,317,Chapter 22_deep_reinforcement_learning Google Colab Python 3.10,"**Describe the bug**
A brief description of the bug and in which notebook/script it lives.
04_q_learning_for_trading
Train Agent
    DDQNAgent.experience_replay()
      q_values[[self.idx, actions]] = targets
      **ValueError: shape mismatch: value array of shape (4096,) could not be broadcast to indexing result of shape (2,4096,3)**

Output is 
(approximately 35 steps)
 10 | 00:00:03 | Agent: -38.6% (-38.6%) | Market:   4.6% (  4.6%) | Wins: 20.0% | eps:  0.960
(approximately 77 steps)
`ValueError                                Traceback (most recent call last)
[<ipython-input-30-bea99005b607>](https://localhost:8080/#) in <cell line: 3>()
     13                                  0.0 if done else 1.0)
     14         if ddqn.train:
---> 15             ddqn.experience_replay()
     16         if done:
     17             break

[<ipython-input-19-a9539b52dba1>](https://localhost:8080/#) in experience_replay(self)
    107 
    108         q_values = self.online_network.predict_on_batch(states)
--> 109         q_values[[self.idx, actions]] = targets
    110 
    111         loss = self.online_network.train_on_batch(x=states, y=q_values)

ValueError: shape mismatch: value array of shape (4096,) could not be broadcast to indexing result of shape (2,4096,3)`

**To Reproduce**
- In case you are not running the current version of the notebook/script as found on GitHub.
NB I previously had a problem creating the TradingEnvironment.observation_space
It complained about being passed pandas Series, instead of numpy ndarray, so I changed code to 
`self.observation_space = spaces.Box(self.data_source.min_values.to_numpy(),
                                            self.data_source.max_values.to_numpy())`

Running on Google Colab (where session restarts wipe out installed talib package)
I am attaching the modified notebook, talib wheel and underlying talib libraries.
I installed the talib package using instructions from another notebook ""Install Ta-lib on Google colab"" (also attached, with modifications to save pieces so a full rebuild is not necessary each new session)
[Copy of Install Ta-lib on Google colab.zip](https://github.com/stefan-jansen/machine-learning-for-trading/files/14071277/Copy.of.Install.Ta-lib.on.Google.colab.zip)

For the extra things, I have a python folder and a data folder in the root of my Google Drive.
[talib_wheel_and_lib_and_config.zip](https://github.com/stefan-jansen/machine-learning-for-trading/files/14071242/talib_wheel_and_lib_and_config.zip)
[04_q_learning_for_trading.zip](https://github.com/stefan-jansen/machine-learning-for-trading/files/14071251/04_q_learning_for_trading.zip)

The python folder also includes the gym environments specifically the one related to this notebook
directory
python/gymenvs/machine_learning_for_trading
should hold the contents of this zip file 
[trading_env.zip](https://github.com/stefan-jansen/machine-learning-for-trading/files/14071337/trading_env.zip)


Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

Create necessary data as per instructions chapter 2 of the book
Run the notebook 

**Expected behavior**
A clear and concise description of what you expected to happen.
Expect the 04_q_learning_from_trading notebook to run as designed by the author (please)

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment**
If you are not using the latest version of the Docker imag:
Google Colab using T4 GPU

 
- OS: [e.g. MacOSX, Windows, Ubuntu]
 - Version [e.g. Big Sur, 10, 20.04]

**Additional context**
Add any other context about the problem here.
Given the mod I had to make for observation_space, here is a runtime value of the observation_space
Box(
[ -0.5186916 -13.186786   -9.157841   -6.9791217  -5.2897873  -1.5290436
  -5.4077215  -0.6155895  -2.762308   -3.9641087], 
[ 0.3321519 11.431712  10.235379   9.135829   8.238228   1.4996951
  5.7050333  5.4152718  2.7126348  2.7631414], 
(10,), float32)

I will continue to troubleshoot the problem, and post further updates I find.
Happy to answer any questions to clarify this post.

Thanks
Martin
",martin0,2607781,closed,False,4,2024-01-27T06:22:16+00:00,2024-01-29T08:12:57+00:00,2024-01-29T08:12:57+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2099198286,316,Install instructions for windows and linux suck - no offence,"At the start of the install document you indicate don't install all the packages but rather install packages for relevant chapter.

Problem 1: at start of each chapter - you don't indicate which packages for the chapter are required - if it needs separate ones per chapter then provide a .yml or .txt file + conda command to install that chapters packages 

Problem 2: you then go on to indicate the packages can all be installed using mt4t-base.txt (Which contradicts the start of your install instructions) - which one is it? As you clearly state don't install all at once.

While your book makes really good sense - the notebooks don't run - each with different package failures.

If possible: its 2024 : Please provide clear install steps for Windows Or Linux environment. (Take out the deprecated stuff and move into separate deprecated file)



Note: Packages do change over time - so maybe create working .yml file that indicates exact versions required. 
i.e. if you setup 1 environment with all working then export the .yml file it should then work on every persons machine for that platform.

i.e. create 1 for osx- export .yml with exact versions of packages needed (after you tested they all run on that platform) - then repeat for  linux and then again for windows.

Apologies if I sound annoyed - Book was an expensive purchase and expected the notebooks to at least run (+ install instructions to work).

Example: Add cell at top of your notebook to install the packages required for that notebook:

import sys
!{sys.executable} -m pip install numpy
!{sys.executable} -m pip install python-binance
!{sys.executable} -m pip install pandas
!{sys.executable} -m pip install statsmodels
!{sys.executable} -m pip install matplotlib",BlockchainPunks,97580331,closed,False,1,2024-01-24T22:10:02+00:00,2024-09-23T10:17:00+00:00,2024-09-23T10:17:00+00:00,,1,1,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2094793899,315,KeyError: 'No object named quandl/wiki/prices in the file',"File [c:\Users\frede\Downloads\trading_env.py:75](file:///C:/Users/frede/Downloads/trading_env.py:75), in DataSource.load_data(self)
     [73](file:///C:/Users/frede/Downloads/trading_env.py:73) idx = pd.IndexSlice
     [74](file:///C:/Users/frede/Downloads/trading_env.py:74) with pd.HDFStore('../data/assets.h5') as store:
---> [75](file:///C:/Users/frede/Downloads/trading_env.py:75)     df = (store['quandl/wiki/prices']
     [76](file:///C:/Users/frede/Downloads/trading_env.py:76)           .loc[idx[:, self.ticker],
     [77](file:///C:/Users/frede/Downloads/trading_env.py:77)                ['adj_close', 'adj_volume', 'adj_low', 'adj_high']]
     [78](file:///C:/Users/frede/Downloads/trading_env.py:78)           .dropna()
     [79](file:///C:/Users/frede/Downloads/trading_env.py:79)           .sort_index())
     [80](file:///C:/Users/frede/Downloads/trading_env.py:80) df.columns = ['close', 'volume', 'low', 'high']
     [81](file:///C:/Users/frede/Downloads/trading_env.py:81) log.info('got data for {}...'.format(self.ticker))

File [f:\miniconda3\envs\ml4t\lib\site-packages\pandas\io\pytables.py:596](file:///F:/miniconda3/envs/ml4t/lib/site-packages/pandas/io/pytables.py:596), in HDFStore.__getitem__(self, key)
    [595](file:///F:/miniconda3/envs/ml4t/lib/site-packages/pandas/io/pytables.py:595) def __getitem__(self, key: str):
--> [596](file:///F:/miniconda3/envs/ml4t/lib/site-packages/pandas/io/pytables.py:596)     return self.get(key)

File [f:\miniconda3\envs\ml4t\lib\site-packages\pandas\io\pytables.py:790](file:///F:/miniconda3/envs/ml4t/lib/site-packages/pandas/io/pytables.py:790), in HDFStore.get(self, key)
    [788](file:///F:/miniconda3/envs/ml4t/lib/site-packages/pandas/io/pytables.py:788) group = self.get_node(key)
    [789](file:///F:/miniconda3/envs/ml4t/lib/site-packages/pandas/io/pytables.py:789) if group is None:
--> [790](file:///F:/miniconda3/envs/ml4t/lib/site-packages/pandas/io/pytables.py:790)     raise KeyError(f""No object named {key} in the file"")
    [791](file:///F:/miniconda3/envs/ml4t/lib/site-packages/pandas/io/pytables.py:791) return self._read_group(group)

KeyError: 'No object named quandl/wiki/prices in the file'

I get this and there seem to be no quandl/wiki/prices in assets.h5. am i missing something or is this just a mistake?",frederikplet,44939848,closed,False,3,2024-01-22T21:35:15+00:00,2024-09-26T18:24:17+00:00,2024-09-23T10:21:41+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2086023661,314,"MultipleTimeSeriesCV, why does it iterate backwards in time?","It looks like the splits are the wrong way around and should be reversed.
The first split will be used first to compute, than the second split should not use information from the first calculation.
This looks like the other way around, please see the plot.

I made a small pice of code for plotting:

    `
      import matplotlib.pyplot as plt
      import numpy as np
      import pandas as pd
    
    # Initialize the MultipleTimeSeriesCV with your parameters
    cv = MultipleTimeSeriesCV(n_splits=5, train_period_length=300, test_period_length=300, lookahead=50)
    
    # Define the colors for different sections of the plot
    train_color = 'blue'  # In-sample data color
    test_color = 'orange'    # Out-of-sample data color
    lookahead_color = 'green'  # Lookahead period color
    
    # Create subplots
    fig, axs = plt.subplots(cv.n_splits, figsize=(10, 15), sharex=True)
    
    # Iterate over each split
    for i, (train_index, test_index) in enumerate(cv.split(data)):
        train = data.iloc[train_index]
        test = data.iloc[test_index]
    
        # Plot training set
        axs[i].plot(train.loc['AAPL'].index, train.loc['AAPL', 'close'], color=train_color, label='Training Set')
        
        # Plot testing set
        axs[i].plot(test.loc['AAPL'].index, test.loc['AAPL', 'close'], color=test_color, label='Testing Set')
    
        # Highlight the lookahead period
        if len(train) > 0 and len(test) > 0:
            lookahead_start_date = train.loc['AAPL'].index[-1]
            lookahead_end_date = test.loc['AAPL'].index[0]
            axs[i].axvspan(lookahead_start_date, lookahead_end_date, color=lookahead_color, alpha=0.3, label='Lookahead Period')
    
        axs[i].legend(loc='best')
        
        # Formatting the plot
        axs[i].set_title(f'Split {i+1}')
        axs[i].set_xlabel('Date')
        axs[i].set_ylabel('Close Price')
    
    plt.tight_layout()
    plt.show()

`

![image](https://github.com/stefan-jansen/machine-learning-for-trading/assets/58643140/e4ca0653-3de9-471e-82a3-3f3693dbb436)
",carstenf,58643140,closed,False,5,2024-01-17T11:49:31+00:00,2024-09-23T15:30:48+00:00,2024-09-23T10:20:15+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2068832128,313,TypeError: download() got multiple values for argument 'start' when using pandas_datareader.data,"**Describe the bug**
I got this error(per title) sometimes when running pandas_datareader.data to download following the 01_pandas_datareader_demo.ipynb


**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment**
![image](https://github.com/stefan-jansen/machine-learning-for-trading/assets/7684883/2e4aad2f-91df-4788-b15e-5cdbfc67fbad)

- OS: [e.g. MacOSX, Windows, Ubuntu]
 - Version [e.g. Big Sur, 11.7.8]
",edward1019,7684883,closed,False,1,2024-01-06T20:55:19+00:00,2024-09-23T10:31:06+00:00,2024-09-23T10:31:05+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2063214720,312,Unable to replicate backtest performance | Chapter 8 02_backtesting_with_zipline,"Running the notebook myself, the backtest performs worse. Note that I've imported the notebook and not made any modifications.

**My run**
![image](https://github.com/stefan-jansen/machine-learning-for-trading/assets/27483525/20d0e3a8-dff8-4c67-94f2-96fd8510f69f)

**From repo**
![image](https://github.com/stefan-jansen/machine-learning-for-trading/assets/27483525/fdbb593d-fb5b-40f6-980c-94c254112635)

**Environment**
Python 3.8
Here's the important dependencies in `requirements.txt` file after running `pip freeze > requirements.txt`
```
TA-Lib==0.4.28
zipline-reloaded==2.1.1
pyfolio-reloaded==0.9.4
```
 
- OS: Windows
- Version: 11",benhsampson,27483525,closed,False,2,2024-01-03T03:58:13+00:00,2024-09-23T10:36:53+00:00,2024-09-23T10:36:53+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2053284375,311,"Twitter data gone? Broken link in ""data/twitter_sentiment.ipynb""","**Describe the bug**
Hi, ""data/twitter_sentiment.ipynb"" is broken. Needs:

'http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip' <- That don't exist

-- mvh.",2f2a,7493314,closed,False,1,2023-12-22T02:30:41+00:00,2024-09-23T10:37:54+00:00,2024-09-23T10:37:54+00:00,,1,0,0,0,0,1,0
stefan-jansen/machine-learning-for-trading,2034510051,310,No object named P in the file,"**Describe the bug**
Hi, unfortunately I saw two errors in the code of chapter 2 and I can't find a solution.

In the file ""01_parse_itch_order_flow_messages.ipynb"" I see:

1.""Cannot serialize the column [primary_market_maker]
because its data contents are not [string] but [integer] object dtype
L"" , processes the binary file and produces the parsed orders stored by message type.

2. ""'No object named P in the file'"", in the last section of the code.

**Screenshots**
![Screenshot 2023-12-10 143240](https://github.com/stefan-jansen/machine-learning-for-trading/assets/1665536/e9b6b5ce-b468-42e9-97e7-1d8263ba035e)


**Environment**
VsCode v1.85
Python v1.8.10
JupytervVsCode extension v2023.11.1003402403
",Impesud,1665536,closed,False,7,2023-12-10T19:53:09+00:00,2024-09-28T12:40:10+00:00,2024-09-28T12:06:53+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,2011232360,309,installation get stuck / mamba env update -n ml4t -f installation/ml4t-base.yml ,"**Describe the bug**
im trying to install the environment and is does not finish and finnaly failed 

**To Reproduce**
MacBook Pro 2.4 GHz 8-Core Intel Core i9
MacOs 13.4 (22F66)
I stalled Miniconda with: Miniconda3-latest-MacOSX-x86_64.pkg
conda install -n base -c conda-forge mamba
not to get libmamba warnings I used -> conda clean --all
then i used for installation:
mamba  env update -n ml4t -f installation/ml4t-base.yml -v

**Expected behavior**
should finish the installation

**Screenshots**
(base) carsten@MacBook-Pro-2 ~ % mamba  env update -n ml4t -f /Users/carsten/Documents/Python/machine-learning-for-trading-main-2/installation/ml4t-base.yml -v
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/conda-forge/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba All targets to download are cached
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/conda-forge/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba All targets to download are cached
info     libmamba Searching index cache file for repo 'https://repo.anaconda.com/pkgs/main/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.8s
pkgs/main/osx-64 (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.8sinfo     libmamba Transfer done for 'pkgs/main/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 200 [https://repo.anaconda.com/pkgs/main/osx-64/repodata.json.zst] 0 bytes
pkgs/main/osx-64 (check zst)                        Checked  0.8s
info     libmamba Searching index cache file for repo 'https://repo.anaconda.com/pkgs/main/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.2s
pkgs/main/noarch (check zst) ━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.2sinfo     libmamba Transfer done for 'pkgs/main/noarch (check zst)'
info     libmamba Transfer finalized, status: 200 [https://repo.anaconda.com/pkgs/main/noarch/repodata.json.zst] 0 bytes
pkgs/main/noarch (check zst)                        Checked  0.3s
info     libmamba Searching index cache file for repo 'https://repo.anaconda.com/pkgs/r/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.2s
pkgs/r/osx-64 (check zst) ━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.2sinfo     libmamba Transfer done for 'pkgs/r/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 200 [https://repo.anaconda.com/pkgs/r/osx-64/repodata.json.zst] 0 bytes
pkgs/r/osx-64 (check zst)                           Checked  0.3s
info     libmamba Searching index cache file for repo 'https://repo.anaconda.com/pkgs/r/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.3s
pkgs/r/noarch (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.3sinfo     libmamba Transfer done for 'pkgs/r/noarch (check zst)'
info     libmamba Transfer finalized, status: 200 [https://repo.anaconda.com/pkgs/r/noarch/repodata.json.zst] 0 bytes
pkgs/r/noarch (check zst)                           Checked  0.4s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/anaconda/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.6s
anaconda/osx-64 (check zst) ━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.6sinfo     libmamba Transfer done for 'anaconda/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 404 [https://conda.anaconda.org/anaconda/osx-64/repodata.json.zst] 0 bytes
anaconda/osx-64 (check zst)                         Checked  0.7s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/anaconda/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.2s
anaconda/noarch (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.2sinfo     libmamba Transfer done for 'anaconda/noarch (check zst)'
info     libmamba Transfer finalized, status: 404 [https://conda.anaconda.org/anaconda/noarch/repodata.json.zst] 0 bytes
anaconda/noarch (check zst)                         Checked  0.3s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/bashtage/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.1s
bashtage/osx-64 (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.1sinfo     libmamba Transfer done for 'bashtage/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/bashtage/osx-64/repodata.json.zst] 0 bytes
bashtage/osx-64 (check zst)                         Checked  0.2s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/bashtage/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.1s
bashtage/noarch (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.1sinfo     libmamba Transfer done for 'bashtage/noarch (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/bashtage/noarch/repodata.json.zst] 0 bytes
bashtage/noarch (check zst)                         Checked  0.2s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/ranaroussi/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.2s
ranaroussi/osx-64 (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.2sinfo     libmamba Transfer done for 'ranaroussi/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/ranaroussi/osx-64/repodata.json.zst] 0 bytes
ranaroussi/osx-64 (check zst)                       Checked  0.2s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/ranaroussi/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.2s
ranaroussi/noarch (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.2sinfo     libmamba Transfer done for 'ranaroussi/noarch (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/ranaroussi/noarch/repodata.json.zst] 0 bytes
ranaroussi/noarch (check zst)                       Checked  0.3s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/powerai/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.1s
powerai/osx-64 (check zst) ━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.1sinfo     libmamba Transfer done for 'powerai/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/powerai/osx-64/repodata.json.zst] 0 bytes
powerai/osx-64 (check zst)                          Checked  0.2s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/powerai/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.4s
powerai/noarch (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.4sinfo     libmamba Transfer done for 'powerai/noarch (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/powerai/noarch/repodata.json.zst] 0 bytes
powerai/noarch (check zst)                          Checked  0.4s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/fastai/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.1s
fastai/osx-64 (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.1sinfo     libmamba Transfer done for 'fastai/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/fastai/osx-64/repodata.json.zst] 0 bytes
fastai/osx-64 (check zst)                           Checked  0.2s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/fastai/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.5s
fastai/noarch (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.5sinfo     libmamba Transfer done for 'fastai/noarch (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/fastai/noarch/repodata.json.zst] 0 bytes
fastai/noarch (check zst)                           Checked  0.6s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/jiayi_anaconda/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.1s
jiayi_anaconda/osx-64 (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.1sinfo     libmamba Transfer done for 'jiayi_anaconda/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/jiayi_anaconda/osx-64/repodata.json.zst] 0 bytes
jiayi_anaconda/osx-64 (check zst)                   Checked  0.2s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/jiayi_anaconda/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.1s
jiayi_anaconda/noarch (check zst) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.1sinfo     libmamba Transfer done for 'jiayi_anaconda/noarch (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/jiayi_anaconda/noarch/repodata.json.zst] 0 bytes
jiayi_anaconda/noarch (check zst)                   Checked  0.2s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/districtdatalabs/osx-64/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.1s
districtdatalabs/osx-64 (check zst) ━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.1sinfo     libmamba Transfer done for 'districtdatalabs/osx-64 (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/districtdatalabs/osx-64/repodata.json.zst] 0 bytes
districtdatalabs/osx-64 (check zst)                 Checked  0.2s
info     libmamba Searching index cache file for repo 'https://conda.anaconda.org/districtdatalabs/noarch/repodata.json'
info     libmamba No valid cache found
info     libmamba Starting to download targets
[+] 0.1s
districtdatalabs/noarch (check zst) ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s Checking  0.1sinfo     libmamba Transfer done for 'districtdatalabs/noarch (check zst)'
info     libmamba Transfer finalized, status: 200 [https://conda.anaconda.org/districtdatalabs/noarch/repodata.json.zst] 0 bytes
districtdatalabs/noarch (check zst)                 Checked  0.2s
info     libmamba Starting to download targets
[+] 1.9s
conda-forge/osx-64 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  10.4MB /  27.5MB @   5.5MB/s  1.9s
conda-forge/noarch ╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.8kB /  12.7MB @ 106.2kB/s  1.9s
pkgs/r/osx-64                                      694.3kB @ 349.1kB/s  2.0s
[+] 2.4s
conda-forge/osx-64 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  13.6MB /  27.5MB @   5.8MB/s  2.4s
conda-forge/noarch ━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 376.8kB /  12.7MB @ 159.8kB/s  2.4s
pkgs/main/noarch                                   697.7kB @ 282.5kB/s  2.5s
[+] 3.6s
conda-forge/osx-64 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━  20.5MB /  27.5MB @   5.7MB/s  3.6s
conda-forge/noarch ━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   1.0MB /  12.7MB @ 291.8kB/s  3.6s
bashtage/osx-64                                      2.4kB @ 668.0 B/s  1.7s
[+] 4.2s
conda-forge/osx-64    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━  23.8MB /  27.5MB @   5.7MB/s  4.2s
conda-forge/noarch    ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   1.3MB /  12.7MB @ 305.8kB/s  4.2s
jiayi_anaconda/osx-64                              123.0 B @  29.0 B/s  0.6s
[+] 5.0s
conda-forge/osx-64      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━  26.3MB /  27.5MB @   5.3MB/s  5.0s
conda-forge/noarch      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   3.5MB /  12.7MB @ 709.5kB/s  5.0s
conda-forge/osx-64                                  27.5MB @   5.4MB/s  5.1s
[+] 5.2s
conda-forge/noarch      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   3.7MB /  12.7MB @ 710.1kB/s  5.2s
pkgs/main/osx-64        ━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 571.8kB /   5.3MB @ 110.3kB/s  5.2s
anaconda/noarch         ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 237.1kB /  ??.?MB @  45.7kB/s  0.1s
ranaroussi/noarch       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s  2.7s
districtdatalabs/noarch                            116.0 B @  22.0 B/s  1.0s
anaconda/noarch                                    462.4kB @  88.0kB/s  0.2s
[+] 5.6s
conda-forge/noarch ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   7.8MB /  12.7MB @   1.4MB/s  5.6s
pkgs/main/osx-64   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 620.9kB /   5.3MB @ 111.3kB/s  5.6s
ranaroussi/noarch                                    1.1kB @ 190.0 B/s  3.2s
[+] 5.7s
conda-forge/noarch ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   8.0MB /  12.7MB @   1.4MB/s  5.7s
pkgs/main/osx-64   ━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 637.3kB /   5.3MB @ 112.0kB/s  5.7s
powerai/osx-64                                     123.0 B @  21.0 B/s  0.5s
[+] 6.0s
conda-forge/noarch ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   8.1MB /  12.7MB @   1.4MB/s  6.0s
pkgs/main/osx-64   ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 653.7kB /   5.3MB @ 109.5kB/s  6.0s
ranaroussi/osx-64                                  123.0 B @  20.0 B/s  0.3s
[+] 6.3s
conda-forge/noarch    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━  11.0MB /  12.7MB @   1.8MB/s  6.3s
pkgs/main/osx-64      ━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 653.7kB /   5.3MB @ 104.2kB/s  6.3s
fastai/noarch                                      569.6kB @  90.2kB/s  1.1s
[+] 6.5s
conda-forge/noarch    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━  11.8MB /  12.7MB @   1.8MB/s  6.5s
pkgs/main/osx-64      ━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 702.8kB /   5.3MB @ 108.3kB/s  6.5s
bashtage/noarch                                    116.0 B @  17.0 B/s  0.2s
[+] 6.6s
conda-forge/noarch      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━  12.3MB /  12.7MB @   1.9MB/s  6.6s
pkgs/main/osx-64        ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 702.8kB /   5.3MB @ 106.6kB/s  6.6s
jiayi_anaconda/noarch                              679.0 B @ 102.0 B/s  0.6s
[+] 6.7s
conda-forge/noarch      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━  12.3MB /  12.7MB @   1.8MB/s  6.7s
pkgs/main/osx-64        ━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 735.6kB /   5.3MB @ 109.7kB/s  6.7s
pkgs/r/noarch           ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s  1.0s
fastai/osx-64           ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s  0.1s
districtdatalabs/osx-64                              2.2kB @ 332.0 B/s  0.2s
conda-forge/noarch                                  12.7MB @   1.9MB/s  6.8s
[+] 6.8s
pkgs/main/osx-64 ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 752.0kB /   5.3MB @ 110.9kB/s  6.8s
pkgs/r/noarch    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   0.0 B /  ??.?MB @  ??.?MB/s  1.1s
powerai/noarch                                       5.1kB @ 746.0 B/s  0.2s
[+] 6.9s
pkgs/main/osx-64 ━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 752.0kB /   5.3MB @ 109.4kB/s  6.9s
pkgs/r/noarch    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   3.1kB /   2.0MB @ 454.0 B/s  1.2s
fastai/osx-64                                        5.1kB @ 733.0 B/s  0.3s
[+] 7.5s
pkgs/main/osx-64 ━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 801.1kB /   5.3MB @ 107.5kB/s  7.5s
pkgs/r/noarch    ━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.7kB /   2.0MB @  14.2kB/s  1.8s
anaconda/osx-64                                      3.0MB @ 401.5kB/s  0.8s
[+] 11.8s
pkgs/main/osx-64 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━   1.2MB /   5.3MB @ 103.3kB/s 11.8s
pkgs/r/noarch    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━━━━━━━━━━━━━━━━   1.9MB /   2.0MB @ 160.7kB/s  6.1spkgs/r/noarch                                        2.0MB @ 172.4kB/s  6.2s
[+] 1m:52.2s
pkgs/main/osx-64 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸━━   5.3MB /   5.3MB @  47.8kB/s 1m:52.2sinfo     libmamba Transfer done for 'pkgs/main/osx-64'
pkgs/main/osx-64                                     5.3MB @  47.6kB/s 1m:52.2s
Channel: conda-forge[osx-64,noarch], platform: osx-64, prio: 10 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/31ce02e0.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/31ce02e0.json.*' for repo index 'conda-forge/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/31ce02e0.json"" for repo conda-forge/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/31ce02e0.solv"" for repo conda-forge/osx-64
Channel: conda-forge[osx-64,noarch], platform: noarch, prio: 10 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/09cdf8bf.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/09cdf8bf.json.*' for repo index 'conda-forge/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/09cdf8bf.json"" for repo conda-forge/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/09cdf8bf.solv"" for repo conda-forge/noarch
Channel: pkgs/main[osx-64,noarch], platform: osx-64, prio: 9 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/7fb2ce72.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/7fb2ce72.json.*' for repo index 'pkgs/main/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/7fb2ce72.json"" for repo pkgs/main/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/7fb2ce72.solv"" for repo pkgs/main/osx-64
Channel: pkgs/main[osx-64,noarch], platform: noarch, prio: 9 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/3e39a7aa.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/3e39a7aa.json.*' for repo index 'pkgs/main/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/3e39a7aa.json"" for repo pkgs/main/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/3e39a7aa.solv"" for repo pkgs/main/noarch
Channel: pkgs/r[osx-64,noarch], platform: osx-64, prio: 8 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/dd44a73b.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/dd44a73b.json.*' for repo index 'pkgs/r/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/dd44a73b.json"" for repo pkgs/r/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/dd44a73b.solv"" for repo pkgs/r/osx-64
Channel: pkgs/r[osx-64,noarch], platform: noarch, prio: 8 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/4ea078d6.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/4ea078d6.json.*' for repo index 'pkgs/r/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/4ea078d6.json"" for repo pkgs/r/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/4ea078d6.solv"" for repo pkgs/r/noarch
Channel: anaconda[osx-64,noarch], platform: osx-64, prio: 7 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/e3427d50.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/e3427d50.json.*' for repo index 'anaconda/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/e3427d50.json"" for repo anaconda/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/e3427d50.solv"" for repo anaconda/osx-64
Channel: anaconda[osx-64,noarch], platform: noarch, prio: 7 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/0c60671d.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/0c60671d.json.*' for repo index 'anaconda/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/0c60671d.json"" for repo anaconda/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/0c60671d.solv"" for repo anaconda/noarch
Channel: bashtage[osx-64,noarch], platform: osx-64, prio: 6 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/0d322024.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/0d322024.json.*' for repo index 'bashtage/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/0d322024.json"" for repo bashtage/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/0d322024.solv"" for repo bashtage/osx-64
Channel: bashtage[osx-64,noarch], platform: noarch, prio: 6 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/9985e476.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/9985e476.json.*' for repo index 'bashtage/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/9985e476.json"" for repo bashtage/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/9985e476.solv"" for repo bashtage/noarch
Channel: ranaroussi[osx-64,noarch], platform: osx-64, prio: 5 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/ec989249.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/ec989249.json.*' for repo index 'ranaroussi/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/ec989249.json"" for repo ranaroussi/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/ec989249.solv"" for repo ranaroussi/osx-64
Channel: ranaroussi[osx-64,noarch], platform: noarch, prio: 5 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/f2f1db2e.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/f2f1db2e.json.*' for repo index 'ranaroussi/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/f2f1db2e.json"" for repo ranaroussi/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/f2f1db2e.solv"" for repo ranaroussi/noarch
Channel: powerai[osx-64,noarch], platform: osx-64, prio: 4 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/cd55de18.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/cd55de18.json.*' for repo index 'powerai/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/cd55de18.json"" for repo powerai/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/cd55de18.solv"" for repo powerai/osx-64
Channel: powerai[osx-64,noarch], platform: noarch, prio: 4 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/0ccf548c.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/0ccf548c.json.*' for repo index 'powerai/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/0ccf548c.json"" for repo powerai/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/0ccf548c.solv"" for repo powerai/noarch
Channel: fastai[osx-64,noarch], platform: osx-64, prio: 3 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/65ae9eab.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/65ae9eab.json.*' for repo index 'fastai/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/65ae9eab.json"" for repo fastai/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/65ae9eab.solv"" for repo fastai/osx-64
Channel: fastai[osx-64,noarch], platform: noarch, prio: 3 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/b057f602.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/b057f602.json.*' for repo index 'fastai/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/b057f602.json"" for repo fastai/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/b057f602.solv"" for repo fastai/noarch
Channel: jiayi_anaconda[osx-64,noarch], platform: osx-64, prio: 2 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/850e1cab.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/850e1cab.json.*' for repo index 'jiayi_anaconda/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/850e1cab.json"" for repo jiayi_anaconda/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/850e1cab.solv"" for repo jiayi_anaconda/osx-64
Channel: jiayi_anaconda[osx-64,noarch], platform: noarch, prio: 2 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/e296a22d.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/e296a22d.json.*' for repo index 'jiayi_anaconda/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/e296a22d.json"" for repo jiayi_anaconda/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/e296a22d.solv"" for repo jiayi_anaconda/noarch
Channel: districtdatalabs[osx-64,noarch], platform: osx-64, prio: 1 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/f30d5fbb.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/f30d5fbb.json.*' for repo index 'districtdatalabs/osx-64'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/f30d5fbb.json"" for repo districtdatalabs/osx-64
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/f30d5fbb.solv"" for repo districtdatalabs/osx-64
Channel: districtdatalabs[osx-64,noarch], platform: noarch, prio: 1 : 0
Cache path:  /Users/carsten/miniconda3/pkgs/cache/788f9cd7.json
info     libmamba Reading cache files '/Users/carsten/miniconda3/pkgs/cache/788f9cd7.json.*' for repo index 'districtdatalabs/noarch'
info     libmamba Reading repodata.json file ""/Users/carsten/miniconda3/pkgs/cache/788f9cd7.json"" for repo districtdatalabs/noarch
info     libmamba Writing libsolv solv file ""/Users/carsten/miniconda3/pkgs/cache/788f9cd7.solv"" for repo districtdatalabs/noarch


Looking for: ['arch', 'alphalens-reloaded', 'arviz', 'autopep8', 'beautifulsoup4', 'bokeh', 'catboost', 'colorlover', 'conda', 'empyrical-reloaded', 'ffmpeg', ""gensim[version='<4.0']"", 'gym', 'gym-box2d', 'html5lib', 'hdbscan', 'ipykernel', 'ipyvolume', 'ipywidgets', 'jupyter', 'jupyter_contrib_nbextensions', 'jupyterlab', 'lightgbm', 'linearmodels', 'lxml', 'matplotlib', 'mplfinance', 'nb_conda', 'nb_conda_kernels', 'nbconvert=5.6.1', 'notebook', 'nltk', 'numpy', 'opencv-python-headless', 'openpyxl', ""pandas[version='<1.3']"", 'pandas-datareader', 'pillow', 'pip', 'plotly', 'pyarrow', 'pydot', 'pyfolio-reloaded', 'pykalman', 'pyldavis', 'pytorch', 'pymc3', 'pytables', 'python=3.8', 'python-graphviz', 'pywavelets', 'quandl', 'requests', 'scikit-learn', 'scipy', 'scrapy', 'seaborn', 'shap', 'snappy', 'spacy', 'statsmodels', 'sympy', 'ta-lib', 'tensorboard', 'tensorflow', 'tensorflow-datasets', 'termcolor', 'textacy', 'textblob', 'tqdm', 'umap-learn', 'wheel', 'widgetsnbextension', 'wordcloud', 'xgboost', 'xlrd', 'yaml', 'yellowbrick', 'yfinance', 'zipline-reloaded']


info     libmamba Adding package record to repo __archspec
info     libmamba Adding package record to repo __osx
info     libmamba Adding package record to repo __unix
info     libmamba Parsing MatchSpec arch
info     libmamba Parsing MatchSpec arch
info     libmamba Adding job: arch
info     libmamba Parsing MatchSpec alphalens-reloaded
info     libmamba Parsing MatchSpec alphalens-reloaded
info     libmamba Adding job: alphalens-reloaded
info     libmamba Parsing MatchSpec arviz
info     libmamba Parsing MatchSpec arviz
info     libmamba Adding job: arviz
info     libmamba Parsing MatchSpec autopep8
info     libmamba Parsing MatchSpec autopep8
info     libmamba Adding job: autopep8
info     libmamba Parsing MatchSpec beautifulsoup4
info     libmamba Parsing MatchSpec beautifulsoup4
info     libmamba Adding job: beautifulsoup4
info     libmamba Parsing MatchSpec bokeh
info     libmamba Parsing MatchSpec bokeh
info     libmamba Adding job: bokeh
info     libmamba Parsing MatchSpec catboost
info     libmamba Parsing MatchSpec catboost
info     libmamba Adding job: catboost
info     libmamba Parsing MatchSpec colorlover
info     libmamba Parsing MatchSpec colorlover
info     libmamba Adding job: colorlover
info     libmamba Parsing MatchSpec conda
info     libmamba Parsing MatchSpec conda
info     libmamba Adding job: conda
info     libmamba Parsing MatchSpec empyrical-reloaded
info     libmamba Parsing MatchSpec empyrical-reloaded
info     libmamba Adding job: empyrical-reloaded
info     libmamba Parsing MatchSpec ffmpeg
info     libmamba Parsing MatchSpec ffmpeg
info     libmamba Adding job: ffmpeg
info     libmamba Parsing MatchSpec gensim[version='<4.0']
info     libmamba Parsing MatchSpec gensim[version='<4.0']
info     libmamba Adding job: gensim[version='<4.0']
info     libmamba Parsing MatchSpec gym
info     libmamba Parsing MatchSpec gym
info     libmamba Adding job: gym
info     libmamba Parsing MatchSpec gym-box2d
info     libmamba Parsing MatchSpec gym-box2d
info     libmamba Adding job: gym-box2d
info     libmamba Parsing MatchSpec html5lib
info     libmamba Parsing MatchSpec html5lib
info     libmamba Adding job: html5lib
info     libmamba Parsing MatchSpec hdbscan
info     libmamba Parsing MatchSpec hdbscan
info     libmamba Adding job: hdbscan
info     libmamba Parsing MatchSpec ipykernel
info     libmamba Parsing MatchSpec ipykernel
info     libmamba Adding job: ipykernel
info     libmamba Parsing MatchSpec ipyvolume
info     libmamba Parsing MatchSpec ipyvolume
info     libmamba Adding job: ipyvolume
info     libmamba Parsing MatchSpec ipywidgets
info     libmamba Parsing MatchSpec ipywidgets
info     libmamba Adding job: ipywidgets
info     libmamba Parsing MatchSpec jupyter
info     libmamba Parsing MatchSpec jupyter
info     libmamba Adding job: jupyter
info     libmamba Parsing MatchSpec jupyter_contrib_nbextensions
info     libmamba Parsing MatchSpec jupyter_contrib_nbextensions
info     libmamba Adding job: jupyter_contrib_nbextensions
info     libmamba Parsing MatchSpec jupyterlab
info     libmamba Parsing MatchSpec jupyterlab
info     libmamba Adding job: jupyterlab
info     libmamba Parsing MatchSpec lightgbm
info     libmamba Parsing MatchSpec lightgbm
info     libmamba Adding job: lightgbm
info     libmamba Parsing MatchSpec linearmodels
info     libmamba Parsing MatchSpec linearmodels
info     libmamba Adding job: linearmodels
info     libmamba Parsing MatchSpec lxml
info     libmamba Parsing MatchSpec lxml
info     libmamba Adding job: lxml
info     libmamba Parsing MatchSpec matplotlib
info     libmamba Parsing MatchSpec matplotlib
info     libmamba Adding job: matplotlib
info     libmamba Parsing MatchSpec mplfinance
info     libmamba Parsing MatchSpec mplfinance
info     libmamba Adding job: mplfinance
info     libmamba Parsing MatchSpec nb_conda
info     libmamba Parsing MatchSpec nb_conda
info     libmamba Adding job: nb_conda
info     libmamba Parsing MatchSpec nb_conda_kernels
info     libmamba Parsing MatchSpec nb_conda_kernels
info     libmamba Adding job: nb_conda_kernels
info     libmamba Parsing MatchSpec nbconvert=5.6.1
info     libmamba Parsing MatchSpec nbconvert=5.6.1
info     libmamba Adding job: nbconvert=5.6.1
info     libmamba Parsing MatchSpec notebook
info     libmamba Parsing MatchSpec notebook
info     libmamba Adding job: notebook
info     libmamba Parsing MatchSpec nltk
info     libmamba Parsing MatchSpec nltk
info     libmamba Adding job: nltk
info     libmamba Parsing MatchSpec numpy
info     libmamba Parsing MatchSpec numpy
info     libmamba Adding job: numpy
info     libmamba Parsing MatchSpec opencv-python-headless
info     libmamba Parsing MatchSpec opencv-python-headless
info     libmamba Adding job: opencv-python-headless
info     libmamba Parsing MatchSpec openpyxl
info     libmamba Parsing MatchSpec openpyxl
info     libmamba Adding job: openpyxl
info     libmamba Parsing MatchSpec pandas[version='<1.3']
info     libmamba Parsing MatchSpec pandas[version='<1.3']
info     libmamba Adding job: pandas[version='<1.3']
info     libmamba Parsing MatchSpec pandas-datareader
info     libmamba Parsing MatchSpec pandas-datareader
info     libmamba Adding job: pandas-datareader
info     libmamba Parsing MatchSpec pillow
info     libmamba Parsing MatchSpec pillow
info     libmamba Adding job: pillow
info     libmamba Parsing MatchSpec pip
info     libmamba Parsing MatchSpec pip
info     libmamba Adding job: pip
info     libmamba Parsing MatchSpec plotly
info     libmamba Parsing MatchSpec plotly
info     libmamba Adding job: plotly
info     libmamba Parsing MatchSpec pyarrow
info     libmamba Parsing MatchSpec pyarrow
info     libmamba Adding job: pyarrow
info     libmamba Parsing MatchSpec pydot
info     libmamba Parsing MatchSpec pydot
info     libmamba Adding job: pydot
info     libmamba Parsing MatchSpec pyfolio-reloaded
info     libmamba Parsing MatchSpec pyfolio-reloaded
info     libmamba Adding job: pyfolio-reloaded
info     libmamba Parsing MatchSpec pykalman
info     libmamba Parsing MatchSpec pykalman
info     libmamba Adding job: pykalman
info     libmamba Parsing MatchSpec pyldavis
info     libmamba Parsing MatchSpec pyldavis
info     libmamba Adding job: pyldavis
info     libmamba Parsing MatchSpec pytorch
info     libmamba Parsing MatchSpec pytorch
info     libmamba Adding job: pytorch
info     libmamba Parsing MatchSpec pymc3
info     libmamba Parsing MatchSpec pymc3
info     libmamba Adding job: pymc3
info     libmamba Parsing MatchSpec pytables
info     libmamba Parsing MatchSpec pytables
info     libmamba Adding job: pytables
info     libmamba Parsing MatchSpec python=3.8
info     libmamba Parsing MatchSpec python=3.8
info     libmamba Adding job: python=3.8
info     libmamba Parsing MatchSpec python-graphviz
info     libmamba Parsing MatchSpec python-graphviz
info     libmamba Adding job: python-graphviz
info     libmamba Parsing MatchSpec pywavelets
info     libmamba Parsing MatchSpec pywavelets
info     libmamba Adding job: pywavelets
info     libmamba Parsing MatchSpec quandl
info     libmamba Parsing MatchSpec quandl
info     libmamba Adding job: quandl
info     libmamba Parsing MatchSpec requests
info     libmamba Parsing MatchSpec requests
info     libmamba Adding job: requests
info     libmamba Parsing MatchSpec scikit-learn
info     libmamba Parsing MatchSpec scikit-learn
info     libmamba Adding job: scikit-learn
info     libmamba Parsing MatchSpec scipy
info     libmamba Parsing MatchSpec scipy
info     libmamba Adding job: scipy
info     libmamba Parsing MatchSpec scrapy
info     libmamba Parsing MatchSpec scrapy
info     libmamba Adding job: scrapy
info     libmamba Parsing MatchSpec seaborn
info     libmamba Parsing MatchSpec seaborn
info     libmamba Adding job: seaborn
info     libmamba Parsing MatchSpec shap
info     libmamba Parsing MatchSpec shap
info     libmamba Adding job: shap
info     libmamba Parsing MatchSpec snappy
info     libmamba Parsing MatchSpec snappy
info     libmamba Adding job: snappy
info     libmamba Parsing MatchSpec spacy
info     libmamba Parsing MatchSpec spacy
info     libmamba Adding job: spacy
info     libmamba Parsing MatchSpec statsmodels
info     libmamba Parsing MatchSpec statsmodels
info     libmamba Adding job: statsmodels
info     libmamba Parsing MatchSpec sympy
info     libmamba Parsing MatchSpec sympy
info     libmamba Adding job: sympy
info     libmamba Parsing MatchSpec ta-lib
info     libmamba Parsing MatchSpec ta-lib
info     libmamba Adding job: ta-lib
info     libmamba Parsing MatchSpec tensorboard
info     libmamba Parsing MatchSpec tensorboard
info     libmamba Adding job: tensorboard
info     libmamba Parsing MatchSpec tensorflow
info     libmamba Parsing MatchSpec tensorflow
info     libmamba Adding job: tensorflow
info     libmamba Parsing MatchSpec tensorflow-datasets
info     libmamba Parsing MatchSpec tensorflow-datasets
info     libmamba Adding job: tensorflow-datasets
info     libmamba Parsing MatchSpec termcolor
info     libmamba Parsing MatchSpec termcolor
info     libmamba Adding job: termcolor
info     libmamba Parsing MatchSpec textacy
info     libmamba Parsing MatchSpec textacy
info     libmamba Adding job: textacy
info     libmamba Parsing MatchSpec textblob
info     libmamba Parsing MatchSpec textblob
info     libmamba Adding job: textblob
info     libmamba Parsing MatchSpec tqdm
info     libmamba Parsing MatchSpec tqdm
info     libmamba Adding job: tqdm
info     libmamba Parsing MatchSpec umap-learn
info     libmamba Parsing MatchSpec umap-learn
info     libmamba Adding job: umap-learn
info     libmamba Parsing MatchSpec wheel
info     libmamba Parsing MatchSpec wheel
info     libmamba Adding job: wheel
info     libmamba Parsing MatchSpec widgetsnbextension
info     libmamba Parsing MatchSpec widgetsnbextension
info     libmamba Adding job: widgetsnbextension
info     libmamba Parsing MatchSpec wordcloud
info     libmamba Parsing MatchSpec wordcloud
info     libmamba Adding job: wordcloud
info     libmamba Parsing MatchSpec xgboost
info     libmamba Parsing MatchSpec xgboost
info     libmamba Adding job: xgboost
info     libmamba Parsing MatchSpec xlrd
info     libmamba Parsing MatchSpec xlrd
info     libmamba Adding job: xlrd
info     libmamba Parsing MatchSpec yaml
info     libmamba Parsing MatchSpec yaml
info     libmamba Adding job: yaml
info     libmamba Parsing MatchSpec yellowbrick
info     libmamba Parsing MatchSpec yellowbrick
info     libmamba Adding job: yellowbrick
info     libmamba Parsing MatchSpec yfinance
info     libmamba Parsing MatchSpec yfinance
info     libmamba Adding job: yfinance
info     libmamba Parsing MatchSpec zipline-reloaded
info     libmamba Parsing MatchSpec zipline-reloaded
info     libmamba Adding job: zipline-reloaded


...several hours later

Assertion failed: (!p2 && d > 0), function solver_addrule, file /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_55fs74onkn/croot/libsolv-suite_1694099474297/work/src/rules.c, line 261.
zsh: abort      mamba env update -n ml4t -f  -v

**Environment**
MacBook Pro 2.4 GHz 8-Core Intel Core i9
MacOs 13.4 (22F66)

",carstenf,58643140,closed,False,1,2023-11-26T22:22:04+00:00,2024-09-23T10:46:53+00:00,2024-09-23T10:46:53+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1919611826,308,--,,VictorCozer2,136805077,closed,False,0,2023-09-29T16:52:53+00:00,2023-09-29T17:42:00+00:00,2023-09-29T17:41:59+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1912538223,307,Chapter 8 02_backtesting_with_zipline custom_loader not working,"Hi, following the 02_backtesting_with_zipline.ipynb

Have the custom loader set it up as:

```
class SignalData(DataSet):
    predictions = Column(dtype=float)
    domain = US_EQUITIES

signal_loader = {SignalData.predictions: DataFrameLoader(SignalData.predictions, 
                                                         predictions)}

class MLSignal(CustomFactor):
    """"""Converting signals to Factor
        so we can rank and filter in Pipeline""""""
    inputs = [SignalData.predictions]
    window_length = 1

    def compute(self, today, assets, out, preds):
        # print(preds) -> [nan, nan....nan, nan],
        out[:] = preds

def compute_signals():
    signals = MLSignal()
    return Pipeline(columns={
        'longs' : signals.top(N_LONGS, mask=signals > 0),
        'shorts': signals.bottom(N_SHORTS, mask=signals < 0)},
            screen=StaticAssets(assets)
    )

# Other functions
results = run_algorithm(start=start_date,
                       end=end_date,
                       initialize=initialize,
                       before_trading_start=before_trading_start,
                       capital_base=1e6,
                       data_frequency='daily',
                       bundle='quandl',
                       custom_loader=signal_loader)
```
I'm using the zipline-reloaded
The `preds` passed to MLSignal.compute function are all [nan, nan....nan, nan], therefore none of the longs and shorts were executed.
Also, the `predictions` generated from `load_predictions` looks fine. Seems that the custom_loader is not recognized somehow?
",weilhuiz,52543962,closed,False,1,2023-09-26T01:49:12+00:00,2023-09-26T04:53:27+00:00,2023-09-26T04:53:27+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1903343453,306,01_parse_itch_order_flow_messages ,,bassel-mt,88371310,closed,False,3,2023-09-19T16:26:55+00:00,2024-09-23T10:47:21+00:00,2024-09-23T10:47:21+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1868575526,305,Chapter 8 02_backtesting_with_zipline,"
```
I am using the code from Chapter 8 of the ML4T book by Stefan Jansen. The file I am using is 02_backtesting_with_zipline
```

Unfortunately, when running this code:
````
start = time()
results = run_algorithm(start=start_date,
                       end=end_date,
                       initialize=initialize,
                       before_trading_start=before_trading_start,
                       capital_base=1e6,
                       data_frequency='daily',
                       bundle='quandl',
                       custom_loader=signal_loader) # need to modify zipline

print('Duration: {:.2f}s'.format(time() - start))
```
`
I get this error:
```
``AttributeError                            Traceback (most recent call last)
c:\Lyubo\Python lessons\Machine-Learning-for-Algorithmic-Trading-Second-Edition-master\08_ml4t_workflow\04_ml4t_workflow_with_zipline\02_backtesting_with_zipline.ipynb Cell 55 in ()
      1 start = time()
----> 2 results = run_algorithm(start=start_date,
      3                        end=end_date,
      4                        initialize=initialize,
      5                        before_trading_start=before_trading_start,
      6                        capital_base=1e6,
      7                        data_frequency='daily',
      8                        bundle='quandl',
      9                        custom_loader=signal_loader) # need to modify zipline
     11 print('Duration: {:.2f}s'.format(time() - start))

File c:\ProgramData\anaconda3\envs\py38class\lib\site-packages\zipline\utils\run_algo.py:397, in run_algorithm(start, end, initialize, capital_base, handle_data, before_trading_start, analyze, data_frequency, bundle, bundle_timestamp, trading_calendar, metrics_set, benchmark_returns, default_extension, extensions, strict_extensions, environ, custom_loader, blotter)
    393 load_extensions(default_extension, extensions, strict_extensions, environ)
    395 benchmark_spec = BenchmarkSpec.from_returns(benchmark_returns)
--> 397 return _run(
    398     handle_data=handle_data,
    399     initialize=initialize,
    400     before_trading_start=before_trading_start,
    401     analyze=analyze,
    402     algofile=None,
    403     algotext=None,
    404     defines=(),
...
   5987 ):
   5988     return self[name]
-> 5989 return object.__getattribute__(self, name)

AttributeError: 'Series' object has no attribute 'append'
```
``

I followed the link to the file that gives the error:
```
File c:\ProgramData\anaconda3\envs\py38class\lib\site-packages\zipline\utils\run_algo.py:397, in run_algorithm(start, end, initialize, capital_base, handle_data, before_trading_start, analyze, data_frequency, bundle, bundle_timestamp, trading_calendar, metrics_set, benchmark_returns, default_extension, extensions, strict_extensions, environ, custom_loader, blotter)


However, I can't see append anywhere in the function itself, so I guess the funciton is dependent on another function in a different package. 

Does anyone know how I can fix this? 





I tried to change the code in the source and basically changed the code of any .py file in the pacakge which has append in it but that didn't work either. 


",lyubomir-vasilev,130079626,closed,False,1,2023-08-27T17:43:54+00:00,2023-09-02T15:27:23+00:00,2023-09-02T15:27:23+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1867733630,304,Chapter 7 - Evaluating signals using alphalens ,"When using the get_clean_factor_and_forward_returns command, I get the following error:
ValueError                                Traceback (most recent call last)
File [c:\ProgramData\anaconda3\envs\py38class\lib\site-packages\pandas\core\frame.py:11610](file:///C:/ProgramData/anaconda3/envs/py38class/lib/site-packages/pandas/core/frame.py:11610), in _reindex_for_setitem(value, index)
  11609 try:
> 11610     reindexed_value = value.reindex(index)._values
  11611 except ValueError as err:
  11612     # raised in MultiIndex.from_tuples, see test_insert_error_msmgs

File [c:\ProgramData\anaconda3\envs\py38class\lib\site-packages\pandas\core\series.py:4923](file:///C:/ProgramData/anaconda3/envs/py38class/lib/site-packages/pandas/core/series.py:4923), in Series.reindex(self, index, axis, method, copy, level, fill_value, limit, tolerance)
   4903 @doc(
   4904     NDFrame.reindex,  # type: ignore[has-type]
   4905     klass=_shared_doc_kwargs[""klass""],
   (...)
   4921         tolerance=None,
   4922     ) -> Series:
-> 4923         return super().reindex(
   4924             index=index,
   4925             method=method,
   4926             copy=copy,
   4927             level=level,
   4928             fill_value=fill_value,
   4929             limit=limit,
   4930             tolerance=tolerance,
   4931         )
...
  11618         ""incompatible index of inserted column with frame index""
  11619     ) from err
  11620 return reindexed_value

TypeError: incompatible index of inserted column with frame index

I am just running the code in the notebook but it didn't work. I checked all the symbols and dates in the lr_factor and trade_prices and I made sure the indexes were compatible, i.e. I filtered the start date of trade_prices such that it matches with the one of lr_factor. Additionally, i checked that all of the assets in the trade_prices columns are present in the lr_factor's index. However, I kept getting this error. 

I decided to proceed in a different way and use the compute_forward_returns function, then to use get_clean_factor. Unfortunately, I got the same error as when using the get_clean_factor_and_forward_returns. The forward returns were computed perfectly, the indexes are the same as the index of lr_factor, but I again get the same error. 
Below is the code where I tried to split the tasks in two:
trade_prices_data = compute_forward_returns(factor= lr_factor,
                                            prices= trade_prices,                                           
                                            periods = (1, 5, 10, 21)
)

<img width=""459"" alt=""image"" src=""https://github.com/stefan-jansen/machine-learning-for-trading/assets/130079626/02b743e6-3a49-4456-8237-69f6fc298099"">

I wasn't very sure exactly what to select for most of the parametres, but below is the code I used. 
lr_factor_data = get_clean_factor(factor= lr_factor,
                                  forward_returns=trade_prices_data,
                                  groupby= None,
                                  binning_by_group=False,
                                  quantiles=5,
                                  bins=None,
                                  groupby_labels=None,
                                  max_loss=0.35,
                                  zero_aware=False
                                  )
Environment: Windows
 C:\ProgramData\anaconda3\envs\py38class


These are all of the packages I have installed:
absl-py @ file:///opt/conda/conda-bld/absl-py_1639803114343/work
aiohttp @ file:///C:/ci/aiohttp_1646806545879/work
aiosignal @ file:///tmp/build/80754af9/aiosignal_1637843061372/work
alabaster==0.7.12
alembic==1.11.2
alpha-vantage==2.3.1
alphalens==0.4.0
amqp @ file:///home/conda/feedstock_root/build_artifacts/amqp_1646564891451/work
anaconda-client @ file:///C:/ci/anaconda-client_1635342752200/work
anaconda-project @ file:///tmp/build/80754af9/anaconda-project_1637161053845/work
ansi2html==1.7.0
anyio @ file:///C:/ci/anyio_1644463701441/work/dist
aplus==0.11.0
appdirs==1.4.4
arctic==1.79.4
argh==0.26.2
argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work
argon2-cffi-bindings @ file:///C:/ci/argon2-cffi-bindings_1644569878360/work
arviz @ file:///home/conda/feedstock_root/build_artifacts/arviz_1633274821480/work
asn1crypto @ file:///tmp/build/80754af9/asn1crypto_1596577642040/work
astor==0.8.1
astroid @ file:///C:/ci/astroid_1639044403229/work
astropy==4.0.2
asttokens @ file:///opt/conda/conda-bld/asttokens_1646925590279/work
astunparse==1.6.3
async-timeout @ file:///tmp/build/80754af9/async-timeout_1637851218186/work
atomicwrites==1.4.0
attrs @ file:///opt/conda/conda-bld/attrs_1642510447205/work
Automat==22.10.0
autopep8 @ file:///tmp/build/80754af9/autopep8_1615918855173/work
Babel @ file:///tmp/build/80754af9/babel_1620871417480/work
backcall==0.2.0
backports.shutil-get-terminal-size==1.0.0
backports.zoneinfo==0.2.1
bcolz-zipline==1.2.6
bcrypt @ file:///C:/ci/bcrypt_1597936263757/work
beautifulsoup4==4.12.2
billiard @ file:///D:/bld/billiard_1636131411089/work
bitarray @ file:///C:/ci/bitarray_1641799256136/work
bkcharts==0.2
black==19.10b0
blake3 @ file:///D:/bld/blake3_1636283080831/work
bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work
blinker==1.4
blis @ file:///D:/bld/cython-blis_1645002897137/work
blosc @ file:///C:/ci/python-blosc_1638539958099/work
blosc2==2.0.0
blpapi==3.17.1
bokeh @ file:///C:/ci/bokeh_1638362940011/work
boto==2.49.0
boto3 @ file:///home/conda/feedstock_root/build_artifacts/boto3_1647500875427/work
botocore @ file:///home/conda/feedstock_root/build_artifacts/botocore_1647478405006/work
Bottleneck==1.3.2
bqplot @ file:///home/conda/feedstock_root/build_artifacts/bqplot_1644594233772/work
branca @ file:///home/conda/feedstock_root/build_artifacts/branca_1609346930842/work
Brotli @ file:///D:/bld/brotli-split_1635846030050/work
brotlipy==0.7.0
bs4==0.0.1
cachetools==4.2.4
catalogue @ file:///D:/bld/catalogue_1638867510751/work
celery @ file:///home/conda/feedstock_root/build_artifacts/celery_1644262149668/work
certifi==2023.7.22
cffi @ file:///C:/ci_310/cffi_1642682485096/work
cftime @ file:///D:/bld/cftime_1646333174733/work
chardet @ file:///D:/bld/chardet_1602255508636/work
charset-normalizer==3.2.0
chart-studio==1.1.0
chartpy==0.1.11
click @ file:///C:/ci/click_1646056799533/work
click-didyoumean @ file:///home/conda/feedstock_root/build_artifacts/click-didyoumean_1642673782881/work
click-plugins==1.1.1
click-repl @ file:///home/conda/feedstock_root/build_artifacts/click-repl_1623347986327/work
cloudpickle==1.3.0
clyent==1.2.2
colorama @ file:///tmp/build/80754af9/colorama_1603211150991/work
colorcet @ file:///home/conda/feedstock_root/build_artifacts/colorcet_1638280441091/work
colorlover==0.3.0
commonmark==0.9.1
comtypes==1.1.10
conda-pack @ file:///tmp/build/80754af9/conda-pack_1611163042455/work
configparser==5.2.0
constantly==15.1.0
contextlib2==0.6.0.post1
contourpy==1.1.0
convertdate @ file:///home/conda/feedstock_root/build_artifacts/convertdate_1642883757836/work
cramjam @ file:///D:/bld/cramjam_1639035096642/work
cryptography @ file:///C:/ci/cryptography_1639472366776/work
cssselect==1.1.0
cufflinks==0.17.3
cvlib==0.2.6
cvxpy==1.3.2
cycler @ file:///tmp/build/80754af9/cycler_1637851556182/work
cymem @ file:///D:/bld/cymem_1636053422146/work
Cython @ file:///C:/ci/cython_1639474807736/work
cytoolz==0.11.0
dash @ file:///home/conda/feedstock_root/build_artifacts/dash_1647257147344/work
dash-bootstrap-components @ file:///home/conda/feedstock_root/build_artifacts/dash-bootstrap-components_1644118775506/work
dash-colorscales @ file:///home/conda/feedstock_root/build_artifacts/dash_colorscales_1593443621680/work
dash-core-components @ file:///home/conda/feedstock_root/build_artifacts/dash-core-components_1645162191927/work
dash-daq @ file:///home/conda/feedstock_root/build_artifacts/dash-daq_1596869611219/work
dash-html-components @ file:///home/conda/feedstock_root/build_artifacts/dash-html-components_1645162180756/work
dash-table==5.0.0
dask @ file:///tmp/build/80754af9/dask-core_1617390489108/work
datashader @ file:///home/conda/feedstock_root/build_artifacts/datashader_1623315462041/work
datashape==0.5.4
DateTime==5.2
debugpy @ file:///C:/ci/debugpy_1637073815078/work
decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work
defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work
Deprecated @ file:///home/conda/feedstock_root/build_artifacts/deprecated_1632758190537/work
deprecation @ file:///home/conda/feedstock_root/build_artifacts/deprecation_1589881437857/work
diff-match-patch @ file:///tmp/build/80754af9/diff-match-patch_1594828741838/work
dill @ file:///home/conda/feedstock_root/build_artifacts/dill_1623610058511/work
distributed @ file:///C:/ci/distributed_1620907381536/work
distro @ file:///home/conda/feedstock_root/build_artifacts/distro_1636872986284/work
dm-tree==0.1.8
docutils @ file:///C:/ci/docutils_1638528665024/work
docx==0.2.4
dtale @ file:///home/conda/feedstock_root/build_artifacts/dtale_1647357168981/work
ecos==2.0.12
eikon @ file:///home/conda/feedstock_root/build_artifacts/eikon_1597983976546/work
empyrical==0.5.5
empyrical-reloaded==0.5.9
en-core-web-md @ https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl
en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl
entrypoints==0.3
enum-compat==0.0.3
ephem @ file:///D:/bld/ephem_1645531741779/work
et-xmlfile==1.1.0
exceptiongroup==1.1.2
exchange-calendars==4.2.8
executing @ file:///opt/conda/conda-bld/executing_1646925071911/work
fake-useragent==0.1.11
fastapi @ file:///home/conda/feedstock_root/build_artifacts/fastapi_1646447241602/work
fastcache==1.1.0
fastparquet @ file:///D:/bld/fastparquet_1643251108536/work
fbprophet @ file:///D:/bld/fbprophet_1599365958928/work
feedfinder2==0.0.4
feedparser @ file:///home/conda/feedstock_root/build_artifacts/feedparser_1624371037925/work
filelock @ file:///opt/conda/conda-bld/filelock_1647002191454/work
financepy==0.193
findatapy==0.1.26
findspark @ file:///home/conda/feedstock_root/build_artifacts/findspark_1644599740637/work
finmarketpy==0.11.11
flake8 @ file:///tmp/build/80754af9/flake8_1615834841867/work
Flask @ file:///tmp/build/80754af9/flask_1634118196080/work
Flask-Compress @ file:///home/conda/feedstock_root/build_artifacts/flask-compress_1646141388556/work
fonttools==4.25.0
formulaic==0.6.4
freetype-py @ file:///home/conda/feedstock_root/build_artifacts/freetype-py_1594318845769/work
frozendict==2.3.8
frozenlist @ file:///C:/ci/frozenlist_1637767289747/work
fsspec @ file:///opt/conda/conda-bld/fsspec_1647268051896/work
funcy @ file:///home/conda/feedstock_root/build_artifacts/funcy_1640095670534/work
future==0.18.2
fxcmpy==1.2.10
gast==0.3.3
gensim @ file:///D:/bld/gensim_1589441310359/work
gevent @ file:///C:/ci/gevent_1628273793586/work
glob2==0.7
gmpy2 @ file:///C:/ci/gmpy2_1645456279018/work
google-auth @ file:///opt/conda/conda-bld/google-auth_1646735974934/work
google-auth-oauthlib==0.4.1
google-pasta==0.2.0
graphlib-backport==1.0.3
graphviz @ file:///tmp/build/80754af9/python-graphviz_1612303637553/work
greenlet @ file:///C:/ci/greenlet_1628888262822/work
grpcio @ file:///C:/ci/grpcio_1637590968244/work
h11==0.14.0
h2==3.2.0
h5py==2.10.0
HeapDict==1.0.1
hijri-converter @ file:///home/conda/feedstock_root/build_artifacts/hijri-converter_1644680066407/work
holidays @ file:///home/conda/feedstock_root/build_artifacts/holidays_1644933126202/work
holoviews @ file:///home/conda/feedstock_root/build_artifacts/holoviews_1645018991041/work
hpack==3.0.0
hsluv @ file:///home/conda/feedstock_root/build_artifacts/hsluv_1614057510686/work
html5lib==1.1
http3==0.6.7
huggingface-hub @ file:///home/conda/feedstock_root/build_artifacts/huggingface_hub_1641988520462/work
hyperframe==5.2.0
hyperlink==21.0.0
idna==3.4
imageio @ file:///home/conda/feedstock_root/build_artifacts/imageio_1594044661732/work
imagesize @ file:///tmp/build/80754af9/imagesize_1637939814114/work
importlib-metadata @ file:///C:/ci/importlib-metadata_1638543108096/work
importlib-resources==6.0.1
imutils==0.5.4
incremental==22.10.0
inflection @ file:///home/conda/feedstock_root/build_artifacts/inflection_1598089801258/work
iniconfig @ file:///tmp/build/80754af9/iniconfig_1602780191262/work
interface-meta==1.3.0
intervaltree @ file:///tmp/build/80754af9/intervaltree_1598376443606/work
ipydatawidgets @ file:///home/conda/feedstock_root/build_artifacts/ipydatawidgets_1609929402090/work
ipykernel @ file:///C:/ci/ipykernel_1647000978151/work/dist/ipykernel-6.9.1-py3-none-any.whl
ipyleaflet @ file:///home/conda/feedstock_root/build_artifacts/ipyleaflet_1639075278024/work
ipympl @ file:///home/conda/feedstock_root/build_artifacts/ipympl_1645174282670/work
ipython @ file:///C:/ci/ipython_1646948643510/work
ipython_genutils==0.2.0
ipyvolume @ file:///home/conda/feedstock_root/build_artifacts/ipyvolume_1618924410810/work
ipyvue @ file:///home/conda/feedstock_root/build_artifacts/ipyvue_1635425457130/work
ipyvuetify @ file:///home/conda/feedstock_root/build_artifacts/ipyvuetify_1644248876173/work
ipywebrtc @ file:///home/conda/feedstock_root/build_artifacts/ipywebrtc_1617018267587/work
ipywidgets @ file:///tmp/build/80754af9/ipywidgets_1634143127070/work
iso3166==2.1.1
iso4217==1.11.20220401
isort @ file:///tmp/build/80754af9/isort_1628603791788/work
itemadapter==0.8.0
itemloaders==1.1.0
itsdangerous @ file:///tmp/build/80754af9/itsdangerous_1621432558163/work
jdcal==1.4.1
jedi @ file:///C:/ci/jedi_1598353661586/work
jellyfish @ file:///D:/bld/jellyfish_1641751695265/work
jieba3k @ file:///D:/bld/jieba3k_1636293213748/work
Jinja2 @ file:///opt/conda/conda-bld/jinja2_1647436528585/work
jmespath @ file:///home/conda/feedstock_root/build_artifacts/jmespath_1647416812516/work
joblib==1.3.1
json5 @ file:///tmp/build/80754af9/json5_1624432770122/work
jsonschema @ file:///C:/ci/jsonschema_1601490672432/work
jupyter==1.0.0
jupyter-client @ file:///opt/conda/conda-bld/jupyter_client_1643638337975/work
jupyter-console @ file:///opt/conda/conda-bld/jupyter_console_1647002188872/work
jupyter-contrib-core==0.3.3
jupyter-contrib-nbextensions @ file:///D:/bld/jupyter_contrib_nbextensions_1602805696229/work
jupyter-core @ file:///C:/ci/jupyter_core_1646976462809/work
jupyter-dash==0.4.1
jupyter-highlight-selected-word @ file:///D:/bld/jupyter_highlight_selected_word_1638383158909/work
jupyter-latex-envs @ file:///D:/bld/jupyter_latex_envs_1614809104175/work
jupyter-nbextensions-configurator @ file:///D:/bld/jupyter_nbextensions_configurator_1611341297787/work
jupyter-server @ file:///opt/conda/conda-bld/jupyter_server_1644494914632/work
jupyterlab @ file:///opt/conda/conda-bld/jupyterlab_1647445413472/work
jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work
jupyterlab-server @ file:///opt/conda/conda-bld/jupyterlab_server_1644500396812/work
jupyterlab-widgets @ file:///tmp/build/80754af9/jupyterlab_widgets_1609884341231/work
kaleido==0.2.1
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.2
keyring @ file:///C:/ci/keyring_1638531661481/work
kiwisolver @ file:///C:/ci/kiwisolver_1644962567532/work
koalas @ file:///home/conda/feedstock_root/build_artifacts/koalas_1634690075417/work
kombu @ file:///D:/bld/kombu_1646562331815/work
korean-lunar-calendar @ file:///home/conda/feedstock_root/build_artifacts/korean_lunar_calendar_1589354365811/work
langcodes @ file:///home/conda/feedstock_root/build_artifacts/langcodes_1636741340529/work
lazy-object-proxy @ file:///C:/ci/lazy-object-proxy_1616529307648/work
libarchive-c==2.9
linearmodels==4.31
llvmlite==0.37.0
locket @ file:///C:/ci/locket_1647006279754/work
Logbook==1.6.0
lru-dict==1.2.0
LunarCalendar==0.0.9
lxml==4.9.3
lz4 @ file:///C:/ci/lz4_1619516660854/work
Mako==1.2.4
Markdown==3.4.4
MarkupSafe==2.1.3
matplotlib==3.7.2
matplotlib-inline @ file:///tmp/build/80754af9/matplotlib-inline_1628242447089/work
mccabe==0.6.1
menuinst @ file:///C:/ci/menuinst_1631733428175/work
MetaTrader5==5.0.45
missingno==0.4.2
mistune==0.8.4
mkl-fft==1.3.0
mkl-random==1.1.0
mkl-service==2.3.0
mock @ file:///tmp/build/80754af9/mock_1607622725907/work
mockextras==1.0.2
modin @ file:///D:/bld/modin_1615928016565/work
more-itertools @ file:///tmp/build/80754af9/more-itertools_1637733554872/work
mpmath==1.2.1
msgpack @ file:///C:/ci/msgpack-python_1612287368835/work
multidict @ file:///C:/ci/multidict_1607362065515/work
multipledispatch==0.6.0
multiprocess @ file:///D:/bld/multiprocess_1635876102526/work
multitasking==0.0.11
munkres==1.1.4
murmurhash @ file:///D:/bld/murmurhash_1636019767292/work
mypy-extensions==0.4.3
nbclassic @ file:///opt/conda/conda-bld/nbclassic_1644943264176/work
nbclient @ file:///tmp/build/80754af9/nbclient_1645431659072/work
nbconvert @ file:///D:/bld/nbconvert_1647040776267/work
nbformat @ file:///tmp/build/80754af9/nbformat_1617383369282/work
nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1613680548246/work
netCDF4==1.5.3
networkx @ file:///opt/conda/conda-bld/networkx_1647437648384/work
newspaper3k @ file:///D:/bld/newspaper3k_1602639957269/work
nltk @ file:///opt/conda/conda-bld/nltk_1645628263994/work
nose==1.3.7
notebook @ file:///C:/ci/notebook_1645002756862/work
numba @ file:///C:/ci/numba_1635186101448/work
numexpr @ file:///C:/ci/numexpr_1614798525447/work
numpy==1.21.0
numpydoc @ file:///opt/conda/conda-bld/numpydoc_1643788541039/work
oauthlib==3.2.2
olefile==0.46
openpyxl @ file:///tmp/build/80754af9/openpyxl_1632777717936/work
opt-einsum==3.3.0
osqp==0.6.3
outcome==1.2.0
packaging @ file:///tmp/build/80754af9/packaging_1637314298585/work
pandas==2.0.3
pandas-datareader==0.10.0
pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work
panel @ file:///home/conda/feedstock_root/build_artifacts/panel_1639076155277/work
param @ file:///home/conda/feedstock_root/build_artifacts/param_1634834173834/work
paramiko @ file:///opt/conda/conda-bld/paramiko_1640109032755/work
parse==1.19.0
parsel==1.8.1
parso==0.7.0
partd @ file:///opt/conda/conda-bld/partd_1647245470509/work
path @ file:///opt/conda/conda-bld/path_1641578212155/work
pathlib2 @ file:///C:/ci/pathlib2_1625585790401/work
pathos @ file:///home/conda/feedstock_root/build_artifacts/pathos_1623937754918/work
pathspec==0.7.0
pathtools==0.1.2
pathy @ file:///home/conda/feedstock_root/build_artifacts/pathy_1635227809952/work
patsy==0.5.3
pdfkit==1.0.0
pdfminer.six==20211012
pep8==1.7.1
pexpect==4.8.0
pickleshare==0.7.5
Pillow==7.2.0
pkginfo @ file:///tmp/build/80754af9/pkginfo_1643162084911/work
platformdirs @ file:///tmp/build/80754af9/platformdirs_1638968569495/work
plotly @ file:///home/conda/feedstock_root/build_artifacts/plotly_1644503417459/work
pluggy @ file:///C:/ci/pluggy_1633697237830/work
ply==3.11
pox @ file:///home/conda/feedstock_root/build_artifacts/pox_1623773830989/work
ppft @ file:///home/conda/feedstock_root/build_artifacts/ppft_1623774454681/work
ppscore @ file:///home/conda/feedstock_root/build_artifacts/ppscore_1610419476864/work
preshed @ file:///D:/bld/preshed_1636077851980/work
progressbar==2.5
progressbar2 @ file:///home/conda/feedstock_root/build_artifacts/progressbar2_1643742461370/work
prometheus-client @ file:///opt/conda/conda-bld/prometheus_client_1643788673601/work
prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1633440160888/work
property-cached==1.6.4
Protego==0.3.0
protobuf==3.20.3
psutil @ file:///C:/ci/psutil_1612298324802/work
ptyprocess @ file:///home/conda/feedstock_root/build_artifacts/ptyprocess_1609419310487/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl
pure-eval @ file:///opt/conda/conda-bld/pure_eval_1646925070566/work
py @ file:///opt/conda/conda-bld/py_1644396412707/work
py-cpuinfo==9.0.0
py4j @ file:///home/conda/feedstock_root/build_artifacts/py4j_1639545792025/work
pyarrow==7.0.0
pyasn1==0.5.0
pyasn1-modules==0.3.0
pycodestyle==2.6.0
pycosat==0.6.3
pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work
pyct @ file:///C:/ci/pyct_1600458386426/work
pycurl==7.44.1
pydantic @ file:///D:/bld/pydantic_1636021198197/work
PyDispatcher==2.0.7
pydocstyle @ file:///tmp/build/80754af9/pydocstyle_1621600989141/work
pydub==0.25.1
pyee==8.2.2
pyemd @ file:///D:/bld/pyemd_1636478530592/work
pyflakes==2.2.0
pyfolio==0.9.2
Pygments @ file:///opt/conda/conda-bld/pygments_1644249106324/work
pyhdfe==0.2.0
PyJWT==1.7.1
pykalman==0.9.5
pyLDAvis @ file:///home/conda/feedstock_root/build_artifacts/pyldavis_1616598811607/work
pylint @ file:///C:/ci/pylint_1639650750934/work
pyls-black @ file:///tmp/build/80754af9/pyls-black_1607553132291/work
pyls-spyder @ file:///tmp/build/80754af9/pyls-spyder_1613849700860/work
pyluach==2.2.0
PyMeeus @ file:///tmp/build/80754af9/pymeeus_1634069098549/work
pymongo==4.0.2
PyNaCl @ file:///C:/ci/pynacl_1595000047588/work
pyodbc @ file:///C:/ci/pyodbc_1647426060948/work
pyOpenSSL @ file:///opt/conda/conda-bld/pyopenssl_1643788558760/work
pyparsing @ file:///tmp/build/80754af9/pyparsing_1635766073266/work
pyphen @ file:///home/conda/feedstock_root/build_artifacts/pyphen_1640641668581/work
pyportfolioopt==1.5.5
pyppeteer==1.0.2
pyproj @ file:///D:/bld/pyproj_1646811431781/work
PyQt5==5.12.3
PyQt5_sip==4.19.18
PyQtChart==5.12
PyQtWebEngine==5.12.1
pyquery==1.4.3
pyreadline==2.1
pyrsistent @ file:///C:/ci/pyrsistent_1636111468851/work
PySocks==1.7.1
pyspark @ file:///home/conda/feedstock_root/build_artifacts/pyspark_1643189326666/work
pystan==2.19.1.1
pytesseract @ file:///home/conda/feedstock_root/build_artifacts/pytesseract_1647306555263/work
pytest==6.2.5
python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work
python-docx==0.8.11
python-engineio==3.9.0
python-interface==1.6.1
python-jsonrpc-server @ file:///tmp/build/80754af9/python-jsonrpc-server_1600278539111/work
python-language-server @ file:///tmp/build/80754af9/python-language-server_1607972495879/work
python-snappy @ file:///D:/bld/python-snappy_1636127269385/work
python-socketio==4.4.0
python-utils @ file:///home/conda/feedstock_root/build_artifacts/python-utils_1642203277698/work
pythreejs @ file:///home/conda/feedstock_root/build_artifacts/pythreejs_1614691447028/work
pytz==2023.3
pytz-deprecation-shim==0.1.0.post0
pyviz-comms @ file:///home/conda/feedstock_root/build_artifacts/pyviz_comms_1625215435439/work
PyWavelets @ file:///C:/ci/pywavelets_1601658407916/work
pywin32==302
pywin32-ctypes==0.2.0
pywinpty @ file:///C:/ci_310/pywinpty_1644230983541/work/target/wheels/pywinpty-2.0.2-cp38-none-win_amd64.whl
PyYAML==6.0
pyzmq @ file:///C:/ci/pyzmq_1638435185959/work
QDarkStyle==2.8.1
qdldl==0.1.7.post0
QtAwesome @ file:///tmp/build/80754af9/qtawesome_1637160816833/work
qtconsole @ file:///opt/conda/conda-bld/qtconsole_1643819126524/work
QtPy @ file:///opt/conda/conda-bld/qtpy_1643087291789/work
Quandl==3.7.0
queuelib==1.6.2
ray==1.6.0
redis @ file:///home/conda/feedstock_root/build_artifacts/redis-py_1645010473018/work
regex @ file:///C:/ci/regex_1629302316714/work
requests==2.31.0
requests-async==0.6.2
requests-file @ file:///home/conda/feedstock_root/build_artifacts/requests-file_1591227832914/work
requests-html==0.10.0
requests-oauthlib==1.3.1
retrying==1.3.3
rfc3986 @ file:///home/conda/feedstock_root/build_artifacts/rfc3986_1620442452971/work
rich @ file:///home/conda/feedstock_root/build_artifacts/rich_1646937619122/work
rise==5.7.1
rope @ file:///opt/conda/conda-bld/rope_1643788605236/work
rsa==4.9
Rtree @ file:///C:/ci/rtree_1618421009405/work
ruamel-yaml-conda @ file:///C:/ci/ruamel_yaml_1616016967756/work
s3transfer @ file:///home/conda/feedstock_root/build_artifacts/s3transfer_1645745825648/work
sacremoses @ file:///home/conda/feedstock_root/build_artifacts/sacremoses_1647361442468/work
scikit-image==0.18.3
scikit-learn @ file:///C:/ci/scikit-learn_1642599125707/work
scipy==1.10.1
Scrapy==2.10.0
scs==3.2.3
seaborn==0.11.1
seasonal==0.3.1
selenium==4.11.2
Send2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work
sentencepiece==0.1.96
service-identity==23.1.0
setproctitle @ file:///D:/bld/setproctitle_1610127780928/work
setuptools-git==1.2
setuptools-scm==7.1.0
sgmllib3k @ file:///home/conda/feedstock_root/build_artifacts/sgmllib3k_1600021450347/work
Shapely==1.8.0
shellingham @ file:///home/conda/feedstock_root/build_artifacts/shellingham_1612179560728/work
simplegeneric==0.8.1
singledispatch @ file:///tmp/build/80754af9/singledispatch_1629321204894/work
sip==4.19.25
six==1.15.0
sklearn==0.0
smart-open @ file:///home/conda/feedstock_root/build_artifacts/smart_open_1630238320325/work
sniffio @ file:///C:/ci/sniffio_1614030707456/work
snowballstemmer @ file:///tmp/build/80754af9/snowballstemmer_1637937080595/work
sortedcollections @ file:///tmp/build/80754af9/sortedcollections_1611172717284/work
sortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work
soupsieve==2.4.1
spacy @ file:///D:/bld/spacy_1644658342625/work
spacy-legacy @ file:///home/conda/feedstock_root/build_artifacts/spacy-legacy_1645713043381/work
spacy-loggers @ file:///home/conda/feedstock_root/build_artifacts/spacy-loggers_1634809367310/work
SpeechRecognition==3.10.0
Sphinx @ file:///tmp/build/80754af9/sphinx_1616268783226/work
sphinxcontrib-applehelp==1.0.2
sphinxcontrib-devhelp==1.0.2
sphinxcontrib-htmlhelp @ file:///tmp/build/80754af9/sphinxcontrib-htmlhelp_1623945626792/work
sphinxcontrib-jsmath==1.0.1
sphinxcontrib-qthelp==1.0.3
sphinxcontrib-serializinghtml @ file:///tmp/build/80754af9/sphinxcontrib-serializinghtml_1624451540180/work
sphinxcontrib-websupport @ file:///tmp/build/80754af9/sphinxcontrib-websupport_1597081412696/work
spyder @ file:///C:/ci/spyder_1616776239898/work
spyder-kernels @ file:///C:/ci/spyder-kernels_1614030842607/work
SQLAlchemy==2.0.19
squarify==0.4.3
srsly @ file:///D:/bld/srsly_1638879736505/work
stack-data @ file:///opt/conda/conda-bld/stack_data_1646927590127/work
starlette @ file:///home/conda/feedstock_root/build_artifacts/starlette-recipe_1641778727195/work
statsmodels==0.14.0
streamz @ file:///home/conda/feedstock_root/build_artifacts/streamz_1633363454258/work
strsimpy @ file:///home/conda/feedstock_root/build_artifacts/strsimpy_1602624469551/work
sympy @ file:///C:/ci/sympy_1635237204453/work
ta==0.10.2
TA-Lib @ file:///D:/bld/ta-lib_1637267093594/work
tables==3.8.0
tabula-py @ file:///C:/ci/tabula-py_1637055935735/work
tabulate @ file:///home/conda/feedstock_root/build_artifacts/tabulate_1614001031686/work
tblib @ file:///tmp/build/80754af9/tblib_1597928476713/work
tenacity @ file:///home/conda/feedstock_root/build_artifacts/tenacity_1626090218611/work
tensorboard @ file:///tmp/build/80754af9/tensorboard_1633093581375/work/tensorboard-2.6.0-py3-none-any.whl
tensorboard-data-server @ file:///C:/ci/tensorboard-data-server_1633035228378/work/tensorboard_data_server-0.6.0-py3-none-any.whl
tensorboard-plugin-wit==1.8.1
tensorflow==2.3.0
tensorflow-estimator @ file:///home/builder/adipietro/tf/tensorflow-estimator_1630508970172/work/tensorflow_estimator-2.6.0-py2.py3-none-any.whl
termcolor==1.1.0
terminado @ file:///C:/ci/terminado_1644322757089/work
testpath @ file:///tmp/build/80754af9/testpath_1624638946665/work
textacy==0.10.0
textblob==0.15.3
textdistance @ file:///tmp/build/80754af9/textdistance_1612461398012/work
thinc @ file:///D:/bld/thinc_1638980369428/work
threadpoolctl==3.2.0
three-merge @ file:///tmp/build/80754af9/three-merge_1607553261110/work
tifffile==2020.10.1
tldextract @ file:///home/conda/feedstock_root/build_artifacts/tldextract_1645411981621/work
tokenizers @ file:///D:/bld/tokenizers_1632285922127/work
toml @ file:///tmp/build/80754af9/toml_1616166611790/work
tomli==2.0.1
toolz @ file:///tmp/build/80754af9/toolz_1636545406491/work
torch==1.11.0
torchvision==0.12.0
tornado @ file:///C:/ci/tornado_1606942392901/work
tqdm @ file:///opt/conda/conda-bld/tqdm_1647339053476/work
traitlets @ file:///tmp/build/80754af9/traitlets_1636710298902/work
traittypes @ file:///home/conda/feedstock_root/build_artifacts/traittypes_1600843364635/work
transformers @ file:///home/conda/feedstock_root/build_artifacts/transformers_1643665047793/work
trio==0.22.2
trio-websocket==0.10.3
Twisted==22.10.0
twisted-iocpsupport==1.0.4
twython==3.9.1
typed-ast @ file:///C:/ci/typed-ast_1624953785070/work
typer @ file:///home/conda/feedstock_root/build_artifacts/typer_1630326630489/work
typing_extensions==4.7.1
tzdata==2022.1
tzlocal==4.1
ujson @ file:///C:/ci/ujson_1611241570789/work
unicodecsv==0.14.1
urllib3 @ file:///opt/conda/conda-bld/urllib3_1643638302206/work
vaderSentiment==3.3.2
vaex-astro @ file:///home/conda/feedstock_root/build_artifacts/vaex-astro_1631339901381/work
vaex-core @ file:///D:/bld/vaex-core_1639773909708/work
vaex-hdf5 @ file:///home/conda/feedstock_root/build_artifacts/vaex-hdf5_1640686607215/work
vaex-jupyter @ file:///home/conda/feedstock_root/build_artifacts/vaex-jupyter_1640686696974/work
vaex-ml @ file:///home/conda/feedstock_root/build_artifacts/vaex-ml_1640688153658/work
vaex-server @ file:///home/conda/feedstock_root/build_artifacts/vaex-server_1644306057562/work
vaex-viz @ file:///home/conda/feedstock_root/build_artifacts/vaex-viz_1640688182221/work
vine @ file:///home/conda/feedstock_root/build_artifacts/vine_1604930048994/work
vispy @ file:///D:/bld/vispy_1644029990752/work
voila @ file:///home/conda/feedstock_root/build_artifacts/voila_1647001037282/work
w3lib==1.22.0
wasabi @ file:///home/conda/feedstock_root/build_artifacts/wasabi_1638865582891/work
watchdog @ file:///C:/ci/watchdog_1612471251191/work
wcwidth @ file:///tmp/build/80754af9/wcwidth_1593447189090/work
webencodings==0.5.1
websocket-client @ file:///C:/ci/websocket-client_1614804473297/work
websockets @ file:///D:/bld/websockets_1645531210500/work
Werkzeug==2.3.6
widgetsnbextension @ file:///C:/ci/widgetsnbextension_1645009558218/work
win-inet-pton==1.1.0
win-unicode-console==0.5
wincertstore==0.2
wordcloud==1.8.1
wrapt==1.12.1
wsproto==1.2.0
xarray @ file:///home/conda/feedstock_root/build_artifacts/xarray_1646254587173/work
xlrd @ file:///tmp/build/80754af9/xlrd_1608072521494/work
XlsxWriter @ file:///tmp/build/80754af9/xlsxwriter_1636633762820/work
xlwings==0.23.0
xlwt==1.3.0
xyzservices @ file:///home/conda/feedstock_root/build_artifacts/xyzservices_1646849493349/work
yapf @ file:///tmp/build/80754af9/yapf_1615749224965/work
yarl @ file:///C:/ci/yarl_1606940076464/work
yellowbrick==1.5
yfinance==0.2.27
zict==2.0.0
zipline-reloaded==3.0.3
zipp @ file:///opt/conda/conda-bld/zipp_1641824620731/work
zope.event==4.5.0
zope.interface==6.0
",lyubomir-vasilev,130079626,closed,False,1,2023-08-25T22:06:41+00:00,2024-09-23T10:49:15+00:00,2024-09-23T10:49:15+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1857153460,303,back testing is not working ,"Hi,

 it looks like zipline back testing is not working. Code from chp 4 and 5 cannot execute and throw errors. anyone would help ?",AkshayVaghani,20482617,closed,False,2,2023-08-18T18:51:36+00:00,2024-09-23T10:50:03+00:00,2024-09-23T10:50:03+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1837184780,302,Daily historical return deciles,"
by_sym = prices.groupby(level='symbol').close
for t in T:
    prices[f'r{t:02}'] = by_sym.pct_change(t)
by_sym = prices.groupby(level='symbol').close
for t in T:
    prices[f'r{t:02}'] = by_sym.pct_change(t)
for t in T:
    prices[f'r{t:02}dec'] = (prices[f'r{t:02}']
                             .groupby(level='date')
                             .apply(lambda x: pd.qcut(x, 
                                                      q=10, 
                                                      labels=False, 
                                                      duplicates='drop')))
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[45], line 2
      1 for t in T:
----> 2     prices[f'r{t:02}dec'] = (prices[f'r{t:02}']
      3                              .groupby(level='date')
      4                              .apply(lambda x: pd.qcut(x, 
      5                                                       q=10, 
      6                                                       labels=False, 
      7                                                       duplicates='drop')))

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\groupby\generic.py:216, in SeriesGroupBy.apply(self, func, *args, **kwargs)
    210 @Appender(
    211     _apply_docs[""template""].format(
    212         input=""series"", examples=_apply_docs[""series_examples""]
    213     )
    214 )
    215 def apply(self, func, *args, **kwargs) -> Series:
--> 216     return super().apply(func, *args, **kwargs)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\groupby\groupby.py:1353, in GroupBy.apply(self, func, *args, **kwargs)
   1351 with option_context(""mode.chained_assignment"", None):
   1352     try:
-> 1353         result = self._python_apply_general(f, self._selected_obj)
   1354     except TypeError:
   1355         # gh-20949
   1356         # try again, with .apply acting as a filtering
   (...)
   1360         # fails on *some* columns, e.g. a numeric operation
   1361         # on a string grouper column
   1363         return self._python_apply_general(f, self._obj_with_exclusions)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\groupby\groupby.py:1402, in GroupBy._python_apply_general(self, f, data, not_indexed_same, is_transform, is_agg)
   1367 @final
   1368 def _python_apply_general(
   1369     self,
   (...)
   1374     is_agg: bool = False,
   1375 ) -> NDFrameT:
   1376     """"""
   1377     Apply function f in python space
   1378 
   (...)
   1400         data after applying f
   1401     """"""
-> 1402     values, mutated = self.grouper.apply(f, data, self.axis)
   1403     if not_indexed_same is None:
   1404         not_indexed_same = mutated

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\groupby\ops.py:767, in BaseGrouper.apply(self, f, data, axis)
    765 # group might be modified
    766 group_axes = group.axes
--> 767 res = f(group)
    768 if not mutated and not _is_indexed_like(res, group_axes, axis):
    769     mutated = True

Cell In[45], line 4, in <lambda>(x)
      1 for t in T:
      2     prices[f'r{t:02}dec'] = (prices[f'r{t:02}']
      3                              .groupby(level='date')
----> 4                              .apply(lambda x: pd.qcut(x, 
      5                                                       q=10, 
      6                                                       labels=False, 
      7                                                       duplicates='drop')))

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\reshape\tile.py:377, in qcut(x, q, labels, retbins, precision, duplicates)
    375 x_np = np.asarray(x)
    376 x_np = x_np[~np.isnan(x_np)]
--> 377 bins = np.quantile(x_np, quantiles)
    379 fac, bins = _bins_to_cuts(
    380     x,
    381     bins,
   (...)
    386     duplicates=duplicates,
    387 )
    389 return _postprocess_for_cut(fac, bins, retbins, dtype, original)

File <__array_function__ internals>:180, in quantile(*args, **kwargs)

File ~\anaconda3\envs\baclass\lib\site-packages\numpy\lib\function_base.py:4371, in quantile(a, q, axis, out, overwrite_input, method, keepdims, interpolation)
   4369 if not _quantile_is_valid(q):
   4370     raise ValueError(""Quantiles must be in the range [0, 1]"")
-> 4371 return _quantile_unchecked(
   4372     a, q, axis, out, overwrite_input, method, keepdims)

File ~\anaconda3\envs\baclass\lib\site-packages\numpy\lib\function_base.py:4383, in _quantile_unchecked(a, q, axis, out, overwrite_input, method, keepdims)
   4375 def _quantile_unchecked(a,
   4376                         q,
   4377                         axis=None,
   (...)
   4380                         method=""linear"",
   4381                         keepdims=False):
   4382     """"""Assumes that q is in [0, 1], and is an ndarray""""""
-> 4383     r, k = _ureduce(a,
   4384                     func=_quantile_ureduce_func,
   4385                     q=q,
   4386                     axis=axis,
   4387                     out=out,
   4388                     overwrite_input=overwrite_input,
   4389                     method=method)
   4390     if keepdims:
   4391         return r.reshape(q.shape + k)

File ~\anaconda3\envs\baclass\lib\site-packages\numpy\lib\function_base.py:3702, in _ureduce(a, func, **kwargs)
   3699 else:
   3700     keepdim = (1,) * a.ndim
-> 3702 r = func(a, **kwargs)
   3703 return r, keepdim

File ~\anaconda3\envs\baclass\lib\site-packages\numpy\lib\function_base.py:4552, in _quantile_ureduce_func(a, q, axis, out, overwrite_input, method)
   4550     else:
   4551         arr = a.copy()
-> 4552 result = _quantile(arr,
   4553                    quantiles=q,
   4554                    axis=axis,
   4555                    method=method,
   4556                    out=out)
   4557 return result

File ~\anaconda3\envs\baclass\lib\site-packages\numpy\lib\function_base.py:4658, in _quantile(arr, quantiles, axis, method, out)
   4650 arr.partition(
   4651     np.unique(np.concatenate(([0, -1],
   4652                               previous_indexes.ravel(),
   4653                               next_indexes.ravel(),
   4654                               ))),
   4655     axis=DATA_AXIS)
   4656 if np.issubdtype(arr.dtype, np.inexact):
   4657     slices_having_nans = np.isnan(
-> 4658         take(arr, indices=-1, axis=DATA_AXIS)
   4659     )
   4660 else:
   4661     slices_having_nans = None

File <__array_function__ internals>:180, in take(*args, **kwargs)

File ~\anaconda3\envs\baclass\lib\site-packages\numpy\core\fromnumeric.py:190, in take(a, indices, axis, out, mode)
     93 @array_function_dispatch(_take_dispatcher)
     94 def take(a, indices, axis=None, out=None, mode='raise'):
     95     """"""
     96     Take elements from an array along an axis.
     97 
   (...)
    188            [5, 7]])
    189     """"""
--> 190     return _wrapfunc(a, 'take', indices, axis=axis, out=out, mode=mode)

File ~\anaconda3\envs\baclass\lib\site-packages\numpy\core\fromnumeric.py:57, in _wrapfunc(obj, method, *args, **kwds)
     54     return _wrapit(obj, method, *args, **kwds)
     56 try:
---> 57     return bound(*args, **kwds)
     58 except TypeError:
     59     # A TypeError occurs if the object does have such a method in its
     60     # class, but its signature is not identical to that of NumPy's. This
   (...)
     64     # Call _wrapit from within the except clause to ensure a potential
     65     # exception has a traceback chain.
     66     return _wrapit(obj, method, *args, **kwds)

IndexError: cannot do a non-empty take from an empty axes.
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment**
If you are not using the latest version of the Docker imag:
 
- OS: [e.g. MacOSX, Windows, Ubuntu]
 - Version [e.g. Big Sur, 10, 20.04]

**Additional context**
Add any other context about the problem here.
",ishikabansal77,133491760,closed,False,2,2023-08-04T18:35:32+00:00,2024-09-23T10:51:19+00:00,2024-09-23T10:51:18+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1837182134,301,rsi,"
prices['rsi'] = prices.groupby(level='symbol').close.apply(RSI)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\frame.py:11610, in _reindex_for_setitem(value, index)
  11609 try:
> 11610     reindexed_value = value.reindex(index)._values
  11611 except ValueError as err:
  11612     # raised in MultiIndex.from_tuples, see test_insert_error_msmgs

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\series.py:4918, in Series.reindex(self, index, axis, method, copy, level, fill_value, limit, tolerance)
   4901 @doc(
   4902     NDFrame.reindex,  # type: ignore[has-type]
   4903     klass=_shared_doc_kwargs[""klass""],
   (...)
   4916     tolerance=None,
   4917 ) -> Series:
-> 4918     return super().reindex(
   4919         index=index,
   4920         method=method,
   4921         copy=copy,
   4922         level=level,
   4923         fill_value=fill_value,
   4924         limit=limit,
   4925         tolerance=tolerance,
   4926     )

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\generic.py:5360, in NDFrame.reindex(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)
   5359 # perform the reindex on the axes
-> 5360 return self._reindex_axes(
   5361     axes, level, limit, tolerance, method, fill_value, copy
   5362 ).__finalize__(self, method=""reindex"")

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\generic.py:5375, in NDFrame._reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)
   5374 ax = self._get_axis(a)
-> 5375 new_index, indexer = ax.reindex(
   5376     labels, level=level, limit=limit, tolerance=tolerance, method=method
   5377 )
   5379 axis = self._get_axis_number(a)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:4268, in Index.reindex(self, target, method, level, limit, tolerance)
   4267 if self._index_as_unique:
-> 4268     indexer = self.get_indexer(
   4269         target, method=method, limit=limit, tolerance=tolerance
   4270     )
   4271 elif self._is_multi:

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:3802, in Index.get_indexer(self, target, method, limit, tolerance)
   3798     return this._get_indexer(
   3799         target, method=method, limit=limit, tolerance=tolerance
   3800     )
-> 3802 return self._get_indexer(target, method, limit, tolerance)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:3823, in Index._get_indexer(self, target, method, limit, tolerance)
   3821     # error: Item ""IndexEngine"" of ""Union[IndexEngine, ExtensionEngine]""
   3822     # has no attribute ""_extract_level_codes""
-> 3823     tgt_values = engine._extract_level_codes(  # type: ignore[union-attr]
   3824         target
   3825     )
   3826 else:

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\_libs\index.pyx:714, in pandas._libs.index.BaseMultiIndexCodesEngine._extract_level_codes()

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\multi.py:143, in MultiIndexUIntEngine._codes_to_ints(self, codes)
    141 # Shift the representation of each level by the pre-calculated number
    142 # of bits:
--> 143 codes <<= self.offsets
    145 # Now sum and OR are in fact interchangeable. This is a simple
    146 # composition of the (disjunct) significant bits of each level (i.e.
    147 # each column in ""codes"") in a single positive integer:

ValueError: operands could not be broadcast together with shapes (2004775,2) (3,) (2004775,2) 

The above exception was the direct cause of the following exception:

TypeError                                 Traceback (most recent call last)
Cell In[44], line 1
----> 1 prices['rsi'] = prices.groupby(level='symbol').close.apply(RSI)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\frame.py:3950, in DataFrame.__setitem__(self, key, value)
   3947     self._setitem_array([key], value)
   3948 else:
   3949     # set column
-> 3950     self._set_item(key, value)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\frame.py:4143, in DataFrame._set_item(self, key, value)
   4133 def _set_item(self, key, value) -> None:
   4134     """"""
   4135     Add series to DataFrame in specified column.
   4136 
   (...)
   4141     ensure homogeneity.
   4142     """"""
-> 4143     value = self._sanitize_column(value)
   4145     if (
   4146         key in self.columns
   4147         and value.ndim == 1
   4148         and not is_extension_array_dtype(value)
   4149     ):
   4150         # broadcast across multiple columns if necessary
   4151         if not self.columns.is_unique or isinstance(self.columns, MultiIndex):

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\frame.py:4867, in DataFrame._sanitize_column(self, value)
   4865     return _reindex_for_setitem(value, self.index)
   4866 elif is_dict_like(value):
-> 4867     return _reindex_for_setitem(Series(value), self.index)
   4869 if is_list_like(value):
   4870     com.require_length_match(value, self.index)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\frame.py:11617, in _reindex_for_setitem(value, index)
  11613     if not value.index.is_unique:
  11614         # duplicate axis
  11615         raise err
> 11617     raise TypeError(
  11618         ""incompatible index of inserted column with frame index""
  11619     ) from err
  11620 return reindexed_value

TypeError: incompatible index of inserted column with frame index

4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment**
If you are not using the latest version of the Docker imag:
 
- OS: [e.g. MacOSX, Windows, Ubuntu]
 - Version [e.g. Big Sur, 10, 20.04]

**Additional context**
Add any other context about the problem here.
",ishikabansal77,133491760,closed,False,2,2023-08-04T18:33:30+00:00,2024-09-23T10:53:15+00:00,2024-09-23T10:53:14+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1834726142,300,"problem in Long-Short Strategy, Part 1: Preparing Alpha Factors and Features","
def compute_bb(close):
    high, mid, low = BBANDS(close, timeperiod=20)
    return pd.DataFrame({'bb_high': high, 'bb_low': low}, index=close.index)

def compute_bb(close):
    high, mid, low = BBANDS(close, timeperiod=20)
    return pd.DataFrame({'bb_high': high, 'bb_low': low}, index=close.index)
​
bb_data = prices.groupby(level='symbol')['close'].apply(compute_bb)
prices = pd.concat([prices, bb_data], axis=1)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[33], line 1
----> 1 prices = pd.concat([prices, bb_data], axis=1)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\reshape\concat.py:372, in concat(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)
    369 elif copy and using_copy_on_write():
    370     copy = False
--> 372 op = _Concatenator(
    373     objs,
    374     axis=axis,
    375     ignore_index=ignore_index,
    376     join=join,
    377     keys=keys,
    378     levels=levels,
    379     names=names,
    380     verify_integrity=verify_integrity,
    381     copy=copy,
    382     sort=sort,
    383 )
    385 return op.get_result()

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\reshape\concat.py:563, in _Concatenator.__init__(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)
    560 self.verify_integrity = verify_integrity
    561 self.copy = copy
--> 563 self.new_axes = self._get_new_axes()

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\reshape\concat.py:633, in _Concatenator._get_new_axes(self)
    631 def _get_new_axes(self) -> list[Index]:
    632     ndim = self._get_result_dim()
--> 633     return [
    634         self._get_concat_axis if i == self.bm_axis else self._get_comb_axis(i)
    635         for i in range(ndim)
    636     ]

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\reshape\concat.py:634, in <listcomp>(.0)
    631 def _get_new_axes(self) -> list[Index]:
    632     ndim = self._get_result_dim()
    633     return [
--> 634         self._get_concat_axis if i == self.bm_axis else self._get_comb_axis(i)
    635         for i in range(ndim)
    636     ]

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\reshape\concat.py:640, in _Concatenator._get_comb_axis(self, i)
    638 def _get_comb_axis(self, i: AxisInt) -> Index:
    639     data_axis = self.objs[0]._get_block_manager_axis(i)
--> 640     return get_objs_combined_axis(
    641         self.objs,
    642         axis=data_axis,
    643         intersect=self.intersect,
    644         sort=self.sort,
    645         copy=self.copy,
    646     )

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\api.py:95, in get_objs_combined_axis(objs, intersect, axis, sort, copy)
     71 """"""
     72 Extract combined index: return intersection or union (depending on the
     73 value of ""intersect"") of indexes on given axis, or None if all objects
   (...)
     92 Index
     93 """"""
     94 obs_idxes = [obj._get_axis(axis) for obj in objs]
---> 95 return _get_combined_index(obs_idxes, intersect=intersect, sort=sort, copy=copy)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\api.py:148, in _get_combined_index(indexes, intersect, sort, copy)
    146         index = index.intersection(other)
    147 else:
--> 148     index = union_indexes(indexes, sort=False)
    149     index = ensure_index(index)
    151 if sort:

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\api.py:293, in union_indexes(indexes, sort)
    290         result = indexes[0]
    292     for other in indexes[1:]:
--> 293         result = result.union(other, sort=None if sort else False)
    294     return result
    296 elif kind == ""array"":

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:3200, in Index.union(self, other, sort)
   3197         return result.sort_values()
   3198     return result
-> 3200 result = self._union(other, sort=sort)
   3202 return self._wrap_setop_result(other, result)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\multi.py:3546, in MultiIndex._union(self, other, sort)
   3541     return MultiIndex.from_arrays(
   3542         zip(*result), sortorder=None, names=result_names
   3543     )
   3545 else:
-> 3546     right_missing = other.difference(self, sort=False)
   3547     if len(right_missing):
   3548         result = self.append(right_missing)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:3501, in Index.difference(self, other, sort)
   3498         return result.sort_values()
   3499     return result
-> 3501 result = self._difference(other, sort=sort)
   3502 return self._wrap_difference_result(other, result)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:3509, in Index._difference(self, other, sort)
   3504 def _difference(self, other, sort):
   3505     # overridden by RangeIndex
   3507     this = self.unique()
-> 3509     indexer = this.get_indexer_for(other)
   3510     indexer = indexer.take((indexer != -1).nonzero()[0])
   3512     label_diff = np.setdiff1d(np.arange(this.size), indexer, assume_unique=True)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:5859, in Index.get_indexer_for(self, target)
   5841 """"""
   5842 Guaranteed return of an indexer even when non-unique.
   5843 
   (...)
   5856 array([0, 2])
   5857 """"""
   5858 if self._index_as_unique:
-> 5859     return self.get_indexer(target)
   5860 indexer, _ = self.get_indexer_non_unique(target)
   5861 return indexer

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:3802, in Index.get_indexer(self, target, method, limit, tolerance)
   3797     target = target.astype(dtype, copy=False)
   3798     return this._get_indexer(
   3799         target, method=method, limit=limit, tolerance=tolerance
   3800     )
-> 3802 return self._get_indexer(target, method, limit, tolerance)

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\base.py:3823, in Index._get_indexer(self, target, method, limit, tolerance)
   3820     engine = self._engine
   3821     # error: Item ""IndexEngine"" of ""Union[IndexEngine, ExtensionEngine]""
   3822     # has no attribute ""_extract_level_codes""
-> 3823     tgt_values = engine._extract_level_codes(  # type: ignore[union-attr]
   3824         target
   3825     )
   3826 else:
   3827     tgt_values = target._get_engine_target()

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\_libs\index.pyx:714, in pandas._libs.index.BaseMultiIndexCodesEngine._extract_level_codes()

File ~\anaconda3\envs\baclass\lib\site-packages\pandas\core\indexes\multi.py:143, in MultiIndexUIntEngine._codes_to_ints(self, codes)
    126 """"""
    127 Transform combination(s) of uint64 in one uint64 (each), in a strictly
    128 monotonic way (i.e. respecting the lexicographic order of integer
   (...)
    139     Integer(s) representing one combination (each).
    140 """"""
    141 # Shift the representation of each level by the pre-calculated number
    142 # of bits:
--> 143 codes <<= self.offsets
    145 # Now sum and OR are in fact interchangeable. This is a simple
    146 # composition of the (disjunct) significant bits of each level (i.e.
    147 # each column in ""codes"") in a single positive integer:
    148 if codes.ndim == 1:
    149     # Single key

ValueError: operands could not be broadcast together with shapes (2061238,2) (3,) (2061238,2)
",ishikabansal77,133491760,closed,False,1,2023-08-03T09:59:12+00:00,2023-08-04T14:43:58+00:00,2023-08-04T14:43:58+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1830828113,299,%load_ext zipline,"
----------------------------------------------------------------
ImportError                    Traceback (most recent call last)
Cell In[5], line 1
----> 1 get_ipython().run_line_magic('load_ext', 'zipline')

File ~\anaconda3\envs\baclass\lib\site-packages\IPython\core\interactiveshell.py:2369, in InteractiveShell.run_line_magic(self, magic_name, line, _stack_depth)
   2367     kwargs['local_ns'] = self.get_local_scope(stack_depth)
   2368 with self.builtin_trap:
-> 2369     result = fn(*args, **kwargs)
   2371 # The code below prevents the output from being displayed
   2372 # when using magics with decodator @output_can_be_silenced
   2373 # when the last Python token in the expression is a ';'.
   2374 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):

File ~\anaconda3\envs\baclass\lib\site-packages\IPython\core\magics\extension.py:33, in ExtensionMagics.load_ext(self, module_str)
     31 if not module_str:
     32     raise UsageError('Missing module name.')
---> 33 res = self.shell.extension_manager.load_extension(module_str)
     35 if res == 'already loaded':
     36     print(""The %s extension is already loaded. To reload it, use:"" % module_str)

File ~\anaconda3\envs\baclass\lib\site-packages\IPython\core\extensions.py:76, in ExtensionManager.load_extension(self, module_str)
     69 """"""Load an IPython extension by its module name.
     70 
     71 Returns the string ""already loaded"" if the extension is already loaded,
     72 ""no load function"" if the module doesn't have a load_ipython_extension
     73 function, or None if it succeeded.
     74 """"""
     75 try:
---> 76     return self._load_extension(module_str)
     77 except ModuleNotFoundError:
     78     if module_str in BUILTINS_EXTS:

File ~\anaconda3\envs\baclass\lib\site-packages\IPython\core\extensions.py:91, in ExtensionManager._load_extension(self, module_str)
     89 with self.shell.builtin_trap:
     90     if module_str not in sys.modules:
---> 91         mod = import_module(module_str)
     92     mod = sys.modules[module_str]
     93     if self._call_load_ipython_extension(mod):

File ~\anaconda3\envs\baclass\lib\importlib\__init__.py:127, in import_module(name, package)
    125             break
    126         level += 1
--> 127 return _bootstrap._gcd_import(name[level:], package, level)

File <frozen importlib._bootstrap>:1030, in _gcd_import(name, package, level)

File <frozen importlib._bootstrap>:1007, in _find_and_load(name, import_)

File <frozen importlib._bootstrap>:986, in _find_and_load_unlocked(name, import_)

File <frozen importlib._bootstrap>:680, in _load_unlocked(spec)

File <frozen importlib._bootstrap_external>:850, in exec_module(self, module)

File <frozen importlib._bootstrap>:228, in _call_with_frames_removed(f, *args, **kwds)

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\__init__.py:29
     27 from .utils.numpy_utils import numpy_version
     28 from .utils.pandas_utils import new_pandas
---> 29 from .utils.run_algo import run_algorithm
     31 # These need to happen after the other imports.
     32 from .algorithm import TradingAlgorithm

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\utils\run_algo.py:24
     22 from zipline.finance import metrics
     23 from zipline.finance.trading import SimulationParameters
---> 24 from zipline.pipeline.data import USEquityPricing
     25 from zipline.pipeline.loaders import USEquityPricingLoader
     27 import zipline.utils.paths as pth

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\pipeline\__init__.py:3
      1 from .classifiers import Classifier, CustomClassifier
      2 from .domain import Domain
----> 3 from .factors import Factor, CustomFactor
      4 from .filters import Filter, CustomFilter
      5 from .term import Term, LoadableTerm, ComputableTerm

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\pipeline\factors\__init__.py:7
      1 from .factor import (
      2     CustomFactor,
      3     Factor,
      4     Latest,
      5     RecarrayField,
      6 )
----> 7 from .basic import (
      8     AnnualizedVolatility,
      9     AverageDollarVolume,
     10     DailyReturns,
     11     EWMA,
     12     ExponentialWeightedMovingAverage,
     13     ExponentialWeightedMovingStdDev,
     14     EWMSTD,
     15     LinearWeightedMovingAverage,
     16     MaxDrawdown,
     17     PeerCount,
     18     PercentChange,
     19     Returns,
     20     SimpleMovingAverage,
     21     VWAP,
     22     WeightedAverageValue,
     23 )
     24 from .events import (
     25     BusinessDaysSincePreviousEvent,
     26     BusinessDaysUntilNextEvent,
     27 )
     28 from .statistical import (
     29     RollingPearson,
     30     RollingSpearman,
   (...)
     34     SimpleBeta,
     35 )

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\pipeline\factors\basic.py:21
      3 from numbers import Number
      4 from numpy import (
      5     arange,
      6     average,
   (...)
     18     errstate as np_errstate,
     19 )
---> 21 from zipline.pipeline.data import EquityPricing
     22 from zipline.utils.input_validation import expect_types
     23 from zipline.utils.math_utils import (
     24     nanargmax,
     25     nanmax,
   (...)
     28     nansum,
     29 )

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\pipeline\data\__init__.py:1
----> 1 from .equity_pricing import EquityPricing, USEquityPricing
      2 from .dataset import (
      3     BoundColumn,
      4     Column,
   (...)
      7     DataSetFamilySlice,
      8 )
     10 __all__ = [
     11     ""BoundColumn"",
     12     ""Column"",
   (...)
     17     ""USEquityPricing"",
     18 ]

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\pipeline\data\equity_pricing.py:7
      4 from zipline.utils.numpy_utils import float64_dtype, categorical_dtype
      6 from ..domain import US_EQUITIES
----> 7 from .dataset import Column, DataSet
     10 class EquityPricing(DataSet):
     11     """"""
     12     :class:`~zipline.pipeline.data.DataSet` containing daily trading prices and
     13     volumes.
     14     """"""

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\pipeline\data\dataset.py:10
      7 from toolz import first
      9 from zipline.currency import Currency
---> 10 from zipline.data.fx import DEFAULT_FX_RATE
     11 from zipline.pipeline.classifiers import Classifier, Latest as LatestClassifier
     12 from zipline.pipeline.domain import Domain, GENERIC

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\data\fx\__init__.py:4
      2 from .in_memory import InMemoryFXRateReader
      3 from .exploding import ExplodingFXRateReader
----> 4 from .hdf5 import HDF5FXRateReader, HDF5FXRateWriter
      6 __all__ = [
      7     ""DEFAULT_FX_RATE"",
      8     ""ExplodingFXRateReader"",
   (...)
     12     ""InMemoryFXRateReader"",
     13 ]

File ~\anaconda3\envs\baclass\lib\site-packages\zipline\data\fx\hdf5.py:96
      1 """"""
      2 HDF5-backed FX Rates
      3 
   (...)
     93 for column i in a data node is the ith element of /index/dts.
     94 """"""
     95 from interface import implements
---> 96 import h5py
     97 import logging
     98 import numpy as np

File ~\anaconda3\envs\baclass\lib\site-packages\h5py\__init__.py:33
     30     else:
     31         raise
---> 33 from . import version
     35 if version.hdf5_version_tuple != version.hdf5_built_version_tuple:
     36     _warn((""h5py is running against HDF5 {0} when it was built against {1}, ""
     37            ""this may cause problems"").format(
     38             '{0}.{1}.{2}'.format(*version.hdf5_version_tuple),
     39             '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)
     40     ))

File ~\anaconda3\envs\baclass\lib\site-packages\h5py\version.py:15
     10 """"""
     11     Versioning module for h5py.
     12 """"""
     14 from collections import namedtuple
---> 15 from . import h5 as _h5
     16 import sys
     17 import numpy

File h5py\h5.pyx:1, in init h5py.h5()

ImportError: DLL load failed while importing defs: The specified procedure could not be found.
",ishikabansal77,133491760,closed,False,1,2023-08-01T09:19:45+00:00,2023-08-04T14:36:35+00:00,2023-08-04T14:36:35+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1830512850,298,Facing issue while installing zipline for Windows,"I am facing issues while installing zipline to run codes in chapter 4.

",ishikabansal77,133491760,closed,False,1,2023-08-01T05:50:52+00:00,2023-08-04T14:34:43+00:00,2023-08-04T14:34:43+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1815861720,297,facing error while installing ta-lib,"!pip install ta-lib==0.4.21


Collecting ta-lib==0.4.21
  Downloading TA-Lib-0.4.21.tar.gz (270 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 270.1/270.1 kB 5.6 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ta-lib==0.4.21) (1.22.4)
Building wheels for collected packages: ta-lib
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  Building wheel for ta-lib (setup.py) ... error
  ERROR: Failed building wheel for ta-lib
  Running setup.py clean for ta-lib
Failed to build ta-lib
ERROR: Could not build wheels for ta-lib, which is required to install pyproject.toml-based projects


was not able to install ta-lib
<img width=""731"" alt=""Screenshot 2023-07-21 192532"" src=""https://github.com/stefan-jansen/machine-learning-for-trading/assets/133491760/6811e248-fa53-4821-8cb2-2bee628eba1c"">
",ishikabansal77,133491760,closed,False,2,2023-07-21T13:58:46+00:00,2023-08-04T14:37:30+00:00,2023-08-04T14:37:30+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1808806082,296,Chapter 2 notebook 1. Key error.,"**Describe the bug**
I have trouble with the keys that the last line of code in the notebook calls. Although I understand the logic behind the code the key simply does not apear to exist.
I've runed it on MacOs and Windows, obtaining in both the same resulsts. I've also changed the nasdaq file many times. I've also tried to acces ""P"" and ""Q"" in many other ways navegating the HDF file but always had little to no luck. Please help me.


KeyError                                  Traceback (most recent call last)
Cell In[17], line 5
      2     print(store.S.keys())
      4     stocks = store['R'].loc[:, ['stock_locate', 'stock']]
----> 5     trades = store['P'].append(store['Q'].rename(columns={'cross_price': 'price'}), sort=False).merge(stocks)
      7 trades['value'] = trades.shares.mul(trades.price)
      8 trades['value_share'] = trades.value.div(trades.value.sum())

File ~/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/pytables.py:596, in HDFStore.__getitem__(self, key)
    595 def __getitem__(self, key: str):
--> 596     return self.get(key)

File ~/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/pytables.py:790, in HDFStore.get(self, key)
    788 group = self.get_node(key)
    789 if group is None:
--> 790     raise KeyError(f""No object named {key} in the file"")
    791 return self._read_group(group)

KeyError: 'No object named P in the file'

",gonzalodes,128628677,closed,False,7,2023-07-17T23:56:05+00:00,2023-08-04T14:38:22+00:00,2023-08-04T14:38:21+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1806215900,295,MESSAGE_TYPES.XLS CHAPTER 2 ,"**Describe the bug**
I was trying to find message_types.xls file used in chapter 2, script 1 but wasn't able to. Kindly help.


",ishikabansal77,133491760,closed,False,2,2023-07-15T16:39:21+00:00,2023-07-15T21:21:19+00:00,2023-07-15T21:21:18+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1793082891,294,SARIMA notebook - huge amount of errors,"**Describe the bug**
About 90% of this notebook shows errors
https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/09_time_series_models/02_arima_models.ipynb

**To Reproduce**
- Load https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/09_time_series_models/02_arima_models.ipynb

Steps to reproduce the behavior:
1. Go to 'https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/09_time_series_models/02_arima_models.ipynb'
2. Scroll down
3. See error

**Expected behavior**
No errors upon loading

**Screenshots**
<img width=""1005"" alt=""image"" src=""https://github.com/stefan-jansen/machine-learning-for-trading/assets/7744628/54406bce-4eb3-418c-ad2f-5285b43c7147"">

<img width=""999"" alt=""image"" src=""https://github.com/stefan-jansen/machine-learning-for-trading/assets/7744628/286da851-b189-4944-8e4d-80caf2bc06cf"">

**Environment**
Chrome browser

**Additional context**
I love this tutorial!
",shimritabraham,7744628,closed,False,1,2023-07-07T08:40:52+00:00,2023-07-18T15:16:00+00:00,2023-07-18T15:15:59+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1773544143,293,Zipline ingest Issues,"I don't understand the steps for backtesting and the environment confuse me: 

In command, after I entered $zipline ingest -b quandl, I got following error: 

Traceback (most recent call last):
  File ""/opt/anaconda3/bin/zipline"", line 8, in <module>
    sys.exit(main())
  File ""/opt/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1130, in __call__
    return self.main(*args, **kwargs)
  File ""/opt/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1055, in main
    rv = self.invoke(ctx)
  File ""/opt/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1657, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/opt/anaconda3/lib/python3.8/site-packages/click/core.py"", line 1404, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/opt/anaconda3/lib/python3.8/site-packages/click/core.py"", line 760, in invoke
    return __callback(*args, **kwargs)
  File ""/opt/anaconda3/lib/python3.8/site-packages/zipline/__main__.py"", line 397, in ingest
    bundles_module.ingest(
  File ""/opt/anaconda3/lib/python3.8/site-packages/zipline/data/bundles/core.py"", line 424, in ingest
    asset_db_writer = AssetDBWriter(assets_db_path)
  File ""/opt/anaconda3/lib/python3.8/site-packages/zipline/assets/asset_writer.py"", line 3, in __init__
    #
  File ""/opt/anaconda3/lib/python3.8/site-packages/zipline/utils/input_validation.py"", line 811, in preprocessor
    return to(arg, **to_kwargs)
  File ""/opt/anaconda3/lib/python3.8/site-packages/zipline/utils/sqlite_utils.py"", line 45, in check_and_create_engine
    return sa.create_engine(""sqlite:///"" + path, future=False)
  File ""<string>"", line 2, in create_engine
  File ""/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/util/deprecations.py"", line 281, in warned
    return fn(*args, **kwargs)  # type: ignore[no-any-return]
  File ""/opt/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/create.py"", line 662, in create_engine
    raise exc.ArgumentError(
sqlalchemy.exc.ArgumentError: The 'future' parameter passed to create_engine() may only be set to True.

I have no idea what should I do. There is no same ERROR in the stack overflow. I don't know how to solve it : (  I am waiting for your solution. Thanks! ",BirdyLiu6471023,113460284,closed,False,1,2023-06-25T22:57:48+00:00,2023-07-17T19:22:21+00:00,2023-07-17T19:22:21+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1766914378,292,Update README.md,update links,LilianYe,4162125,closed,False,0,2023-06-21T07:16:12+00:00,2023-07-18T21:39:18+00:00,2023-07-18T21:39:18+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1763777918,291,Dependent variable of market beta estimation,"https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/11_decision_trees_random_forests/00_data_prep.ipynb

Why minus market premium when estimating market beta?
`factor_data['return_1m'] -= factor_data['Mkt-RF']`

The dependent variable should be the difference between stock return and risk-free rate
`factor_data['return_1m'] -= factor_data['RF']`",mk0417,27696880,closed,False,1,2023-06-19T15:33:36+00:00,2023-07-18T15:20:07+00:00,2023-07-18T15:19:52+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1699118217,290,ML4T Workflow - Vectorized versus event driven - script data.py not found on githup package,"I cannot find the data.py script mentioned in the section ""Vectorized versus event driven backtesting"". Can someone provide it ?",gmolinaro-git,51878343,closed,False,1,2023-05-07T15:41:04+00:00,2023-07-17T19:24:01+00:00,2023-07-17T19:24:01+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1662975817,289,Please make this repository private,,mary-gatech,130494758,closed,False,0,2023-04-11T18:01:16+00:00,2023-04-11T18:02:50+00:00,2023-04-11T18:02:50+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1655265281,288,04_q_learning_for_trading -> experience_replay(self) --> predict q values gives shape mismatch error,"### Error:

This line of code:
q_values[[self.idx, actions]] = targets 
Gives an error:
Exception has occurred: ValueError
shape mismatch: value array of shape (4096,) could not be broadcast to indexing result of shape (2,4096,3)
![bug reported v2](https://user-images.githubusercontent.com/129945927/230040259-71dc56f8-e790-4891-9bf0-1354df5c0975.jpg)

### Steps to reproduce: 
run 04_q_learning_for_trading.ipynb

### Environment: 
python 3.9",yeonmo317,129945927,closed,False,1,2023-04-05T09:30:24+00:00,2023-08-04T14:44:11+00:00,2023-08-04T14:44:11+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1631109284,287,Mamba env installation fails,"**Describe the bug**
The installation of the python packages by means of the ml4t.yml file is failing.
I am getting the error: 

Encountered problems while solving:
  - package numpy-1.19.1-py36hbc911f0_0 requires mkl >=2019.4,<2021.0a0, but none of the providers can be installed
  - package libsqlite-3.40.0-h753d276_0 requires libzlib >=1.2.13,<1.3.0a0, but none of the providers can be installed



**To Reproduce**
1. Create a DOCKERFILE inside of the installation folder.
    The content should be 
    
    # Start from a core stack version
    FROM jupyter/datascience-notebook:python-3.8
    # Install in the default python3 environment
    COPY ./linux/ml4t.yml /var/tmp/ml4t.yml
    RUN conda create -n ml4t python=3.8
    RUN mamba env update -n ml4t -f /var/tmp/ml4t.yml
    RUN conda activate ml4t
2. run docker image build --tag jupyter/base-notebook:my_version .




",hacky1610,18327894,closed,False,1,2023-03-19T20:40:27+00:00,2023-08-04T14:40:19+00:00,2023-08-04T14:40:19+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1630251043,286,07_linear_models/05_predicting_stock_returns_with_linear_regression.ipynb--->Syntax error for top_coeffs of Ridge Regression,"best_alpha = ridge_scores.groupby('alpha').ic.mean().idxmax() 
fig, axes = plt.subplots(ncols=2, figsize=(15, 5)) 
plot_ic_distribution(ridge_scores[ridge_scores.alpha == best_alpha],
                     ax=axes[0]) 
axes[0].set_title('Daily Information Coefficients')
top_coeffs = ridge_coeffs.loc[best_alpha].abs().sort_values().head(10).index  #I think there may be some error
top_coeffs.tolist() 
ridge_coeffs.loc[best_alpha, top_coeffs].sort_values().plot.barh(ax=axes[1],
                                                                 title='Top 10 Coefficients') 
sns.despine()
fig.tight_layout()

######################################################
-> ""top_coeffs = ridge_coeffs.loc[best_alpha].abs().sort_values().head(10).index""  maybe use sort_values(ascending = False) is better here?Since we want to caculate the most relevant coefficients,which means have top 10 high values?",Fdu-lijiachen,103441607,closed,False,1,2023-03-18T09:44:35+00:00,2023-08-04T14:42:17+00:00,2023-08-04T14:42:16+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1629738659,285,01_pandas_datareader_demo.ipynb from chapter 2,"**Describe the bug**
A brief description of the bug and in which notebook/script it lives.
In the notebook located at this issue's title, pandas.datareader for yahoo finance is broken. Changing FB (which no longer exists as a ticker) to META didn't make any difference. On both occasions, the following error is thrown: string indices must be integers
I noticed FB is also fetched under the Quandl section, but I didn't complete the entire notebook since I noticed it seems nothing gets saved to disk to be used in later exercises.

**To Reproduce**
- In case you are not running the current version of the notebook/script as found on GitHub.
Ran as is.


Changed the code as below. Seems to work fine.
start = datetime(2014, 1, 1)
end = datetime(2017, 5, 24)

yahoo= yf.download('META', start=start, end=end)
yahoo.info()
",sgjohnson1981,49596637,closed,False,2,2023-03-17T18:18:22+00:00,2023-08-04T14:41:01+00:00,2023-08-04T14:41:01+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1627262876,284,04_q_learning_for_trading -> Error during Observation Space creation,"**Describe the bug**
The creation of a Box in trading_env.py:242 is failung. The Datatype of the parameters is not correct. It has to be an nd_array.


Steps to reproduce the behavior:
1. OPen 04_q_learning_for_trading
2. Execute all

**Expected behavior**
No error occurs

**Screenshots**
![image](https://user-images.githubusercontent.com/18327894/225560525-7b43f107-1aa1-48fc-9e4f-a84a191d9867.png)

**Environment**
jupyter/datascience-notebook:latest
 
Solution:
    self.observation_space = spaces.Box(self.data_source.min_values.to_numpy(),
                                            self.data_source.max_values.to_numpy())
",hacky1610,18327894,closed,False,1,2023-03-16T11:20:36+00:00,2023-08-04T14:44:22+00:00,2023-08-04T14:44:22+00:00,,1,1,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1620088907,283,Cannot create stooq data,"**Describe the bug**
In create_stooq_data.ipynb
Cannot Add symbols. 
This line `df = pd.read_csv(f'https://stooq.com/db/l/?g={code}', sep='        ').apply(lambda x: x.str.strip()) `
 raises an error EmptyDataError: No columns to parse from file
Basically, it cannot properly parse the data from the url into a csv.

**Screenshots**
![Selection_169](https://user-images.githubusercontent.com/7363000/224503591-35b35892-c6b0-4a8e-a2c5-86a6a7e85683.png)
![Selection_170](https://user-images.githubusercontent.com/7363000/224503601-d9bd8ba6-df49-4d1a-a4fe-5cb1e840f654.png)


**Environment**
If you are not using the latest version of the Docker imag:
 
- OS:Ubuntu 18.04
 -Python 3.8

**Additional context**
Add any other context about the problem here.
",doruirimescu,7363000,closed,False,8,2023-03-11T17:44:12+00:00,2025-02-26T05:09:02+00:00,2023-08-04T14:41:18+00:00,,1,1,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1590896489,282,Zipline for minute bars,"HI @stefan-jansen - in the high-frequency trading section of chapter 12, you're saying that Zipline's Pipeline doesn't work for minute data. Is it now supported by your zipline-reloaded fork?",ir0nt0ad,14354402,closed,False,1,2023-02-19T23:05:33+00:00,2023-03-05T17:15:20+00:00,2023-03-05T17:15:20+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1590476120,281,12_gradient_boosting_machines/06_evaluate_trading_signals.ipynb KeyError: 'No object named predictions/1/1134/63/7/500 in the file',"I ran the 05_trading_signals_with_lightgbm_and_catboost.ipynb. Took something like 4 days, but not errors.

When running this cell in 06_evaluate_trading_signals:
lookahead = 1
topn = 10
for best in range(topn):
    best_params = get_cb_params(catboost_daily_ic, t=lookahead, best=best)
    key = get_cb_key(lookahead, best_params)
    rounds = str(int(best_params.boost_rounds))
    if best == 0:
        best_predictions = pd.read_hdf(results_path / 'tuning_catboost.h5', 'predictions/' + key)
        best_predictions = best_predictions[rounds].to_frame(best)
    else:
        best_predictions[best] = pd.read_hdf(results_path / 'tuning_catboost.h5',
                                             'predictions/' + key)[rounds]
best_predictions = best_predictions.sort_index()

I receive this error:

---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/Volumes/PortableSSD/machine-learning-for-trading/12_gradient_boosting_machines/06_evaluate_trading_signals.ipynb Cell 110 in <cell line: 3>()
      [6](vscode-notebook-cell:/Volumes/PortableSSD/machine-learning-for-trading/12_gradient_boosting_machines/06_evaluate_trading_signals.ipynb#Y214sZmlsZQ%3D%3D?line=5) rounds = str(int(best_params.boost_rounds))
      [7](vscode-notebook-cell:/Volumes/PortableSSD/machine-learning-for-trading/12_gradient_boosting_machines/06_evaluate_trading_signals.ipynb#Y214sZmlsZQ%3D%3D?line=6) if best == 0:
----> [8](vscode-notebook-cell:/Volumes/PortableSSD/machine-learning-for-trading/12_gradient_boosting_machines/06_evaluate_trading_signals.ipynb#Y214sZmlsZQ%3D%3D?line=7)     best_predictions = pd.read_hdf(results_path / 'tuning_catboost.h5', 'predictions/' + key)
      [9](vscode-notebook-cell:/Volumes/PortableSSD/machine-learning-for-trading/12_gradient_boosting_machines/06_evaluate_trading_signals.ipynb#Y214sZmlsZQ%3D%3D?line=8)     best_predictions = best_predictions[rounds].to_frame(best)
     [10](vscode-notebook-cell:/Volumes/PortableSSD/machine-learning-for-trading/12_gradient_boosting_machines/06_evaluate_trading_signals.ipynb#Y214sZmlsZQ%3D%3D?line=9) else:

File ~/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/pytables.py:425, in read_hdf(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)
    420                 raise ValueError(
    421                     ""key must be provided when HDF5 ""
    422                     ""file contains multiple datasets.""
    423                 )
    424         key = candidate_only_group._v_pathname
--> 425     return store.select(
    426         key,
    427         where=where,
    428         start=start,
    429         stop=stop,
    430         columns=columns,
    431         iterator=iterator,
    432         chunksize=chunksize,
    433         auto_close=auto_close,
    434     )
    435 except (ValueError, TypeError, KeyError):
    436     if not isinstance(path_or_buf, HDFStore):
    437         # if there is an error, close the store if we opened it.

File ~/opt/miniconda3/envs/ml4t/lib/python3.8/site-packages/pandas/io/pytables.py:820, in HDFStore.select(self, key, where, start, stop, columns, iterator, chunksize, auto_close)
    818 group = self.get_node(key)
    819 if group is None:
--> 820     raise KeyError(f""No object named {key} in the file"")
    822 # create the storer and axes
    823 where = _ensure_term(where, scope_level=1)

KeyError: 'No object named predictions/1/1134/63/7/500 in the file'",dhsusf,93278840,closed,False,1,2023-02-18T21:29:52+00:00,2023-08-04T14:41:36+00:00,2023-08-04T14:41:36+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1574846278,280,Interests,,Cali07,124034861,closed,False,0,2023-02-07T18:40:05+00:00,2023-02-07T18:40:29+00:00,2023-02-07T18:40:29+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1516988371,278,"Chapter 12, 04_preparing_the_model_data.ipynb","In the  deciles cells, `pd.qcut` doesn't want to handle series which are all `NaN` and throws  ""IndexError: cannot do a non-empty take from an empty axes."" I'm using pandas 1.5.2.

 I solved it somewhat crudely like this:

**Daily historical return deciles**
```
for t in T:
    prices[f'r{t:02}dec'] = (prices[f'r{t:02}']
                             .dropna()
                             .groupby(level='date')
                             .apply(lambda x: pd.qcut(x, 
                                                      q=10, 
                                                      labels=False, 
                                                      duplicates='drop')))
```

**Daily sector return deciles**
```
for t in T:
    prices[f'r{t:02}q_sector'] = (prices
                                  .groupby(['date', 'sector'])[f'r{t:02}']
                                  .transform(lambda x: pd.qcut(x, 
                                                               q=5, 
                                                               labels=False, 
                                                               duplicates='drop')
                                                       if not x.isnull().all() else np.nan))
```",ir0nt0ad,14354402,closed,False,3,2023-01-03T06:41:42+00:00,2024-09-13T02:41:15+00:00,2023-01-05T01:18:57+00:00,,2,2,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1515821220,277,Error in training the agent: not enough values to unpack in trading_environment.step,"Hi!

I currently face the following error:

Traceback (most recent call last):
  File ""c:\Users\marcm\OneDrive\Bureaublad\Programming\Maathouse Algorithm Trading\MachineLearning\Jansen\Deep_RL.py"", line 263, in <module>
    next_state, reward, done, _ = trading_environment.step(action) 
  File ""C:\Users\marcm\OneDrive\Bureaublad\Programming\Maathouse Algorithm Trading\MachineLearning\lib\site-packages\gym\wrappers\time_limit.py"", line 50, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
ValueError: not enough values to unpack (expected 5, got 4)

Anyone faced the same problem or know how to solve it? Seems to be an error in one of the Core API for Environment, Wrapper, ActionWrapper, RewardWrapper and ObservationWrapper.

Regards,

Maathouse
",Maathouse,36130317,closed,False,1,2023-01-01T20:42:08+00:00,2023-01-03T00:23:08+00:00,2023-01-03T00:23:08+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1515312176,276,Would someone please provide all data for data/assets.h5 downloadable?,"Becasue of some data source is NOT accessible now, so it is very  welcomed if someone provide data/assets.h5 contain all data downloadable.

Thanks a lot!

",joshua-xia,5258151,closed,False,2,2023-01-01T04:41:22+00:00,2023-01-03T01:02:34+00:00,2023-01-03T01:02:34+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1511788168,275,ValueError: Please set your QUANDL_API_KEY environment variable and retry.,"after run: zipline ingest -b quandl

I get the following error:

ValueError: Please set your QUANDL_API_KEY environment variable and retry.

I am using windows 10. what should I do?",quant2008,116711018,closed,False,6,2022-12-27T13:08:03+00:00,2023-08-18T19:01:16+00:00,2023-01-01T21:10:48+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1508478642,274,README update,"Just wanted to play around with some of your notebooks, particularly the budgeting section.

Thanks for this.",calvinceecee,96885737,closed,False,1,2022-12-22T20:05:25+00:00,2023-01-01T21:36:58+00:00,2023-01-01T21:36:58+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1492134320,273,Ch 7 - 07_logistic_regression_macro_data.ipynb - process/analysis,"Ch. 7, script #07_logistic_regression_macro_data.ipynb - process/analysis

The 'target' of the regression is whether or not the current moving average calculation for a 20 day period.  Shouldn't the regression exclude the first 20 observations, because it's not possible to have a 1?  (i.e. can't have 1 until observation 21)

Thanks",cconw,7128015,closed,False,1,2022-12-12T15:23:48+00:00,2023-01-01T21:16:02+00:00,2023-01-01T21:16:01+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1490390447,272,05_predicting_stock_returns_with_linear_regression.ipynb || ValueError: cannot reindex on an axis with duplicate labels,"I've been at this for a few hours trying to debug myself and understand this error and I simply cannot figure it out.  It's equally perplexing to me because the ridge_scores visualization has a virtually identical call to seaborn earlier in the script and that works.  If anyone has any ideas, I would appreciate it! 

**this specific line:**

> ax = sns.lineplot(x='alpha', 
>                 y='ic', data=lasso_scores, 
>                 estimator=np.mean, label='Mean', ax=axes[0])

**is generating this error:**

> ---------------------------------------------------------------------------
> ValueError                                Traceback (most recent call last)
> Cell In [144], line 6
>       4 best_alpha_mean = scores_by_alpha['mean'].idxmax()
>       5 best_alpha_median = scores_by_alpha['median'].idxmax()
> ----> 6 ax = sns.lineplot(x='alpha', 
>       7                 y='ic', data=lasso_scores, 
>       8                 estimator=np.mean, label='Mean', ax=axes[0])
>       9 scores_by_alpha['median'].plot(logx=True, ax=axes[0], label='Median')
>      11 axes[0].axvline(best_alpha_mean, ls='--', c='k', lw=1, label='Max. Mean')
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\seaborn\relational.py:639, in lineplot(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)
>     636 color = kwargs.pop(""color"", kwargs.pop(""c"", None))
>     637 kwargs[""color""] = _default_color(ax.plot, hue, color, kwargs)
> --> 639 p.plot(ax, kwargs)
>     640 return ax
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\seaborn\relational.py:423, in _LinePlotter.plot(self, ax, kws)
>     415 # TODO How to handle NA? We don't want NA to propagate through to the
>     416 # estimate/CI when some values are present, but we would also like
>     417 # matplotlib to show ""gaps"" in the line when all values are missing.
>    (...)
>     420 
>     421 # Loop over the semantic subsets and add to the plot
>     422 grouping_vars = ""hue"", ""size"", ""style""
> --> 423 for sub_vars, sub_data in self.iter_data(grouping_vars, from_comp_data=True):
>     425     if self.sort:
>     426         sort_vars = [""units"", orient, other]
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\seaborn\_oldcore.py:1028, in VectorPlotter.iter_data(self, grouping_vars, reverse, from_comp_data, by_facet, allow_empty, dropna)
>    1023 grouping_vars = [
>    1024     var for var in grouping_vars if var in self.variables
>    1025 ]
>    1027 if from_comp_data:
> -> 1028     data = self.comp_data
>    1029 else:
>    1030     data = self.plot_data
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\seaborn\_oldcore.py:1134, in VectorPlotter.comp_data(self)
>    1132         else:
>    1133             comp_col = pd.Series(dtype=float, name=var)
> -> 1134         comp_data.insert(0, var, comp_col)
>    1136     self._comp_data = comp_data
>    1138 return self._comp_data
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\frame.py:4447, in DataFrame.insert(self, loc, column, value, allow_duplicates)
>    4444 if not isinstance(loc, int):
>    4445     raise TypeError(""loc must be int"")
> -> 4447 value = self._sanitize_column(value)
>    4448 self._mgr.insert(loc, column, value)
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\frame.py:4535, in DataFrame._sanitize_column(self, value)
>    4533 # We can get there through loc single_block_path
>    4534 if isinstance(value, (DataFrame, Series)):
> -> 4535     return _reindex_for_setitem(value, self.index)
>    4537 if is_list_like(value):
>    4538     com.require_length_match(value, self.index)
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\frame.py:11008, in _reindex_for_setitem(value, index)
>   11004 except ValueError as err:
>   11005     # raised in MultiIndex.from_tuples, see test_insert_error_msmgs
>   11006     if not value.index.is_unique:
>   11007         # duplicate axis
> > 11008         raise err
>   11010     raise TypeError(
>   11011         ""incompatible index of inserted column with frame index""
>   11012     ) from err
>   11013 return reindexed_value
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\frame.py:11003, in _reindex_for_setitem(value, index)
>   11001 # GH#4107
>   11002 try:
> > 11003     reindexed_value = value.reindex(index)._values
>   11004 except ValueError as err:
>   11005     # raised in MultiIndex.from_tuples, see test_insert_error_msmgs
>   11006     if not value.index.is_unique:
>   11007         # duplicate axis
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\series.py:4672, in Series.reindex(self, *args, **kwargs)
>    4668         raise TypeError(
>    4669             ""'index' passed as both positional and keyword argument""
>    4670         )
>    4671     kwargs.update({""index"": index})
> -> 4672 return super().reindex(**kwargs)
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\generic.py:4966, in NDFrame.reindex(self, *args, **kwargs)
>    4963     return self._reindex_multi(axes, copy, fill_value)
>    4965 # perform the reindex on the axes
> -> 4966 return self._reindex_axes(
>    4967     axes, level, limit, tolerance, method, fill_value, copy
>    4968 ).__finalize__(self, method=""reindex"")
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\generic.py:4986, in NDFrame._reindex_axes(self, axes, level, limit, tolerance, method, fill_value, copy)
>    4981 new_index, indexer = ax.reindex(
>    4982     labels, level=level, limit=limit, tolerance=tolerance, method=method
>    4983 )
>    4985 axis = self._get_axis_number(a)
> -> 4986 obj = obj._reindex_with_indexers(
>    4987     {axis: [new_index, indexer]},
>    4988     fill_value=fill_value,
>    4989     copy=copy,
>    4990     allow_dups=False,
>    4991 )
>    4992 # If we've made a copy once, no need to make another one
>    4993 copy = False
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\generic.py:5032, in NDFrame._reindex_with_indexers(self, reindexers, fill_value, copy, allow_dups)
>    5029     indexer = ensure_platform_int(indexer)
>    5031 # TODO: speed up on homogeneous DataFrame objects (see _reindex_multi)
> -> 5032 new_data = new_data.reindex_indexer(
>    5033     index,
>    5034     indexer,
>    5035     axis=baxis,
>    5036     fill_value=fill_value,
>    5037     allow_dups=allow_dups,
>    5038     copy=copy,
>    5039 )
>    5040 # If we've made a copy once, no need to make another one
>    5041 copy = False
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\internals\managers.py:676, in BaseBlockManager.reindex_indexer(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice, use_na_proxy)
>     674 # some axes don't allow reindexing with dups
>     675 if not allow_dups:
> --> 676     self.axes[axis]._validate_can_reindex(indexer)
>     678 if axis >= self.ndim:
>     679     raise IndexError(""Requested axis not found in manager"")
> 
> File c:\Users\cc\miniforge3\envs\ml4t\lib\site-packages\pandas\core\indexes\base.py:4121, in Index._validate_can_reindex(self, indexer)
>    4119 # trying to reindex on an axis with duplicates
>    4120 if not self._index_as_unique and len(indexer):
> -> 4121     raise ValueError(""cannot reindex on an axis with duplicate labels"")
> 
> ValueError: cannot reindex on an axis with duplicate labels",cconw,7128015,closed,False,3,2022-12-11T20:03:40+00:00,2024-03-20T07:07:54+00:00,2023-01-01T21:17:23+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1489874225,271,`ddqn.experience`'s `action` data type problem in `22_deep_reinforcement_learning`/`04_q_learning_for_trading.ipynb`,"**Describe the bug**
- When the train goes on for hundreds of episodes, it randomly generates a `np.ndarray` data type object in the `ddqn.experience.action` list. At very beginning, the interval is once per episode. Then, it becomes once per every 20 steps, and finally every step. 

**To Reproduce**
1. Run the training for like 300 episodes.
2. Execute the below code, which will show a list of nth `ddqn.experience` whose `action` is not an `integer`. 
```
all_action_list = [ddqn.experience[i][1] for i in range(len(ddqn.experience))]
for i in range(len(all_action_list)):
    if type(all_action_list[i]) != int:
        print(i)
```

**Question**
- Is this intended? Any purpose for this? ",DavidHJong,31554275,closed,False,1,2022-12-11T11:46:09+00:00,2023-01-03T00:34:43+00:00,2023-01-03T00:34:43+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1483878385,270,memory problem in 22_deep_reinforcement_learning\04_q_learning_for_trading.ipynb,,DavidHJong,31554275,closed,False,0,2022-12-08T06:58:14+00:00,2022-12-08T07:20:38+00:00,2022-12-08T07:20:38+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1473932482,269,Memory hog problem in `22_deep_reinforcement_learning`/`04_q_learning_for_trading.ipynb`,"**Describe the bug**
- As training goes on, the memory usage by the process increases. It continuously goes up until hit around 10GB in episode 280. In later episodes, memory usage goes up around 70MB per episode, as shown in [04_q_learning_for_trading.ipynb](https://github.com/DavidHJong/machine-learning-for-trading/blob/e514b7a47d9df1cda453161b35b63e0ed54955eb/22_deep_reinforcement_learning/04_q_learning_for_trading.ipynb)'s `Train Agent` cell's output. 

**What I did to find this**
1. In [04_q_learning_for_trading.ipynb](https://github.com/DavidHJong/machine-learning-for-trading/blob/e514b7a47d9df1cda453161b35b63e0ed54955eb/22_deep_reinforcement_learning/04_q_learning_for_trading.ipynb), I added a cell `monitor RAM usage` with 2 functions to monitor the process memory usage and call these functions multiple times in the `Train Agent` cell. 
2. I recorded the process memory changes and save it into [a .csv file](https://github.com/DavidHJong/machine-learning-for-trading/blob/main/22_deep_reinforcement_learning/memory_change_record.csv) and visualize it in [a .xlsx file](https://github.com/DavidHJong/machine-learning-for-trading/blob/main/22_deep_reinforcement_learning/memory_change_record.xlsx).

**Steps to reproduce the behavior:**
1. To see in which steps the major memory changes occurred, open the[ `memory_change_record.xlsx` file](https://github.com/DavidHJong/machine-learning-for-trading/blob/main/22_deep_reinforcement_learning/memory_change_record.xlsx) 
2. To reproduce this record, run [04_q_learning_for_trading.ipynb](https://github.com/DavidHJong/machine-learning-for-trading/blob/main/22_deep_reinforcement_learning/04_q_learning_for_trading.ipynb). 

**Questions**
 - Is this memory leak? 
 - What is the main part of code contributing to this increasing memory usage? 
 - Any way to keep the memory usage constant or at a small size as training goes on? 

**Why it matters**
 - An increasing memory usage seems like a major reason for the slowing down in later episode training. 
",DavidHJong,31554275,closed,False,1,2022-12-03T11:21:11+00:00,2023-01-03T00:27:19+00:00,2023-01-03T00:27:18+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1472230051,268,Crypto,I don't have a bug or antyhing but just an observation ... all of the crypto markets or most of them ... all give away the trade and tick data as well as the orderbook websockets for free ... if you check out the bybit and binance api you will see you can get all this historical data for free ... i was wondering if anyone has any experience translating all this information from this book over to crypto ... because it seems like it is this gigantic mess just to get info from the source and then you have to do all kinds of manipulation to it ... but crypto you just download it and its all done for you in nice and neat columns ,quantfreedom,95321908,closed,False,1,2022-12-02T02:46:50+00:00,2023-01-01T21:34:39+00:00,2023-01-01T21:34:39+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1464706406,267,about the order book itch nasdaq total view,"Hi,the order book is in the dataframe file named pd.HDFStore(order_book_store) right?!
if i want to import historical data with a defined datetime how can i do that?
thanks for the help. ",Bandita88,103201837,closed,False,1,2022-11-25T15:47:43+00:00,2023-01-01T21:20:29+00:00,2023-01-01T21:20:29+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1446309986,266,MESSAGE_TYPES ON nyse order book construction,"Hi everyone, i am tring to construct the nyse order book using the https://www.nasdaqtrader.com/content/technicalsupport/specifications/dataproducts/NQTVITCHSpecification.pdf
But i don't understad something... 
i have to copy and past ALL the data in this link in a file and so on import on jupyter? 
Because in the write code i see one first table example like this format belove:
NAME   OFFSET   LENGHT  VALUE   NOTES
XXXX     XXXX      XXXXX     XXXXX  XXXXX

and after i find also an other one table with the single tipe message specify. 
MESSAGE_TYPE    VALUE/DESCRIPTION
XXXXX                    XXXXXX

Could you give me an help and explanantion? and maybe link for the message_types file? 

Really thanks 

Also, i am using the directly connection with the itch link withouth downloading the file. 
Is it correct? or i must download the file and then do i accede with the http url? 
",Bandita88,103201837,closed,False,4,2022-11-12T07:39:34+00:00,2023-07-17T05:29:36+00:00,2022-11-15T17:50:22+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1440536034,265,QUANDL site is permanently down,"**Describe the bug**
A brief description of the bug and in which notebook/script it lives.

**zipline ingest -b quandl**

Error 
ValueError: Please set your QUANDL_API_KEY environment variable and retry.
After giving a dummy - set QUANDL_API_KEY=X
urllib.error.URLError: <urlopen error [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond>

Is there a way to digest the zipline bundle offline from local computer ? ",shaktisd,2882134,closed,False,1,2022-11-08T16:37:28+00:00,2023-01-01T21:33:46+00:00,2023-01-01T21:33:46+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1435070419,264,Stooq hd5 format can not be created ,"ValueError: No objects to concatenate

This error occurs if it try to generate the file even though I have dowloaded the files in the correct directory. I even changed the Index names in the Files from <TICKER> and <NAME> to ticker and name but still this mistake occurs. The problem is there are to many txt files with probably different Index names <TICKER> ....  ",schullino1,114085923,closed,False,2,2022-11-03T18:05:11+00:00,2023-01-03T00:39:02+00:00,2023-01-03T00:39:01+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1426180108,263,Code issue: Chapter 22_deep_reinforcement_learning  04_q_learning_for_trading,"I tried to reproduce the Q-Learning for trading, but there is an error when I run this notebook. I prepared data as instructed in data/create_datasets.ipynb. However, I still got the following error when creating the environment from trading_env.py

![image](https://user-images.githubusercontent.com/28869032/198383080-07cefe21-658f-4202-802a-b261560488b0.png)

",KoalaChelsea,28869032,closed,False,3,2022-10-27T19:40:52+00:00,2023-01-03T00:37:24+00:00,2023-01-03T00:37:23+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1422164984,262,Chapter 5: EffcientFrontier now working properly with SCS solver,"**Describe the bug**
Using the SCS solver in the optimize_weights function (02_backtest_with_pf_optimization.ipynb) doesn't work as there's no impact on the portfolio valuation (remains at 10M) and throws warning: [ 08:14:54.359874]: WARNING: 2013-01-07 Error parsing inputs

**To Reproduce**
Run the 02_backtest_with_pf_optimization.ipynb file

Solved by:
Removing the solver argument
",BibiTheLamb,29162158,closed,False,1,2022-10-25T09:36:39+00:00,2022-10-25T09:38:04+00:00,2022-10-25T09:37:16+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1416255688,261,create dateset,"https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange={}&render=download
exchanges = ['NASDAQ', 'AMEX', 'NYSE']

the data been deleted？",xtwxfxk,78169,closed,False,1,2022-10-20T09:09:02+00:00,2023-01-03T00:41:26+00:00,2023-01-03T00:41:14+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1402522693,260,Seek help to address zipline installation problem,"Hi,

I am a reader of the book ""Machine learning for algorithmic trading"". I encountered a problem when installing/using the zipline library (as described in details in the pull request). I will be very grateful if I can get some help. 

Actually, I am not sure how to reach out for such problems--not sure if it's appropriate to create a pull request either, and my apology if that is the case! I have also sent an email to feedback@packtpub.com and questions@packtpub.com asking the same question.

Thanks a lot!
JC",jianjiu-chen,115439040,closed,False,2,2022-10-10T02:52:08+00:00,2023-01-01T21:37:55+00:00,2023-01-01T21:37:55+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1374998267,259,Can't print Limit Order Price Distribution,"**Describe the bug**
While working with file 02_rebuild_nasdaq_order_book.ipynb, at the very end there's a print of the Limit Order Price Distribution. The graph comes but there are no bars. Code has been correctly executed previously, with no errors.

**To Reproduce**

Steps to reproduce the behavior:
1. Run 02_rebuild_nasdaq_order_book.ipynb file
2. For data, consider Nasdaq file 01302020.NASDAQ_ITCH50.bin
3. In Buy-Sell Order Distribution section, plotting the graph throws error

![Capture d’écran de 2022-09-15 21-29-04](https://user-images.githubusercontent.com/29162158/190492635-ea6cb07b-55e3-4276-b29a-c200a1a69a9b.png)


**Expected behavior**
Plot the graph

**Environment**
If you are not using the latest version of the Docker imag:
 
- OS: Ubuntu
 - Version 18.04.6 LTS

**Additional context**
Python 3.6.9
",BibiTheLamb,29162158,closed,False,1,2022-09-15T19:41:47+00:00,2022-09-15T20:32:02+00:00,2022-09-15T20:32:02+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1362261842,258,mamba env update -n ml4t -f ml4t-base.yml has multiple error when installing,"I have followed the installation instruction and got the below error : 


**\ Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\notebook\nbextensions.py"", line 203, in install_nbextension_python
    m, nbexts = _get_nbextension_metadata(module)
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\notebook\nbextensions.py"", line 1107, in _get_nbextension_metadata
    m = import_item(module)
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\traitlets\utils\importstring.py"", line 38, in import_item
    return __import__(parts[0])
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\latex_envs\__init__.py"", line 3, in <module>
    from . import latex_envs
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\latex_envs\latex_envs.py"", line 20, in <module>
    from nbconvert.exporters.exporter import Exporter
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\__init__.py"", line 4, in <module>
    from .exporters import *
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\exporters\__init__.py"", line 3, in <module>
    from .html import HTMLExporter
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\exporters\html.py"", line 12, in <module>
    from jinja2 import contextfilter
ImportError: cannot import name 'contextfilter' from 'jinja2' (C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\jinja2\__init__.py)

- Config option `kernel_spec_manager_class` not recognized by `EnableJupyterNbextensionsConfiguratorApp`.
Enabling: jupyter_nbextensions_configurator
- Writing config: C:\Users\Administrator\anaconda3\envs\ml4t\etc\jupyter
    - Validating...
      jupyter_nbextensions_configurator 0.4.1 ok
Enabling notebook nbextension nbextensions_configurator/config_menu/main...
Enabling tree nbextension nbextensions_configurator/tree_tab/main...

- Traceback (most recent call last):
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\Scripts\jupyter-contrib-nbextension-script.py"", line 6, in <module>
    from jupyter_contrib_nbextensions.application import main
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\jupyter_contrib_nbextensions\application.py"", line 15, in <module>
    from jupyter_contrib_nbextensions.install import (
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\jupyter_contrib_nbextensions\install.py"", line 12, in <module>
    import latex_envs
ModuleNotFoundError: No module named 'latex_envs'

done
ERROR conda.core.link:_execute(730): An error occurred while installing package 'conda-forge::jupyter_latex_envs-1.4.6-py38haa244fe_1001'.
Rolling back transaction: done
class: LinkError
message:
post-link script failed for package conda-forge::jupyter_latex_envs-1.4.6-py38haa244fe_1001
location of failed script: C:\Users\Administrator\anaconda3\envs\ml4t\Scripts\.jupyter_latex_envs-post-link.bat
==> script messages <==
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\notebook\nbextensions.py"", line 203, in install_nbextension_python
    m, nbexts = _get_nbextension_metadata(module)
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\notebook\nbextensions.py"", line 1107, in _get_nbextension_metadata
    m = import_item(module)
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\traitlets\utils\importstring.py"", line 38, in import_item
    return __import__(parts[0])
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\latex_envs\__init__.py"", line 3, in <module>
    from . import latex_envs
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\latex_envs\latex_envs.py"", line 20, in <module>
    from nbconvert.exporters.exporter import Exporter
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\__init__.py"", line 4, in <module>
    from .exporters import *
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\exporters\__init__.py"", line 3, in <module>
    from .html import HTMLExporter
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\exporters\html.py"", line 12, in <module>
    from jinja2 import contextfilter
ImportError: cannot import name 'contextfilter' from 'jinja2' (C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\jinja2\__init__.py)

==> script output <==
stdout:
C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET DISTUTILS_USE_SDK=1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET MSSdk=1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET ""VS_VERSION=15.0""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET ""VS_MAJOR=15""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET ""VS_YEAR=2017""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""MSYS2_ARG_CONV_EXCL=/AI;/AL;/OUT;/out""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""MSYS2_ENV_CONV_EXCL=CL""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""PY_VCRUNTIME_REDIST=C:\Users\Administrator\anaconda3\envs\ml4t\bin\vcruntime140.dll""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""CXX=cl.exe""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""CC=cl.exe""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""VSINSTALLDIR=""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""NEWER_VS_WITH_OLDER_VC=0""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>for /F ""usebackq tokens=*"" %i in (`vswhere.exe -nologo -products * -version [15.0,16.0) -property installationPath`) do (set ""VSINSTALLDIR=%i\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist """" (for /F ""usebackq tokens=*"" %i in (`vswhere.exe -nologo -products * -requires Microsoft.VisualStudio.Component.VC.v141.x86.x64 -property installationPath`) do (
set ""VSINSTALLDIR=%i\""
 set ""NEWER_VS_WITH_OLDER_VC=1""
) )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist """" (set ""VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist ""C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\"" (set ""VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist ""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\"" (set ""VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist ""C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\"" (set ""VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF NOT """" == """" (
set ""INCLUDE=;""
 set ""LIB=;""
 set ""CMAKE_PREFIX_PATH=;""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>call :GetWin10SdkDir

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>call :GetWin10SdkDirHelper HKLM\SOFTWARE\Wow6432Node  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE\Wow6432Node  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKLM\SOFTWARE  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 exit /B 1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>exit /B 0

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>for /F %i in ('dir /ON /B ""\include\10.*""') DO (SET WindowsSDKVer=%~i )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 (echo ""Didn't find any windows 10 SDK. I'm not sure if things will work, but let's try..."" )  else (echo Windows SDK version found as: """" )
Windows SDK version found as: """"

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF ""win-64"" == ""win-64"" (
set ""BITS=64""
 set ""CMAKE_PLAT=x64""
)  ELSE (
set ""BITS=32""
 set ""CMAKE_PLAT=Win32""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF 2017 GEQ 2019 (
set ""CMAKE_GEN=Visual Studio 15 2017""
 set ""USE_NEW_CMAKE_GEN_SYNTAX=1""
)  ELSE (
IF ""win-64"" == ""win-64"" (set ""CMAKE_GEN=Visual Studio 15 2017 Win64"" )  else (set ""CMAKE_GEN=Visual Studio 15 2017"" )
 set ""USE_NEW_CMAKE_GEN_SYNTAX=0""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>echo ""NEWER_VS_WITH_OLDER_VC=0""
""NEWER_VS_WITH_OLDER_VC=0""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF ""0"" == ""1"" (
set ""CMAKE_GEN=Visual Studio 16 2019""
 set ""USE_NEW_CMAKE_GEN_SYNTAX=1""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF ""Visual Studio 15 2017 Win64"" == """" SET ""CMAKE_GENERATOR=Visual Studio 15 2017 Win64""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF ""0"" == ""1"" (
IF """" == """" SET ""CMAKE_GENERATOR_PLATFORM=x64""
 IF """" == """" SET ""CMAKE_GENERATOR_TOOLSET=v141""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>pushd C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>CALL ""VC\Auxiliary\Build\vcvars64.bat"" -vcvars_ver=14.16

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>popd

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>call :GetWin10SdkDirHelper HKLM\SOFTWARE\Wow6432Node  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE\Wow6432Node  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKLM\SOFTWARE  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 exit /B 1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>exit /B 0

stderr: The system cannot find the file specified.
The system cannot find the path specified.
The system cannot find the path specified.

return code: 1

kwargs:
{}

Traceback (most recent call last):
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\exceptions.py"", line 1082, in __call__
    return func(*args, **kwargs)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda_env\cli\main.py"", line 80, in do_call
    exit_code = getattr(module, func_name)(args, parser)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda_env\cli\main_update.py"", line 126, in execute
    result[installer_type] = installer.install(prefix, specs, args, env)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\mamba\mamba_env.py"", line 95, in mamba_install
    conda_transaction.execute()
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\core\link.py"", line 281, in execute
    self._execute(tuple(concat(interleave(itervalues(self.prefix_action_groups)))))
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\core\link.py"", line 744, in _execute
    raise CondaMultiError(tuple(concatv(
conda.CondaMultiErrorclass: LinkError
message:
post-link script failed for package conda-forge::jupyter_latex_envs-1.4.6-py38haa244fe_1001
location of failed script: C:\Users\Administrator\anaconda3\envs\ml4t\Scripts\.jupyter_latex_envs-post-link.bat
==> script messages <==
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\notebook\nbextensions.py"", line 203, in install_nbextension_python
    m, nbexts = _get_nbextension_metadata(module)
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\notebook\nbextensions.py"", line 1107, in _get_nbextension_metadata
    m = import_item(module)
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\traitlets\utils\importstring.py"", line 38, in import_item
    return __import__(parts[0])
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\latex_envs\__init__.py"", line 3, in <module>
    from . import latex_envs
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\latex_envs\latex_envs.py"", line 20, in <module>
    from nbconvert.exporters.exporter import Exporter
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\__init__.py"", line 4, in <module>
    from .exporters import *
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\exporters\__init__.py"", line 3, in <module>
    from .html import HTMLExporter
  File ""C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\nbconvert\exporters\html.py"", line 12, in <module>
    from jinja2 import contextfilter
ImportError: cannot import name 'contextfilter' from 'jinja2' (C:\Users\Administrator\anaconda3\envs\ml4t\lib\site-packages\jinja2\__init__.py)

==> script output <==
stdout:
C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET DISTUTILS_USE_SDK=1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET MSSdk=1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET ""VS_VERSION=15.0""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET ""VS_MAJOR=15""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>SET ""VS_YEAR=2017""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""MSYS2_ARG_CONV_EXCL=/AI;/AL;/OUT;/out""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""MSYS2_ENV_CONV_EXCL=CL""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""PY_VCRUNTIME_REDIST=C:\Users\Administrator\anaconda3\envs\ml4t\bin\vcruntime140.dll""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""CXX=cl.exe""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""CC=cl.exe""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""VSINSTALLDIR=""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>set ""NEWER_VS_WITH_OLDER_VC=0""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>for /F ""usebackq tokens=*"" %i in (`vswhere.exe -nologo -products * -version [15.0,16.0) -property installationPath`) do (set ""VSINSTALLDIR=%i\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist """" (for /F ""usebackq tokens=*"" %i in (`vswhere.exe -nologo -products * -requires Microsoft.VisualStudio.Component.VC.v141.x86.x64 -property installationPath`) do (
set ""VSINSTALLDIR=%i\""
 set ""NEWER_VS_WITH_OLDER_VC=1""
) )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist """" (set ""VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist ""C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\"" (set ""VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist ""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\"" (set ""VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if not exist ""C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\"" (set ""VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\"" )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF NOT """" == """" (
set ""INCLUDE=;""
 set ""LIB=;""
 set ""CMAKE_PREFIX_PATH=;""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>call :GetWin10SdkDir

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>call :GetWin10SdkDirHelper HKLM\SOFTWARE\Wow6432Node  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE\Wow6432Node  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKLM\SOFTWARE  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 exit /B 1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>exit /B 0

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>for /F %i in ('dir /ON /B ""\include\10.*""') DO (SET WindowsSDKVer=%~i )

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 (echo ""Didn't find any windows 10 SDK. I'm not sure if things will work, but let's try..."" )  else (echo Windows SDK version found as: """" )
Windows SDK version found as: """"

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF ""win-64"" == ""win-64"" (
set ""BITS=64""
 set ""CMAKE_PLAT=x64""
)  ELSE (
set ""BITS=32""
 set ""CMAKE_PLAT=Win32""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF 2017 GEQ 2019 (
set ""CMAKE_GEN=Visual Studio 15 2017""
 set ""USE_NEW_CMAKE_GEN_SYNTAX=1""
)  ELSE (
IF ""win-64"" == ""win-64"" (set ""CMAKE_GEN=Visual Studio 15 2017 Win64"" )  else (set ""CMAKE_GEN=Visual Studio 15 2017"" )
 set ""USE_NEW_CMAKE_GEN_SYNTAX=0""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>echo ""NEWER_VS_WITH_OLDER_VC=0""
""NEWER_VS_WITH_OLDER_VC=0""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF ""0"" == ""1"" (
set ""CMAKE_GEN=Visual Studio 16 2019""
 set ""USE_NEW_CMAKE_GEN_SYNTAX=1""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF ""Visual Studio 15 2017 Win64"" == """" SET ""CMAKE_GENERATOR=Visual Studio 15 2017 Win64""

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>IF ""0"" == ""1"" (
IF """" == """" SET ""CMAKE_GENERATOR_PLATFORM=x64""
 IF """" == """" SET ""CMAKE_GENERATOR_TOOLSET=v141""
)

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>pushd C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>CALL ""VC\Auxiliary\Build\vcvars64.bat"" -vcvars_ver=14.16

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>popd

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>call :GetWin10SdkDirHelper HKLM\SOFTWARE\Wow6432Node  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE\Wow6432Node  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKLM\SOFTWARE  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE  1>nul 2>&1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>if errorlevel 1 exit /B 1

C:\Users\Administrator\anaconda3\envs\ml4t\Scripts>exit /B 0

stderr: The system cannot find the file specified.
The system cannot find the path specified.
The system cannot find the path specified.

return code: 1

kwargs:
{}

: <exception str() failed>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Administrator\anaconda3\Scripts\mamba-script.py"", line 10, in <module>
    sys.exit(main())
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\mamba\mamba.py"", line 871, in main
    return mamba_env.main()
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\mamba\mamba_env.py"", line 105, in main
    return conda_env_main()
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda_env\cli\main.py"", line 91, in main
    return conda_exception_handler(do_call, args, parser)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\exceptions.py"", line 1374, in conda_exception_handler
    return_value = exception_handler(func, *args, **kwargs)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\exceptions.py"", line 1085, in __call__
    return self.handle_exception(exc_val, exc_tb)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\exceptions.py"", line 1116, in handle_exception
    return self.handle_application_exception(exc_val, exc_tb)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\exceptions.py"", line 1132, in handle_application_exception
    self._print_conda_exception(exc_val, exc_tb)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\exceptions.py"", line 1136, in _print_conda_exception
    print_conda_exception(exc_val, exc_tb)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\exceptions.py"", line 1059, in print_conda_exception
    stderrlog.error(""\n%r\n"", exc_val)
  File ""C:\Users\Administrator\anaconda3\lib\logging\__init__.py"", line 1471, in error
    self._log(ERROR, msg, args, **kwargs)
  File ""C:\Users\Administrator\anaconda3\lib\logging\__init__.py"", line 1585, in _log
    self.handle(record)
  File ""C:\Users\Administrator\anaconda3\lib\logging\__init__.py"", line 1594, in handle
    if (not self.disabled) and self.filter(record):
  File ""C:\Users\Administrator\anaconda3\lib\logging\__init__.py"", line 807, in filter
    result = f.filter(record)
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\gateways\logging.py"", line 61, in filter
    record.msg = record.msg % new_args
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\__init__.py"", line 132, in __repr__
    errs.append(e.__repr__())
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\__init__.py"", line 71, in __repr__
    return '%s: %s' % (self.__class__.__name__, text_type(self))
  File ""C:\Users\Administrator\anaconda3\lib\site-packages\conda\__init__.py"", line 90, in __str__
    return text_type(self.message % self._kwargs)
TypeError: %i format: a number is required, not dict**


I have also tried 

mamba env update -n ml4t -f installation/windows/ml4t.yml 

PS. after conda env create -n ml4t python=3.8 , my jupyter note book doesn't appear ml4t kernal, so I am not sure if this is installed successfully ? 

Thank you.
",lililinuk,34682964,closed,False,1,2022-09-05T16:28:55+00:00,2023-01-03T00:44:58+00:00,2023-01-03T00:44:58+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1360484548,257,Chapter 08 :- 02_vestorized_backtest.ipynb,"**Line 11**
```
long_signals = ((predictions
                .where(predictions > 0)
                .rank(axis=1, ascending=False) > N_LONG)
                .astype(int))
short_signals = ((predictions
                  .where(predictions < 0)
                  .rank(axis=1) > N_SHORT)
                 .astype(int))
```

- wouldn't we want to long stock in the Top 15 highest expected return? e.g. Rank 1 to 15 with the highest expected return rank first (descending order). Hence we need ` rank <= 15` ?
- and short the lowest expected return stock from 1 to 15 again, due to lowest return rank first?



",Quantuary,43050453,closed,False,1,2022-09-02T18:12:10+00:00,2023-01-03T00:46:36+00:00,2023-01-03T00:46:35+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1355486135,256,Chapter 20. build_us_stock_dataset variable AttributeError,"Does anyone has an issue on chapter 20 ""20_autoencoders_for_conditional_risk_factors/04_build_us_stock_dataset.ipynb"" with the variable ""yf-symbols = yf.Tickers(all_symbols) ?

I have got this error : AttributeError: 'float' object has no attribute 'upper'

![image](https://user-images.githubusercontent.com/99275634/187408589-3a6bbcf1-a733-4702-a351-144288512588.png)
",Jano32,99275634,closed,False,1,2022-08-30T09:49:06+00:00,2023-01-03T00:48:18+00:00,2023-01-03T00:48:18+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1342818715,255,where is clusterable_data.npy,"where to find ""clusterable_data.npy"" which is used in notebook ""13_unsupervised_learning\03_clustering_algorithms\05_density_based_clustering.ipynb""?",oldwain,12002057,closed,False,1,2022-08-18T09:21:01+00:00,2022-08-20T08:48:59+00:00,2022-08-20T08:48:59+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1342578394,254,Minor fixes regarding connecting Nasdaq sample data storage,This PR fixes minor things regarding connection to Nasdaq ITCH sample data storage.,tomas-rampas,930520,closed,False,0,2022-08-18T05:29:03+00:00,2023-01-03T00:56:36+00:00,2023-01-03T00:56:36+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1338049555,253,"Working with Order Book Data: NASDAQ ITCH, Invalid URL and variable","**Describe the bug**
When running 
```file_name = may_be_download(urljoin(FTP_URL, SOURCE_FILE))```

First, there isnt a variable called FTP_URL, and when changed to HTTPS_URL, you get an invalid URL error. 

This doesnt work:
```HTTPS_URL = 'https://emi.nasdaq.com/ITCH/Nasdaq ITCH/'```

This DOES work:
```HTTPS_URL = 'https://emi.nasdaq.com/ITCH/Nasdaq%20ITCH/'```
**To Reproduce**
- Go to: run 01_parse_itch_order_flow_messages.ipynb from the main branch
- Run the first 8 cells

",marctheshark3,35350738,closed,False,3,2022-08-13T20:24:26+00:00,2023-01-03T00:57:58+00:00,2023-01-03T00:57:57+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1323413191,252,InvalidArgumentError: Nan in summary histogram for: hidden/kernel_0 [Op:WriteHistogramSummary],"[17_deep_learning]   
02_how_to_use_tensorflow     
6  Train Model

when run the code:

training=model.fit(X,       
          Y, 
          epochs=50,
          validation_split=.2,
          batch_size=128, 
          verbose=1, 
          callbacks=[tb_callback])

encountered an error：

Epoch 1/50
  2/313 [..............................] - ETA: 1:32 - loss: nan - accuracy: 0.4648WARNING:tensorflow:Callbacks method `on_train_batch_begin` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_begin` time: 0.0070s). Check your callbacks.
WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.5843s). Check your callbacks.
306/313 [============================>.] - ETA: 0s - loss: nan - accuracy: 0.4987
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Input In [25], in <cell line: 1>()
----> 1 training=model.fit(X,       
      2           Y, 
      3           epochs=50,
      4           validation_split=.2,
      5           batch_size=128, 
      6           verbose=1, 
      7           callbacks=[tb_callback])

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\keras\engine\training.py:108, in enable_multi_worker.<locals>._method_wrapper(self, *args, **kwargs)
    106 def _method_wrapper(self, *args, **kwargs):
    107   if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108     return method(self, *args, **kwargs)
    110   # Running inside `run_distribute_coordinator` already.
    111   if dc_context.get_current_worker_context():

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\keras\engine\training.py:1137, in Model.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1134   val_logs = {'val_' + name: val for name, val in val_logs.items()}
   1135   epoch_logs.update(val_logs)
-> 1137 callbacks.on_epoch_end(epoch, epoch_logs)
   1138 training_logs = epoch_logs
   1139 if self.stop_training:

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\keras\callbacks.py:412, in CallbackList.on_epoch_end(self, epoch, logs)
    410 for callback in self.callbacks:
    411   if getattr(callback, '_supports_tf_logs', False):
--> 412     callback.on_epoch_end(epoch, logs)
    413   else:
    414     if numpy_logs is None:  # Only convert once.

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\keras\callbacks.py:2182, in TensorBoard.on_epoch_end(self, epoch, logs)
   2179 self._log_epoch_metrics(epoch, logs)
   2181 if self.histogram_freq and epoch % self.histogram_freq == 0:
-> 2182   self._log_weights(epoch)
   2184 if self.embeddings_freq and epoch % self.embeddings_freq == 0:
   2185   self._log_embeddings(epoch)

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\keras\callbacks.py:2234, in TensorBoard._log_weights(self, epoch)
   2232 for weight in layer.weights:
   2233   weight_name = weight.name.replace(':', '_')
-> 2234   summary_ops_v2.histogram(weight_name, weight, step=epoch)
   2235   if self.write_images:
   2236     self._log_weight_as_image(weight, weight_name, epoch)

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\ops\summary_ops_v2.py:836, in histogram(name, tensor, family, step)
    827 def function(tag, scope):
    828   # Note the identity to move the tensor to the CPU.
    829   return gen_summary_ops.write_histogram_summary(
    830       _summary_state.writer._resource,  # pylint: disable=protected-access
    831       _choose_step(step),
    832       tag,
    833       array_ops.identity(tensor),
    834       name=scope)
--> 836 return summary_writer_function(name, tensor, function, family=family)

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\ops\summary_ops_v2.py:764, in summary_writer_function(name, tensor, function, family)
    762   return control_flow_ops.no_op()
    763 with ops.device(""cpu:0""):
--> 764   op = smart_cond.smart_cond(
    765       should_record_summaries(), record, _nothing, name="""")
    766   if not context.executing_eagerly():
    767     ops.add_to_collection(ops.GraphKeys._SUMMARY_COLLECTION, op)  # pylint: disable=protected-access

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\framework\smart_cond.py:54, in smart_cond(pred, true_fn, false_fn, name)
     52 if pred_value is not None:
     53   if pred_value:
---> 54     return true_fn()
     55   else:
     56     return false_fn()

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\ops\summary_ops_v2.py:758, in summary_writer_function.<locals>.record()
    755 def record():
    756   with ops.name_scope(name_scope), summary_op_util.summary_scope(
    757       name, family, values=[tensor]) as (tag, scope):
--> 758     with ops.control_dependencies([function(tag, scope)]):
    759       return constant_op.constant(True)

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\ops\summary_ops_v2.py:829, in histogram.<locals>.function(tag, scope)
    827 def function(tag, scope):
    828   # Note the identity to move the tensor to the CPU.
--> 829   return gen_summary_ops.write_histogram_summary(
    830       _summary_state.writer._resource,  # pylint: disable=protected-access
    831       _choose_step(step),
    832       tag,
    833       array_ops.identity(tensor),
    834       name=scope)

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\ops\gen_summary_ops.py:478, in write_histogram_summary(writer, step, tag, values, name)
    476   pass
    477 try:
--> 478   return write_histogram_summary_eager_fallback(
    479       writer, step, tag, values, name=name, ctx=_ctx)
    480 except _core._SymbolicException:
    481   pass  # Add nodes to the TensorFlow graph.

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\ops\gen_summary_ops.py:497, in write_histogram_summary_eager_fallback(writer, step, tag, values, name, ctx)
    495 _inputs_flat = [writer, step, tag, values]
    496 _attrs = (""T"", _attr_T)
--> 497 _result = _execute.execute(b""WriteHistogramSummary"", 0, inputs=_inputs_flat,
    498                            attrs=_attrs, ctx=ctx, name=name)
    499 _result = None
    500 return _result

File F:\Python\Miniconda3\envs\ml4t\lib\site-packages\tensorflow\python\eager\execute.py:59, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57 try:
     58   ctx.ensure_initialized()
---> 59   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                       inputs, attrs, num_outputs)
     61 except core._NotOkStatusException as e:
     62   if name is not None:

InvalidArgumentError: Nan in summary histogram for: hidden/kernel_0 [Op:WriteHistogramSummary]",circle2nd,109115560,closed,False,0,2022-07-31T09:45:30+00:00,2023-01-03T00:59:03+00:00,2023-01-03T00:59:03+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1323225124,251,"shape mismatch: value array of shape (4096,) could not be broadcast to indexing result of shape (2,4096,3)","
Trying to run 04_q_learning_for_trading results in the following error:

    118 print(targets.shape)
    119 print(targets)
--> 120 q_values[[self.idx, actions]] = targets
    122 loss = self.online_network.train_on_batch(x=states, y=q_values)
    123 self.losses.append(loss)

ValueError: shape mismatch: value array of shape (4096,) could not be broadcast to indexing result of shape (2,4096,3)

Running tensorflow 2.9.2 (GPU)
",mohala562,930331,closed,False,5,2022-07-30T17:16:50+00:00,2023-01-03T01:03:04+00:00,2023-01-03T01:00:51+00:00,,1,1,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1302038412,250,cp2. edgar_xbrl,"**Describe the bug**
A brief description of the bug and in which notebook/script it lives.

Bad zip file for 2021 Q1~2022Q2

**To Reproduce**
- In case you are not running the current version of the notebook/script as found on GitHub.

Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.
![제목 없음](https://user-images.githubusercontent.com/41133124/178496877-db6f687d-7b3e-4bee-abd7-aeb51e4662bf.png)

**Environment**
If you are not using the latest version of the Docker imag:
 
- OS: windows 11

**Additional context**
Add any other context about the problem here.
",tonygjwns,41133124,closed,False,2,2022-07-12T13:06:43+00:00,2022-07-26T09:54:57+00:00,2022-07-26T03:06:20+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1292877989,249,#Chapter 11 - 00_data_prep.ipynb Broken links,"Hi, 

In Chapter 11 and the first notebook 00_data_prep.ipynb the links send me to a 404 error. There are three links that do not work and are found in the following sentences:

_""We create the dataset here and store it in our [data] folder to facilitate reuse in later chapters.""

""The assets.h5 store can be generated using the the notebook [create_datasets] in the [data] directory in the root directory of this repo for instruction to download the following dataset.""_

Naturally I would need access to these to follow along with the example so I would appreciate it if you can take a look into this as soon as possible.

Thanks!",saad-ibrahim,54000785,closed,False,1,2022-07-04T09:18:53+00:00,2022-07-26T03:10:30+00:00,2022-07-26T03:10:30+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1292613312,248,RuntimeError: Cannot set name on a level of a MultiIndex. . Use 'MultiIndex.set_names' instead.,"**Describe the bug**
I am getting the above runtime error while executing alphalens_signals_quality.ipynb in line from Alphalens library function 
factor_data = get_clean_factor_and_forward_returns(factor=factor, prices=trade_prices,quantiles=5, periods=(1, 5, 10, 21)) . Appears to be a compatibility issue with pandas library . 

**To Reproduce**
- In case you are not running the current version of the notebook/script as found on GitHub.

Steps to reproduce the behavior:
1. Go to ' https://github.com/PacktPublishing/Machine-Learning-for-Algorithmic-Trading-Second-Edition/blob/master/11_decision_trees_random_forests/06_alphalens_signals_quality.ipynb'
2. Execute the above file 
3. Scroll down to statement -  
factor_data = get_clean_factor_and_forward_returns(factor=factor, prices=trade_prices,quantiles=5, periods=(1, 5, 10, 21)) . 
4. See error  [ attached screenshot ] 

**Expected behavior**
A clear and concise description of what you expected to happen.

**Screenshots**
If applicable, add screenshots to help explain your problem.

**Environment**

I am using  - 
OS:  Windows 10 
alphalens-0.3.6 
pandas 1.3.5 
python 3.7 

![error_alphalens_utils_get_clean_factor_and_forward_returns](https://user-images.githubusercontent.com/36529883/177082686-647a3b37-3880-4058-bd9c-35febe348d29.jpg)

**Additional context**
Add any other context about the problem here.
",lumiereUmbra,36529883,closed,False,1,2022-07-04T04:43:18+00:00,2023-01-03T01:01:52+00:00,2023-01-03T01:01:52+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1284701432,247,create_datasets.ipynb - Metadata on US-traded companies - file format change,"The NASDAQ, AMEX and NYSE file format changed:

1. The `marketcap` field is now `market cap`.
2. The values in `market cap` are no longer strings with suffixes but rather numerics and no longer require conversion. ",barashe,10500355,closed,False,1,2022-06-25T20:03:54+00:00,2022-07-26T02:36:41+00:00,2022-07-26T02:36:41+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1279496089,246,"Chapter 12, Section 4, Dollar Volume Rank is Never Used","Hello, 
Going through this section, I noticed that the Dollar Volume Rank, meant to restrict the trading universe, seems to never be used in the rest of the this chapter.  
Thanks! ",majidhosseini1,10819253,closed,False,1,2022-06-22T04:25:32+00:00,2022-07-26T02:35:09+00:00,2022-07-26T02:35:09+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1244036873,245,Chapter 4: 06_performance_eval_alphalens.ipynb,"**Describe the bug**
1. 
The `Mean Period Wise Return` chart produces by the `create summary_tear_sheet` is different from the same chart produces using output from `mean_return_by_quantile`. e.g. 10D shows a wider spread in the book version, using `mean_return_by_quantile`. However the tear sheet method shows 5D is superior.
Can anyone shed some light please?

tear sheet method:
![Screenshot from 2022-05-22 13-44-02](https://user-images.githubusercontent.com/43050453/169677501-ddb8bcd6-2032-482a-b37f-4ecc1ab3077d.png)

vs

`mean_return_by_quantile` methods:
![Screenshot from 2022-05-22 13-43-46](https://user-images.githubusercontent.com/43050453/169677499-25207707-41d4-4300-afcc-624cc8f91006.png)




",Quantuary,43050453,closed,False,1,2022-05-21T17:10:51+00:00,2022-07-26T02:44:22+00:00,2022-07-26T02:44:22+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1237571404,244,Kalman Filter and denoising ,"The example in denoising data with Kalman Filter seems that is not applied properly. The initial value is odd (relates to the guess? )  

https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/04_alpha_factor_research/03_kalman_filter_and_wavelets.ipynb 

Here is the pic from the book (second version, Kindle edition) 

![image](https://user-images.githubusercontent.com/26636184/168662081-a0157af0-44dd-45f1-b01e-5ff68eeb9fe0.png)

",ghost,10137,closed,False,2,2022-05-16T18:52:34+00:00,2022-07-26T02:46:04+00:00,2022-07-26T02:46:04+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1237345568,243,Download and store STOOQ data,"In the notebook `Download and store STOOQ data`. I have downloaded manually the files to get the STOOQ data but I’m facing some problems with the code.

Trying to add symbols `EmptyDataError: No columns to parse from file` (that’s obvious cause I’m reading .csv files and if you download the data manually you got .txt files.

And if a try to store it in the assets.h5 file I got `FileNotFoundError: [Errno 2] No such file or directory: 'stooq\\tickers\\jp\\tse stocks.csv'` (the folder structure I got is the one of the stooq web page (data/freq/market/asset_class).

Which is the easiest way to solve it? 

Thanks in advance,
",mresquivias,40606211,closed,False,1,2022-05-16T15:39:21+00:00,2022-07-26T02:40:52+00:00,2022-07-26T02:40:52+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1215239164,242,docs(various): Fix typos. Also closes #233,"Rather than continue making tiny PR's, these are all combined. 

One file had to be renamed because the typo was in the filename.",ryanrussell,523300,closed,False,0,2022-04-26T01:36:12+00:00,2022-07-26T02:26:27+00:00,2022-07-26T02:26:26+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1215170695,241,docs(19.01): Typo,,ryanrussell,523300,closed,False,0,2022-04-26T00:02:42+00:00,2022-07-26T02:49:18+00:00,2022-07-26T02:49:17+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1215169309,240,docs(19.04): Fix Typoes,,ryanrussell,523300,closed,False,0,2022-04-26T00:00:09+00:00,2022-07-26T02:48:48+00:00,2022-07-26T02:48:48+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1208958280,239,Chapter 4 - 01_feature_engineering,"Hello, I have a problem downloading data as described in the file 'create_datasets.ipynb', in the 'Wiki prices metadata' section. I can only download single records and when trying to download the entire database, I receive permission error.
As Quandl is no longer available I'm trying to download from links: ""https://data.nasdaq.com/api/v3/databases/SCF/data?download_type=full&api_key=YOURAPIKEY""
or
""https://data.nasdaq.com/api/v3/databases/SCF/data?download_type=partial&api_key=YOURAPIKEY""

Is there any other source with a .csv file that I could import for further analysis?",1JarekNowak,33979197,closed,False,2,2022-04-19T23:14:27+00:00,2022-07-26T02:44:57+00:00,2022-07-26T02:44:57+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1202890450,238,Chapter 18. 08_backtesting_with_zipline.ipynb Error,"**Describe the bug**
I checked the following error while running
Chapter 18. 08_backtesting_with_zipline.ipynb
What should I do?

This book is very helpful to my study. Thank you.

**Import&Settings**
```
from pathlib import Path
from collections import defaultdict
from time import time
import warnings

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import pandas_datareader.data as web
from logbook import Logger, StderrHandler, INFO, WARNING

from zipline import run_algorithm
from zipline.api import (attach_pipeline, pipeline_output,
                         date_rules, time_rules, record,
                         schedule_function, commission, slippage,
                         set_slippage, set_commission, set_max_leverage,
                         order_target, order_target_percent,
                         get_open_orders, cancel_order)
from zipline.data import bundles
from zipline.utils.run_algo import load_extensions
from zipline.pipeline import Pipeline, CustomFactor
from zipline.pipeline.data import Column, DataSet
from zipline.pipeline.domain import US_EQUITIES
from zipline.pipeline.filters import StaticAssets
from zipline.pipeline.loaders import USEquityPricingLoader
from zipline.pipeline.loaders.frame import DataFrameLoader
from trading_calendars import get_calendar

import pyfolio as pf
from pyfolio.plotting import plot_rolling_returns, plot_rolling_sharpe
from pyfolio.timeseries import forecast_cone_bootstrap

from alphalens.tears import (create_returns_tear_sheet,
                             create_summary_tear_sheet,
                             create_full_tear_sheet)

from alphalens.performance import mean_return_by_quantile
from alphalens.plotting import plot_quantile_returns_bar
from alphalens.utils import get_clean_factor_and_forward_returns, rate_of_return
```

**Alphalens Analysis**
```
def get_trade_prices(tickers):
    prices = (pd.read_hdf(DATA_STORE, 'quandl/wiki/prices').swaplevel().sort_index())
    prices.index.names = ['symbol', 'date']
    prices = prices.loc[idx[tickers, '2010':'2018'], 'adj_open']
    return (prices
            .unstack('symbol')
            .sort_index()
            .shift(-1)
            .tz_localize('UTC'))

predictions = (pd.read_hdf(results_path / 'predictions.h5', 'predictions')
               .iloc[:, :4]
               .mean(1)
               .to_frame('prediction'))

factor = (predictions
          .unstack('symbol')
          .asfreq('D')
          .dropna(how='all')
          .stack()
          .tz_localize('UTC', level='date')
          .sort_index())
tickers = factor.index.get_level_values('symbol').unique()
trade_prices = get_trade_prices(tickers).dropna()
factor_data = get_clean_factor_and_forward_returns(factor=factor,
                                                   prices=trade_prices,
                                                   quantiles=5,
                                                   max_loss=1.0,
                                                   periods=(1, 5, 10, 21)).sort_index()
```
**Load zipline extensions**
```
load_extensions(default=True,
                extensions=[],
                strict=True,
                environ=None)
```
```
log_handler = StderrHandler(format_string='[{record.time:%Y-%m-%d %H:%M:%S.%f}]: ' +
                            '{record.level_name}: {record.func_name}: {record.message}',
                            level=WARNING)
log_handler.push_application()
log = Logger('Algorithm')
```
**Algo Params**
```
N_LONGS = 25
N_SHORTS = 25
MIN_POSITIONS = 10
```

**Load Data**
```
def load_predictions(bundle):
    predictions = (pd.read_hdf(results_path / 'predictions.h5', 'predictions')
                   .iloc[:, :4]
                   .mean(1)
                   .to_frame('prediction'))
    tickers = predictions.index.get_level_values('symbol').unique().tolist()

    assets = bundle.asset_finder.lookup_symbols(tickers, as_of_date=None)
    predicted_sids = pd.Int64Index([asset.sid for asset in assets])
    ticker_map = dict(zip(tickers, predicted_sids))

    return (predictions
            .unstack('symbol')
            .rename(columns=ticker_map)
            .prediction
            .tz_localize('UTC')), assets
```
**Others codes is Same as GitHub example**

**Initialize Algorithm**
```
def initialize(context):
    context.longs = context.shorts = None
    set_slippage(slippage.FixedSlippage(spread=0.00))
    schedule_function(rebalance,
                      date_rules.every_day(),
                      time_rules.market_open(hours=1, minutes=30))
    schedule_function(record_vars,
                      date_rules.every_day(),
                      time_rules.market_close())

    pipeline = compute_signals()
    attach_pipeline(pipeline, 'signals')

def before_trading_start(context, data):
    output = pipeline_output('signals')
    longs = pipeline_output('signals').longs.astype(int)
    shorts = pipeline_output('signals').shorts.astype(int)
    holdings = context.portfolio.positions.keys()
    if longs.sum() > MIN_POSITIONS and shorts.sum() > MIN_POSITIONS:
        context.longs = longs[longs!=0].index
        context.shorts = shorts[shorts!=0].index
        context.divest = holdings - set(context.longs) - set(context.shorts)
    else:
        context.longs = context.shorts = pd.Index([])
        context.divest = set(holdings)

def rebalance(context, data):
    for symbol, open_orders in get_open_orders().items():
        for open_order in open_orders:
            cancel_order(open_order)       
    for stock in context.divest:
        order_target(stock, target=0)
    if not (context.longs.empty and context.shorts.empty):
        for stock in context.shorts:
            order_target_percent(stock, -1 / len(context.shorts) / 2)
        for stock in context.longs:
            order_target_percent(stock, 1 / len(context.longs))

def record_vars(context, data):
    record(leverage=context.account.leverage,
           longs=context.longs,
           shorts=context.shorts)
```

**Run Algorithm**
```
dates = predictions.index.get_level_values('date')
start_date, end_date = dates.min(), dates.max()
```
```
start = time()
results = run_algorithm(start=start_date,
                        end=end_date,
                        initialize=initialize,
                        before_trading_start=before_trading_start,
                        capital_base=1e5,
                        data_frequency='daily',
                        bundle='quandl',
                        custom_loader=signal_loader)  # need to modify zipline
```
**PyFolio Analysis**
```
returns, positions, transactions = pf.utils.extract_rets_pos_txn_from_zipline(results)
benchmark = web.DataReader('SP500', 'fred', '2011', '2017').squeeze()
benchmark = benchmark.pct_change().tz_localize('UTC')
```
**Custom Plots**
```
LIVE_DATE = '2016-01-01'
fig, axes = plt.subplots(ncols=2, figsize=(16, 5))
plot_rolling_returns(returns,
                     factor_returns=benchmark,
                     live_start_date=LIVE_DATE,
                     logy=False,
                     cone_std=2,
                     legend_loc='best',
                     volatility_match=False,
                     cone_function=forecast_cone_bootstrap,
                     ax=axes[0])
plot_rolling_sharpe(returns, ax=axes[1], rolling_window=63)
axes[0].set_title('Cumulative Returns - In and Out-of-Sample')
axes[1].set_title('Rolling Sharpe Ratio (3 Months)')
fig.tight_layout()
fig.savefig((results_path / 'pyfolio_out_of_sample').as_posix(), dpi=300)
```
```
KeyError: ""Passing list-likes to .loc or [] with any missing labels is no longer supported. The following labels were missing: DatetimeIndex(['2011-04-27 00:00:00+00:00', '2011-04-28 00:00:00+00:00',\n               '2011-04-29 00:00:00+00:00', '2011-05-02 00:00:00+00:00',\n               '2011-05-03 00:00:00+00:00',\n               ...\n               '2017-12-21 00:00:00+00:00', '2017-12-22 00:00:00+00:00',\n               '2017-12-26 00:00:00+00:00', '2017-12-27 00:00:00+00:00',\n               '2017-12-28 00:00:00+00:00'],\n              dtype='datetime64[ns, UTC]', length=493, freq=None). See [https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike""](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike%22)
 ```

**error screenshot**
![image](https://user-images.githubusercontent.com/70154806/163124699-40402e0e-5953-4eaf-b93a-f9765c076309.png)


- OS/Version:[Windows10]

**Additional context**
Add any other context about the problem here.
",silent0506,70154806,closed,False,3,2022-04-13T07:46:13+00:00,2023-01-03T01:04:29+00:00,2023-01-03T01:04:29+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1192384794,237,The step to create stooq/jp/tse/stocks/prices dataset.,"For Chapter 11, the Japanese market long-short strategy, I can not find a place to download the `stooq/jp/tse/stocks/prices` data set. The `create_dateset` file doesn't contain the content. Is there an additional file? ",YukoOshima,17468913,closed,False,0,2022-04-04T22:34:34+00:00,2022-04-05T23:12:56+00:00,2022-04-05T23:12:56+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1178042772,236,Chapter 2: Working with Order Book Data: NASDAQ ITCH,"When attempting to run the code:

buy_per_min.timestamp = buy_per_min.timestamp.add(utc_offset).astype(int)

I get the error TypeError: cannot astype a datetimelike from [datetime64[ns]] to [int32]

Help!",djkamm,43007854,closed,False,2,2022-03-23T12:18:37+00:00,2022-04-13T13:36:59+00:00,2022-04-13T13:36:58+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1174199283,235,extract_interesting_date_ranges  from 03_pyfolio_demo Modeling Event Risk,"**Describe the bug**
From 03_pyfolio_demo, Modelling Event Risk. 
Can't find any other events other than from the screenshots. I trace the file back to `interesting_periods.py`  it shows a list of periods. 
 
 
**Screenshots**
![image](https://user-images.githubusercontent.com/16059227/159111800-40cdab31-6beb-4ea4-b1ab-184b2d775324.png)



**Expected behavior**
There should be more than 4 keys, and Period['Flash Crash'] or ['Fukushima] should be included. 

Perhaps, am I doing something wrong? 




",CDLim0906,16059227,closed,False,1,2022-03-19T07:25:24+00:00,2023-01-03T01:05:49+00:00,2023-01-03T01:05:48+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1171898637,234,The ftp://emi.nasdaq.com/ITCH can not connect anymore,"The ftp://emi.nasdaq.com/ITCH can not connect anymore.
Where can I find the data to download?",YukoOshima,17468913,closed,False,1,2022-03-17T03:52:54+00:00,2022-03-18T03:31:21+00:00,2022-03-18T03:31:21+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1163567364,233,02_yfinance_demo.ipynb tiny mistake ,"**Describe the bug**

in the Section Data Download with proxy server，I tried it.

msgt.option_chain(proxy=PROXY_SERVER)

the last line ""msgt"" should change to msft, then it works

",ZhaiCong,26532631,closed,False,0,2022-03-09T07:05:52+00:00,2022-07-26T02:26:32+00:00,2022-07-26T02:26:32+00:00,,0,0,0,0,0,0,0
stefan-jansen/machine-learning-for-trading,1163424266,232,Understanding trading env,"Hi, I couldn't understand why here: https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/22_deep_reinforcement_learning/trading_env.py#L171, n_trades is computed as `n_trades = end_position - start_position`. I would really appreciate it if you can clarify. Thank you in advance!",davide97l,41103541,closed,False,2,2022-03-09T02:44:15+00:00,2022-03-16T16:49:57+00:00,2022-03-16T16:49:57+00:00,,0,0,0,0,0,0,0
