repo_full_name,pr_id,comment_id,user_login,user_id,created_at,updated_at,body,is_review_comment,path,position,diff_hunk,reactions_total,reactions_plus1,reactions_minus1,reactions_laugh,reactions_hooray,reactions_confused,reactions_heart
keras-team/keras,2513074374,2084163209,JyotinderSingh,33001894,2025-05-12T08:47:22+00:00,2025-05-12T08:48:15+00:00,"This may be a cleaner way to write this

```suggestion
        # Base case: Unbatched data
        batch_size = 1
        if len(images.shape) == 4:
            # This is a batch of images (4D input) 
            batch_size = self.backend.core.shape(images)[0]

        random_values = self.backend.random.uniform(
                shape=(batch_size,),
                minval=0,
                maxval=1,
                seed=seed,
            )
```",False,,,,0,0,0,0,0,0,0
keras-team/keras,2513074374,2085999177,JyotinderSingh,33001894,2025-05-13T06:16:51+00:00,2025-05-13T06:18:17+00:00,"Nit: The conditional logic is too deeply nested and reduces clarity. Maybe we can rewrite this as follows:

```py
            # Determine if the input was batched
            is_batched = (len(xs.shape) == 4)

            # If batched, select the first image from the batch for inspection.
            # Otherwise, use the transformed image directly.
            # `image_to_inspect` will always be a 3D tensor.
            if is_batched:
                image_to_inspect = transformed[0]
            else:
                image_to_inspect = transformed

            if data_format == ""channels_last"":
                # image_to_inspect has shape (H, W, C), get the first channel [:, :, 0]
                channel_data = image_to_inspect[:, :, 0]
            else: # data_format == ""channels_first""
                # image_to_inspect has shape (C, H, W), get the first channel [0, :, :]
                channel_data = image_to_inspect[0, :, :]

            unique_vals = np.unique(channel_data)
            self.assertEqual(len(unique_vals), 1)

```",False,,,,0,0,0,0,0,0,0
keras-team/keras,2513074374,2085999792,JyotinderSingh,33001894,2025-05-13T06:17:22+00:00,2025-05-13T06:18:17+00:00,"Nit

```suggestion
            # batched inputs
            (np.full((1, 4, 4, 3), 128, dtype=np.float32), ""channels_last""),
```",False,,,,0,0,0,0,0,0,0
keras-team/keras,2513074374,2084163209,JyotinderSingh,33001894,2025-05-12T08:47:22+00:00,2025-05-12T08:48:15+00:00,"This may be a cleaner way to write this

```suggestion
        # Base case: Unbatched data
        batch_size = 1
        if len(images.shape) == 4:
            # This is a batch of images (4D input) 
            batch_size = self.backend.core.shape(images)[0]

        random_values = self.backend.random.uniform(
                shape=(batch_size,),
                minval=0,
                maxval=1,
                seed=seed,
            )
```",True,keras/src/layers/preprocessing/image_preprocessing/random_grayscale.py,,"@@ -59,12 +59,20 @@ def __init__(self, factor=0.5, data_format=None, seed=None, **kwargs):
     def get_random_transformation(self, images, training=True, seed=None):
         if seed is None:
             seed = self._get_seed_generator(self.backend._backend)
-        random_values = self.backend.random.uniform(
-            shape=(self.backend.core.shape(images)[0],),
-            minval=0,
-            maxval=1,
-            seed=seed,
-        )
+        if len(images.shape) == 4:
+            random_values = self.backend.random.uniform(
+                shape=(self.backend.core.shape(images)[0],),
+                minval=0,
+                maxval=1,
+                seed=seed,
+            )
+        else:
+            random_values = self.backend.random.uniform(
+                shape=(1,),
+                minval=0,
+                maxval=1,
+                seed=seed,
+            )",0,0,0,0,0,0,0
keras-team/keras,2513074374,2085999177,JyotinderSingh,33001894,2025-05-13T06:16:51+00:00,2025-05-13T06:18:17+00:00,"Nit: The conditional logic is too deeply nested and reduces clarity. Maybe we can rewrite this as follows:

```py
            # Determine if the input was batched
            is_batched = (len(xs.shape) == 4)

            # If batched, select the first image from the batch for inspection.
            # Otherwise, use the transformed image directly.
            # `image_to_inspect` will always be a 3D tensor.
            if is_batched:
                image_to_inspect = transformed[0]
            else:
                image_to_inspect = transformed

            if data_format == ""channels_last"":
                # image_to_inspect has shape (H, W, C), get the first channel [:, :, 0]
                channel_data = image_to_inspect[:, :, 0]
            else: # data_format == ""channels_first""
                # image_to_inspect has shape (C, H, W), get the first channel [0, :, :]
                channel_data = image_to_inspect[0, :, :]

            unique_vals = np.unique(channel_data)
            self.assertEqual(len(unique_vals), 1)

```",True,keras/src/layers/preprocessing/image_preprocessing/random_grayscale_test.py,,"@@ -80,15 +80,28 @@ def test_grayscale_with_single_color_image(self):
         test_cases = [
             (np.full((1, 4, 4, 3), 128, dtype=np.float32), ""channels_last""),
             (np.full((1, 3, 4, 4), 128, dtype=np.float32), ""channels_first""),
+            # unbatched inputs
+            (np.full((4, 4, 3), 128, dtype=np.float32), ""channels_last""),
+            (np.full((3, 4, 4), 128, dtype=np.float32), ""channels_first""),
         ]
 
         for xs, data_format in test_cases:
             layer = layers.RandomGrayscale(factor=1.0, data_format=data_format)
             transformed = ops.convert_to_numpy(layer(xs))
-
-            if data_format == ""channels_last"":
-                unique_vals = np.unique(transformed[0, :, :, 0])
-                self.assertEqual(len(unique_vals), 1)
+            
+            if len(xs.shape)==4:
+                # batched inputs
+                if data_format == ""channels_last"":
+                    unique_vals = np.unique(transformed[0, :, :, 0])
+                    self.assertEqual(len(unique_vals), 1)
+                else:
+                    unique_vals = np.unique(transformed[0, 0, :, :])
+                    self.assertEqual(len(unique_vals), 1)
             else:
-                unique_vals = np.unique(transformed[0, 0, :, :])
-                self.assertEqual(len(unique_vals), 1)
+                # unbatched inputs
+                if data_format == ""channels_last"":
+                    unique_vals = np.unique(transformed[ :, :, 0])
+                    self.assertEqual(len(unique_vals), 1)
+                else:
+                    unique_vals = np.unique(transformed[ 0, :, :])
+                    self.assertEqual(len(unique_vals), 1)",0,0,0,0,0,0,0
keras-team/keras,2513074374,2085999792,JyotinderSingh,33001894,2025-05-13T06:17:22+00:00,2025-05-13T06:18:17+00:00,"Nit

```suggestion
            # batched inputs
            (np.full((1, 4, 4, 3), 128, dtype=np.float32), ""channels_last""),
```",True,keras/src/layers/preprocessing/image_preprocessing/random_grayscale_test.py,5.0,"@@ -80,15 +80,28 @@ def test_grayscale_with_single_color_image(self):
         test_cases = [
             (np.full((1, 4, 4, 3), 128, dtype=np.float32), ""channels_last""),",0,0,0,0,0,0,0
keras-team/keras,2512272091,2083690235,fchollet,710255,2025-05-12T01:24:41+00:00,2025-05-12T01:24:47+00:00,Can we refactor this logic into a standalone function that we could reuse across all callbacks that need this functionality?,False,,,,0,0,0,0,0,0,0
keras-team/keras,2512272091,2083690235,fchollet,710255,2025-05-12T01:24:41+00:00,2025-05-12T01:24:47+00:00,Can we refactor this logic into a standalone function that we could reuse across all callbacks that need this functionality?,True,keras/src/callbacks/model_checkpoint.py,53.0,"@@ -197,6 +183,44 @@ def __init__(
                     f""filepath={self.filepath}""
                 )
 
+    def _set_monitor_op(self):",0,0,0,0,0,0,0
keras-team/keras,2510619007,2085959552,rkazants,35459624,2025-05-13T05:42:46+00:00,2025-05-13T05:42:47+00:00,"do we really need this code? Why `x1, x2 = _align_operand_types(x1, x2, ""select()"")` does not help? The main logic of this helper to align arguments",False,,,,0,0,0,0,0,0,0
keras-team/keras,2510619007,2085960831,rkazants,35459624,2025-05-13T05:44:09+00:00,2025-05-13T05:44:09+00:00,why it is needed?,False,,,,0,0,0,0,0,0,0
keras-team/keras,2510619007,2086063453,Mohamed-Ashraf273,117025882,2025-05-13T06:57:53+00:00,2025-05-13T07:39:53+00:00,"Yeah, we need this code,  ```_align_operand_types``` alone isn't sufficient to handle cases where xi is an integer or float. Without this fix, related tests fail.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2510619007,2086074560,Mohamed-Ashraf273,117025882,2025-05-13T07:05:19+00:00,2025-05-13T07:29:39+00:00,"This change handles cases where a list contains ```OpenVINOKerasTensor``` elements. I first encountered the issue while running ```split``` tests, the tests took an unusually long time and never completed. After investigating, I found that the issue was caused by split returning a list of ```OpenVINOKerasTensor``` objects, and the issue was at line where ```np.array(x)``` was trying to convert them. Although Iâ€™m not sure whether it would have eventually finished, it was taking far too long, I solved it by converting each element in the list to a numpy first if it is an ```OpenVINOKerasTensor``` object.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2510619007,2086131884,Mohamed-Ashraf273,117025882,2025-05-13T07:35:53+00:00,2025-05-13T07:35:53+00:00,"![image](https://github.com/user-attachments/assets/14d71922-74cb-4a23-929e-0f4722cc46cb)
Look tests stop here and take too long and never finish.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2510619007,2086138559,Mohamed-Ashraf273,117025882,2025-05-13T07:39:36+00:00,2025-05-13T07:39:36+00:00,"![image](https://github.com/user-attachments/assets/32d7cd7b-d6e9-403c-b62a-86ab6a7beefb)
![image](https://github.com/user-attachments/assets/38238501-eb27-4dd6-96f7-c94237f7c022)
",False,,,,0,0,0,0,0,0,0
keras-team/keras,2510619007,2085959552,rkazants,35459624,2025-05-13T05:42:46+00:00,2025-05-13T05:42:47+00:00,"do we really need this code? Why `x1, x2 = _align_operand_types(x1, x2, ""select()"")` does not help? The main logic of this helper to align arguments",True,keras/src/backend/openvino/numpy.py,130.0,"@@ -1426,7 +1478,44 @@ def vectorize(pyfunc, *, excluded=None, signature=None):
 
 
 def where(condition, x1, x2):
-    raise NotImplementedError(""`where` is not supported with openvino backend"")
+    condition = get_ov_output(condition)
+    if x1 is None and x2 is None:
+        nonzero_indices = ov_opset.non_zero(condition)
+        return OpenVINOKerasTensor(nonzero_indices.output(0))
+    if x1 is None:
+        return OpenVINOKerasTensor(condition)
+    if x2 is None:
+        raise ValueError(""x2 must be provided if x1 is specified."")
+
+    x1_type = get_ov_output(x1).get_element_type()
+    x2_type = get_ov_output(x2).get_element_type()
+
+    if x1_type == Type.boolean or x2_type == Type.boolean:
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2)
+    elif isinstance(x1, float):
+        x2 = get_ov_output(x2)
+        if x2_type.is_integral():
+            x1 = get_ov_output(x1)
+        else:
+            x1 = get_ov_output(x1, x2_type)
+    elif isinstance(x2, float):
+        x1 = get_ov_output(x1)
+        if x1_type.is_integral():
+            x2 = get_ov_output(x2)
+        else:
+            x2 = get_ov_output(x2, x1_type)
+    elif isinstance(x2, int):
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2, x1_type)
+    elif isinstance(x1, int):
+        x2 = get_ov_output(x2)
+        x1 = get_ov_output(x1, x2_type)
+    else:
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2)",0,0,0,0,0,0,0
keras-team/keras,2510619007,2085960831,rkazants,35459624,2025-05-13T05:44:09+00:00,2025-05-13T05:44:09+00:00,why it is needed?,True,keras/src/backend/openvino/core.py,14.0,"@@ -450,8 +450,16 @@ def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):
 def convert_to_numpy(x):
     if isinstance(x, np.ndarray):
         return x
-    elif isinstance(x, (int, float, list, tuple)):
+    elif isinstance(x, (int, float)):
         return np.array(x)
+    elif isinstance(x, (list, tuple)):
+        x_new = []
+        for elem in x:
+            if isinstance(elem, OpenVINOKerasTensor):
+                x_new.append(convert_to_numpy(elem))
+            else:
+                x_new.append(elem)
+        return np.array(x_new)",0,0,0,0,0,0,0
keras-team/keras,2510619007,2086063453,Mohamed-Ashraf273,117025882,2025-05-13T06:57:53+00:00,2025-05-13T07:39:53+00:00,"Yeah, we need this code,  ```_align_operand_types``` alone isn't sufficient to handle cases where xi is an integer or float. Without this fix, related tests fail.",True,keras/src/backend/openvino/numpy.py,130.0,"@@ -1426,7 +1478,44 @@ def vectorize(pyfunc, *, excluded=None, signature=None):
 
 
 def where(condition, x1, x2):
-    raise NotImplementedError(""`where` is not supported with openvino backend"")
+    condition = get_ov_output(condition)
+    if x1 is None and x2 is None:
+        nonzero_indices = ov_opset.non_zero(condition)
+        return OpenVINOKerasTensor(nonzero_indices.output(0))
+    if x1 is None:
+        return OpenVINOKerasTensor(condition)
+    if x2 is None:
+        raise ValueError(""x2 must be provided if x1 is specified."")
+
+    x1_type = get_ov_output(x1).get_element_type()
+    x2_type = get_ov_output(x2).get_element_type()
+
+    if x1_type == Type.boolean or x2_type == Type.boolean:
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2)
+    elif isinstance(x1, float):
+        x2 = get_ov_output(x2)
+        if x2_type.is_integral():
+            x1 = get_ov_output(x1)
+        else:
+            x1 = get_ov_output(x1, x2_type)
+    elif isinstance(x2, float):
+        x1 = get_ov_output(x1)
+        if x1_type.is_integral():
+            x2 = get_ov_output(x2)
+        else:
+            x2 = get_ov_output(x2, x1_type)
+    elif isinstance(x2, int):
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2, x1_type)
+    elif isinstance(x1, int):
+        x2 = get_ov_output(x2)
+        x1 = get_ov_output(x1, x2_type)
+    else:
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2)",0,0,0,0,0,0,0
keras-team/keras,2510619007,2086074560,Mohamed-Ashraf273,117025882,2025-05-13T07:05:19+00:00,2025-05-13T07:29:39+00:00,"This change handles cases where a list contains ```OpenVINOKerasTensor``` elements. I first encountered the issue while running ```split``` tests, the tests took an unusually long time and never completed. After investigating, I found that the issue was caused by split returning a list of ```OpenVINOKerasTensor``` objects, and the issue was at line where ```np.array(x)``` was trying to convert them. Although Iâ€™m not sure whether it would have eventually finished, it was taking far too long, I solved it by converting each element in the list to a numpy first if it is an ```OpenVINOKerasTensor``` object.",True,keras/src/backend/openvino/core.py,14.0,"@@ -450,8 +450,16 @@ def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):
 def convert_to_numpy(x):
     if isinstance(x, np.ndarray):
         return x
-    elif isinstance(x, (int, float, list, tuple)):
+    elif isinstance(x, (int, float)):
         return np.array(x)
+    elif isinstance(x, (list, tuple)):
+        x_new = []
+        for elem in x:
+            if isinstance(elem, OpenVINOKerasTensor):
+                x_new.append(convert_to_numpy(elem))
+            else:
+                x_new.append(elem)
+        return np.array(x_new)",0,0,0,0,0,0,0
keras-team/keras,2510619007,2086131884,Mohamed-Ashraf273,117025882,2025-05-13T07:35:53+00:00,2025-05-13T07:35:53+00:00,"![image](https://github.com/user-attachments/assets/14d71922-74cb-4a23-929e-0f4722cc46cb)
Look tests stop here and take too long and never finish.",True,keras/src/backend/openvino/core.py,14.0,"@@ -450,8 +450,16 @@ def convert_to_tensor(x, dtype=None, sparse=None, ragged=None):
 def convert_to_numpy(x):
     if isinstance(x, np.ndarray):
         return x
-    elif isinstance(x, (int, float, list, tuple)):
+    elif isinstance(x, (int, float)):
         return np.array(x)
+    elif isinstance(x, (list, tuple)):
+        x_new = []
+        for elem in x:
+            if isinstance(elem, OpenVINOKerasTensor):
+                x_new.append(convert_to_numpy(elem))
+            else:
+                x_new.append(elem)
+        return np.array(x_new)",0,0,0,0,0,0,0
keras-team/keras,2510619007,2086138559,Mohamed-Ashraf273,117025882,2025-05-13T07:39:36+00:00,2025-05-13T07:39:36+00:00,"![image](https://github.com/user-attachments/assets/32d7cd7b-d6e9-403c-b62a-86ab6a7beefb)
![image](https://github.com/user-attachments/assets/38238501-eb27-4dd6-96f7-c94237f7c022)
",True,keras/src/backend/openvino/numpy.py,130.0,"@@ -1426,7 +1478,44 @@ def vectorize(pyfunc, *, excluded=None, signature=None):
 
 
 def where(condition, x1, x2):
-    raise NotImplementedError(""`where` is not supported with openvino backend"")
+    condition = get_ov_output(condition)
+    if x1 is None and x2 is None:
+        nonzero_indices = ov_opset.non_zero(condition)
+        return OpenVINOKerasTensor(nonzero_indices.output(0))
+    if x1 is None:
+        return OpenVINOKerasTensor(condition)
+    if x2 is None:
+        raise ValueError(""x2 must be provided if x1 is specified."")
+
+    x1_type = get_ov_output(x1).get_element_type()
+    x2_type = get_ov_output(x2).get_element_type()
+
+    if x1_type == Type.boolean or x2_type == Type.boolean:
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2)
+    elif isinstance(x1, float):
+        x2 = get_ov_output(x2)
+        if x2_type.is_integral():
+            x1 = get_ov_output(x1)
+        else:
+            x1 = get_ov_output(x1, x2_type)
+    elif isinstance(x2, float):
+        x1 = get_ov_output(x1)
+        if x1_type.is_integral():
+            x2 = get_ov_output(x2)
+        else:
+            x2 = get_ov_output(x2, x1_type)
+    elif isinstance(x2, int):
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2, x1_type)
+    elif isinstance(x1, int):
+        x2 = get_ov_output(x2)
+        x1 = get_ov_output(x1, x2_type)
+    else:
+        x1 = get_ov_output(x1)
+        x2 = get_ov_output(x2)",0,0,0,0,0,0,0
keras-team/keras,2509728912,2085961638,rkazants,35459624,2025-05-13T05:44:43+00:00,2025-05-13T05:44:43+00:00,please remove this file. It is needed only for developer purpose,False,,,,0,0,0,0,0,0,0
keras-team/keras,2509728912,2085963589,rkazants,35459624,2025-05-13T05:46:23+00:00,2025-05-13T05:46:23+00:00,"these two reshapes are not needed. please remove it.
reshapes below are sufficient.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2509728912,2085961638,rkazants,35459624,2025-05-13T05:44:43+00:00,2025-05-13T05:44:43+00:00,please remove this file. It is needed only for developer purpose,True,pytest.ini,,"@@ -0,0 +1,3 @@
+[pytest]",0,0,0,0,0,0,0
keras-team/keras,2509728912,2085963589,rkazants,35459624,2025-05-13T05:46:23+00:00,2025-05-13T05:46:23+00:00,"these two reshapes are not needed. please remove it.
reshapes below are sufficient.",True,keras/src/backend/openvino/numpy.py,,"@@ -1191,7 +1191,27 @@ def ones_like(x, dtype=None):
 
 
 def outer(x1, x2):
-    raise NotImplementedError(""`outer` is not supported with openvino backend"")
+    x1 = get_ov_output(x1)
+    x2 = get_ov_output(x2)
+
+    x1, x2 = _align_operand_types(x1, x2, ""outer()"")
+
+    x1_flatten = ov_opset.reshape(
+        x1, ov_opset.constant([-1], Type.i32), False
+    ).output(0)
+    x2_flatten = ov_opset.reshape(
+        x2, ov_opset.constant([-1], Type.i32), False
+    ).output(0)",0,0,0,0,0,0,0
keras-team/keras,2509278252,2085968066,rkazants,35459624,2025-05-13T05:50:15+00:00,2025-05-13T05:50:15+00:00,remove these changes,False,,,,0,0,0,0,0,0,0
keras-team/keras,2509278252,2085968376,rkazants,35459624,2025-05-13T05:50:30+00:00,2025-05-13T05:50:31+00:00,remove it. it is needed only for developer purpose,False,,,,0,0,0,0,0,0,0
keras-team/keras,2509278252,2085968066,rkazants,35459624,2025-05-13T05:50:15+00:00,2025-05-13T05:50:15+00:00,remove these changes,True,.gitignore,,"@@ -19,4 +19,5 @@ examples/**/*.jpg
 .python-version
 .coverage
 *coverage.xml
-.ruff_cache
\ No newline at end of file
+.ruff_cache
+venv/",0,0,0,0,0,0,0
keras-team/keras,2509278252,2085968376,rkazants,35459624,2025-05-13T05:50:30+00:00,2025-05-13T05:50:31+00:00,remove it. it is needed only for developer purpose,True,pytest.ini,,"@@ -0,0 +1,3 @@
+[pytest]",0,0,0,0,0,0,0
keras-team/keras,2506591990,2085970378,rkazants,35459624,2025-05-13T05:52:22+00:00,2025-05-13T05:52:22+00:00,please remove it,False,,,,0,0,0,0,0,0,0
keras-team/keras,2506591990,2085970536,rkazants,35459624,2025-05-13T05:52:31+00:00,2025-05-13T05:52:31+00:00,please remove it,False,,,,0,0,0,0,0,0,0
keras-team/keras,2506591990,2085970378,rkazants,35459624,2025-05-13T05:52:22+00:00,2025-05-13T05:52:22+00:00,please remove it,True,.gitignore,6.0,"@@ -19,4 +19,5 @@ examples/**/*.jpg
 .python-version
 .coverage
 *coverage.xml
-.ruff_cache
\ No newline at end of file
+.ruff_cache",0,0,0,0,0,0,0
keras-team/keras,2506591990,2085970536,rkazants,35459624,2025-05-13T05:52:31+00:00,2025-05-13T05:52:31+00:00,please remove it,True,pytest.ini,1.0,"@@ -0,0 +1,3 @@
+[pytest]",0,0,0,0,0,0,0
keras-team/keras,2503377193,2077570293,rkazants,35459624,2025-05-07T12:56:51+00:00,2025-05-07T12:56:51+00:00,"no need, pls remove this file. it is only for developer purpose.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2503377193,2077570293,rkazants,35459624,2025-05-07T12:56:51+00:00,2025-05-07T12:56:51+00:00,"no need, pls remove this file. it is only for developer purpose.",True,pytest.ini,,"@@ -0,0 +1,3 @@
+[pytest]",0,0,0,0,0,0,0
keras-team/keras,2500554851,2076783463,fchollet,710255,2025-05-07T04:52:02+00:00,2025-05-07T04:52:35+00:00,Please add indent,False,,,,0,0,0,0,0,0,0
keras-team/keras,2500554851,2076783557,fchollet,710255,2025-05-07T04:52:09+00:00,2025-05-07T04:52:35+00:00,TPU,False,,,,0,0,0,0,0,0,0
keras-team/keras,2500554851,2076783660,fchollet,710255,2025-05-07T04:52:18+00:00,2025-05-07T04:52:35+00:00,Please use 4-space indent,False,,,,0,0,0,0,0,0,0
keras-team/keras,2500554851,2076783463,fchollet,710255,2025-05-07T04:52:02+00:00,2025-05-07T04:52:35+00:00,Please add indent,True,keras/src/backend/jax/nn.py,,"@@ -205,9 +205,9 @@ def _pool(
         initial_value: the initial value for the reduction.
         reduce_fn: a reduce function of the form `(T, T) -> T`.
         pool_size: a sequence of `N` integers, representing the window size to
-            reduce over.
+          reduce over.
         strides: a sequence of `N` integers, representing the inter-window
-            strides (default: `(1, ..., 1)`).
+        strides (default: `(1, ..., 1)`).",0,0,0,0,0,0,0
keras-team/keras,2500554851,2076783557,fchollet,710255,2025-05-07T04:52:09+00:00,2025-05-07T04:52:35+00:00,TPU,True,keras/src/backend/jax/nn.py,,"@@ -1126,16 +1126,17 @@ def wrap_flash_attention(
     decoder_segment_ids,
     custom_mask=None,
     attn_logits_soft_cap=None,
+    head_shards=1,
+    q_seq_shards=1,
 ):
     if decoder_segment_ids is not None:
         assert query.shape[2] == decoder_segment_ids.q.shape[1], (
-            ""Sharding along sequence dimension not allowed in tpu kernel ""
-            ""attention""
+            ""Sharding along sequence dimension not allowed""
+            "" in tpu kernel attention""",0,0,0,0,0,0,0
keras-team/keras,2500554851,2076783660,fchollet,710255,2025-05-07T04:52:18+00:00,2025-05-07T04:52:35+00:00,Please use 4-space indent,True,keras/src/backend/jax/nn.py,,"@@ -1168,6 +1169,37 @@ def dot_product_attention(
     flash_attention=None,
     attn_logits_soft_cap=None,
 ):
+    """"""Computes dot-product attention given query, key, and value.
+
+    This is the core computation of attention that is used in transformers.
+    For TPU platforms, flash attention optimizations are automatically applied
+    when possible, and sharding parameters are inferred from the layout map
+    in the current distribution context.
+
+    Args:
+        query: Queries with shape `[batch, time, heads,
+          depth_k]`.",0,0,0,0,0,0,0
keras-team/keras,2499990569,2083303285,fchollet,710255,2025-05-10T22:46:02+00:00,2025-05-10T22:46:02+00:00,Would the integration prevent the use of `jax.jit` with Keras layers?,False,,,,0,0,0,0,0,0,0
keras-team/keras,2499990569,2083303285,fchollet,710255,2025-05-10T22:46:02+00:00,2025-05-10T22:46:02+00:00,Would the integration prevent the use of `jax.jit` with Keras layers?,True,keras/src/random/random_test.py,18.0,"@@ -380,12 +381,11 @@ def test_uniform_dtype_validation(self):
         reason=""This test requires `jax` as the backend."",
     )
     def test_dropout_jax_jit_stateless(self):
-        import jax
         import jax.numpy as jnp
 
         x = ops.ones(3)
 
-        @jax.jit
+        @nnx.jit",0,0,0,0,0,0,0
keras-team/keras,2498396029,2073811296,divyashreepathihalli,78194266,2025-05-05T16:54:17+00:00,2025-05-05T16:59:21+00:00,"I am very confused about the logic here.

- why is FA disabled if inputs are sharded?
",False,,,,0,0,0,0,0,0,0
keras-team/keras,2498396029,2073812589,divyashreepathihalli,78194266,2025-05-05T16:55:11+00:00,2025-05-05T17:03:41+00:00,"this condition is weird
if FA is enabled and inputs are sharded and is not running on TPU - you are disabling FA? why? can you please explain?
following this you are checking if running on TPU and FA is enabled - this will never be true if inputs are sharded - whats the point?",False,,,,0,0,0,0,0,0,0
keras-team/keras,2498396029,2073816560,divyashreepathihalli,78194266,2025-05-05T16:58:06+00:00,2025-05-05T16:59:05+00:00,lets verify numerics remain consistent with this updated code to mask,False,,,,0,0,0,0,0,0,0
keras-team/keras,2498396029,2073811296,divyashreepathihalli,78194266,2025-05-05T16:54:17+00:00,2025-05-05T16:59:21+00:00,"I am very confused about the logic here.

- why is FA disabled if inputs are sharded?
",True,keras/src/backend/jax/nn.py,,"@@ -1177,54 +1178,111 @@ def dot_product_attention(
             f""Received: query.shape={query.shape}, key.shape={key.shape}, ""
             f""value.shape={value.shape}.""
         )
-    if flash_attention is None:
-        flash_attention = _can_use_flash_attention(query, key, value, bias)
-    elif flash_attention is True:
-        # Use `raise_error=True` to provide more details if the inputs failed to
-        # use flash attention
-        _can_use_flash_attention(query, key, value, bias, raise_error=True)
 
-    if jax.devices()[0].platform == ""tpu"":
-        # Transpose to ('batch', 'heads', 'length', 'kv')
-        query = jnp.transpose(query, axes=(0, 2, 1, 3))
-        key = jnp.transpose(key, axes=(0, 2, 1, 3))
-        value = jnp.transpose(value, axes=(0, 2, 1, 3))
-        B, H, S, KV = query.shape
-
-        segment_ids = jnp.ones([B, S])
-        # {token_ids, padding_mask, segment_ids} enable packing
-        out = wrap_flash_attention(
-            query,
-            key,
-            value,
-            decoder_segment_ids=splash_attention_kernel.SegmentIds(
-                segment_ids, segment_ids
-            ),
-            custom_mask=mask,
-            attn_logits_soft_cap=attn_logits_soft_cap,
+    # Check if inputs are sharded
+    inputs_sharded = any(
+        hasattr(t, ""sharding"") and not t.sharding.is_fully_replicated
+        for t in (query, key, value)
+    )
+    is_tpu = jax.devices()[0].platform == ""tpu""
+
+    # Determine flash attention compatibility",0,0,0,0,0,0,0
keras-team/keras,2498396029,2073812589,divyashreepathihalli,78194266,2025-05-05T16:55:11+00:00,2025-05-05T17:03:41+00:00,"this condition is weird
if FA is enabled and inputs are sharded and is not running on TPU - you are disabling FA? why? can you please explain?
following this you are checking if running on TPU and FA is enabled - this will never be true if inputs are sharded - whats the point?",True,keras/src/backend/jax/nn.py,,"@@ -1177,54 +1178,111 @@ def dot_product_attention(
             f""Received: query.shape={query.shape}, key.shape={key.shape}, ""
             f""value.shape={value.shape}.""
         )
-    if flash_attention is None:
-        flash_attention = _can_use_flash_attention(query, key, value, bias)
-    elif flash_attention is True:
-        # Use `raise_error=True` to provide more details if the inputs failed to
-        # use flash attention
-        _can_use_flash_attention(query, key, value, bias, raise_error=True)
 
-    if jax.devices()[0].platform == ""tpu"":
-        # Transpose to ('batch', 'heads', 'length', 'kv')
-        query = jnp.transpose(query, axes=(0, 2, 1, 3))
-        key = jnp.transpose(key, axes=(0, 2, 1, 3))
-        value = jnp.transpose(value, axes=(0, 2, 1, 3))
-        B, H, S, KV = query.shape
-
-        segment_ids = jnp.ones([B, S])
-        # {token_ids, padding_mask, segment_ids} enable packing
-        out = wrap_flash_attention(
-            query,
-            key,
-            value,
-            decoder_segment_ids=splash_attention_kernel.SegmentIds(
-                segment_ids, segment_ids
-            ),
-            custom_mask=mask,
-            attn_logits_soft_cap=attn_logits_soft_cap,
+    # Check if inputs are sharded
+    inputs_sharded = any(
+        hasattr(t, ""sharding"") and not t.sharding.is_fully_replicated
+        for t in (query, key, value)
+    )
+    is_tpu = jax.devices()[0].platform == ""tpu""
+
+    # Determine flash attention compatibility
+    if flash_attention is None:
+        flash_attention = (
+            not inputs_sharded or is_tpu
+        ) and _can_use_flash_attention(query, key, value, bias)
+    elif flash_attention and inputs_sharded and not is_tpu:",0,0,0,0,0,0,0
keras-team/keras,2498396029,2073816560,divyashreepathihalli,78194266,2025-05-05T16:58:06+00:00,2025-05-05T16:59:05+00:00,lets verify numerics remain consistent with this updated code to mask,True,keras/src/backend/jax/nn.py,,"@@ -1177,54 +1178,111 @@ def dot_product_attention(
             f""Received: query.shape={query.shape}, key.shape={key.shape}, ""
             f""value.shape={value.shape}.""
         )
-    if flash_attention is None:
-        flash_attention = _can_use_flash_attention(query, key, value, bias)
-    elif flash_attention is True:
-        # Use `raise_error=True` to provide more details if the inputs failed to
-        # use flash attention
-        _can_use_flash_attention(query, key, value, bias, raise_error=True)
 
-    if jax.devices()[0].platform == ""tpu"":
-        # Transpose to ('batch', 'heads', 'length', 'kv')
-        query = jnp.transpose(query, axes=(0, 2, 1, 3))
-        key = jnp.transpose(key, axes=(0, 2, 1, 3))
-        value = jnp.transpose(value, axes=(0, 2, 1, 3))
-        B, H, S, KV = query.shape
-
-        segment_ids = jnp.ones([B, S])
-        # {token_ids, padding_mask, segment_ids} enable packing
-        out = wrap_flash_attention(
-            query,
-            key,
-            value,
-            decoder_segment_ids=splash_attention_kernel.SegmentIds(
-                segment_ids, segment_ids
-            ),
-            custom_mask=mask,
-            attn_logits_soft_cap=attn_logits_soft_cap,
+    # Check if inputs are sharded
+    inputs_sharded = any(
+        hasattr(t, ""sharding"") and not t.sharding.is_fully_replicated
+        for t in (query, key, value)
+    )
+    is_tpu = jax.devices()[0].platform == ""tpu""
+
+    # Determine flash attention compatibility
+    if flash_attention is None:
+        flash_attention = (
+            not inputs_sharded or is_tpu
+        ) and _can_use_flash_attention(query, key, value, bias)
+    elif flash_attention and inputs_sharded and not is_tpu:
+        flash_attention = False
+
+    # TPU-specific flash attention path
+    if is_tpu and flash_attention:
+        # Transpose to ('batch', 'heads', 'length', 'head_dim')
+        query_tpu_layout = jnp.transpose(query, axes=(0, 2, 1, 3))
+        key_tpu_layout = jnp.transpose(key, axes=(0, 2, 1, 3))
+        value_tpu_layout = jnp.transpose(value, axes=(0, 2, 1, 3))
+
+        bs, num_heads, q_len, head_dim = query_tpu_layout.shape
+
+        # Apply scale to query if provided
+        if scale is not None:
+            # TPU kernel applies 1/sqrt(head_dim) internally, to achieve
+            # overall QK^T * scale, scale query by (scale * sqrt(head_dim))
+            query_tpu_layout = query_tpu_layout * (scale * math.sqrt(head_dim))
+
+        # Create segment IDs for Splash Attention (for packing/batching)
+        segment_ids = jnp.zeros([bs, q_len], dtype=jnp.int32)
+        decoder_segment_ids = splash_attention_kernel.SegmentIds(
+            q=segment_ids, kv=segment_ids
         )
-        out = jnp.transpose(out, axes=(0, 2, 1, 3))
-        return out
 
-    # `dot_product_attention` is only available in jax>=0.4.31
+        # Process mask for Splash Attention
+        custom_mask = None",0,0,0,0,0,0,0
keras-team/keras,2493538526,2070847236,fchollet,710255,2025-05-01T21:45:23+00:00,2025-05-01T21:46:04+00:00,Please also add the function as raising `NotImplementedError` in the OpenVino `numpy.py`,False,,,,1,1,0,0,0,0,0
keras-team/keras,2493538526,2070847729,fchollet,710255,2025-05-01T21:45:58+00:00,2025-05-01T21:46:04+00:00,Is floatx the right dtype to cast to or should be infer from the input dtype?,False,,,,1,1,0,0,0,0,0
keras-team/keras,2493538526,2070847236,fchollet,710255,2025-05-01T21:45:23+00:00,2025-05-01T21:46:04+00:00,Please also add the function as raising `NotImplementedError` in the OpenVino `numpy.py`,True,keras/src/backend/openvino/excluded_concrete_tests.txt,4.0,"@@ -9,6 +9,7 @@ NumpyDtypeTest::test_any
 NumpyDtypeTest::test_argpartition
 NumpyDtypeTest::test_array
 NumpyDtypeTest::test_bartlett
+NumpyDtypeTest::test_blackman",1,1,0,0,0,0,0
keras-team/keras,2493538526,2070847729,fchollet,710255,2025-05-01T21:45:58+00:00,2025-05-01T21:46:04+00:00,Is floatx the right dtype to cast to or should be infer from the input dtype?,True,keras/src/backend/numpy/numpy.py,6.0,"@@ -390,6 +390,11 @@ def right_shift(x, y):
     return bitwise_right_shift(x, y)
 
 
+def blackman(x):
+    x = convert_to_tensor(x)
+    return np.blackman(x).astype(config.floatx())",1,1,0,0,0,0,0
keras-team/keras,2491474991,2070641353,rkazants,35459624,2025-05-01T18:40:17+00:00,2025-05-01T18:40:17+00:00,"also, please remove `NumpyDtypeTest::test_flip` above",False,,,,0,0,0,0,0,0,0
keras-team/keras,2491474991,2070641782,rkazants,35459624,2025-05-01T18:40:39+00:00,2025-05-01T18:40:40+00:00,not needed. use `ov_opset`,False,,,,0,0,0,0,0,0,0
keras-team/keras,2491474991,2070641353,rkazants,35459624,2025-05-01T18:40:17+00:00,2025-05-01T18:40:17+00:00,"also, please remove `NumpyDtypeTest::test_flip` above",True,keras/src/backend/openvino/excluded_concrete_tests.txt,9.0,"@@ -82,7 +82,6 @@ NumpyOneInputOpsCorrectnessTest::test_cumprod
 NumpyOneInputOpsCorrectnessTest::test_diag",0,0,0,0,0,0,0
keras-team/keras,2491474991,2070641782,rkazants,35459624,2025-05-01T18:40:39+00:00,2025-05-01T18:40:40+00:00,not needed. use `ov_opset`,True,keras/src/backend/openvino/numpy.py,,"@@ -1,4 +1,5 @@
 import numpy as np
+import openvino.runtime.opset1 as ov1",0,0,0,0,0,0,0
keras-team/keras,2489509461,2069583181,fchollet,710255,2025-04-30T22:36:24+00:00,2025-04-30T22:36:30+00:00,If this is intended to be public in the API then we should provide an explanation and a usage example in the docstring,False,,,,0,0,0,0,0,0,0
keras-team/keras,2489509461,2069593117,cantonios,2538739,2025-04-30T22:40:01+00:00,2025-04-30T22:40:01+00:00,"Great!  Yes, I'd like it to be public so I can inherit from it without needing to import internal keras APIs.

Is this location (within `optimizer.py`) okay, or would you prefer it somewhere else, (e.g. in its own file)?",False,,,,0,0,0,0,0,0,0
keras-team/keras,2489509461,2069583181,fchollet,710255,2025-04-30T22:36:24+00:00,2025-04-30T22:36:30+00:00,If this is intended to be public in the API then we should provide an explanation and a usage example in the docstring,True,keras/src/optimizers/optimizer.py,12.0,"@@ -23,5 +24,90 @@ class Optimizer(BackendOptimizer, base_optimizer.BaseOptimizer):
     pass
 
 
+@keras_export(""keras.optimizers.VariableUpdater"")
+class VariableUpdater:
+    """"""Allows special handling of variable updates.""""""",0,0,0,0,0,0,0
keras-team/keras,2489509461,2069593117,cantonios,2538739,2025-04-30T22:40:01+00:00,2025-04-30T22:40:01+00:00,"Great!  Yes, I'd like it to be public so I can inherit from it without needing to import internal keras APIs.

Is this location (within `optimizer.py`) okay, or would you prefer it somewhere else, (e.g. in its own file)?",True,keras/src/optimizers/optimizer.py,12.0,"@@ -23,5 +24,90 @@ class Optimizer(BackendOptimizer, base_optimizer.BaseOptimizer):
     pass
 
 
+@keras_export(""keras.optimizers.VariableUpdater"")
+class VariableUpdater:
+    """"""Allows special handling of variable updates.""""""",0,0,0,0,0,0,0
keras-team/keras,2489024825,2066727329,rkazants,35459624,2025-04-29T14:40:43+00:00,2025-04-29T14:40:44+00:00,please also remove `NumpyOneInputOpsCorrectnessTest::test_ravel` below,False,,,,0,0,0,0,0,0,0
keras-team/keras,2489024825,2066727329,rkazants,35459624,2025-04-29T14:40:43+00:00,2025-04-29T14:40:44+00:00,please also remove `NumpyOneInputOpsCorrectnessTest::test_ravel` below,True,keras/src/backend/openvino/excluded_concrete_tests.txt,6.0,"@@ -42,7 +42,6 @@ NumpyDtypeTest::test_outer_
 NumpyDtypeTest::test_power
 NumpyDtypeTest::test_prod
 NumpyDtypeTest::test_quantile
-NumpyDtypeTest::test_ravel
 NumpyDtypeTest::test_repeat
 NumpyDtypeTest::test_roll",0,0,0,0,0,0,0
keras-team/keras,2488968726,2067094465,fchollet,710255,2025-04-29T18:15:54+00:00,2025-04-29T18:15:55+00:00,"This API is basically single-user, so please keep it underscore-private",False,,,,0,0,0,0,0,0,0
keras-team/keras,2488968726,2067094932,fchollet,710255,2025-04-29T18:16:04+00:00,2025-04-29T18:16:04+00:00,Likewise here,False,,,,0,0,0,0,0,0,0
keras-team/keras,2488968726,2067698125,JyotinderSingh,33001894,2025-04-30T01:08:13+00:00,2025-04-30T01:08:13+00:00,Done,False,,,,0,0,0,0,0,0,0
keras-team/keras,2488968726,2067698204,JyotinderSingh,33001894,2025-04-30T01:08:21+00:00,2025-04-30T01:08:22+00:00,Done,False,,,,0,0,0,0,0,0,0
keras-team/keras,2488968726,2067725044,hertschuh,1091026,2025-04-30T01:50:02+00:00,2025-04-30T01:50:02+00:00,"Wait, this one does not have an underscore.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2488968726,2067842789,JyotinderSingh,33001894,2025-04-30T04:41:49+00:00,2025-04-30T04:41:49+00:00,Had forgotten to upload that change. Fixed now!,False,,,,0,0,0,0,0,0,0
keras-team/keras,2488968726,2069023143,hertschuh,1091026,2025-04-30T16:15:30+00:00,2025-04-30T16:15:30+00:00,"Wait, this needs to move to `__init__`. Otherwise it will overwrite your registrations.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2488968726,2069035015,hertschuh,1091026,2025-04-30T16:20:48+00:00,2025-04-30T16:20:48+00:00,"Never mind, this is `__init__`.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2488968726,2067094465,fchollet,710255,2025-04-29T18:15:54+00:00,2025-04-29T18:15:55+00:00,"This API is basically single-user, so please keep it underscore-private",True,keras/src/layers/layer.py,,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):
                 return rematerialized_activation_call_wrapper
         return layer_call
 
+    def register_call_context_args(self, *names):",0,0,0,0,0,0,0
keras-team/keras,2488968726,2067094932,fchollet,710255,2025-04-29T18:16:04+00:00,2025-04-29T18:16:04+00:00,Likewise here,True,keras/src/layers/layer.py,,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):
                 return rematerialized_activation_call_wrapper
         return layer_call
 
+    def register_call_context_args(self, *names):
+        """"""Register call-context args to be propagated by this layer.
+
+        This is useful in registering custom context-args with predefined
+        layer and model classes so that they know which arguments to
+        propagate down their call-stack.
+
+        Can be invoked any time *before* the layer is called.
+
+        Example:
+        ```
+        class Inner(layers.Layer):
+            call_context_args = (""foo_mode"",)
+
+            def call(self, x, foo_mode=False):
+                # If foo_mode=True add 1, otherwise add 0
+                add_val = ops.where(foo_mode, 1.0, 0.0)
+                return x + add_val
+
+        class Outer(layers.Layer):
+            def __init__(self):
+                super().__init__()
+                self.inner = Inner()
+
+            def call(self, x):
+                # We donâ€™t explicitly pass foo_mode hereâ€”Base Layer.__call__
+                # should inject it into `self.inner`
+                return self.inner(x)
+
+        sample_input = np.array([[1.0], [2.0]])
+
+        # Sequential model
+        seq = models.Sequential([Outer()])
+
+        # Tell the Sequential model to propagate foo_mode down
+        # the call-stack
+        seq.register_call_context_args(""foo_mode"")
+
+        # foo_mode=True -> input + 1
+        out_true = seq(sample_input, foo_mode=True)
+        """"""
+        if self._called:
+            raise RuntimeError(
+                ""Cannot add call-context args after the layer has been called.""
+            )
+        self._call_context_args = self._call_context_args | set(names)
+
+    @property
+    def call_context_args(self):",0,0,0,0,0,0,0
keras-team/keras,2488968726,2067698125,JyotinderSingh,33001894,2025-04-30T01:08:13+00:00,2025-04-30T01:08:13+00:00,Done,True,keras/src/layers/layer.py,,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):
                 return rematerialized_activation_call_wrapper
         return layer_call
 
+    def register_call_context_args(self, *names):",0,0,0,0,0,0,0
keras-team/keras,2488968726,2067698204,JyotinderSingh,33001894,2025-04-30T01:08:21+00:00,2025-04-30T01:08:22+00:00,Done,True,keras/src/layers/layer.py,,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):
                 return rematerialized_activation_call_wrapper
         return layer_call
 
+    def register_call_context_args(self, *names):
+        """"""Register call-context args to be propagated by this layer.
+
+        This is useful in registering custom context-args with predefined
+        layer and model classes so that they know which arguments to
+        propagate down their call-stack.
+
+        Can be invoked any time *before* the layer is called.
+
+        Example:
+        ```
+        class Inner(layers.Layer):
+            call_context_args = (""foo_mode"",)
+
+            def call(self, x, foo_mode=False):
+                # If foo_mode=True add 1, otherwise add 0
+                add_val = ops.where(foo_mode, 1.0, 0.0)
+                return x + add_val
+
+        class Outer(layers.Layer):
+            def __init__(self):
+                super().__init__()
+                self.inner = Inner()
+
+            def call(self, x):
+                # We donâ€™t explicitly pass foo_mode hereâ€”Base Layer.__call__
+                # should inject it into `self.inner`
+                return self.inner(x)
+
+        sample_input = np.array([[1.0], [2.0]])
+
+        # Sequential model
+        seq = models.Sequential([Outer()])
+
+        # Tell the Sequential model to propagate foo_mode down
+        # the call-stack
+        seq.register_call_context_args(""foo_mode"")
+
+        # foo_mode=True -> input + 1
+        out_true = seq(sample_input, foo_mode=True)
+        """"""
+        if self._called:
+            raise RuntimeError(
+                ""Cannot add call-context args after the layer has been called.""
+            )
+        self._call_context_args = self._call_context_args | set(names)
+
+    @property
+    def call_context_args(self):",0,0,0,0,0,0,0
keras-team/keras,2488968726,2067725044,hertschuh,1091026,2025-04-30T01:50:02+00:00,2025-04-30T01:50:02+00:00,"Wait, this one does not have an underscore.",True,keras/src/layers/layer.py,,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):
                 return rematerialized_activation_call_wrapper
         return layer_call
 
+    def register_call_context_args(self, *names):
+        """"""Register call-context args to be propagated by this layer.
+
+        This is useful in registering custom context-args with predefined
+        layer and model classes so that they know which arguments to
+        propagate down their call-stack.
+
+        Can be invoked any time *before* the layer is called.
+
+        Example:
+        ```
+        class Inner(layers.Layer):
+            call_context_args = (""foo_mode"",)
+
+            def call(self, x, foo_mode=False):
+                # If foo_mode=True add 1, otherwise add 0
+                add_val = ops.where(foo_mode, 1.0, 0.0)
+                return x + add_val
+
+        class Outer(layers.Layer):
+            def __init__(self):
+                super().__init__()
+                self.inner = Inner()
+
+            def call(self, x):
+                # We donâ€™t explicitly pass foo_mode hereâ€”Base Layer.__call__
+                # should inject it into `self.inner`
+                return self.inner(x)
+
+        sample_input = np.array([[1.0], [2.0]])
+
+        # Sequential model
+        seq = models.Sequential([Outer()])
+
+        # Tell the Sequential model to propagate foo_mode down
+        # the call-stack
+        seq.register_call_context_args(""foo_mode"")
+
+        # foo_mode=True -> input + 1
+        out_true = seq(sample_input, foo_mode=True)
+        """"""
+        if self._called:
+            raise RuntimeError(
+                ""Cannot add call-context args after the layer has been called.""
+            )
+        self._call_context_args = self._call_context_args | set(names)
+
+    @property
+    def call_context_args(self):",0,0,0,0,0,0,0
keras-team/keras,2488968726,2067842789,JyotinderSingh,33001894,2025-04-30T04:41:49+00:00,2025-04-30T04:41:49+00:00,Had forgotten to upload that change. Fixed now!,True,keras/src/layers/layer.py,,"@@ -1697,6 +1697,58 @@ def rematerialized_activation_call_wrapper(*args, **kwargs):
                 return rematerialized_activation_call_wrapper
         return layer_call
 
+    def register_call_context_args(self, *names):
+        """"""Register call-context args to be propagated by this layer.
+
+        This is useful in registering custom context-args with predefined
+        layer and model classes so that they know which arguments to
+        propagate down their call-stack.
+
+        Can be invoked any time *before* the layer is called.
+
+        Example:
+        ```
+        class Inner(layers.Layer):
+            call_context_args = (""foo_mode"",)
+
+            def call(self, x, foo_mode=False):
+                # If foo_mode=True add 1, otherwise add 0
+                add_val = ops.where(foo_mode, 1.0, 0.0)
+                return x + add_val
+
+        class Outer(layers.Layer):
+            def __init__(self):
+                super().__init__()
+                self.inner = Inner()
+
+            def call(self, x):
+                # We donâ€™t explicitly pass foo_mode hereâ€”Base Layer.__call__
+                # should inject it into `self.inner`
+                return self.inner(x)
+
+        sample_input = np.array([[1.0], [2.0]])
+
+        # Sequential model
+        seq = models.Sequential([Outer()])
+
+        # Tell the Sequential model to propagate foo_mode down
+        # the call-stack
+        seq.register_call_context_args(""foo_mode"")
+
+        # foo_mode=True -> input + 1
+        out_true = seq(sample_input, foo_mode=True)
+        """"""
+        if self._called:
+            raise RuntimeError(
+                ""Cannot add call-context args after the layer has been called.""
+            )
+        self._call_context_args = self._call_context_args | set(names)
+
+    @property
+    def call_context_args(self):",0,0,0,0,0,0,0
keras-team/keras,2488968726,2069023143,hertschuh,1091026,2025-04-30T16:15:30+00:00,2025-04-30T16:15:30+00:00,"Wait, this needs to move to `__init__`. Otherwise it will overwrite your registrations.",True,keras/src/layers/layer.py,19.0,"@@ -306,21 +306,21 @@ def __init__(
         self._losses_override = []
 
         self._call_signature = inspect.signature(self.call)
-        call_signature_parameters = [
+        self.call_signature_parameters = [
             p.name for p in self._call_signature.parameters.values()
         ]
-        self._call_has_training_arg = ""training"" in call_signature_parameters
-        self._call_has_mask_arg = ""mask"" in call_signature_parameters
+        self._call_has_training_arg = (
+            ""training"" in self.call_signature_parameters
+        )
+        self._call_has_mask_arg = ""mask"" in self.call_signature_parameters
 
         # 1. collect names that should be autoâ€‘propagated
-        builtin_context_args = {""training""}
-        custom_context_args = set(getattr(self, ""call_context_args"", ()))
-        self._call_ctx_args = builtin_context_args | custom_context_args
+        self._call_context_args = {""training""}",0,0,0,0,0,0,0
keras-team/keras,2488968726,2069035015,hertschuh,1091026,2025-04-30T16:20:48+00:00,2025-04-30T16:20:48+00:00,"Never mind, this is `__init__`.",True,keras/src/layers/layer.py,19.0,"@@ -306,21 +306,21 @@ def __init__(
         self._losses_override = []
 
         self._call_signature = inspect.signature(self.call)
-        call_signature_parameters = [
+        self.call_signature_parameters = [
             p.name for p in self._call_signature.parameters.values()
         ]
-        self._call_has_training_arg = ""training"" in call_signature_parameters
-        self._call_has_mask_arg = ""mask"" in call_signature_parameters
+        self._call_has_training_arg = (
+            ""training"" in self.call_signature_parameters
+        )
+        self._call_has_mask_arg = ""mask"" in self.call_signature_parameters
 
         # 1. collect names that should be autoâ€‘propagated
-        builtin_context_args = {""training""}
-        custom_context_args = set(getattr(self, ""call_context_args"", ()))
-        self._call_ctx_args = builtin_context_args | custom_context_args
+        self._call_context_args = {""training""}",0,0,0,0,0,0,0
keras-team/keras,2488390099,2072497061,fchollet,710255,2025-05-04T01:34:50+00:00,2025-05-04T01:34:50+00:00,There is not `math` namespace -- just export to `ops`,False,,,,0,0,0,0,0,0,0
keras-team/keras,2488390099,2072636142,DavidLandup0,60978046,2025-05-04T14:44:56+00:00,2025-05-04T14:44:56+00:00,"Whoops - good point. Accidentally left it in due to the original `keras.ops.numpy` location.
Removed!",False,,,,0,0,0,0,0,0,0
keras-team/keras,2488390099,2072497061,fchollet,710255,2025-05-04T01:34:50+00:00,2025-05-04T01:34:50+00:00,There is not `math` namespace -- just export to `ops`,True,keras/src/ops/math.py,,"@@ -1044,3 +1044,101 @@ def logdet(x):
     if any_symbolic_tensors((x,)):
         return Logdet().symbolic_call(x)
     return backend.math.logdet(x)
+
+
+class ViewAsComplex(Operation):
+    def call(self, x):
+        x = backend.convert_to_tensor(x)
+        if len(x.shape) < 1 or x.shape[-1] != 2:
+            raise ValueError(
+                ""Input tensor's last dimension must be 2 (real and imaginary).""
+            )
+        return x[..., 0] + 1j * x[..., 1]
+
+    def compute_output_spec(self, x):
+        return KerasTensor(shape=x.shape[:-1], dtype=""complex64"")
+
+
+class ViewAsReal(Operation):
+    def call(self, x):
+        x = backend.convert_to_tensor(x)
+        real_part = backend.numpy.real(x)
+        imag_part = backend.numpy.imag(x)
+        return backend.numpy.stack((real_part, imag_part), axis=-1)
+
+    def compute_output_spec(self, x):
+        return KerasTensor(shape=x.shape + (2,), dtype=""float32"")
+
+
+@keras_export([""keras.ops.view_as_complex"", ""keras.ops.math.view_as_complex""])
+def view_as_complex(x):
+    """"""Converts a real tensor with shape `(..., 2)` to a complex tensor,
+    where the last dimension represents the real and imaginary components
+    of a complex tensor.
+
+    Args:
+        x: A real tensor with last dimension of size 2.
+
+    Returns:
+        A complex tensor with shape `x.shape[:-1]`.
+
+    Example:
+
+    ```
+    >>> import numpy as np
+    >>> from keras import ops
+
+    >>> real_imag = np.array([[1.0, 2.0], [3.0, 4.0]])
+    >>> complex_tensor = ops.view_as_complex(real_imag)
+    >>> complex_tensor
+    array([1.+2.j, 3.+4.j])
+    ```
+    """"""
+    if any_symbolic_tensors((x,)):
+        return ViewAsComplex().symbolic_call(x)
+
+    x = backend.convert_to_tensor(x)
+    if len(x.shape) < 1 or x.shape[-1] != 2:
+        raise ValueError(
+            ""Last dimension of input must be size 2 (real and imaginary). ""
+            f""Received shape: {x.shape}""
+        )
+    real_part = x[..., 0]
+    imag_part = x[..., 1]
+
+    return backend.cast(real_part, dtype=""complex64"") + 1j * backend.cast(
+        imag_part, dtype=""complex64""
+    )
+
+
+@keras_export([""keras.ops.view_as_real"", ""keras.ops.math.view_as_real""])",0,0,0,0,0,0,0
keras-team/keras,2488390099,2072636142,DavidLandup0,60978046,2025-05-04T14:44:56+00:00,2025-05-04T14:44:56+00:00,"Whoops - good point. Accidentally left it in due to the original `keras.ops.numpy` location.
Removed!",True,keras/src/ops/math.py,,"@@ -1044,3 +1044,101 @@ def logdet(x):
     if any_symbolic_tensors((x,)):
         return Logdet().symbolic_call(x)
     return backend.math.logdet(x)
+
+
+class ViewAsComplex(Operation):
+    def call(self, x):
+        x = backend.convert_to_tensor(x)
+        if len(x.shape) < 1 or x.shape[-1] != 2:
+            raise ValueError(
+                ""Input tensor's last dimension must be 2 (real and imaginary).""
+            )
+        return x[..., 0] + 1j * x[..., 1]
+
+    def compute_output_spec(self, x):
+        return KerasTensor(shape=x.shape[:-1], dtype=""complex64"")
+
+
+class ViewAsReal(Operation):
+    def call(self, x):
+        x = backend.convert_to_tensor(x)
+        real_part = backend.numpy.real(x)
+        imag_part = backend.numpy.imag(x)
+        return backend.numpy.stack((real_part, imag_part), axis=-1)
+
+    def compute_output_spec(self, x):
+        return KerasTensor(shape=x.shape + (2,), dtype=""float32"")
+
+
+@keras_export([""keras.ops.view_as_complex"", ""keras.ops.math.view_as_complex""])
+def view_as_complex(x):
+    """"""Converts a real tensor with shape `(..., 2)` to a complex tensor,
+    where the last dimension represents the real and imaginary components
+    of a complex tensor.
+
+    Args:
+        x: A real tensor with last dimension of size 2.
+
+    Returns:
+        A complex tensor with shape `x.shape[:-1]`.
+
+    Example:
+
+    ```
+    >>> import numpy as np
+    >>> from keras import ops
+
+    >>> real_imag = np.array([[1.0, 2.0], [3.0, 4.0]])
+    >>> complex_tensor = ops.view_as_complex(real_imag)
+    >>> complex_tensor
+    array([1.+2.j, 3.+4.j])
+    ```
+    """"""
+    if any_symbolic_tensors((x,)):
+        return ViewAsComplex().symbolic_call(x)
+
+    x = backend.convert_to_tensor(x)
+    if len(x.shape) < 1 or x.shape[-1] != 2:
+        raise ValueError(
+            ""Last dimension of input must be size 2 (real and imaginary). ""
+            f""Received shape: {x.shape}""
+        )
+    real_part = x[..., 0]
+    imag_part = x[..., 1]
+
+    return backend.cast(real_part, dtype=""complex64"") + 1j * backend.cast(
+        imag_part, dtype=""complex64""
+    )
+
+
+@keras_export([""keras.ops.view_as_real"", ""keras.ops.math.view_as_real""])",0,0,0,0,0,0,0
keras-team/keras,2484638697,2064087939,mattdangerw,1389937,2025-04-28T16:50:09+00:00,2025-04-28T16:50:09+00:00,"This controls the epoch iterator, so it sets `steps` for `predict` and `evaluate` and `steps_per_epoch` for `fit`. I'm not sure if `KERAS_MAX_STEPS` or `KERAS_MAX_STEPS_PER_EPOCH` is a better name.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2484638697,2064260494,mattdangerw,1389937,2025-04-28T18:29:25+00:00,2025-04-28T18:29:25+00:00,switching to max steps per epoch,False,,,,0,0,0,0,0,0,0
keras-team/keras,2484638697,2064087939,mattdangerw,1389937,2025-04-28T16:50:09+00:00,2025-04-28T16:50:09+00:00,"This controls the epoch iterator, so it sets `steps` for `predict` and `evaluate` and `steps_per_epoch` for `fit`. I'm not sure if `KERAS_MAX_STEPS` or `KERAS_MAX_STEPS_PER_EPOCH` is a better name.",True,keras/src/backend/config.py,,"@@ -304,7 +308,10 @@ def keras_home():
     _backend = os.environ[""KERAS_BACKEND""]
     if _backend:
         _BACKEND = _backend
-
+if ""KERAS_MAX_EPOCHS"" in os.environ:
+    _MAX_EPOCHS = int(os.environ[""KERAS_MAX_EPOCHS""])
+if ""KERAS_MAX_STEPS"" in os.environ:
+    _MAX_STEPS = int(os.environ[""KERAS_MAX_STEPS""])",0,0,0,0,0,0,0
keras-team/keras,2484638697,2064260494,mattdangerw,1389937,2025-04-28T18:29:25+00:00,2025-04-28T18:29:25+00:00,switching to max steps per epoch,True,keras/src/backend/config.py,,"@@ -304,7 +308,10 @@ def keras_home():
     _backend = os.environ[""KERAS_BACKEND""]
     if _backend:
         _BACKEND = _backend
-
+if ""KERAS_MAX_EPOCHS"" in os.environ:
+    _MAX_EPOCHS = int(os.environ[""KERAS_MAX_EPOCHS""])
+if ""KERAS_MAX_STEPS"" in os.environ:
+    _MAX_STEPS = int(os.environ[""KERAS_MAX_STEPS""])",0,0,0,0,0,0,0
keras-team/keras,2484403588,2062986573,rkazants,35459624,2025-04-28T06:44:35+00:00,2025-04-28T06:44:36+00:00,not correct implementation. See other PRs as examples,False,,,,0,0,0,0,0,0,0
keras-team/keras,2484403588,2062986573,rkazants,35459624,2025-04-28T06:44:35+00:00,2025-04-28T06:44:36+00:00,not correct implementation. See other PRs as examples,True,keras/src/backend/openvino/numpy.py,8.0,"@@ -965,9 +965,8 @@ def log2(x):
 
 
 def logaddexp(x1, x2):
-    raise NotImplementedError(
-        ""`logaddexp` is not supported with openvino backend""
-    )
+    m = maximum(x1, x2)
+    return add(m, log(add(exp(subtract(x1, m)), exp(subtract(x2, m)))))",0,0,0,0,0,0,0
keras-team/keras,2480610563,2073222818,rkazants,35459624,2025-05-05T10:45:50+00:00,2025-05-05T10:45:51+00:00,not needed because `get_ov_output` already performs `convert`. So please remove,False,,,,0,0,0,0,0,0,0
keras-team/keras,2480610563,2073223590,rkazants,35459624,2025-05-05T10:46:31+00:00,2025-05-05T10:46:31+00:00,not needed,False,,,,0,0,0,0,0,0,0
keras-team/keras,2480610563,2073224007,rkazants,35459624,2025-05-05T10:46:52+00:00,2025-05-05T10:46:52+00:00,"I think `num = get_ov_output(num, Type.i32)` should be enought for all cases",False,,,,0,0,0,0,0,0,0
keras-team/keras,2480610563,2073228478,rkazants,35459624,2025-05-05T10:50:52+00:00,2025-05-05T10:50:52+00:00,create `return_step` variable and assign. `return_step` should be wrapped with `OpenVINOKerasTensor`,False,,,,0,0,0,0,0,0,0
keras-team/keras,2480610563,2073908247,vi-shruti,41509646,2025-05-05T17:52:39+00:00,2025-05-05T17:52:39+00:00,"Thanks for your review, @rkazants.
I've incorporated the rest of the changes but removing this seems to cause type errors.",False,,,,0,0,0,0,0,0,0
keras-team/keras,2480610563,2073222818,rkazants,35459624,2025-05-05T10:45:50+00:00,2025-05-05T10:45:51+00:00,not needed because `get_ov_output` already performs `convert`. So please remove,True,keras/src/backend/openvino/numpy.py,61.0,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):
 def linspace(
     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0
 ):
-    raise NotImplementedError(
-        ""`linspace` is not supported with openvino backend""
-    )
+    """"""
+    Return evenly spaced numbers over a specified interval.
+
+    Returns `num` evenly spaced samples, calculated over the
+    interval [`start`, `stop`].
+
+    The endpoint of the interval can optionally be excluded.
+
+    Parameters
+    ----------
+    start : array_like
+        The starting value of the sequence.
+    stop : array_like
+        The end value of the sequence, unless `endpoint` is set to False.
+        In that case, the sequence consists of all but the last of ``num + 1``
+        evenly spaced samples, so that `stop` is excluded.  Note that the step
+        size changes when `endpoint` is False.
+    num : int, optional
+        Number of samples to generate. Default is 50. Must be non-negative.
+    endpoint : bool, optional
+        If True, `stop` is the last sample. Otherwise, it is not included.
+        Default is True.
+    retstep : bool, optional
+        If True, return (`samples`, `step`), where `step` is the spacing
+        between samples.
+    dtype : dtype, optional
+        The type of the output array.  If `dtype` is not given, the data type
+        is inferred from `start` and `stop`. The inferred dtype will never be
+        an integer; `float` is chosen even if the arguments would produce an
+        array of integers.
+    axis : int, optional
+        The axis in the result to store the samples.  Relevant only if start
+        or stop are array-like.  By default (0), the samples will be along a
+        new axis inserted at the beginning. Use -1 to get an axis at the end.
+
+    Returns
+    -------
+    samples : ndarray
+        There are `num` equally spaced samples in the closed interval
+        ``[start, stop]`` or the half-open interval ``[start, stop)``
+        (depending on whether `endpoint` is True or False).
+    step : float, optional
+        Only returned if `retstep` is True
+
+        Size of spacing between samples.
+    """"""
+
+    dtype = standardize_dtype(dtype) or config.floatx()
+    out_dtype = OPENVINO_DTYPES[dtype]
+    dtype = OPENVINO_DTYPES[config.floatx()]
+
+    start = get_ov_output(start, dtype)
+    stop = get_ov_output(stop, dtype)
+    start = ov_opset.convert(start, dtype).output(0)
+    stop = ov_opset.convert(stop, dtype).output(0)",0,0,0,0,0,0,0
keras-team/keras,2480610563,2073223590,rkazants,35459624,2025-05-05T10:46:31+00:00,2025-05-05T10:46:31+00:00,not needed,True,keras/src/backend/openvino/numpy.py,64.0,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):
 def linspace(
     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0
 ):
-    raise NotImplementedError(
-        ""`linspace` is not supported with openvino backend""
-    )
+    """"""
+    Return evenly spaced numbers over a specified interval.
+
+    Returns `num` evenly spaced samples, calculated over the
+    interval [`start`, `stop`].
+
+    The endpoint of the interval can optionally be excluded.
+
+    Parameters
+    ----------
+    start : array_like
+        The starting value of the sequence.
+    stop : array_like
+        The end value of the sequence, unless `endpoint` is set to False.
+        In that case, the sequence consists of all but the last of ``num + 1``
+        evenly spaced samples, so that `stop` is excluded.  Note that the step
+        size changes when `endpoint` is False.
+    num : int, optional
+        Number of samples to generate. Default is 50. Must be non-negative.
+    endpoint : bool, optional
+        If True, `stop` is the last sample. Otherwise, it is not included.
+        Default is True.
+    retstep : bool, optional
+        If True, return (`samples`, `step`), where `step` is the spacing
+        between samples.
+    dtype : dtype, optional
+        The type of the output array.  If `dtype` is not given, the data type
+        is inferred from `start` and `stop`. The inferred dtype will never be
+        an integer; `float` is chosen even if the arguments would produce an
+        array of integers.
+    axis : int, optional
+        The axis in the result to store the samples.  Relevant only if start
+        or stop are array-like.  By default (0), the samples will be along a
+        new axis inserted at the beginning. Use -1 to get an axis at the end.
+
+    Returns
+    -------
+    samples : ndarray
+        There are `num` equally spaced samples in the closed interval
+        ``[start, stop]`` or the half-open interval ``[start, stop)``
+        (depending on whether `endpoint` is True or False).
+    step : float, optional
+        Only returned if `retstep` is True
+
+        Size of spacing between samples.
+    """"""
+
+    dtype = standardize_dtype(dtype) or config.floatx()
+    out_dtype = OPENVINO_DTYPES[dtype]
+    dtype = OPENVINO_DTYPES[config.floatx()]
+
+    start = get_ov_output(start, dtype)
+    stop = get_ov_output(stop, dtype)
+    start = ov_opset.convert(start, dtype).output(0)
+    stop = ov_opset.convert(stop, dtype).output(0)
+
+    if isinstance(num, OpenVINOKerasTensor):
+        num = get_ov_output(num, Type.i32)
+    elif isinstance(num, int):
+        num = ov_opset.constant(num, Type.i32).output(0)
+    else:
+        raise TypeError(""`num` must be an int or OpenVINOKerasTensor."")
+    num = ov_opset.convert(num, Type.i32).output(0)",0,0,0,0,0,0,0
keras-team/keras,2480610563,2073224007,rkazants,35459624,2025-05-05T10:46:52+00:00,2025-05-05T10:46:52+00:00,"I think `num = get_ov_output(num, Type.i32)` should be enought for all cases",True,keras/src/backend/openvino/numpy.py,,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):
 def linspace(
     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0
 ):
-    raise NotImplementedError(
-        ""`linspace` is not supported with openvino backend""
-    )
+    """"""
+    Return evenly spaced numbers over a specified interval.
+
+    Returns `num` evenly spaced samples, calculated over the
+    interval [`start`, `stop`].
+
+    The endpoint of the interval can optionally be excluded.
+
+    Parameters
+    ----------
+    start : array_like
+        The starting value of the sequence.
+    stop : array_like
+        The end value of the sequence, unless `endpoint` is set to False.
+        In that case, the sequence consists of all but the last of ``num + 1``
+        evenly spaced samples, so that `stop` is excluded.  Note that the step
+        size changes when `endpoint` is False.
+    num : int, optional
+        Number of samples to generate. Default is 50. Must be non-negative.
+    endpoint : bool, optional
+        If True, `stop` is the last sample. Otherwise, it is not included.
+        Default is True.
+    retstep : bool, optional
+        If True, return (`samples`, `step`), where `step` is the spacing
+        between samples.
+    dtype : dtype, optional
+        The type of the output array.  If `dtype` is not given, the data type
+        is inferred from `start` and `stop`. The inferred dtype will never be
+        an integer; `float` is chosen even if the arguments would produce an
+        array of integers.
+    axis : int, optional
+        The axis in the result to store the samples.  Relevant only if start
+        or stop are array-like.  By default (0), the samples will be along a
+        new axis inserted at the beginning. Use -1 to get an axis at the end.
+
+    Returns
+    -------
+    samples : ndarray
+        There are `num` equally spaced samples in the closed interval
+        ``[start, stop]`` or the half-open interval ``[start, stop)``
+        (depending on whether `endpoint` is True or False).
+    step : float, optional
+        Only returned if `retstep` is True
+
+        Size of spacing between samples.
+    """"""
+
+    dtype = standardize_dtype(dtype) or config.floatx()
+    out_dtype = OPENVINO_DTYPES[dtype]
+    dtype = OPENVINO_DTYPES[config.floatx()]
+
+    start = get_ov_output(start, dtype)
+    stop = get_ov_output(stop, dtype)
+    start = ov_opset.convert(start, dtype).output(0)
+    stop = ov_opset.convert(stop, dtype).output(0)
+
+    if isinstance(num, OpenVINOKerasTensor):
+        num = get_ov_output(num, Type.i32)
+    elif isinstance(num, int):
+        num = ov_opset.constant(num, Type.i32).output(0)
+    else:
+        raise TypeError(""`num` must be an int or OpenVINOKerasTensor."")",0,0,0,0,0,0,0
keras-team/keras,2480610563,2073228478,rkazants,35459624,2025-05-05T10:50:52+00:00,2025-05-05T10:50:52+00:00,create `return_step` variable and assign. `return_step` should be wrapped with `OpenVINOKerasTensor`,True,keras/src/backend/openvino/numpy.py,,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):
 def linspace(
     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0
 ):
-    raise NotImplementedError(
-        ""`linspace` is not supported with openvino backend""
-    )
+    """"""
+    Return evenly spaced numbers over a specified interval.
+
+    Returns `num` evenly spaced samples, calculated over the
+    interval [`start`, `stop`].
+
+    The endpoint of the interval can optionally be excluded.
+
+    Parameters
+    ----------
+    start : array_like
+        The starting value of the sequence.
+    stop : array_like
+        The end value of the sequence, unless `endpoint` is set to False.
+        In that case, the sequence consists of all but the last of ``num + 1``
+        evenly spaced samples, so that `stop` is excluded.  Note that the step
+        size changes when `endpoint` is False.
+    num : int, optional
+        Number of samples to generate. Default is 50. Must be non-negative.
+    endpoint : bool, optional
+        If True, `stop` is the last sample. Otherwise, it is not included.
+        Default is True.
+    retstep : bool, optional
+        If True, return (`samples`, `step`), where `step` is the spacing
+        between samples.
+    dtype : dtype, optional
+        The type of the output array.  If `dtype` is not given, the data type
+        is inferred from `start` and `stop`. The inferred dtype will never be
+        an integer; `float` is chosen even if the arguments would produce an
+        array of integers.
+    axis : int, optional
+        The axis in the result to store the samples.  Relevant only if start
+        or stop are array-like.  By default (0), the samples will be along a
+        new axis inserted at the beginning. Use -1 to get an axis at the end.
+
+    Returns
+    -------
+    samples : ndarray
+        There are `num` equally spaced samples in the closed interval
+        ``[start, stop]`` or the half-open interval ``[start, stop)``
+        (depending on whether `endpoint` is True or False).
+    step : float, optional
+        Only returned if `retstep` is True
+
+        Size of spacing between samples.
+    """"""
+
+    dtype = standardize_dtype(dtype) or config.floatx()
+    out_dtype = OPENVINO_DTYPES[dtype]
+    dtype = OPENVINO_DTYPES[config.floatx()]
+
+    start = get_ov_output(start, dtype)
+    stop = get_ov_output(stop, dtype)
+    start = ov_opset.convert(start, dtype).output(0)
+    stop = ov_opset.convert(stop, dtype).output(0)
+
+    if isinstance(num, OpenVINOKerasTensor):
+        num = get_ov_output(num, Type.i32)
+    elif isinstance(num, int):
+        num = ov_opset.constant(num, Type.i32).output(0)
+    else:
+        raise TypeError(""`num` must be an int or OpenVINOKerasTensor."")
+    num = ov_opset.convert(num, Type.i32).output(0)
+
+    zero_i = ov_opset.constant(0, Type.i32).output(0)
+    one_i = ov_opset.constant(1, Type.i32).output(0)
+    axis_i = ov_opset.constant(axis, Type.i32).output(0)
+
+    div = ov_opset.subtract(num, one_i).output(0) if endpoint else num
+    div = ov_opset.convert(div, dtype).output(0)
+
+    zero = ov_opset.constant(0.0, dtype).output(0)
+    one = ov_opset.constant(1.0, dtype).output(0)
+    num_f = ov_opset.convert(num, dtype).output(0)
+    seq = ov_opset.range(zero, num_f, one, dtype).output(0)
+
+    ndim = len(start.shape)
+    dims = ov_opset.concat(
+        [
+            ov_opset.constant([-1], Type.i32),
+            ov_opset.constant([1] * ndim, Type.i32),
+        ],
+        0,
+    ).output(0)
+    seq = ov_opset.reshape(seq, dims, False).output(0)
+
+    delta = ov_opset.subtract(stop, start).output(0)
+
+    cond = ov_opset.greater(div, zero).output(0)
+    nan_const = ov_opset.constant(float(""nan""), dtype).output(0)
+    step_val = ov_opset.divide(delta, div).output(0)
+    step = ov_opset.select(cond, step_val, nan_const).output(0)
+
+    target_shape = ov_opset.concat(
+        [
+            ov_opset.constant([1], Type.i64),
+            ov_opset.shape_of(start).output(0),
+        ],
+        0,
+    ).output(0)
+    step = ov_opset.reshape(step, target_shape, False).output(0)
+
+    eq_zero = ov_opset.equal(step, zero).output(0)
+    any_zero = ov_opset.reduce_logical_or(
+        eq_zero, ov_opset.constant([0], Type.i32), False
+    ).output(0)
+
+    y_norm = ov_opset.multiply(seq, step).output(0)
+    y_denorm = ov_opset.multiply(
+        ov_opset.divide(seq, div).output(0),
+        delta,
+    ).output(0)
+    y_pos = ov_opset.convert(
+        ov_opset.select(any_zero, y_denorm, y_norm).output(0), dtype
+    ).output(0)
+
+    y_zero = ov_opset.convert(
+        ov_opset.multiply(seq, delta).output(0), dtype
+    ).output(0)
+    y = ov_opset.add(
+        ov_opset.convert(ov_opset.select(cond, y_pos, y_zero).output(0), dtype),
+        start,
+    ).output(0)
+
+    if endpoint:
+        idx = ov_opset.subtract(num, one_i).output(0)
+        idx = ov_opset.convert(idx, Type.i32).output(0)
+        idx_tensor = ov_opset.broadcast(idx, target_shape).output(0)
+        stop_tensor = ov_opset.broadcast(stop, target_shape).output(0)
+        y = ov_opset.scatter_elements_update(
+            y, idx_tensor, stop_tensor, 0
+        ).output(0)
+
+    if axis != 0:
+        rank = ov_opset.rank(y).output(0)
+        axis_p1 = ov_opset.add(axis_i, one_i).output(0)
+        pre = ov_opset.range(one_i, axis_p1, one_i).output(0)
+        post = ov_opset.range(axis_p1, rank, one_i).output(0)
+        zero_i = ov_opset.reshape(
+            zero_i, ov_opset.constant([1], Type.i32), False
+        ).output(0)
+        perm = ov_opset.concat([pre, zero_i, post], 0).output(0)
+        y = ov_opset.transpose(y, perm).output(0)
+
+    y = ov_opset.convert(y, out_dtype).output(0)
+    if retstep:
+        return OpenVINOKerasTensor(y), ov_opset.convert(step, out_dtype).output(",0,0,0,0,0,0,0
keras-team/keras,2480610563,2073908247,vi-shruti,41509646,2025-05-05T17:52:39+00:00,2025-05-05T17:52:39+00:00,"Thanks for your review, @rkazants.
I've incorporated the rest of the changes but removing this seems to cause type errors.",True,keras/src/backend/openvino/numpy.py,61.0,"@@ -910,9 +910,156 @@ def less_equal(x1, x2):
 def linspace(
     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0
 ):
-    raise NotImplementedError(
-        ""`linspace` is not supported with openvino backend""
-    )
+    """"""
+    Return evenly spaced numbers over a specified interval.
+
+    Returns `num` evenly spaced samples, calculated over the
+    interval [`start`, `stop`].
+
+    The endpoint of the interval can optionally be excluded.
+
+    Parameters
+    ----------
+    start : array_like
+        The starting value of the sequence.
+    stop : array_like
+        The end value of the sequence, unless `endpoint` is set to False.
+        In that case, the sequence consists of all but the last of ``num + 1``
+        evenly spaced samples, so that `stop` is excluded.  Note that the step
+        size changes when `endpoint` is False.
+    num : int, optional
+        Number of samples to generate. Default is 50. Must be non-negative.
+    endpoint : bool, optional
+        If True, `stop` is the last sample. Otherwise, it is not included.
+        Default is True.
+    retstep : bool, optional
+        If True, return (`samples`, `step`), where `step` is the spacing
+        between samples.
+    dtype : dtype, optional
+        The type of the output array.  If `dtype` is not given, the data type
+        is inferred from `start` and `stop`. The inferred dtype will never be
+        an integer; `float` is chosen even if the arguments would produce an
+        array of integers.
+    axis : int, optional
+        The axis in the result to store the samples.  Relevant only if start
+        or stop are array-like.  By default (0), the samples will be along a
+        new axis inserted at the beginning. Use -1 to get an axis at the end.
+
+    Returns
+    -------
+    samples : ndarray
+        There are `num` equally spaced samples in the closed interval
+        ``[start, stop]`` or the half-open interval ``[start, stop)``
+        (depending on whether `endpoint` is True or False).
+    step : float, optional
+        Only returned if `retstep` is True
+
+        Size of spacing between samples.
+    """"""
+
+    dtype = standardize_dtype(dtype) or config.floatx()
+    out_dtype = OPENVINO_DTYPES[dtype]
+    dtype = OPENVINO_DTYPES[config.floatx()]
+
+    start = get_ov_output(start, dtype)
+    stop = get_ov_output(stop, dtype)
+    start = ov_opset.convert(start, dtype).output(0)
+    stop = ov_opset.convert(stop, dtype).output(0)",0,0,0,0,0,0,0
keras-team/keras,2475055982,2058904903,hertschuh,1091026,2025-04-24T17:15:54+00:00,2025-04-24T18:18:23+00:00,"let's rename to
- `buildin_context_args`
- `custom_context_args`",False,,,,0,0,0,0,0,0,0
keras-team/keras,2475055982,2058905941,hertschuh,1091026,2025-04-24T17:16:17+00:00,2025-04-24T18:18:23+00:00,Let's rename to `_call_has_context_arg`.,False,,,,0,0,0,0,0,0,0
